title: Query & Agent Workflow
slides:
  - title: "How we build the query set"
    content: |
      - Queries are written/curated manually (human-in-the-loop) and consolidated into `data/benchmark/v1_items.jsonl`.
      - We export a readable view to `data/benchmark/v1_items_readable.md` for review.
  - title: "Coverage constraints"
    content: |
      - We curate 449 tutorials (`data/tutorial_list.yaml`), but only convert those with real Galaxy tools (skip intros/interactive/meta-only tutorials).
      - 186 tutorials already have Codex queries; ~263 still await tool metadata before query generation.
  - title: "Prediction agent"
    content: |
      - `python3 -m scripts.eval.run_v1_agent_eval --agent llm` prompts an LLM with:
        • the user query text
        • a shortlist of candidate tools retrieved from the local usegalaxy tool catalog (`data/tool_catalog/*`).
      - Configs: `--provider`, `--api-url`, `--model`, `--candidate-k`, `--top-k`.
  - title: "Agent flow diagram"
    content: |
      ```
      ╔════════════════════════════════════════╗
      ║ Prediction agent flow                   ║
      ║ Inputs → agent(LLM) → Outputs           ║
      ╠════════════════════════════════════════╣
      ║ Inputs:                                 ║
      ║  • Codex query + topic/dataset hint     ║
      ║  • Candidate tools from the gold file   ║
      ║  • API key/model/delay params            ║
      ╠════════════════════════════════════════╣
      ║ Process: send prompt to the LLM         ║
      ║ Outputs: JSONL predictions + logging    ║
      ╚════════════════════════════════════════╝
      ```
  - title: "Evaluation approach"
    content: |
      - Gold refers to `data/benchmark/v1_items.jsonl`, our authoritative benchmark listing the true tools per tutorial; both the prompt’s candidate set and the evaluator rely on it.
      - Run `python3 -m scripts.eval.evaluate_recommendations` + `--normalize-tools` with cutoffs `k=1,3,5,10`.
      - Latest 100-query snapshot (see `eval_results.md`): Hit@1=0.84, Hit@3/5=0.95, Hit@10=0.96.
  - title: "Next steps"
    content: |
      - Finish generating queries for the remaining ~263 tutorials when their tool metadata is available.
      - Current accuracy is inflated because each tutorial’s candidate set is tiny; we plan to maintain a topic/tag → tool catalog to give the agent a richer pool before ranking, so future slides/reporting will reflect more realistic scores.

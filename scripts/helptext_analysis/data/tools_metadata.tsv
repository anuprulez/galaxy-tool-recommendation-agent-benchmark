tool_id	name	description	panel_section_id	panel_section_name	help_text
toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0	Collapse Collection	into single dataset in order of the collection	collection_operations	Collection Operations	Combines a list collection into a single file dataset with option to include dataset names or merge common header line.
toolshed.g2.bx.psu.edu/repos/iuc/collection_element_identifiers/collection_element_identifiers/0.0.2	Extract element identifiers	of a list collection	collection_operations	Collection Operations	This tool takes a list-type collection and produces a text dataset as output, containing the element identifiers of all datasets contained in the collection.
__SAMPLE_SHEET_TO_TABULAR__	Sample sheet to tabular		collection_operations	Collection Operations	"======== Synopsis ======== Takes a sample sheet dataset collection and converts the sample sheet metadata into a tabular dataset. The output is a tab-separated file where each row corresponds to an element in the sample sheet collection. The first column contains the element identifier, followed by columns for each metadata field defined in the sample sheet. When ""Include column headers"" is enabled, the first row will contain the column names, with ""element_identifier"" as the first column followed by the names from the sample sheet column definitions."
toolshed.g2.bx.psu.edu/repos/bgruening/split_file_on_column/tp_split_on_column/0.4	Split file	according to the values of a column	collection_operations	Collection Operations	"What it does
 This tool splits a file into different smaller files using a specific column. It will work like the group tool, but every group is saved to its own file. You have the option to include the header (first line) in all splitted files. If you have a header and don't want keep it, please remove it before you use this tool. For example with the ""Remove beginning of a file"" tool. ----- 
Example
 Splitting a file without header on column 5 from this:: chr7 56632 56652 cluster 1 chr7 56736 56756 cluster 1 chr7 56761 56781 cluster 2 chr7 56772 56792 cluster 2 chr7 56775 56795 cluster 2 will produce 2 files with different clusters:: chr7 56632 56652 cluster 1 chr7 56736 56756 cluster 1 chr7 56761 56781 cluster 2 chr7 56772 56792 cluster 2 chr7 56775 56795 cluster 2"
toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.2	Split file	to dataset collection	collection_operations	Collection Operations	"Split file into a dataset collection
 This tool splits a data set consisting of records into multiple data sets within a collection. A record can be for instance simply a line, a FASTA sequence (header + sequence), a FASTQ sequence (headers + sequence + qualities), etc. The important property is that the records either have a specific length (e.g. 4 lines for FASTQ) or that the beginning/end of a new record can be specified by a regular expression, e.g. "".
"" for lines or "">.
"" for FASTA. The tool has presets for text, tabular data sets (which are split after each line), FASTA (new records start with "">.*""), FASTQ (records consist of 4 lines), SDF (records start with ""^BEGIN IONS"") and MGF (records end with ""^$$$$""). For other data types the text delimiting records or the number of lines making up a record can be specified manually using the generic splitter. If the generic splitter is used, an option is also available to split records either before or after the separator. If a preset filetype is used, this is selected automatically (after for SDF, before for all others). If splitting by line (or by some other item, like a FASTA entry or an MGF record), the splitting can be either done alternatingly, in original record order, or at random. If t records are to be distributed to n new data sets, then the i-th record goes to data set * floor(i / t * n) (for batch), * i % n (for alternating), or * a random data set For instance, t=5 records are distributed as follows on n=2 data sets = === === ==== i bat alt rand = === === ==== 0 0 0 0 1 0 1 1 2 0 0 1 3 1 1 0 4 1 0 0 = === === ==== If the five records are distributed on n=3 data sets: = === === ==== i bat alt rand = === === ==== 0 0 0 0 1 0 1 1 2 1 2 2 3 1 0 0 4 2 1 1 = === === ==== Note that there are no guarantees when splitting at random that every result file will be non-empty, so downstream tools should be able to gracefully handle empty files. If a tabular file is used as input, you may choose to split by line or by column. If split by column, a new file is created for each unique value in the column. In addition, (Python) regular expressions may be used to transform the value in the column to a new value. Caution should be used with this feature, as it could transform all values to the same value, or other unexpected behavior. The default regular expression uses each value in the column without modifying it. Two modes are available for the tool. For the main mode, the number of output files is selected. In this case, records are shared out between this number of files. Alternatively, 'chunking mode' can be selected, which puts a fixed number of records (the 'chunk size') into each output file."
toolshed.g2.bx.psu.edu/repos/imgteam/unzip/unzip/6.0+galaxy2	Unzip	Unzip a file	collection_operations	Collection Operations	"Unzip an archive containing file(s) of various types.
 If multiple files are asked to be retained, the output will be a collection containing all files within the ZIP or TAR archive."
toolshed.g2.bx.psu.edu/repos/ecology/iabiodiv_smartbiodiv_med_environ/iabiodiv_smartbiodiv_med_environ/0.1.0	Adds environment variables	From Copernicus and etopo given geolocalized and timestamped observations	get_data	Get Data	"What it does
 This tool allows you to augment your dataset by adding some environmental variables grabbed from CMEMS, etopo, and woa. It is a galaxy tool built on the medenv python package available at https://github.com/jeremyfix/medenv. It will augment the dataset with environmental variables as provided by copernicus and etopo based on the informations coming from an ""observation file"". This version of the tool gets environmental data from etopo for the bathymetry and otherwise copernicus mediterranean products. We expect the dataset you provide to contain the coordinates of the observation: latitude, longitude, depth, and time. The time is expected in ISO 8601: YYYY-MM-DDTHH:MM:SSZ The 
cmems_username
 and 
cmems_password
 are required only if you want to add variables provided by CMEMS. These must be defined in ""User -> Preferences -> Manage Information"" The CMEMS data are collected from the following products : - med-cmcc: https://resources.marine.copernicus.eu/product-detail/MEDSEA_MULTIYEAR_PHY_006_004/INFORMATION - med-ogs: https://resources.marine.copernicus.eu/product-detail/MEDSEA_MULTIYEAR_BGC_006_008/INFORMATION The bathymetry is obtained with the ETOPO at : - https://www.ngdc.noaa.gov/mgg/global/relief/ETOPO1/data/bedrock/grid_registered/netcdf/ETOPO1_Bed_g_gmt4.grd.gz We request data around the point with a given tolerance and average the collected values. The medenv package does not force that averaging and could return a raster. A later update of the Galaxy tool could take this into account. The features you can request are the following, some indexed by depth : - ""bathymetry"", (etopo) - ""temperature"", - ""salinity"", - ""chlorophyl-a"", - ""nitrate"", - ""phosphate"", - ""ammonium"", - ""phytoplankton-carbon-biomass"", - ""oxygen"", - ""net-primary-production"", - ""ph"", - ""alkalinity"", - ""dissolved-inorganic-carbon"", - ""northward-water-velocity"", - ""eastward-water-velocity"". and others not indexed by depth : - ""mixed-layer-thickness"", - ""sea-surface-temperature"", - ""sea-surface-salinity"", - ""sea-surface-above-geoid"", - ""surface-partial-pressure-co2"", - ""surface-co2-flux"". 
 Copernicus credentials 
 To request Copernicus, you need to have an account on the https://resources.marine.copernicus.eu platform. Then, you need to provide your credentials in the user-preference section of Galaxy."
toolshed.g2.bx.psu.edu/repos/ecology/argo_getdata/argo_getdata/0.1.15+galaxy0	Argo data access	for global ocean in situ observing system	get_data	Get Data	"========================================================== Argo data access to global ocean in situ observing system. ========================================================== In order to ease Argo data analysis for the vast majority of standard users, we implemented in argopy different levels of verbosity and data processing to hide or simply remove variables only meaningful to experts. Let argopy manage data wrangling, and focus on your scientific analysis. If you don‚Äôt know in which category you would place yourself, try to answer the following questions: - [] what is a WMO number ? - [] what is the difference between Delayed and Real Time data mode ? - [] what is an adjusted parameter ? - [] what a QC flag of 3 means ? If you don‚Äôt answer to more than 1 question: you probably will feel more comfortable with the standard or research user modes. By default, all argopy data fetchers are set to work with a standard user mode, the other possible modes are research and expert. 
What it does
 This tool based on argopy is able to fetch (i.e. access, download, format) Argo data into netcdf files. 
Input description
 - This tool provides 3 user modes: - üèÑ expert mode return all the Argo data, without any postprocessing, - üèä standard mode simplifies the dataset, remove most of its jargon and return a priori good data, - üö£ research mode simplifies the dataset to its heart, preserving only data of the highest quality for research studies, including studies sensitive to small pressure and salinity bias (e.g. calculations of global ocean heat content or mixed layer depth). - The tool provides 3 different data selection methods: - üó∫ For a space/time domain (to select data for a rectangular space/time domain) - ü§ñ For one or more floats (if you know the Argo float unique identifier number called a WMO number you can use this mode) - ‚öì For one or more profiles (specify the float WMO platform number and the profile cycle number(s) to retrieve profiles) - The tool can retrieve data from : - ‚≠ê the Ifremer 
erddap
 server (Default). The erddap server database is updated daily and doesn‚Äôt require you to download anymore data than what you need. You can select this data source with the keyword erddap and methods described below. The Ifremer erddap dataset is based on mono-profile files of the GDAC. Since this is the most efficient method to fetcher Argo data, it‚Äôs the default data source in argopy. - With this tool you can thus get access to the following Argo data: - the 
phy
 dataset, for physical parameters. This dataset provides data from floats that measure temperature, salinity, pressure, without limitation in depth. It is available from all Available data sources. Since this is the most common Argo data subset it‚Äôs selected with the phy keyword by default in argopy. - the 
bgc
 dataset, for biogeochemical parameters.This dataset provides data from floats that measure temperature, salinity, pressure and oxygen, pH, nitrate, chlorophyll, backscatter, irradiance, without limitation in depth. You can select this dataset with the keyword bgc and methods described below. 
Output
 You can retrieve the Argo data in one netcdf file. 
More about argopy
 https://argopy.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/ecology/copernicusmarine/copernicusmarine/1.3.3+galaxy2	Copernicus Marine Data Store	retrieve marine data	get_data	Get Data	"============================ Copernicus Marine Data Store ============================ 
 Context 
 This tool is a wrapper to retrieve data from the Copernicus Marine Environment Monitoring Service (CMEMS). - It allows to retrieve data from the Copernicus Marine Service. - Any user willing to use this tool needs to 
create a new account <https://data.marine.copernicus.eu/login>
_. - Set your Copernicus CMEMS API Key via: User > Preferences > Manage Information - Enter your username and password for Copernicus CMEMS - Compose your request directly on Copernicus Marine Data Store - Choose there which data interest you click on the download button - Then on the top right click again on the big download butto - Log in - Click on ""Automate"" - You should have a pop-up window called ""Automate download"" - Copy the "">_Command-Line Interface"" proposed there - Back on Galaxy paste it in the input field ""Paste API Request"". For more information on the Command-Line Interface (CLI) go on 
Copernicus Marine Toolbox CLI - Subset <https://help.marine.copernicus.eu/en/articles/7972861-copernicus-marine-toolbox-cli-subset>
 
 Input 
 Command line from the Copernicus marine services copy paste as a text. 
 Output 
 A netcdf file containing the the data chose by the user from the Copernicus Marine Data Store."
toolshed.g2.bx.psu.edu/repos/iuc/sra_tools/sam_dump/3.1.1+galaxy1	Download and Extract Reads in BAM	format from NCBI SRA	get_data	Get Data	"What it does?
 This tool extracts data (in BAM_ format) from the Short Read Archive (SRA) at the National Center for Biotechnology Information (NCBI). It is based on the sam-dump_ utility of the SRA Toolkit and returns a collection of NGS data containing one file for each accession number provided. 
How to use it?
 There are three ways in which you can download data: 1. Plain text input of accession number(s) 2. Providing a list of accessions from file 3. Extracting data from an already uploaded SRA dataset Below we discuss each in detail. ------ 
Plain text input of accession number(s)
 When you type an accession number (e.g., 
SRR1582967
) into 
Accession
 box and click 
Execute
 the tool will fetch the data for you. You can also provide a list of multiple accession numbers (e.g. 
SRR3141592, SRR271828, SRR112358
). ----- 
Providing a list of accessions from file
 A more realistic scenario is when you want to upload a number of datasets at once. To do this you need a list of accession, where there is only one accession per line (see below for information on how to generate such a file). Once you have this file: 1. Upload it into your history using Galaxy's upload tool 2. Once the list of accessions is uploaded choose 
List of SRA accessions, one per line
 from 
select input type
 dropdown 3. Choose uploaded file within the 
sra accession list
 field 4. Click 
Execute
 ----- 
Extract data from an already uploaded SRA dataset
 If an SRA dataset is already present in the history, the sequencing data can be extracted in a human-readable data format (fastq, sam, bam) by setting 
select input type
 drop-down to 
SRA archive in current history
. ----- 
How to generate accession lists
 1. Go to 
SRA Run Selector
 by clicking this link_ 2. Find the study you are interested in by typing a search term within the 
Search
 box. This can be a word (e.g., 
mitochondria
) or an accession you have gotten from a paper (e.g., 
SRR1582967
). 3. Once you click on the study of interest you will see the number of datasets in this study within the 
Related SRA data
 box 4. Click on the Runs number 5. On the page that would open you will see 
Accession List
 button 6. Clicking of this button will produce a file that you will need to upload into Galaxy and use as the input to this tool. ----- .. _sam-dump: https://github.com/ncbi/sra-tools .. _BAM: https://samtools.github.io/hts-specs/SAMv1.pdf .. _collection: https://galaxyproject.org/tutorials/collections/ .. _link: https://trace.ncbi.nlm.nih.gov/Traces/index.html?view=run_browser&display=reads For credits, information, support and bug reports, please refer ato https://github.com/galaxyproject/tools-iuc."
toolshed.g2.bx.psu.edu/repos/iuc/sra_tools/fastq_dump/3.1.1+galaxy1	Download and Extract Reads in FASTQ	format from NCBI SRA	get_data	Get Data	"A string of characters and/or variables. The variables can be one of: $ac: accession, $si: spot id, $sn: spot name, $sg: spot group (barcode), $sl: spot length in bases, $ri: read number, $rn: read name, $rl: read length in bases. '[]' could be used for an optional output: if all vars in [] yield empty values whole group is not printed. Empty value is empty string or for numeric variables. Ex: @$sn[
$rn]/$ri '
$rn' is omitted if name is empty"""
toolshed.g2.bx.psu.edu/repos/iuc/sra_tools/sra_pileup/2.10.3	Download and Generate Pileup Format	from NCBI SRA	get_data	Get Data	This tool produces pileup format from sra archives using sra-pileup. The sra-pileup program is developed at NCBI, and is available at http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software. Galaxy tool wrapper originally written by Matt Shirley (mdshw5 at gmail.com). Wrapper modified by Philip Mabon ( philip.mabon at phac-aspc.gc.ca ). Tool dependencies, clean-up and bug-fixes by Marius van den Beek (m.vandenbeek at gmail.com). For support and bug reports contact Matt Shirley or Marius van den Beek or go to https://github.com/galaxyproject/tools-iuc.
toolshed.g2.bx.psu.edu/repos/iuc/ebi_metagenomics_run_downloader/ebi_metagenomics_run_downloader/0.1.0	Download run data	from EBI Metagenomics database	get_data	Get Data	"What it does
 The European Bioinformatics Institute (EMBL-EBI) maintains the world‚Äôs most comprehensive range of freely available and up-to-date molecular databases This tool download data related to a run in EBI Metagenomics database."
toolshed.g2.bx.psu.edu/repos/iuc/ebi_search_rest_results/ebi_search_rest_results/0.1.1	EBI Search	to obtain search results on resources and services hosted at the EBI	get_data	Get Data	"What it does
 The European Bioinformatics Institute (EMBL-EBI) maintains the world‚Äôs most comprehensive range of freely available and up-to-date molecular databases. EBI Search, also named as 'EB-eye', is a scalable search engine that: - provides text search functionality and uniform access to resources and services hosted at the European Bioinformatics Institute (EMBL-EBI) - is based on the consolidated Apache Lucene technology - exposes both a Web and RESTful Web Services interfaces - provides inter-domain navigation via a network of cross-references Here, sample clients provided by EBI is used"
toolshed.g2.bx.psu.edu/repos/iuc/ega_download_client/pyega3/5.0.2+galaxy0	EGA Download Client		get_data	Get Data	The pyEGA3 download client is a python-based tool for viewing and downloading files from authorized EGA datasets. .. class:: Warning mark Data is stored unencrypted on the user's Galaxy account. Confidential data from the EGA could be left unprotected when uploaded to a public Galaxy server. Make sure to read the EGA Data Access Agreement (DAA_) before uploading any data to Galaxy from the EGA. If this applies to you, we recommend you follow the GalaxySensitiveData-ELIXIR-IS_ page for updates on encrypted data at rest. If you have an EGA account, you can set your EGA credentials in the user preferences menu of Galaxy. Otherwise, default EGA credentials with access to an example dataset will be used. pyEGA3 uses the EGA Data API and has several key features: - Files are transferred over secure https connections and received unencrypted, so no need for decryption after download. - Downloads resume from where they left off in the event that the connection is interrupted. - pyEGA3 supports file segmenting and parallelized download of segments, improving overall performance. - After download completes, file integrity is verified using checksums. - pyEGA3 implements the GA4GH-compliant htsget protocol for download of genomic ranges for data files with accompanying index files. .. _DAA: https://ega-archive.org/files/Example_DAA.doc .. _GalaxySensitiveData-ELIXIR-IS: https://github.com/elixir-europe/GalaxySensitiveData-ELIXIR_IS
toolshed.g2.bx.psu.edu/repos/ecology/wormsmeasurements/WormsMeasurements/0.1.2	Enrich dataset with WoRMS	Enrich dataset with measurement type data from WoRMS	get_data	Get Data	"================== 
What it does ?
 ================== This tool requests WoRMS (World Register of Marine Species) to get data about a specific by accessing the entry returned by an its scientific name, it looks for the measurementType(s) requested by the user and select the associated measurement value to add it to a dataset. =================== 
How to use it ?
 =================== ## Parameters: - 
data
: a dataset containing a variable of scientific names. - 
list of measurement types
: a list of measurements types present in WoRMS (ex: Development, Fecundity, Size ...) separated by ','. - 
scientific names column name
: the name of column in the dataset containing scientific names. - 
include attributes inherited from parent taxon
: usefull when the data you are looking for are incomplete. - 
one hot encoding on the measurement types
: each possible values of a measurementType becomes a column encoded in 0/1 - 
exclude_NA
: exclude lines with missing measurement value ## Outputs: The inputed dataset with columns of measurement types or measurements 
Example of input data :
 ""decimalLatitude"" ""decimalLongitude"" ""scientificName"" -49.355 70.215 ""Abatus cordatus"" ""planktotrophic"" NA -66.963303 163.223297 ""Ctenocidaris spinosa"" -42.45 -74.75833333 ""Loxechinus albus"" -37.606167 176.705167 ""Ogmocidaris benhami"" -36.201698 175.834198 ""Peronella hinemoae"" -37.494667 176.672501 ""Phormosoma bursarium"" -43.469 173.572 ""Pseudechinus huttoni"" -47.7 179.45 ""Pseudechinus novaezealandiae"" -74.72 164.2183333 ""Sterechinus neumayeri"" -70.51166667 -8.801 ""Sterechinus sp"" 
Example of output data :
 ""decimalLatitude"" ""decimalLongitude"" ""scientificName"" ""Development"" "" Fecundity"" -49.355 70.215 ""Abatus cordatus"" ""planktotrophic"" NA -66.963303 163.223297 ""Ctenocidaris spinosa"" ""direct development"" NA -42.45 -74.75833333 ""Loxechinus albus"" ""planktotrophic"" NA -37.606167 176.705167 ""Ogmocidaris benhami"" ""planktotrophic"" NA -36.201698 175.834198 ""Peronella hinemoae"" ""planktotrophic"" NA -37.494667 176.672501 ""Phormosoma bursarium"" ""planktotrophic"" NA -43.469 173.572 ""Pseudechinus huttoni"" ""planktotrophic"" NA -47.7 179.45 ""Pseudechinus novaezealandiae"" ""planktotrophic"" NA -74.72 164.2183333 ""Sterechinus neumayeri"" ""planktotrophic"" NA -70.51166667 -8.801 ""Sterechinus sp"" NA NA"
toolshed.g2.bx.psu.edu/repos/iuc/sra_tools/fasterq_dump/3.1.1+galaxy1	Faster Download and Extract Reads in FASTQ	format from NCBI SRA	get_data	Get Data	"A string of characters and/or variables. The variables can be one of: $ac: accession, $si: spot id, $sn: spot name, $sg: spot group (barcode), $sl: spot length in bases, $ri: read number, $rn: read name, $rl: read length in bases. '[]' could be used for an optional output: if all vars in [] yield empty values whole group is not printed. Empty value is empty string or for numeric variables. Ex: @$sn[
$rn]/$ri '
$rn' is omitted if name is empty"""
microbial_import1	Get Microbial Data		get_data	Get Data	"This tool will allow you to obtain various genomic datasets for any completed Microbial Genome Project as listed at NCBI_. .. _NCBI: http://www.ncbi.nlm.nih.gov/genomes/lproks.cgi?view=1 Current datasets available include 1. CDS 2. tRNA 3. rRNA 4. FASTA Sequences 5. GeneMark Annotations 6. GeneMarkHMM Annotations 7. Glimmer3 Annotations ----- Organisms in 
bold
 are available at the UCSC Browser."
toolshed.g2.bx.psu.edu/repos/earlhaminst/ensembl_get_feature_info/get_feature_info/1.0.0	Get features by Ensembl ID	using REST API	get_data	Get Data	"What it does
 Retrieve feature information in JSON format from Ensembl using its REST API. Uses the 
""POST lookup/id""
 API endpoint. .. 
""POST lookup/id"": https://rest.ensembl.org/documentation/info/lookup_post"
toolshed.g2.bx.psu.edu/repos/earlhaminst/ensembl_get_genetree/get_genetree/1.0.0	Get gene tree by Ensembl ID	using REST API	get_data	Get Data	"What it does
 Retrieve a gene tree from Ensembl using its REST API. Uses the 
""GET genetree/id/:id""
 and 
""GET genetree/member/id/:species/:id""
 API endpoints. .. 
""GET genetree/id/:id"": https://rest.ensembl.org/documentation/info/genetree .. 
""GET genetree/member/id/:species/:id"": https://rest.ensembl.org/documentation/info/genetree_species_member_id"
toolshed.g2.bx.psu.edu/repos/earlhaminst/ensembl_get_sequences/get_sequences/1.0.0	Get sequences by Ensembl ID	using REST API	get_data	Get Data	"What it does
 Retrieves FASTA sequences from Ensembl using its REST API. Uses the 
""POST sequence/id""
 API endpoint. .. 
""POST sequence/id"": https://rest.ensembl.org/documentation/info/sequence_id_post"
toolshed.g2.bx.psu.edu/repos/ecology/spocc_occ/spocc_occ/1.2.2	Get species occurrences data	from GBIF, OBIS, ALA, iNAT and others	get_data	Get Data	"=========================== Get species occurences data =========================== 
What it does
 Search species occurences across a single or many data sources. | 
How to use it
 Enter a species scientific name, be careful that the tool is case sensitive. Eg : Canis lupus. Select one or more data source. It can be any combination of gbif, bison, inat, ebird, ecoengine and/or vertnet. | 
Output
 The tool returns a table with the species observations available in the chosen databases. Output file will have the following attributes : name, longituden latitude, prov, data, key. prov is the datasouce and key the occurence corresponding unique identifier. | 
How it works
 This tool use the spocc R package (Spocc::occ) : A programmatic interface to many species occurrence data sources, including : - Global Biodiversity Information Facility : GBIF https://www.gbif.org/ - USGSs' Biodiversity Information Serving Our Nation : BISON https://bison.usgs.gov/#home - iNaturalist : iNat https://www.inaturalist.org/, - eBird https://ebird.org/home - Berkeley Ecoinformatics Engine : EcoEngine https://ecoengine.berkeley.edu/ - VertNet http://vertnet.org/ - Integrated Digitized Biocollections : iDigBio https://www.idigbio.org/ - Ocean Biogeographic Information System : OBIS https://obis.org/ - Atlas of Living Australia : ALA https://www.ala.org.au/. Includes functionality for retrieving species occurrence data, and combining those data. | 
Sources
 Original source : https://cran.r-project.org/web/packages/spocc/index.html Reference manual : https://cran.r-project.org/web/packages/spocc/spocc.pdf"
toolshed.g2.bx.psu.edu/repos/iuc/iedb_api/iedb_api/2.15.3+galaxy1	IEDB	MHC Binding prediction	get_data	Get Data	"The dataset should have on allele per line. The allele may be followed by an optional comma-separated list of peptide lengths, e.g.: HLA-A
03:01,8,9,10 HLA-B
07:02,9"
toolshed.g2.bx.psu.edu/repos/iuc/query_impc/query_impc/0.9.0	IMPC	query tool	get_data	Get Data	"What it does
 With this tool, it is possible to submit various types of queries to the IMPC database. Select the desired query from the drop down menu. As input both MGI IDs or gene symbols are allowed (even mixed). If you want to input more than one ID, separate them with a comma without spaces (eg: MGI:104636,MGI:104637). If a mixed input is retrieved, the order after the mapping will not be maintained. Note that if the mapping between the two types of IDs doesn't retrieves a result, that ID will not be included in the query input, resulting in an error if all of the IDs are not mapped. The output will be a table containing the data. For the phenotypes, is possible to give as input both MP term IDs or HP terms IDs since they will be mapped to MP terms (also here the order of the input will not be maintained). For both genes and phenotypes mapping, check the ""View details"" section of the job to check if some of them were not mapped (typo errors/ID not present in the database). For queries requiring an IMPReSS pipeline ID, here_ is possible to find a complete list with details about each pipeline. For query 7 no inputs are required and you can choose if including genes without identified phenotypes or not. In query number 9, a top level phenotype category is required as input. On IMPC, phenotypes are divided into 20 categories to summarize wich systems are mainly influenced by the phenotype. In the database they are 24, since some of them are splitted into different groups: +-----------------------------------------+---------------------------------------+ | Top level phenotype category name | top level phenotype category ID | +=========================================+=======================================+ | Immune system phenotype | MP:0005387 | +-----------------------------------------+---------------------------------------+ | Integument phenotype | MP:0010771 | +-----------------------------------------+---------------------------------------+ | Adipose tissue phenotype | MP:0005375 | +-----------------------------------------+---------------------------------------+ | Hearing/vestibular/ear phenotype | MP:0005377 | +-----------------------------------------+---------------------------------------+ | Hematopoietic system phenotype | MP:0005397 | +-----------------------------------------+---------------------------------------+ | Craniofacial phenotype | MP:0005382 | +-----------------------------------------+---------------------------------------+ | Cardiovascular system phenotype | MP:0005385 | +-----------------------------------------+---------------------------------------+ | Renal/urinary system phenotype | MP:0005367 | +-----------------------------------------+---------------------------------------+ | Homeostasis/metabolism phenotype | MP:0005376 | +-----------------------------------------+---------------------------------------+ | Pigmentation phenotype | MP:0001186 | +-----------------------------------------+---------------------------------------+ | Limbs/digits/tail phenotype | MP:0005371 | +-----------------------------------------+---------------------------------------+ | Nervous system phenotype | MP:0003631 | +-----------------------------------------+---------------------------------------+ | Vision/eye phenotype | MP:0005391 | +-----------------------------------------+---------------------------------------+ | Liver/biliary system phenotype | MP:0005370 | +-----------------------------------------+---------------------------------------+ | Respiratory system phenotype | MP:0005388 | +-----------------------------------------+---------------------------------------+ | Behavior/neurological phenotype | MP:0005386 | +-----------------------------------------+---------------------------------------+ | Skeleton phenotype | MP:0005390 | +-----------------------------------------+---------------------------------------+ | Mortality/aging | MP:0010768 | +-----------------------------------------+---------------------------------------+ | Reproductive system phenotype | MP:0005389 | +-----------------------------------------+---------------------------------------+ | Endocrine/exocrine gland phenotype | MP:0005379 | +-----------------------------------------+---------------------------------------+ | Growth/size/body region phenotype | MP:0005378 | +-----------------------------------------+---------------------------------------+ | Embryo phenotype | MP:0005380 | +-----------------------------------------+---------------------------------------+ | Muscle phenotype | MP:0005369 | +-----------------------------------------+---------------------------------------+ | Digestive/alimentary phenotype | MP:0005381 | +-----------------------------------------+---------------------------------------+ | | Moreover, the when the output of a query is a list of genes, the user can choose if the output will be MGI IDs or gene symbols. Please note that it is not possible to map a gene, it will had the same ID as the beggining. For each query is possible to choose if include or not an header row. Note that not all tools have an option to remove it automatically. In this case the user will have to remove it using the tool ""Remove beginning of a file"". The headers for each query are the following: +---------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+ | Query | Output header columns | +===================================================================================================+================================================================================+ |Extract all measured phenotypes related to a gene |MP term name, MP term ID | +---------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+ |Extract all genes having a particular phenotype or a set of phenotypes |Gene accession ID/Gene symbol, Gene name, Gene bundle url | +---------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+ |Extract all phenotypes which are present in a particular gene set |MP term ID, MP term name, genes | +---------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+ |Extract images with a particular phenotype or a set of phenotypes |External sample ID, Gene symbol, Biological sample group, Sex, Colony ID, | | |Zygosity, Parameter name, Download url, Thumbnail url | +---------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+ |Which IMPReSS parameters have been measured for a particular knockout |IMPReSS Parameter name | +---------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+ |Which IMPRess parameters identified a significant finding for a particular knockout |IMPReSS Parameter name, p-value | +---------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+ |Full table of genes and all identified phenotypes |Gene accession ID/Gene symbol, Identified phenotypes | +---------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+ |Extract all genes names and ID measured in a specific IMPReSS pipeline |Gene accession ID/Gene symbol, Gene name | +---------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+ |Extract all genes and corresponding phenotypes related to a particular top level phenotype category|Gene accession ID/Gene symbol, Significant mp term ID, Significant mp term name | +---------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+ .. _here: https://www.mousephenotype.org/impress/pipelines"
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0	NCBI Accession Download	Download sequences from GenBank/RefSeq by accession through the NCBI ENTREZ API	get_data	Get Data	"What it does
 Given a file containing a list of NCBI accession numbers or a direct entry of accession numbers in the tool text input box, this tool will download the corresponding sequence records via the NCBI API. 
Limitations
 - For protein sequence downloads, only fasta format is supported - To avoid rate-limits imposed by the NCBI API, records are downloaded sequentially with a delay between requests. This may make it impractical to use this tool to download many (>100) records. 
Output
 A collection of sequence records in the desired format."
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_datasets/datasets_download_gene/18.14.0+galaxy0	NCBI Datasets Gene	download gene sequences and metadata	get_data	Get Data	".. class:: infomark 
What it does
 Downloads gene data from NCBI using the 
datasets
_ command-line tool. Retrieve gene sequences, transcripts, proteins, and annotation reports. 
Query Options
 ============= ================================================================ Method Description ============= ================================================================ Gene ID NCBI Gene ID (e.g., 672 for BRCA1) Symbol Gene symbol with taxon (e.g., TP53 in human) Accession RefSeq nucleotide (NM_) or protein (NP_/WP_) accession Taxon All genes for a taxon (large downloads) ============= ================================================================ ---- 
Key Options
 - 
Ortholog retrieval
: Get orthologous genes across taxa (vertebrates/insects) - 
Taxon filter
: Limit WP_ accession results to specific organisms - 
Flanking sequence
: Include nucleotides upstream/downstream (WP_ only) - 
FASTA filter
: Subset output to specific accessions 
Outputs (Eukaryote)
 - 
Gene Data Report
: Tabular metadata (ID, symbol, description, coordinates) - 
Gene Product Report
: Detailed transcript/protein information - 
Sequences
: Gene, RNA, protein, CDS, 5'/3' UTR FASTA files 
Outputs (Prokaryote)
 Prokaryotic genes (WP_ accessions) use a different report format with: accession, description, EC number, gene symbol, protein info. 
Examples
 Download human BRCA1:: Query by: Gene ID Gene ID: 672 Download TP53 orthologs in rodents:: Query by: Symbol Symbol: tp53 Ortholog: rodentia .. _datasets: https://www.ncbi.nlm.nih.gov/datasets/"
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_datasets/datasets_download_genome/18.14.0+galaxy0	NCBI Datasets Genomes	download genome sequence, annotation and metadata	get_data	Get Data	".. class:: infomark 
What it does
 Downloads genome assemblies from NCBI using the 
datasets
_ command-line tool. Retrieve genome sequences, annotations, and metadata by accession or taxon. 
Query Options
 - 
By Accession
: NCBI Assembly (GCF_/GCA_) or BioProject accession - 
By Taxon
: Taxonomy ID, scientific name, or common name 
Filters
 ==================== =============================================== Filter Description ==================== =============================================== Reference only Limit to reference/representative assemblies Annotated only Include only genomes with annotations Assembly level Chromosome, complete, contig, or scaffold Assembly source RefSeq (GCF_) or GenBank (GCA_) Exclude atypical Remove atypical assemblies (e.g., partial) MAG filter Include/exclude metagenome-assembled genomes Date range Filter by release date ==================== =============================================== ---- .. class:: warningmark 
Note
: The ""Reference only"" filter returns only RefSeq (GCF_) assemblies. If a taxon has only GenBank (GCA_) assemblies, this filter will return no results with a misleading error message. It is a NCBI datasets bug (not a Galaxy bug). 
Outputs
 - 
Data Report
: Tabular metadata for matching assemblies - 
Genome FASTA
: Genomic sequences (nested collection by accession) - 
Annotation files
: GFF3, GTF, GenBank flat files - 
Protein/RNA/CDS
: Amino acid and nucleotide sequences - 
Sequence Report
: Per-sequence metadata (chromosome, length, etc.) .. _datasets: https://www.ncbi.nlm.nih.gov/datasets/"
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_eutils_ecitmatch/ncbi_eutils_ecitmatch/1.2	NCBI ECitMatch	search NCBI for citations in PubMed	get_data	Get Data	"NCBI ECitMatch ============== Search for citation PubMed IDs. These can be provided via a tabular file, or via direct input. If provided via file, the columns should be ordered: 1. Journal Name 2. Year 3. Volume 4. First Page 5. Author Name 6. Citation Key An example query: +---------------+--------------------------+ | Parameter | Value | +===============+==========================+ | Journal Title | proc natl acad sci u s a | +---------------+--------------------------+ | Year | 1991 | +---------------+--------------------------+ | Volume | 88 | +---------------+--------------------------+ | First Page | 3248 | +---------------+--------------------------+ | Author Name | mann bj | +---------------+--------------------------+ | Citation Key | citation_1 | +---------------+--------------------------+ Usage Guidelines and Requirements ================================= Frequency, Timing, and Registration of E-utility URL Requests ------------------------------------------------------------- In order not to overload the E-utility servers, NCBI recommends that users limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays. Failure to comply with this policy may result in an IP address being blocked from accessing NCBI. Minimizing the Number of Requests --------------------------------- If a task requires searching for and/or downloading a large number of records, it is much more efficient to use the Entrez History to upload and/or retrieve these records in batches rather than using separate requests for each record. Please refer to Application 3 in Chapter 3 for an example. Many thousands of IDs can be uploaded using a single EPost request, and several hundred records can be downloaded using one EFetch request. Disclaimer and Copyright Issues ------------------------------- In accordance with requirements of NCBI's E-Utilities, we must provide the following disclaimer: Please note that abstracts in PubMed may incorporate material that may be protected by U.S. and foreign copyright laws. All persons reproducing, redistributing, or making commercial use of this information are expected to adhere to the terms and conditions asserted by the copyright holder. Transmission or reproduction of protected items beyond that allowed by fair use (PDF) as defined in the copyright laws requires the written permission of the copyright owners. NLM provides no legal advice concerning distribution of copyrighted materials. Please consult your legal counsel. If you wish to do a large data mining project on PubMed data, you can enter into a licensing agreement and lease the data for free from NLM. For more information on this please see 
https://www.nlm.nih.gov/databases/download/data_distrib_main.html <https://www.nlm.nih.gov/databases/download/data_distrib_main.html>
 The 
full disclaimer <https://www.ncbi.nlm.nih.gov/home/about/policies/>
 is available on their website Liability ~~~~~~~~~ For documents and software available from this server, the U.S. Government does not warrant or assume any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed. Endorsement ~~~~~~~~~~~ NCBI does not endorse or recommend any commercial products, processes, or services. The views and opinions of authors expressed on NCBI's Web sites do not necessarily state or reflect those of the U.S. Government, and they may not be used for advertising or product endorsement purposes. External Links ~~~~~~~~~~~~~~ Some NCBI Web pages may provide links to other Internet sites for the convenience of users. NCBI is not responsible for the availability or content of these external sites, nor does NCBI endorse, warrant, or guarantee the products, services, or information described or offered at these other Internet sites. Users cannot assume that the external sites will abide by the same Privacy Policy to which NCBI adheres. It is the responsibility of the user to examine the copyright and licensing restrictions of linked pages and to secure all necessary permissions."
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_eutils_efetch/ncbi_eutils_efetch/1.2	NCBI EFetch	fetch records from NCBI	get_data	Get Data	"NCBI Entrez EFetch ================== Responds to a list of UIDs in a given database with the corresponding data records in a specified format. Example Queries --------------- Fetch PMIDs 17284678 and 9997 as text abstracts: +----------------------+--------------------------------------+ | Parameter | Value | +======================+======================================+ | NCBI Database to Use | PubMed | +----------------------+--------------------------------------+ | ID List | 17284678 9997 | +----------------------+--------------------------------------+ | Output Format | Abstract | +----------------------+--------------------------------------+ Fetch FASTA for a transcript and its protein product (GIs 312836839 and 34577063) +----------------------+--------------------------------------+ | Parameter | Value | +======================+======================================+ | NCBI Database to Use | Protein | +----------------------+--------------------------------------+ | ID List | 312836839 34577063 | +----------------------+--------------------------------------+ | Output Format | Fasta | +----------------------+--------------------------------------+ Usage Guidelines and Requirements ================================= Frequency, Timing, and Registration of E-utility URL Requests ------------------------------------------------------------- In order not to overload the E-utility servers, NCBI recommends that users limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays. Failure to comply with this policy may result in an IP address being blocked from accessing NCBI. Minimizing the Number of Requests --------------------------------- If a task requires searching for and/or downloading a large number of records, it is much more efficient to use the Entrez History to upload and/or retrieve these records in batches rather than using separate requests for each record. Please refer to Application 3 in Chapter 3 for an example. Many thousands of IDs can be uploaded using a single EPost request, and several hundred records can be downloaded using one EFetch request. Disclaimer and Copyright Issues ------------------------------- In accordance with requirements of NCBI's E-Utilities, we must provide the following disclaimer: Please note that abstracts in PubMed may incorporate material that may be protected by U.S. and foreign copyright laws. All persons reproducing, redistributing, or making commercial use of this information are expected to adhere to the terms and conditions asserted by the copyright holder. Transmission or reproduction of protected items beyond that allowed by fair use (PDF) as defined in the copyright laws requires the written permission of the copyright owners. NLM provides no legal advice concerning distribution of copyrighted materials. Please consult your legal counsel. If you wish to do a large data mining project on PubMed data, you can enter into a licensing agreement and lease the data for free from NLM. For more information on this please see 
https://www.nlm.nih.gov/databases/download/data_distrib_main.html <https://www.nlm.nih.gov/databases/download/data_distrib_main.html>
 The 
full disclaimer <https://www.ncbi.nlm.nih.gov/home/about/policies/>
 is available on their website Liability ~~~~~~~~~ For documents and software available from this server, the U.S. Government does not warrant or assume any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed. Endorsement ~~~~~~~~~~~ NCBI does not endorse or recommend any commercial products, processes, or services. The views and opinions of authors expressed on NCBI's Web sites do not necessarily state or reflect those of the U.S. Government, and they may not be used for advertising or product endorsement purposes. External Links ~~~~~~~~~~~~~~ Some NCBI Web pages may provide links to other Internet sites for the convenience of users. NCBI is not responsible for the availability or content of these external sites, nor does NCBI endorse, warrant, or guarantee the products, services, or information described or offered at these other Internet sites. Users cannot assume that the external sites will abide by the same Privacy Policy to which NCBI adheres. It is the responsibility of the user to examine the copyright and licensing restrictions of linked pages and to secure all necessary permissions."
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_eutils_egquery/ncbi_eutils_egquery/1.2	NCBI EGQuery	Provides the number of records retrieved in all Entrez databases by a single text query.	get_data	Get Data	"NCBI Entrez EGQuery =================== Provides the number of records retrieved in all Entrez databases by a single text query. Example Queries --------------- +----------------------+-------------+ | Parameter | Value | +======================+=============+ | Term | Cancer | +----------------------+-------------+ Usage Guidelines and Requirements ================================= Frequency, Timing, and Registration of E-utility URL Requests ------------------------------------------------------------- In order not to overload the E-utility servers, NCBI recommends that users limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays. Failure to comply with this policy may result in an IP address being blocked from accessing NCBI. Minimizing the Number of Requests --------------------------------- If a task requires searching for and/or downloading a large number of records, it is much more efficient to use the Entrez History to upload and/or retrieve these records in batches rather than using separate requests for each record. Please refer to Application 3 in Chapter 3 for an example. Many thousands of IDs can be uploaded using a single EPost request, and several hundred records can be downloaded using one EFetch request. Disclaimer and Copyright Issues ------------------------------- In accordance with requirements of NCBI's E-Utilities, we must provide the following disclaimer: Please note that abstracts in PubMed may incorporate material that may be protected by U.S. and foreign copyright laws. All persons reproducing, redistributing, or making commercial use of this information are expected to adhere to the terms and conditions asserted by the copyright holder. Transmission or reproduction of protected items beyond that allowed by fair use (PDF) as defined in the copyright laws requires the written permission of the copyright owners. NLM provides no legal advice concerning distribution of copyrighted materials. Please consult your legal counsel. If you wish to do a large data mining project on PubMed data, you can enter into a licensing agreement and lease the data for free from NLM. For more information on this please see 
https://www.nlm.nih.gov/databases/download/data_distrib_main.html <https://www.nlm.nih.gov/databases/download/data_distrib_main.html>
 The 
full disclaimer <https://www.ncbi.nlm.nih.gov/home/about/policies/>
 is available on their website Liability ~~~~~~~~~ For documents and software available from this server, the U.S. Government does not warrant or assume any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed. Endorsement ~~~~~~~~~~~ NCBI does not endorse or recommend any commercial products, processes, or services. The views and opinions of authors expressed on NCBI's Web sites do not necessarily state or reflect those of the U.S. Government, and they may not be used for advertising or product endorsement purposes. External Links ~~~~~~~~~~~~~~ Some NCBI Web pages may provide links to other Internet sites for the convenience of users. NCBI is not responsible for the availability or content of these external sites, nor does NCBI endorse, warrant, or guarantee the products, services, or information described or offered at these other Internet sites. Users cannot assume that the external sites will abide by the same Privacy Policy to which NCBI adheres. It is the responsibility of the user to examine the copyright and licensing restrictions of linked pages and to secure all necessary permissions."
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_eutils_einfo/ncbi_eutils_einfo/1.2	NCBI EInfo	fetch NCBI database metadata	get_data	Get Data	"NCBI Entrez EInfo ================= Provides the number of records indexed in each field of a given database, the date of the last update of the database, and the available links from the database to other Entrez databases. Usage Guidelines and Requirements ================================= Frequency, Timing, and Registration of E-utility URL Requests ------------------------------------------------------------- In order not to overload the E-utility servers, NCBI recommends that users limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays. Failure to comply with this policy may result in an IP address being blocked from accessing NCBI. Minimizing the Number of Requests --------------------------------- If a task requires searching for and/or downloading a large number of records, it is much more efficient to use the Entrez History to upload and/or retrieve these records in batches rather than using separate requests for each record. Please refer to Application 3 in Chapter 3 for an example. Many thousands of IDs can be uploaded using a single EPost request, and several hundred records can be downloaded using one EFetch request. Disclaimer and Copyright Issues ------------------------------- In accordance with requirements of NCBI's E-Utilities, we must provide the following disclaimer: Please note that abstracts in PubMed may incorporate material that may be protected by U.S. and foreign copyright laws. All persons reproducing, redistributing, or making commercial use of this information are expected to adhere to the terms and conditions asserted by the copyright holder. Transmission or reproduction of protected items beyond that allowed by fair use (PDF) as defined in the copyright laws requires the written permission of the copyright owners. NLM provides no legal advice concerning distribution of copyrighted materials. Please consult your legal counsel. If you wish to do a large data mining project on PubMed data, you can enter into a licensing agreement and lease the data for free from NLM. For more information on this please see 
https://www.nlm.nih.gov/databases/download/data_distrib_main.html <https://www.nlm.nih.gov/databases/download/data_distrib_main.html>
 The 
full disclaimer <https://www.ncbi.nlm.nih.gov/home/about/policies/>
 is available on their website Liability ~~~~~~~~~ For documents and software available from this server, the U.S. Government does not warrant or assume any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed. Endorsement ~~~~~~~~~~~ NCBI does not endorse or recommend any commercial products, processes, or services. The views and opinions of authors expressed on NCBI's Web sites do not necessarily state or reflect those of the U.S. Government, and they may not be used for advertising or product endorsement purposes. External Links ~~~~~~~~~~~~~~ Some NCBI Web pages may provide links to other Internet sites for the convenience of users. NCBI is not responsible for the availability or content of these external sites, nor does NCBI endorse, warrant, or guarantee the products, services, or information described or offered at these other Internet sites. Users cannot assume that the external sites will abide by the same Privacy Policy to which NCBI adheres. It is the responsibility of the user to examine the copyright and licensing restrictions of linked pages and to secure all necessary permissions."
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_eutils_elink/ncbi_eutils_elink/1.2	NCBI ELink	link UIDs from one database to another	get_data	Get Data	"NCBI Entrez ELink ================= Responds to a list of UIDs in a given database with either a list of related UIDs (and relevancy scores) in the same database or a list of linked UIDs in another Entrez database; checks for the existence of a specified link from a list of one or more UIDs; creates a hyperlink to the primary LinkOut provider for a specific UID and database, or lists LinkOut URLs and attributes for multiple UIDs. Commands -------- Example Queries --------------- Link from protein to gene +----------------------+--------------------------------------+ | Parameter | Value | +======================+======================================+ | From NCBI Database | Protein | +----------------------+--------------------------------------+ | Elink Command | Neighbor | +----------------------+--------------------------------------+ | To NCBI Database | Gene | +----------------------+--------------------------------------+ | ID List | 15718680 157427902 | +----------------------+--------------------------------------+ Find related articles to PMID 20210808 with scores +----------------------+--------------------------------------+ | Parameter | Value | +======================+======================================+ | From NCBI Database | PubMed | +----------------------+--------------------------------------+ | Elink Command | Scored Neighbors | +----------------------+--------------------------------------+ | To NCBI Database | PubMed | +----------------------+--------------------------------------+ | ID List | 20210808 | +----------------------+--------------------------------------+ List all possible links from two protein GIs +----------------------+--------------------------------------+ | Parameter | Value | +======================+======================================+ | From NCBI Database | Protein | +----------------------+--------------------------------------+ | Elink Command | ACheck | +----------------------+--------------------------------------+ | ID List | 15718680 157427902 | +----------------------+--------------------------------------+ List all possible links from two protein GIs to PubMed +----------------------+--------------------------------------+ | Parameter | Value | +======================+======================================+ | From NCBI Database | Protein | +----------------------+--------------------------------------+ | Elink Command | ACheck | +----------------------+--------------------------------------+ | To NCBI Database | PubMed | +----------------------+--------------------------------------+ | ID List | 15718680 157427902 | +----------------------+--------------------------------------+ Check whether two nuccore sequences have ""related sequences"" links. +----------------------+--------------------------------------+ | Parameter | Value | +======================+======================================+ | From NCBI Database | Nuccore | +----------------------+--------------------------------------+ | Elink Command | NCheck | +----------------------+--------------------------------------+ | ID List | 21614549 219152114 | +----------------------+--------------------------------------+ List the LinkOut URLs for non-library providers for two pubmed abstracts. +----------------------+--------------------------------------+ | Parameter | Value | +======================+======================================+ | From NCBI Database | Pubmed | +----------------------+--------------------------------------+ | Elink Command | Links | +----------------------+--------------------------------------+ | ID List | 19880848 19822630 | +----------------------+--------------------------------------+ Find links to full text providers for two PubMed abstracts. +----------------------+--------------------------------------+ | Parameter | Value | +======================+======================================+ | From NCBI Database | Pubmed | +----------------------+--------------------------------------+ | Elink Command | Provider Links | +----------------------+--------------------------------------+ | ID List | 19880848 19822630 | +----------------------+--------------------------------------+ Usage Guidelines and Requirements ================================= Frequency, Timing, and Registration of E-utility URL Requests ------------------------------------------------------------- In order not to overload the E-utility servers, NCBI recommends that users limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays. Failure to comply with this policy may result in an IP address being blocked from accessing NCBI. Minimizing the Number of Requests --------------------------------- If a task requires searching for and/or downloading a large number of records, it is much more efficient to use the Entrez History to upload and/or retrieve these records in batches rather than using separate requests for each record. Please refer to Application 3 in Chapter 3 for an example. Many thousands of IDs can be uploaded using a single EPost request, and several hundred records can be downloaded using one EFetch request. Disclaimer and Copyright Issues ------------------------------- In accordance with requirements of NCBI's E-Utilities, we must provide the following disclaimer: Please note that abstracts in PubMed may incorporate material that may be protected by U.S. and foreign copyright laws. All persons reproducing, redistributing, or making commercial use of this information are expected to adhere to the terms and conditions asserted by the copyright holder. Transmission or reproduction of protected items beyond that allowed by fair use (PDF) as defined in the copyright laws requires the written permission of the copyright owners. NLM provides no legal advice concerning distribution of copyrighted materials. Please consult your legal counsel. If you wish to do a large data mining project on PubMed data, you can enter into a licensing agreement and lease the data for free from NLM. For more information on this please see 
https://www.nlm.nih.gov/databases/download/data_distrib_main.html <https://www.nlm.nih.gov/databases/download/data_distrib_main.html>
 The 
full disclaimer <https://www.ncbi.nlm.nih.gov/home/about/policies/>
 is available on their website Liability ~~~~~~~~~ For documents and software available from this server, the U.S. Government does not warrant or assume any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed. Endorsement ~~~~~~~~~~~ NCBI does not endorse or recommend any commercial products, processes, or services. The views and opinions of authors expressed on NCBI's Web sites do not necessarily state or reflect those of the U.S. Government, and they may not be used for advertising or product endorsement purposes. External Links ~~~~~~~~~~~~~~ Some NCBI Web pages may provide links to other Internet sites for the convenience of users. NCBI is not responsible for the availability or content of these external sites, nor does NCBI endorse, warrant, or guarantee the products, services, or information described or offered at these other Internet sites. Users cannot assume that the external sites will abide by the same Privacy Policy to which NCBI adheres. It is the responsibility of the user to examine the copyright and licensing restrictions of linked pages and to secure all necessary permissions."
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_eutils_epost/ncbi_eutils_epost/1.2	NCBI EPost	post UIDs to NCBI History Server	get_data	Get Data	"NCBI Entrez EPost ================= Accepts a list of UIDs from a given database, stores the set on the History Server, and responds with an NCBI History reference. Usage Guidelines and Requirements ================================= Frequency, Timing, and Registration of E-utility URL Requests ------------------------------------------------------------- In order not to overload the E-utility servers, NCBI recommends that users limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays. Failure to comply with this policy may result in an IP address being blocked from accessing NCBI. Minimizing the Number of Requests --------------------------------- If a task requires searching for and/or downloading a large number of records, it is much more efficient to use the Entrez History to upload and/or retrieve these records in batches rather than using separate requests for each record. Please refer to Application 3 in Chapter 3 for an example. Many thousands of IDs can be uploaded using a single EPost request, and several hundred records can be downloaded using one EFetch request. Disclaimer and Copyright Issues ------------------------------- In accordance with requirements of NCBI's E-Utilities, we must provide the following disclaimer: Please note that abstracts in PubMed may incorporate material that may be protected by U.S. and foreign copyright laws. All persons reproducing, redistributing, or making commercial use of this information are expected to adhere to the terms and conditions asserted by the copyright holder. Transmission or reproduction of protected items beyond that allowed by fair use (PDF) as defined in the copyright laws requires the written permission of the copyright owners. NLM provides no legal advice concerning distribution of copyrighted materials. Please consult your legal counsel. If you wish to do a large data mining project on PubMed data, you can enter into a licensing agreement and lease the data for free from NLM. For more information on this please see 
https://www.nlm.nih.gov/databases/download/data_distrib_main.html <https://www.nlm.nih.gov/databases/download/data_distrib_main.html>
 The 
full disclaimer <https://www.ncbi.nlm.nih.gov/home/about/policies/>
 is available on their website Liability ~~~~~~~~~ For documents and software available from this server, the U.S. Government does not warrant or assume any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed. Endorsement ~~~~~~~~~~~ NCBI does not endorse or recommend any commercial products, processes, or services. The views and opinions of authors expressed on NCBI's Web sites do not necessarily state or reflect those of the U.S. Government, and they may not be used for advertising or product endorsement purposes. External Links ~~~~~~~~~~~~~~ Some NCBI Web pages may provide links to other Internet sites for the convenience of users. NCBI is not responsible for the availability or content of these external sites, nor does NCBI endorse, warrant, or guarantee the products, services, or information described or offered at these other Internet sites. Users cannot assume that the external sites will abide by the same Privacy Policy to which NCBI adheres. It is the responsibility of the user to examine the copyright and licensing restrictions of linked pages and to secure all necessary permissions."
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_eutils_esearch/ncbi_eutils_esearch/1.2	NCBI ESearch	search NCBI Databases by text query	get_data	Get Data	"NCBI Entrez ESearch =================== Responds to a text query with the list of matching UIDs in a given database (for later use in ESummary, EFetch or ELink), along with the term translations of the query. Example Queries --------------- Search in PubMed with the term cancer for abstracts that have an Entrez date within the last 60 days: +----------------------+-------------+ | Parameter | Value | +======================+=============+ | NCBI Database to Use | PubMed | +----------------------+-------------+ | Term | Cancer | +----------------------+-------------+ | Datetype | Entrez Date | +----------------------+-------------+ | In past N Days | 60 | +----------------------+-------------+ Search PubMed Central for free full text articles containing the query stem cells: +----------------------+--------------------------------------+ | Parameter | Value | +======================+======================================+ | NCBI Database to Use | PubMedCentral | +----------------------+--------------------------------------+ | Term | Stem Cells AND free fulltext[filter] | +----------------------+--------------------------------------+ Search in Nucleotide for all tRNAs: +----------------------+--------------------------------------+ | Parameter | Value | +======================+======================================+ | NCBI Database to Use | Nucleotide | +----------------------+--------------------------------------+ | Term | biomol trna[prop] | +----------------------+--------------------------------------+ Search in Protein for a molecular weight range: +----------------------+--------------------------------------+ | Parameter | Value | +======================+======================================+ | NCBI Database to Use | Protein | +----------------------+--------------------------------------+ | Term | 70000:90000[molecular weight] | +----------------------+--------------------------------------+ Usage Guidelines and Requirements ================================= Frequency, Timing, and Registration of E-utility URL Requests ------------------------------------------------------------- In order not to overload the E-utility servers, NCBI recommends that users limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays. Failure to comply with this policy may result in an IP address being blocked from accessing NCBI. Minimizing the Number of Requests --------------------------------- If a task requires searching for and/or downloading a large number of records, it is much more efficient to use the Entrez History to upload and/or retrieve these records in batches rather than using separate requests for each record. Please refer to Application 3 in Chapter 3 for an example. Many thousands of IDs can be uploaded using a single EPost request, and several hundred records can be downloaded using one EFetch request. Disclaimer and Copyright Issues ------------------------------- In accordance with requirements of NCBI's E-Utilities, we must provide the following disclaimer: Please note that abstracts in PubMed may incorporate material that may be protected by U.S. and foreign copyright laws. All persons reproducing, redistributing, or making commercial use of this information are expected to adhere to the terms and conditions asserted by the copyright holder. Transmission or reproduction of protected items beyond that allowed by fair use (PDF) as defined in the copyright laws requires the written permission of the copyright owners. NLM provides no legal advice concerning distribution of copyrighted materials. Please consult your legal counsel. If you wish to do a large data mining project on PubMed data, you can enter into a licensing agreement and lease the data for free from NLM. For more information on this please see 
https://www.nlm.nih.gov/databases/download/data_distrib_main.html <https://www.nlm.nih.gov/databases/download/data_distrib_main.html>
 The 
full disclaimer <https://www.ncbi.nlm.nih.gov/home/about/policies/>
 is available on their website Liability ~~~~~~~~~ For documents and software available from this server, the U.S. Government does not warrant or assume any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed. Endorsement ~~~~~~~~~~~ NCBI does not endorse or recommend any commercial products, processes, or services. The views and opinions of authors expressed on NCBI's Web sites do not necessarily state or reflect those of the U.S. Government, and they may not be used for advertising or product endorsement purposes. External Links ~~~~~~~~~~~~~~ Some NCBI Web pages may provide links to other Internet sites for the convenience of users. NCBI is not responsible for the availability or content of these external sites, nor does NCBI endorse, warrant, or guarantee the products, services, or information described or offered at these other Internet sites. Users cannot assume that the external sites will abide by the same Privacy Policy to which NCBI adheres. It is the responsibility of the user to examine the copyright and licensing restrictions of linked pages and to secure all necessary permissions."
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_eutils_esummary/ncbi_eutils_esummary/1.2	NCBI ESummary	fetch summary of history/ids	get_data	Get Data	"NCBI Entrez ESummary ==================== Responds to a list of UIDs from a given database with the corresponding document summaries. Example Queries --------------- Search against protein: +----------------------+--------------------------------------+ | Parameter | Value | +======================+======================================+ | NCBI Database to Use | Protein | +----------------------+--------------------------------------+ | ID List | 28800982 28628843 | +----------------------+--------------------------------------+ Usage Guidelines and Requirements ================================= Frequency, Timing, and Registration of E-utility URL Requests ------------------------------------------------------------- In order not to overload the E-utility servers, NCBI recommends that users limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time during weekdays. Failure to comply with this policy may result in an IP address being blocked from accessing NCBI. Minimizing the Number of Requests --------------------------------- If a task requires searching for and/or downloading a large number of records, it is much more efficient to use the Entrez History to upload and/or retrieve these records in batches rather than using separate requests for each record. Please refer to Application 3 in Chapter 3 for an example. Many thousands of IDs can be uploaded using a single EPost request, and several hundred records can be downloaded using one EFetch request. Disclaimer and Copyright Issues ------------------------------- In accordance with requirements of NCBI's E-Utilities, we must provide the following disclaimer: Please note that abstracts in PubMed may incorporate material that may be protected by U.S. and foreign copyright laws. All persons reproducing, redistributing, or making commercial use of this information are expected to adhere to the terms and conditions asserted by the copyright holder. Transmission or reproduction of protected items beyond that allowed by fair use (PDF) as defined in the copyright laws requires the written permission of the copyright owners. NLM provides no legal advice concerning distribution of copyrighted materials. Please consult your legal counsel. If you wish to do a large data mining project on PubMed data, you can enter into a licensing agreement and lease the data for free from NLM. For more information on this please see 
https://www.nlm.nih.gov/databases/download/data_distrib_main.html <https://www.nlm.nih.gov/databases/download/data_distrib_main.html>
 The 
full disclaimer <https://www.ncbi.nlm.nih.gov/home/about/policies/>
 is available on their website Liability ~~~~~~~~~ For documents and software available from this server, the U.S. Government does not warrant or assume any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed. Endorsement ~~~~~~~~~~~ NCBI does not endorse or recommend any commercial products, processes, or services. The views and opinions of authors expressed on NCBI's Web sites do not necessarily state or reflect those of the U.S. Government, and they may not be used for advertising or product endorsement purposes. External Links ~~~~~~~~~~~~~~ Some NCBI Web pages may provide links to other Internet sites for the convenience of users. NCBI is not responsible for the availability or content of these external sites, nor does NCBI endorse, warrant, or guarantee the products, services, or information described or offered at these other Internet sites. Users cannot assume that the external sites will abide by the same Privacy Policy to which NCBI adheres. It is the responsibility of the user to examine the copyright and licensing restrictions of linked pages and to secure all necessary permissions."
toolshed.g2.bx.psu.edu/repos/ecology/obis_data/obis_data/0.0.2	OBIS occurences	retrieve data	get_data	Get Data	"=========================== Get species occurences data =========================== 
What it does
 Search and retrieve species occurences across OBIS database. | 
How to use it
 Enter a species scientific name, be careful that the tool is case sensitive. Eg : Scomber scombrus. Or enter the latitude longitude of the area you want to retrieve data from. | 
Output
 The tool returns a table with the species observations available in OBIS database. Output file will have at least the following attributes : BasisOfRecords, longitude, latitude, species, individualcount. | 
How it works
 This tool use the robis R package. Includes functionality for retrieving species occurrence data, and combining those data."
toolshed.g2.bx.psu.edu/repos/iuc/openalex_explorer/openalex_explorer/0.1.0+galaxy0	OpenAlex explorer	Fetch citing papers from OpenAlex using DOI, openAlex ID, or title	get_data	Get Data	"This tool fetches citing papers from OpenAlex for a paper specified by OpenAlex ID, DOI, or Title. You can optionally download available Open Access PDFs. 
Outputs:
 - summary.txt: summary of total, OA, and closed access citing papers - citing_papers.tsv: list of citing papers with details (title, DOI, OA)"
toolshed.g2.bx.psu.edu/repos/galaxyp/dbbuilder/dbbuilder/0.3.4	Protein Database Downloader		get_data	Get Data	"Output
 Creates a FASTA file of specified protein sequences for comparison with experimental MS/MS data in search algorithm. 
External Links
 - Galaxy-P_101_ shows usage Protein Database Downloader tool in the creation of a workflow - UniProtKB_ provides additional information about the UniProt Knowledgebase .. _Galaxy-P_101: http://msi-galaxy-p.readthedocs.org/en/latest/sections/galaxyp_101.html .. _UniProtKB: http://www.uniprot.org/help/uniprotkb 
Additional Protein Fasta URLs
 
HUMAN GUT METAPROTEOME:
 * 61MB gzip http://www.bork.embl.de/~arumugam/Qin_et_al_2010/frequent_microbe_proteins.fasta.gz 
MOUSE GUT MICROBIOTA:
 * See: http://gigadb.org/dataset/view/id/100114/token/mZlMYJIF04LshpgP"
toolshed.g2.bx.psu.edu/repos/iuc/enasearch_retrieve_data/enasearch_retrieve_data/0.1.1.0	Retrieve ENA data	(other than taxon and project)	get_data	Get Data	"What it does
 This tool retrieve ENA data (other than taxon and project)"
toolshed.g2.bx.psu.edu/repos/iuc/enasearch_retrieve_taxons/enasearch_retrieve_taxons/0.1.1.0	Retrieve ENA taxon data		get_data	Get Data	"What it does
 This tool retrieve ENA taxon data"
toolshed.g2.bx.psu.edu/repos/iuc/enasearch_retrieve_run_report/enasearch_retrieve_run_report/0.1.1.0	Retrieve a run report		get_data	Get Data	"What it does
 This tool retrieve a run report for an accession id"
toolshed.g2.bx.psu.edu/repos/iuc/enasearch_retrieve_analysis_report/enasearch_retrieve_analysis_report/0.1.1.0	Retrieve an analysis report		get_data	Get Data	"What it does
 This tool retrieve an analysis report for an accession id"
toolshed.g2.bx.psu.edu/repos/ecology/retrieve_bold/retrieve_bold/1.3.0+galaxy0	Retrieve bold	Functions to search in Bold and download the available sequences of each subtaxa (get_fasta)	get_data	Get Data	".. class:: infomark 
What it does
 This tool helps you get one or multiple list of fasta sequences from BOLD corresponding to the taxa and markers you have specified : BOLD Systems 
https://boldsystems.org/
 
_ .. class:: infomark 
Input
 A list of taxa in a txt format file and a selection of markers (one or more) from different organisms. The categories only exist to help users and diminish complexity. All markers selected will be researched for all provided taxa, therefore you don't need to specify the same marker in multiple categories. 
_ .. class:: infomark 
Output
 Each ""Taxa"" and ""Markers"" request will produce a fasta file, all files will be available in a collection. This tool will also produce a log message specifying the available markers for each taxa. .. class:: warning 
If no marker are selected or the markers selected are not found for the taxa, the tool will not produce a file. If you need, you can check the markers available for the different taxa in the ""Log message"" produced"
toolshed.g2.bx.psu.edu/repos/ecology/sdmpredictors_list_layers/sdmpredictors_list_layers/0.2.15+galaxy0	SdmPredictors List Layers	from environmental predictors for species distribution modeling	get_data	Get Data	".. class:: infomark 
What it does
 This tool helps you get one or multiple list of environemental layers from multiple sources, including : WorldClim 
https://www.worldclim.org/
 ENVIREM 
https://envirem.github.io/
 Bio-ORACLE 
https://bio-oracle.org/
 and MARSPEC 
http://www.marspec.org/
 ____ .. class:: infomark 
Output
 Eatch ""New layer list"" request will produce a tabular file, all files will be available in a collection .. class:: warning 
If no Environement are selected, or if the option specified does not exist, the tool will produce an empty file."
toolshed.g2.bx.psu.edu/repos/iuc/enasearch_search_data/enasearch_search_data/0.1.1.0	Search ENA data	given a query	get_data	Get Data	"What it does
 This tool retrieve ENA taxon data"
toolshed.g2.bx.psu.edu/repos/galaxyp/uniprotxml_downloader/uniprotxml_downloader/2.5.0	UniProt	download proteome as XML or fasta	get_data	Get Data	UniProtKB/TrEMBL (unreviewed)is a large, automatically annotated database that may contain redundant sequences, but there is a higher chance peptides will be identified. UniProtKB/Swiss-Prot (reviewed) is a smaller, manually annotated database with less of a chance peptides will be identified but less sequence redundancy
toolshed.g2.bx.psu.edu/repos/galaxyp/unipept/unipept/6.2.4+galaxy1	Unipept	retrieve taxonomy for peptides	get_data	Get Data	isoleucine (I) and leucine (L) are equated when matching tryptic peptides to UniProt records
upload1	Upload File	from your computer	get_data	Get Data	"Auto-detect
 The system will attempt to detect Axt, Fasta, Fastqsolexa, Gff, Gff3, Html, Lav, Maf, Tabular, Wiggle, Bed and Interval (Bed with headers) formats. If your file is not detected properly as one of the known formats, it most likely means that it has some format problems (e.g., different number of columns on different rows). You can still coerce the system to set your data to the format you think it should be. You can also upload compressed files, which will automatically be decompressed. ----- 
Ab1
 A binary sequence file in 'ab1' format with a '.ab1' file extension. You must manually select this 'File Format' when uploading the file. ----- 
Axt
 blastz pairwise alignment format. Each alignment block in an axt file contains three lines: a summary line and 2 sequence lines. Blocks are separated from one another by blank lines. The summary line contains chromosomal position and size information about the alignment. It consists of 9 required fields. ----- 
Bam
 A binary file compressed in the BGZF format with a '.bam' file extension. ----- 
Bed
 * Tab delimited format (tabular) * Does not require header line * Contains 3 required fields: - chrom - The name of the chromosome (e.g. chr3, chrY, chr2_random) or contig (e.g. ctgY1). - chromStart - The starting position of the feature in the chromosome or contig. The first base in a chromosome is numbered 0. - chromEnd - The ending position of the feature in the chromosome or contig. The chromEnd base is not included in the display of the feature. For example, the first 100 bases of a chromosome are defined as chromStart=0, chromEnd=100, and span the bases numbered 0-99. * May contain 9 additional optional BED fields: - name - Defines the name of the BED line. This label is displayed to the left of the BED line in the Genome Browser window when the track is open to full display mode or directly to the left of the item in pack mode. - score - A score between 0 and 1000. If the track line useScore attribute is set to 1 for this annotation data set, the score value will determine the level of gray in which this feature is displayed (higher numbers = darker gray). - strand - Defines the strand - either '+' or '-'. - thickStart - The starting position at which the feature is drawn thickly (for example, the start codon in gene displays). - thickEnd - The ending position at which the feature is drawn thickly (for example, the stop codon in gene displays). - itemRgb - An RGB value of the form R,G,B (e.g. 255,0,0). If the track line itemRgb attribute is set to ""On"", this RBG value will determine the display color of the data contained in this BED line. NOTE: It is recommended that a simple color scheme (eight colors or less) be used with this attribute to avoid overwhelming the color resources of the Genome Browser and your Internet browser. - blockCount - The number of blocks (exons) in the BED line. - blockSizes - A comma-separated list of the block sizes. The number of items in this list should correspond to blockCount. - blockStarts - A comma-separated list of block starts. All of the blockStart positions should be calculated relative to chromStart. The number of items in this list should correspond to blockCount. * Example:: chr22 1000 5000 cloneA 960 + 1000 5000 0 2 567,488, 0,3512 chr22 2000 6000 cloneB 900 - 2000 6000 0 2 433,399, 0,3601 ----- 
Fasta
 A sequence in FASTA format consists of a single-line description, followed by lines of sequence data. The first character of the description line is a greater-than ("">"") symbol in the first column. All lines should be shorter than 80 characters:: >sequence1 atgcgtttgcgtgc gtcggtttcgttgc >sequence2 tttcgtgcgtatag tggcgcggtga ----- 
FastqSolexa
 FastqSolexa is the Illumina (Solexa) variant of the Fastq format, which stores sequences and quality scores in a single file:: @seq1 GACAGCTTGGTTTTTAGTGAGTTGTTCCTTTCTTT +seq1 hhhhhhhhhhhhhhhhhhhhhhhhhhPW@hhhhhh @seq2 GCAATGACGGCAGCAATAAACTCAACAGGTGCTGG +seq2 hhhhhhhhhhhhhhYhhahhhhWhAhFhSIJGChO Or:: @seq1 GAATTGATCAGGACATAGGACAACTGTAGGCACCAT +seq1 40 40 40 40 35 40 40 40 25 40 40 26 40 9 33 11 40 35 17 40 40 33 40 7 9 15 3 22 15 30 11 17 9 4 9 4 @seq2 GAGTTCTCGTCGCCTGTAGGCACCATCAATCGTATG +seq2 40 15 40 17 6 36 40 40 40 25 40 9 35 33 40 14 14 18 15 17 19 28 31 4 24 18 27 14 15 18 2 8 12 8 11 9 ----- 
Gff
 GFF lines have nine required fields that must be tab-separated. ----- 
Gff3
 The GFF3 format addresses the most common extensions to GFF, while preserving backward compatibility with previous formats. ----- 
Interval (Genomic Intervals)
 - Tab delimited format (tabular) - File must start with definition line in the following format (columns may be in any order).:: #CHROM START END STRAND - CHROM - The name of the chromosome (e.g. chr3, chrY, chr2_random) or contig (e.g. ctgY1). - START - The starting position of the feature in the chromosome or contig. The first base in a chromosome is numbered 0. - END - The ending position of the feature in the chromosome or contig. The chromEnd base is not included in the display of the feature. For example, the first 100 bases of a chromosome are defined as chromStart=0, chromEnd=100, and span the bases numbered 0-99. - STRAND - Defines the strand - either '+' or '-'. - Example:: #CHROM START END STRAND NAME COMMENT chr1 10 100 + exon myExon chrX 1000 10050 - gene myGene ----- 
Lav
 Lav is the primary output format for BLASTZ. The first line of a .lav file begins with #:lav.. ----- 
MAF
 TBA and multiz multiple alignment format. The first line of a .maf file begins with ##maf. This word is followed by white-space-separated ""variable=value"" pairs. There should be no white space surrounding the ""="". ----- 
Scf
 A binary sequence file in 'scf' format with a '.scf' file extension. You must manually select this 'File Format' when uploading the file. ----- 
Sff
 A binary file in 'Standard Flowgram Format' with a '.sff' file extension. ----- 
Tabular (tab delimited)
 Any data in tab delimited format (tabular) ----- 
Table (delimiter-separated)
 Any delimiter-separated tabular data (CSV or TSV). ----- 
Wig
 The wiggle format is line-oriented. Wiggle data is preceded by a track definition line, which adds a number of options for controlling the default display of this track. ----- 
Other text type
 Any text file"
toolshed.g2.bx.psu.edu/repos/ecology/xarray_import_data/xarray_import_data/0.1.0	Xarray Import Data	Import a dataset from an OPeNDAP URL and convert it to a local NetCDF file using xarray.	get_data	Get Data	"================== Xarray Import Data ================== 
What it does
 Open a dataset from an OPeNDAP URL and convert it to a NetCDF file using xarray and netcdf4. | 
How to use it
 Pass a valid OPeNDAP URL to the 
opendap_url
 parameter. The tool will download the dataset and save it as a local NetCDF file. For big datasets consider subsetting the dataset in the url using 
<URL>?var_name1[start:step:end],var_name2[start:step:end]
 | 
Links
 https://docs.xarray.dev/en/stable/user-guide/io.html#opendap https://www.opendap.org/"
lftp	downloads	via lftps	get_data	Get Data	".. class:: infomark 
What this tool does
 This tool downloads a list ir URLs via lftp and creates a collection of files."
toolshed.g2.bx.psu.edu/repos/iuc/fastq_dl/fastq_dl/3.0.1+galaxy1	fastq-dl	Download FASTQ files from ENA	get_data	Get Data	"This tool downloads FASTQ files from the European Nucleotide Archive (ENA) based on a list of ENA accession IDs. You can provide either accession IDs in text format or upload a file containing accession IDs (one per line). The tool also allows you to group downloaded data by experiment or sample and can optionally retrieve only metadata without downloading the FASTQ files. Input Types ----------- You can select from two types of inputs: 1. 
ENA Accession IDs (Text Input)
: - Provide a list of ENA accession IDs (e.g., Study, Sample, Experiment, or Run accessions) separated by whitespace. 2. 
Accession IDs File
: - Provide a file containing a list of ENA accession IDs, one per line. Parameters ---------- - 
Group by Experiment
: This option groups the downloaded runs by the experiment accession, which can be useful if you need to process data related to a specific experiment. - 
Group by Sample
: This option groups the downloaded runs by the sample accession. - 
Only Download Metadata
: Select this option if you only want to retrieve metadata without downloading the actual FASTQ files. This is useful if you need information about the runs but do not need the raw sequence data. Outputs ------- The tool generates three types of outputs: 1. 
Metadata Files
: This collection contains metadata files for each accession, in 
.tsv
 format, which provide details about the corresponding run. 2. 
Single-End Data
: If the input FASTQ files contain single-end reads, those files will be placed into a separate collection. In 
.fastq.gz
 format. 3. 
Paired-End Data
: If the input FASTQ files contain paired-end reads, those files will be grouped into pairs (forward and reverse). The paired files will also be placed in a separate collection and will be in 
.fastq.gz
 format."
toolshed.g2.bx.psu.edu/repos/iuc/pysradb_search/pysradb_search/1.4.2+galaxy2	pysradb search	sequence metadata from SRA/ENA	get_data	Get Data	".. class:: infomark 
Purpose
 pysradb allows to retrieve metadata, such as run accession numbers, from SRA and ENA based on multiple criteria: - Database: SRA or ENA - Query keywords - Accession number: a relevant study/experiment/sample/run accession number - Organism: scientific name of the sample organism - Library layout: paired or single-end reads - Sample size: rounded to the nearest megabase - Publication date - Sequencing platform: Illumina, Nanopore or PacBio - Library selection: method used to select and/or enrich the material being sequenced - Library source: Type of source material that is being sequenced - Library preparation strategy: sequencing technique intended for the library ------ .. class:: infomark 
Outputs
 pysradb generates three different output types: - Raw metadata file - Statistics for the search query - Graphs to illustrate the search results ------ .. class:: infomark 
Sequencing instruments
 
Comparisons between HiSeq instruments
 HiSeq 3000/4000 provides some improvements with respect the previous model HiSeq 2500: - HiSeq 3000/4000 genere up to 1.5 Tb and 5 Tb reads per run. - HiSeq 3000/4000 use patterned flow cell technology originally developed for HiSeq X platforms. - HiSeq 3000/4000 run 3 times faster and yield 65% more reads per lane. - HiSeq 3000/4000 patterned flow cells contain billions of nanowells at fixed, known positions on the flow cell. The structured organization enables clustering at higher densities compared to non-pattern HiSeq designs. However, the HiSeq 3000/4000 also have some also some limitations with respect to HiSeq 2500: - HiSeq 3000/4000 are not recommended for low complexity sequencing. Applications such as non-unique amplicons, 16S, are currently not recommended. - Libraries with low complexity within the first 25 bases of a read are not expected to produce high quality data. - Library size restrictions. Libraries that are too long can result in polyclonal clusters that span more than 1 well, these will not pass filter. Smaller libraries will preferentially amplify with Illumina's new kinetic exclusion amplification so tight library distributions ranging from 300-500 bp are recommended. - Very low tolerance for adapter dimers. Even as little as 1% adapter dimer can take up ~6% of sequencing reads, 10% contamination will take up 84% of reads. Illumina recommends you keep adapter contamination below 0.5% of your entire library. - Higher duplication rates as compared to HiSeq 2500. - Low quality read 2 (entire HiSeq 3000 install base is affected). HiSeq 3000/4000 support DNA-seq, RNA-seq , ChIP-Seq, mate-pair, small RNA and exome library preparation. Any library preparation where there is enough sequence diversity is currently supported. Amplicon, 16S and applications with low sequencing diversity are currently not supported on the HiSeq 3000 / 4000. HiSeq 2500 is considered the most reliable model according to different sources. 
What type of read quality is expected from the HiSeq 3000/4000 ?
 - 2 x 50bp ‚â•85% bases > Q30 - 2 x 75bp ‚â•80% bases > Q30 - 2 x 150bp ‚â•75% of bases >Q30 
What is the difference between MiSeq and HiSeq?
 HiSeq and MiSeq platforms are among the most widely used platform to study microbial communities. But the two platforms differ in the length and amount of reads. MiSeq can run 600 cycles to produce 200 million 300 bp reads, on the other hand, HiSeq 2500 can run 500 cycles to produce 120 million 250 bp. 
What are the differences between HiSeq and NovaSeq?
 The Illumina NovaSeq provides a massive upgrade in sequencing throughput compared to the HiSeq 4000. There are more stringent library requirements and requires a larger sample size. Due to the vast amount of data produced by the NovaSeq and the known issue of index swapping, unique dual-indexed libraries are required. 
What are the characteristics of HiSeq X instruments?
 - HiSeq X is recommended for whole genome sequencing only (including whole bisulfite sequencing). This means that it is not adequate for RNA-seq, exome, ChIP-seq or small RNA-seq applications. - Plant and animal samples can be sequenced on the HiSeq X. - Expect coverate is over 30x or approximately 375 million reads per lane by loading one sample per lane. - Hiseq X Ten generates utilize 2x150 base pair read configurations and has slightly better GC coverage than the HiSeq 2500. 
What are the differences between MiSeq and Nextseq?
 The NextSeq Series of systems delivers the power of high-throughput sequencing with the simplicity of a desktop sequencer. NextSeq instruments represent an improvement when compared with Miseq, despite generating sorter reads (150bp, compared to MiSeq 250bp). NextSeq is recommended in the following applications & methods: - Exome & large panel sequencing (enrichment-based) - Single-cell profiling (scRNA-Seq, scDNA-Seq, oligo tagging assays) - Transcriptome sequencing (total RNA-Seq, mRNA-Seq, gene expression profiling) - Methylation sequencing - Metagenomic profiling (shotgun metagenomics, metatranscriptomics) - Cell-free sequencing & liquid biopsy analysis Regarding the maximum number of reads per ran, MiSeq can generate 25 million, vs 400 million generated by the Nextseq 550 instrument. MiSeq recommended for sequencing samples of low diversity. 
What are the differences between HiSeq and NextSeq?
 The main technical difference between HiSeq and NextSeq will be the number of dyes each machines use. HiSeq uses traditional color coding with four different dyes, while NextSeq uses two dyes. This does not give any practical differences in terms of the data quality, but the trend in illumina sequencers are more into the direction of reducing the number of dyes. 
What is the difference between Nextseq and NovaSeq?
 The NovaSeq 6000 system offers deep and broad coverage and is recommended for large whole-genome sequencing (human, plant, animal) projects. It generates 250 bp reads, with 20 billion maximum reads per run. NovaSeq 6000 instruments have not application based restrictions. 
Illumina maximum read-length summary
 - MiSeq: between 300 and 600 bp - NextSeq: 300 bp - HiSeq 2500: between 250 and 500 bp (depending of the sofware) - HiSeq 4000: 150 bp - HiSeq X: 150 bp 
Nanopore models - single-molecule ultra-long-read sequencing
 Nanopore sequencing provides the longest read lengths, from 500 bp to the current record of 2.3 Mb, with 10-30-kb genomic libraries being common. Even after error correction, sequencing error rates of corrected nanopore reads (1.5-9%) are still higher than those of corrected PacBio reads (<1%). 
PacBio SMRT instruments - single-molecule long-read low-error rate sequencing
 PacBio Sequel II CLR sequencing represents a major advancement in sequencing throughput over previous PacBio platforms with the production of more sequencing data and longer reads versus RS II and the Sequel I. The PacBio HiFi sequencing method yields highly accurate long-read sequencing datasets with read lengths averaging 10-25 kb and accuracies greater than 99.5%."
toolshed.g2.bx.psu.edu/repos/imgteam/imagecoordinates_flipaxis/imagecoordinates_flipaxis/0.1	Switch axis coordinates	Switches the axes of an image and flips the y axis.	imaging	Imaging	Makes x the horizontal axis (left to right) and y the vertical axis (bottom to top), like in a coordinate system.
toolshed.g2.bx.psu.edu/repos/iuc/collection_column_join/collection_column_join/0.0.3	Column join	on multiple datasets	join__subtract_and_group	Join, Subtract and Group	"Joins lists of tabular datasets together on a field. ----- 
Example
 To join three files, with headers, based on the first column: 
First file (in_1)
:: #KEY c2 c3 c4 one 1-1 1-2 1-3 two 1-4 1-5 1-6 three 1-7 1-8 1-9 
Second File (in_2)
:: #KEY c2 c3 c4 one 2-1 2-2 2-3 two 2-4 2-5 2-6 three 2-7 2-8 2-9 
Third file (in_3)
:: #KEY c2 c3 c4 one 3-3 3-2 3-3 two 3-4 3-5 3-6 three 3-7 3-8 3-9 
Joining
 the files, using 
identifier column of 1
 and a 
header lines of 1
, will return:: #KEY in_1_c2 in_1_c3 in_1_c4 in_2_c2 in_2_c3 in_2_c4 in_3_c2 in_3_c3 in_3_c4 one 1-1 1-2 1-3 2-1 2-2 2-3 3-3 3-2 3-3 three 1-7 1-8 1-9 2-7 2-8 2-9 3-7 3-8 3-9 two 1-4 1-5 1-6 2-4 2-5 2-6 3-4 3-5 3-6 
Joining
 the files, using 
identifier column of 1
 and a 
header lines of 1
, but disabling 
Add column name to header
, will return:: #KEY in_1 in_1 in_1 in_2 in_2 in_2 in_3 in_3 in_3 one 1-1 1-2 1-3 2-1 2-2 2-3 3-3 3-2 3-3 three 1-7 1-8 1-9 2-7 2-8 2-9 3-7 3-8 3-9 two 1-4 1-5 1-6 2-4 2-5 2-6 3-4 3-5 3-6"
toolshed.g2.bx.psu.edu/repos/devteam/cluster/gops_cluster_1/1.0.0	Cluster	the intervals of a dataset	operate_on_genomic_intervals	Operate on Genomic Intervals	".. class:: infomark 
TIP:
 If your dataset does not appear in the pulldown menu, it means that it is not in interval format. Use ""edit attributes"" to set chromosome, start, end, and strand columns. ----- 
Screencasts!
 See Galaxy Interval Operation Screencasts_ (right click to open this link in another window). .. _Screencasts: https://galaxyproject.org/learn/interval-operations/ ----- 
Syntax
 - 
Maximum distance
 is greatest distance in base pairs allowed between intervals that will be considered ""clustered"". 
Negative
 values for distance are allowed, and are useful for clustering intervals that overlap. - 
Minimum intervals per cluster
 allow a threshold to be set on the minimum number of intervals to be considered a cluster. Any area with less than this minimum will not be included in the output. - 
Merge clusters into single intervals
 outputs intervals that span the entire cluster. - 
Find cluster intervals; preserve comments and order
 filters out non-cluster intervals while maintaining the original ordering and comments in the file. - 
Find cluster intervals; output grouped by clusters
 filters out non-cluster intervals, but outputs the cluster intervals so that they are grouped together. Comments and original ordering in the file are lost. ----- 
Examples
 Find Clusters: .. image:: gops_clusterFind.gif Merge Clusters: .. image:: gops_clusterMerge.gif"
toolshed.g2.bx.psu.edu/repos/iuc/ena_upload/ena_upload/0.9.0+galaxy0	ENA Upload tool	Submission of (meta)data to the European Nucleotide Archive (ENA)	send_data	Send Data	This is a wrapper for the ENA upload tool in https://github.com/usegalaxy-eu/ena-upload-cli. The input metadata can be submitted following the tabular format of the templates or their excel spreadsheet equivalent in https://github.com/ELIXIR-Belgium/ENA-metadata-templates. This template repo provides ready to use sheets for every ENA sample checklist and is automatically updated. .. class:: warningmark The ENA upload tool won't work unless you have provided an ENA Webin ID in User > Preferences > Manage Information > ENA Webin account details.
toolshed.g2.bx.psu.edu/repos/iuc/ena_webin_cli/ena_webin_cli/9.0.1+galaxy1	ENA Webin CLI	Submission of consensus sequences to the European Nucleotide Archive (ENA)	send_data	Send Data	This tool is a wrapper for the ENA Webin CLI submission tool (https://ena-docs.readthedocs.io/en/latest/submit/general-guide/webin-cli.html). .. class:: warningmark The ENA upload tool won't work unless you have provided an ENA Webin ID in User > Preferences > Manage Information > ENA Webin account details.
export_remote	Export datasets	to repositories	send_data	Send Data	
toolshed.g2.bx.psu.edu/repos/imgteam/curl_post/curl_post/0.0.2	cURL	send cURL POST requests	send_data	Send Data	"What it does
 Uses cURL to send a file via POST."
toolshed.g2.bx.psu.edu/repos/mvdbeek/add_input_name_as_column/addName/0.2.0	Add input name as column	to an existing tabular file	text_manipulation	Text Manipulation	"What it does
 Adds a new column with the name of the input file as it appears in the history. By default the column is appended. ----- .. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert"
toolshed.g2.bx.psu.edu/repos/bgruening/add_line_to_file/add_line_to_file/0.1.0	Add line to file	writes a line of text at the begining or end of a text file.	text_manipulation	Text Manipulation	"What it does
 This tool adds the input text to the beginning (header) or the end (footer) of the input text file."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy3	Advanced Cut	columns from a table (cut)	text_manipulation	Text Manipulation	"What it does
 This tool runs the 
cut
 unix command, which extract or delete columns from a file. ----- Field List Example: 
1,3,7
 - Cut specific fields/characters. 
3-
 - Cut from the third field/character to the end of the line. 
2-5
 - Cut from the second to the fifth field/character. 
-8
 - Cut from the first to the eight field/characters. Input Example:: fruit color price weight apple red 1.4 0.5 orange orange 1.5 0.3 banana yellow 0.9 0.3 Output Example ( 
Keeping fields 1,3,4
 ):: fruit price weight apple 1.4 0.5 orange 1.5 0.3 banana 0.9 0.3 Output Example ( 
Discarding field 2
 ):: fruit price weight apple 1.4 0.5 orange 1.5 0.3 banana 0.9 0.3 Output Example ( 
Keeping 3 characters
 ):: fru app ora ban @REFERENCES@"
toolshed.g2.bx.psu.edu/repos/galaxyp/regex_find_replace/regexColumn1/1.0.3	Column Regex Find And Replace		text_manipulation	Text Manipulation	".. class:: warningmark 
This tool will attempt to reuse the metadata from your first input.
 To change metadata assignments click on the ""edit attributes"" link of the history item generated by this tool. .. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- This tool goes line by line through the specified input file and if the text in the selected column matches a specified regular expression pattern replaces the text with the corresponding specified replacement. This tool can be used to change between the chromosome naming conventions of UCSC and Ensembl. For example to remove the 
chr
 part of the reference sequence name in the first column of this GFF file:: ##gff-version 2 ##Date: Thu Mar 23 11:21:17 2006 ##bed2gff.pl $Rev: 601 $ ##Input file: ./database/files/61c6c604e0ef50b280e2fd9f1aa7da61.dat chr1 bed2gff CCDS1000.1_cds_0_0_chr1_148325916_f 148325916 148325975 . + . score ""0""; chr21 bed2gff CCDS13614.1_cds_0_0_chr21_32707033_f 32707033 32707192 . + . score ""0""; chrX bed2gff CCDS14606.1_cds_0_0_chrX_122745048_f 122745048 122745924 . + . score ""0""; Setting:: using column: c1 Find Regex: chr([0-9]+|X|Y|M[Tt]?) Replacement: \1 produces:: ##gff-version 2 ##Date: Thu Mar 23 11:21:17 2006 ##bed2gff.pl $Rev: 601 $ ##Input file: ./database/files/61c6c604e0ef50b280e2fd9f1aa7da61.dat 1 bed2gff CCDS1000.1_cds_0_0_chr1_148325916_f 148325916 148325975 . + . score ""0""; 21 bed2gff CCDS13614.1_cds_0_0_chr21_32707033_f 32707033 32707192 . + . score ""0""; X bed2gff CCDS14606.1_cds_0_0_chrX_122745048_f 122745048 122745924 . + . score ""0""; This tool uses Python regular expressions with the 
re.sub()
 function. More information about Python regular expressions can be found here: http://docs.python.org/library/re.html. The regex 
chr([0-9]+|X|Y|M)
 means start with text 
chr
 followed by either: one or more digits, or the letter X, or the letter Y, or the letter M (optionally followed by a single letter T or t). Note that the parentheses 
()
 capture patterns in the text that can be used in the replacement text by using a backslash-number reference: 
\1
 In the replacement pattern, use the special token #{input_name} to insert the input dataset's display name. The name can be modified by a second find/replace check. Suppose you want to insert the sample id of your dataset, named 
Sample ABC123
, into the dataset itself, which currently contains the lines:: Data 1 Data 2 Data 3 You can use the following checks:: Find Regex: Data Replacement: #{input_name} Data Find Regex: Sample (\S+) Replacement: \1 The result will be:: ABC123 Data 1 ABC123 Data 2 ABC123 Data 3 Galaxy aggressively escapes input supplied to tools, so if something is not working please let us know and we can look into whether this is the cause. Also if you would like help constructing regular expressions for your inputs, please let us know at help@msi.umn.edu."
toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1	Compute	on rows	text_manipulation	Text Manipulation	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
What it does
 This tool computes an expression on every row of a dataset and appends or inserts the result as a new column (field). Several expressions can be specified and will be applied sequentially to each row. 
Expression rules
 - Columns are referenced with 
c
 and a 
number
. For example, 
c1
 refers to the first column of a tab-delimited file - The following built-in Python functions are available for use in expressions:: abs | all | any | ascii | bin | bool | chr | complex | divmod | float | format | hex | int | len | list map | max | min | oct | ord | pow | range | reversed | round | set | sorted | str | sum | type acos | acosh | asin | asinh | atan | atan2 | atanh | cbrt | ceil | comb | copysign | cos | cosh | degrees dist | erf | erfc | exp | exp2 | expm1 | fabs | factorial | floor | fmod | frexp | fsum | gamma | gcd hypot | inf | isclose | isfinite | isinf | isnan | isqrt | ldexp | lgamma | log | log10 | log1p | log2 modf | nextafter | perm | pow | prod | remainder | sin | sqrt | tan | tanh | tau | trunc | ulp - In addition the numpy function 
format_float_positional
 is available to control the formatting of floating point numbers. - Expressions can be chained, and the tool will keep track of newly added columns while working through the chain. This means you can reference a column that was created as the result of a previous expression in later ones. ----- 
Simple examples
 If this is your input:: chr1 151077881 151077918 2 200 - chr1 151081985 151082078 3 500 + computing ""c4 * c5"" will produce:: chr1 151077881 151077918 2 200 - 400 chr1 151081985 151082078 3 500 + 1500 You can also use this tool to evaluate expressions. For example, computing ""c3 >= c2"" for the input above will result in the following:: chr1 151077881 151077918 2 200 - True chr1 151081985 151082078 3 500 + True Similarly, computing ""type(c2) == type(c3) will return:: chr1 151077881 151077918 2 200 - True chr1 151081985 151082078 3 500 + True ----- 
Error handling
 The tool will always fail on syntax errors in and other unrecoverable parsing errors with any of your expressions. For other problems, however, it offers control over how they should be handled: 1. The default for ""Autodetect column types"" is ""Yes"", which means the tool will evaluate each column value as the type that Galaxy assumes for the column. This default behavior will allow you to write simpler expressions. The arithmetic expression ""c4 * c5"" from the first simple example, for instance, works only because Galaxy realizes that c4 and c5 are integer columns. Occasionally, this autodetection can cause issues. A common such situation are missing values in columns that Galaxy thinks are of numeric type. If you're getting errors like ""Failed to convert some of the columns in line #X ..."", a solution might be to turn off column type autodetection. The price you will have to pay for doing so is that now you will have to handle type conversions yourself. In the first example you would now have to use the epression: ""int(c4) * int(c5)"". 2. By default, if any expression references columns that are not existing before that expression gets computed, the tool will fail, but you can uncheck the ""Fail on references to non-existent columns"" option. If you do so, the result will depend on your choice for ""If an expression cannot be computed for a row"" (see 3.) 3. The default for rows, for which an expression fails to compute is, again, to fail the tool run, but you can also choose to: - skip the row on output This is a simple way to only keep lines conforming to an expected standard. It is also easy to mask problems with your expressions with this option so take a look at the results and try to understand what gets skipped and for what reasons (the stdout of the tool will contain information about both). - keep the row unchanged This can be a good solution if your input contains special separator lines that don't follow the general tabular format of other lines and you would like to keep those lines - produce an empty column value for the row This will use the empty string as a substitute for non-computable items. Different from the ""keep the row unchanged option"" the problematic line will have a column added or changed. This option is a good choice for inputs in which all rows have the same tabular layout where you want to make sure that the same is true for the output, i.e. that all output lines still have the same number of columns. - fill in a replacement value This option is very similar to the previous one, but lets you control the replacement value. 
Example
 In the following input:: chr1 151077881 151077918 2 200 - chr1 151081985 151082078 3 500 + chr1 151090031 151090938 4 700 the last line does not have a strand column. This violates the bed file format specification, which says that unknown strand is to be encoded as 
.
 in the strand column. You can fix the file with the following tool run: 
Add expression
: 
c6
 
Mode of the operation
: 
Replace
 
Use new column to replace column number
: 
6
 
Fail on references to non-existent columns
: 
No
 
If an expression cannot be computed for a row
: 
Fill in a replacement value
 
Replacement value
: 
."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cat/9.5+galaxy3	Concatenate datasets	tail-to-head (cat)	text_manipulation	Text Manipulation	".. class:: warningmark 
WARNING:
 Be careful not to concatenate datasets of different kinds (e.g., sequences with intervals). This tool does not check if the datasets being concatenated are in the same format. ----- 
What it does
 Concatenates datasets ----- 
Example
 Concatenating Dataset:: chrX 151087187 151087355 A 0 - chrX 151572400 151572481 B 0 + with Dataset1:: chr1 151242630 151242955 X 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - and with Dataset2:: chr2 100000030 200000955 P 0 + chr2 100000015 200000999 Q 0 + will result in the following:: chrX 151087187 151087355 A 0 - chrX 151572400 151572481 B 0 + chr1 151242630 151242955 X 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - chr2 100000030 200000955 P 0 + chr2 100000015 200000999 Q 0 +"
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_text_file_with_recurring_lines/9.5+galaxy3	Create text file	with recurring lines	text_manipulation	Text Manipulation	".. class:: infomark 
What it does
 This tool creates a text file with recurring lines. You can specify a bunch of characters or entire sentences. The entire string will be printed X times separated by a line break. X can be either given by the use as a number or calculated by a given file. In case the user provides a file, the line number will be used as X."
toolshed.g2.bx.psu.edu/repos/iuc/filter_tabular/filter_tabular/3.3.1	Filter Tabular		text_manipulation	Text Manipulation	"Python offset indexes or slices. Examples: 
 
Column offset indexes: 0,3,1 (selects the first,fourth, and second columns)
 
Negative column numbers: -1,-2 (selects the last, and second last columns)
 
python slices ( slice(start, stop[, step]) select a range of columns): 
 
 
0:3 or :3 (selects the first 3 columns)
 
3:5 (selects the fourth and fifth columns)
 
2: (selects all columns after the second)
 
-2: (selects the last 2 columns)
 
2::-1 (selects the first 3 columns n reverse order: third,second,first)"
toolshed.g2.bx.psu.edu/repos/devteam/dna_filtering/histogram_rpy/1.0.3	Histogram	of a numeric column	text_manipulation	Text Manipulation	".. class:: infomark 
TIP:
 To remove comment lines that do not begin with a 
#
 character, use 
Text Manipulation->Remove beginning
 .. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
Syntax
 This tool computes a histogram of the numerical values in a column of a dataset. - All invalid, blank and comment lines in the dataset are skipped. The number of skipped lines is displayed in the resulting history item. - 
Column for x axis
 - only numerical columns are possible. - 
Number of breaks(bars)
 - breakpoints between histogram cells. Value of '0' will determine breaks automatically. - 
Plot title
 - the histogram title. - 
Label for x axis
 - the label of the x axis for the histogram. - 
Include smoothed density
 - if checked, the resulting graph will join the given corresponding points with line segments. ----- 
Example
 - Input file:: 1 68 4.1 2 71 4.6 3 62 3.8 4 75 4.4 5 58 3.2 6 60 3.1 7 67 3.8 8 68 4.1 9 71 4.3 10 69 3.7 - Create a histogram on column 2 of the above dataset. .. image:: histogram2.png"
toolshed.g2.bx.psu.edu/repos/iuc/jq/jq/1.0	JQ	process JSON	text_manipulation	Text Manipulation	"JQ == jq is a lightweight and flexible JSON processor. Brief Examples -------------- See 
the manual <https://stedolan.github.io/jq/manual/>
__ for a much more detailed guide on using JQ. Select an Attribute ~~~~~~~~~~~~~~~~~~~ Given an input like the following :: {""foo"": 42, ""bar"": ""less interesting data""} To select just the value of 
foo
, supply the filter 
.foo
 Loop over an Array ~~~~~~~~~~~~~~~~~~ Given an input like the following :: [{""foo"": 1123}, {""foo"": 6536}, {""foo"": 5321}] To select the values of 
foo
, supply the filter 
.[].foo
 or 
.[] | .foo
. This will produce a file with one number per line. If you wish to select multiple things: :: [{""foo"": 1123, ""bar"": ""a""}, {""foo"": 6536, ""bar"": ""b""}, {""foo"": 5321, ""bar"": ""c""}] To select the values of 
foo
 AND 
bar
, supply the filter 
.[] | [.foo, .bar]
. This will produce and output array like: :: [ [1123, ""a""] [6536, ""b""] [5321, ""c""] ] A common next step is to turn this into a tabular output which more Galaxy tools can work with. This can be done by checking the box for tabular. This will invoke the JQ filter of 
@tsv
 at the end of the processing chain, and produce a tabular file."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_easyjoin_tool/9.5+galaxy3	Join	two files	text_manipulation	Text Manipulation	"What it does
 This tool joins two tabular files based on a common key column. ----- 
Example
 
First file
:: Fruit Color Apple red Banana yellow Orange orange Melon green 
Second File
:: Fruit Price Orange 7 Avocado 8 Apple 4 Banana 3 
Joining
 both files, using 
key column 1
 and a 
header line
, will return:: Fruit Color Price Apple red 4 Avocado . 8 Banana yellow 3 Melon green . Orange orange 7 .. class:: infomark * Input files need not be sorted. * The header line (
Fruit Color Price
) was joined and kept as first line. * Missing values ( Avocado's color, missing from the first file ) are replaced with a period character."
toolshed.g2.bx.psu.edu/repos/bgruening/join_files_on_column_fuzzy/join_files_on_column_fuzzy/1.0.1	Join two files	on column allowing a small difference	text_manipulation	Text Manipulation	"Join two files on a common column. It is necessary to provide an allowed difference between both values as the maximum absolute difference or maximum parts per million (ppm) for matching. Two modes are available: 1) In the 
best match
 mode: For each value in file 1 only the best matching value of file 2 is reported. In case of multiple best matches, only the closest match is reported. 2) In the 
all matches
 mode: All matches within the defined distance are reported. Be aware that file 1 is the template file and therefore the same value in file 2 can be matched to multiple values in file 1 ------ 
Example
 
Input file 1
 :: 1 2 3 4 5 
Input file 2
 :: 1.1 1.2 2.2 3.3 4.4 
Joined file1 and 2
 with best match and absolute distance 0.3:: 1 1.1 0.1 2 2.2 0.2 3 3.3 0.3 
Joined file1 and 2
 with all matches and absolute distance 0.3:: 1 1.1 0.1 1 1.2 0.2 2 2.2 0.2 3 3.3 0.3"
toolshed.g2.bx.psu.edu/repos/devteam/merge_cols/mergeCols1/1.0.2	Merge Columns	together	text_manipulation	Text Manipulation	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
What it does
 This tool merges columns together. Any number of valid columns can be merged in any order. ----- 
Example
 Input dataset (five columns: c1, c2, c3, c4, and c5):: 1 10 1000 gene1 chr 2 100 1500 gene2 chr merging columns ""
c5,c1
"" will return:: 1 10 1000 gene1 chr chr1 2 100 1500 gene2 chr chr2 .. class:: warningmark Note that all original columns are preserved and the result of merge is added as the rightmost column."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_multijoin_tool/9.5+galaxy3	Multi-Join	(combine multiple files)	text_manipulation	Text Manipulation	"What it does
 This tool joins multiple tabular files based on a common key column. ----- 
Example
 To join three files, based on the 4th column, and keeping the 7th,8th,9th columns: 
First file (AAA)
:: chr4 888449 890171 FBtr0308778 0 + 266 1527 1722 chr4 972167 979017 FBtr0310651 0 - 3944 6428 6850 chr4 972186 979017 FBtr0089229 0 - 3944 6428 6831 chr4 972186 979017 FBtr0089231 0 - 3944 6428 6831 chr4 972186 979017 FBtr0089233 0 - 3944 6428 6831 chr4 995793 996435 FBtr0111046 0 + 7 166 642 chr4 995793 997931 FBtr0111044 0 + 28 683 2138 chr4 995793 997931 FBtr0111045 0 + 28 683 2138 chr4 1034029 1047719 FBtr0089223 0 - 5293 13394 13690 ... 
Second File (BBB)
:: chr4 90286 134453 FBtr0309803 0 + 657 29084 44167 chr4 251355 266499 FBtr0089116 0 + 56 1296 15144 chr4 252050 266506 FBtr0308086 0 + 56 1296 14456 chr4 252050 266506 FBtr0308087 0 + 56 1296 14456 chr4 252053 266528 FBtr0300796 0 + 56 1296 14475 chr4 252053 266528 FBtr0300800 0 + 56 1296 14475 chr4 252055 266528 FBtr0300798 0 + 56 1296 14473 chr4 252055 266528 FBtr0300799 0 + 56 1296 14473 chr4 252541 266528 FBtr0300797 0 + 56 1296 13987 ... 
Third file (CCC)
:: chr4 972167 979017 FBtr0310651 0 - 9927 6738 6850 chr4 972186 979017 FBtr0089229 0 - 9927 6738 6831 chr4 972186 979017 FBtr0089231 0 - 9927 6738 6831 chr4 972186 979017 FBtr0089233 0 - 9927 6738 6831 chr4 995793 996435 FBtr0111046 0 + 5 304 642 chr4 995793 997931 FBtr0111044 0 + 17 714 2138 chr4 995793 997931 FBtr0111045 0 + 17 714 2138 chr4 1034029 1047719 FBtr0089223 0 - 17646 13536 13690 ... 
Joining
 the files, using 
key column 4
, 
value columns 7,8,9
 and a 
header line
, will return:: key AAA__V7 AAA__V8 AAA__V9 BBB__V7 BBB__V8 BBB__V9 CCC__V7 CCC__V8 CCC__V9 FBtr0089116 0 0 0 56 1296 15144 0 0 0 FBtr0089223 5293 13394 13690 0 0 0 17646 13536 13690 FBtr0089229 3944 6428 6831 0 0 0 9927 6738 6831 FBtr0089231 3944 6428 6831 0 0 0 9927 6738 6831 FBtr0089233 3944 6428 6831 0 0 0 9927 6738 6831 FBtr0111044 28 683 2138 0 0 0 17 714 2138 FBtr0111045 28 683 2138 0 0 0 17 714 2138 FBtr0111046 7 166 642 0 0 0 5 304 642 FBtr0300796 0 0 0 56 1296 14475 0 0 0 ... .. class:: infomark Input files need not be sorted."
toolshed.g2.bx.psu.edu/repos/iuc/query_tabular/query_tabular/3.3.2	Query Tabular	using sqlite sql	text_manipulation	Text Manipulation	"Python offset indexes or slices. Examples: 
 
Column offset indexes: 0,3,1 (selects the first,fourth, and second columns)
 
Negative column numbers: -1,-2 (selects the last, and second last columns)
 
python slices ( slice(start, stop[, step]) select a range of columns): 
 
 
0:3 or :3 (selects the first 3 columns)
 
3:5 (selects the fourth and fifth columns)
 
2: (selects all columns after the second)
 
-2: (selects the last 2 columns)
 
2::-1 (selects the first 3 columns n reverse order: third,second,first)"
toolshed.g2.bx.psu.edu/repos/iuc/gff3_rebase/gff3.rebase/1.2	Rebase GFF3 features	against parent features	text_manipulation	Text Manipulation	"What it does
 Often the genomic data processing/analysis process requires a workflow like the following: - select some features from a genome - export the sequences associated with those regions - analyse those exports with some tool like Blast For display, especially in software like JBrowse, it is convenient to know where in the original genome the analysis results would fall. E.g. if a transmembrane domain is detected at bases 10-20 of an analysed protein, where should this be displayed relative to the parent genome? This tool helps fill that gap, by rebasing some analysis results against the parent features which were originally analysed. 
Example Inputs
 For a ""child"" set of annotations like:: #gff-version 3 cds42 blastp match_part 1 50 1e-40 . . ID=m00001;Notes=RNAse A Protein And a parent set of annotations like:: #gff-version 3 PhageBob maker cds 300 600 . + . ID=cds42 One could imagine that during the analysis process, the user had exported the parent annotation into some sequence:: >cds42 M...... and then analysed those results, producing the ""child"" annotation file. This tool will then localize the results properly against the parent:: #gff-version 3 PhageBob blastp match_part 300 449 1e-40 + . ID=m00001;Notes=RNAse A Protein which will allow you to display the results in the correct location in visualizations. 
Options
 There are two optional flags which can be passed. The Interpro flag selectively ignores features which shouldn't be included in the output (i.e. 
polypeptide
), and a couple of qualifiers that aren't useful (
status
, 
Target
) The ""Map Protein..."" flag says that you translated the sequences during the genomic export process, analysing protein sequences. This indicates to the software that the bases should be multiplied by three to obtain the correct DNA locations."
toolshed.g2.bx.psu.edu/repos/galaxyp/regex_find_replace/regex1/1.0.3	Regex Find And Replace		text_manipulation	Text Manipulation	"This tool goes line by line through the specified input file and replaces text which matches the specified regular expression patterns with its corresponding specified replacement. This tool uses Python regular expressions. More information about Python regular expressions can be found here: http://docs.python.org/library/re.html. To convert an Ilumina FATSQ sequence id from the CAVASA 8 format:: @EAS139:136:FC706VJ:2:2104:15343:197393 1:Y:18:ATCACG GGGTGATGGCCGCTGCCGATGGCGTCAAATCCCACC +EAS139:136:FC706VJ:2:2104:15343:197393 1:Y:18:ATCACG IIIIIIIIIIIIIIIIIIIIIIIIIIIIII9IG9IC To the CASAVA 7 format:: @EAS139_FC706VJ:2:2104:15343:197393#0/1 GGGTGATGGCCGCTGCCGATGGCGTCAAATCCCACC +EAS139_FC706VJ:2:2104:15343:197393#0/1 IIIIIIIIIIIIIIIIIIIIIIIIIIIIII9IG9IC Use Settings:: Find Regex: ^([@+][A-Z0-9]+):\d+:(\S+)\s(\d).
$ Replacement: \1_\2#0/\3 Note that the parentheses 
()
 capture patterns in the text that can be used in the replacement text by using a backslash-number reference: 
\1
 The regex 
^([@+][A-Z0-9]+):\d+:(\S+) (\d).
$
 means:: ^ - start the match at the beginning of the line of text ( - start a group (1), that is a string of matched text, that can be back-referenced in the replacement as \1 [@+] - matches either a @ or + character [A-Z0-9]+ - matches an uppercase letter or a digit, the plus sign means to match 1 or more such characters ) - end a group (1), that is a string of matched text, that can be back-referenced in the replacement as \1 :\d+: - matches a colon followed by one or more digits followed by a colon character (\S+) - matches one or more non-whitespace charcters, the enclosing parentheses make this a group (2) that can back-referenced in the replacement text as \2 \s - matches a whitespace character (\d) - matches a single digit character, the enclosing parentheses make this a group (3) that can back-referenced in the replacement text as \3 .* - dot means match any character, asterisk means zero more more matches $ - the regex must match to the end of the line of text In the replacement pattern, use the special token #{input_name} to insert the input dataset's display name. The name can be modified by a second find/replace check. Suppose you want to insert the sample id of your dataset, named 
Sample ABC123**, into the dataset itself, which currently contains the lines:: Data 1 Data 2 Data 3 You can use the following checks:: Find Regex: Data Replacement: #{input_name} Data Find Regex: Sample (\S+) Replacement: \1 The result will be:: ABC123 Data 1 ABC123 Data 2 ABC123 Data 3 Galaxy aggressively escapes input supplied to tools, so if something is not working please let us know and we can look into whether this is the cause. Also if you would like help constructing regular expressions for your inputs, please let us know at help@msi.umn.edu."
toolshed.g2.bx.psu.edu/repos/kellrott/regex_replace/regex_replace/1.0.0	Regex Replace	Regular Expression replacement using the Python re module	text_manipulation	Text Manipulation	Perform regular expression replacement using the Python re engine. Example: ======== Search String:: [^\s]+_(\w+).* Replace String:: \1 On the input file:: LJP001_BT20_24H:DMSO LJP001_BT20_24H:H20 LJP001_BT20_6H:DMSO Outputs:: 24H 24H 6H Additional Options ================== Replace Count: Will define the number of occurrences that can be replaced by the substitution. A count of 0 is equivalent to 'replace all' Ignore Case: Make searches case insensitive Multi-line: Do the search across the entire contents of the file. If not checked, then replacements occur line by line Dot-All: With this checked, the '.' in a matching pattern will also match new-line characters. Generally used with 'multi-line' search
toolshed.g2.bx.psu.edu/repos/iuc/column_remove_by_header/column_remove_by_header/1.0	Remove columns	by heading	text_manipulation	Text Manipulation	Removes or keeps columns based upon user provided values. Hint: If any of the column names you would like to specify contains special (non-ASCII) characters, you can specify these using their Unicode escape sequences.
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy3	Replace	parts of text	text_manipulation	Text Manipulation	"What it does
 This tool finds $ replaces text in an input dataset. .. class:: infomark The 
pattern to find
 can be a simple text string, or a perl 
regular expression
 string (depending on 
pattern is a regex
 check-box). .. class:: infomark When using regular expressions, the 
replace pattern
 can contain back-references ( e.g. \1 ) .. class:: infomark This tool uses 
Perl regular expression <https://perldoc.perl.org/perlre>
_ syntax. ----- 
Examples of 
regular-expression
 Find Patterns
 - 
HELLO
 The word 'HELLO' (case sensitive). - 
AG.T
 The letters A,G followed by any single character, followed by the letter T. - 
A{4,}
 Four or more consecutive A's. - 
chr2[012]\t
 The words 'chr20' or 'chr21' or 'chr22' followed by a tab character. - 
hsa-mir-([^ ]+)
 The text 'hsa-mir-' followed by one-or-more non-space characters. When using parenthesis, the matched content of the parenthesis can be accessed with 
\1
 in the 
replace
 pattern. 
Examples of Replace Patterns
 - 
WORLD
 The word 'WORLD' will be placed whereever the find pattern was found. - 
FOO-$&-BAR
 Each time the find pattern is found, it will be surrounded with 'FOO-' at the begining and '-BAR' at the end. 
$&
 (dollar-ampersand) represents the matched find pattern. - 
$1
 The text which matched the first parenthesis in the Find Pattern. ----- 
Example 1
 
Find Pattern:
 HELLO 
Replace Pattern:
 WORLD 
Regular Expression:
 no 
Replace what:
 entire line Every time the word HELLO is found, it will be replaced with the word WORLD. ----- 
Example 2
 
Find Pattern:
 ^chr 
Replace Pattern:
 (empty) 
Regular Expression:
 yes 
Replace what:
 column 11 If column 11 (of every line) begins with ther letters 'chr', they will be removed. Effectively, it'll turn ""chr4"" into ""4"" and ""chrXHet"" into ""XHet"" ----- 
Perl's Regular Expression Syntax
 The Find & Replace tool searches the data for lines containing or not containing a match to the given pattern. A Regular Expression is a pattern descibing a certain amount of text. - 
( ) { } [ ] . * ? + \ ^ $
 are all special characters. 
\
 can be used to ""escape"" a special character, allowing that special character to be searched for. - 
^
 matches the beginning of a string(but not an internal line). - 
(
 .. 
)
 groups a particular pattern. - 
{
 n or n, or n,m 
}
 specifies an expected number of repetitions of the preceding pattern. - 
{n}
 The preceding item is matched exactly n times. - 
{n,}
 The preceding item ismatched n or more times. - 
{n,m}
 The preceding item is matched at least n times but not more than m times. - 
[
 ... 
]
 creates a character class. Within the brackets, single characters can be placed. A dash (-) may be used to indicate a range such as 
a-z
. - 
.
* Matches any single character except a newline. - 
 The preceding item will be matched zero or more times. - 
?
 The preceding item is optional and matched at most once. - 
+
 The preceding item will be matched one or more times. - 
^
 has two meaning: - matches the beginning of a line or string. - indicates negation in a character class. For example, [^...] matches every character except the ones inside brackets. - 
$
 matches the end of a line or string. - 
\|
 Separates alternate possibilities. - 
\d
 matches a single digit - 
\w
 matches a single letter or digit or an underscore. - 
\s** matches a single white-space (space or tabs)."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_column/9.5+galaxy3	Replace Text	in a specific column	text_manipulation	Text Manipulation	"What it does
 This tool performs find & replace operation on a specified column in a given file. .. class:: infomark The 
pattern to find
 uses the 
extended regular
 expression syntax (same as running 'awk --re-interval'). .. class:: infomark 
TIP:
 If you need more complex patterns, use the 
awk
 tool. ----- 
Examples of Find Patterns
 - 
HELLO
 The word 'HELLO' (case sensitive). - 
AG.T
 The letters A,G followed by any single character, followed by the letter T. - 
A{4,}
 Four or more consecutive A's. - 
chr2[012]\t
 The words 'chr20' or 'chr21' or 'chr22' followed by a tab character. - 
hsa-mir-([^ ]+)
 The text 'hsa-mir-' followed by one-or-more non-space characters. When using parenthesis, the matched content of the parenthesis can be accessed with 
\1
 in the 
replace
 pattern. 
Examples of Replace Patterns
 - 
WORLD
 The word 'WORLD' will be placed whereever the find pattern was found. - 
FOO-&-BAR
 Each time the find pattern is found, it will be surrounded with 'FOO-' at the begining and '-BAR' at the end. 
&
 (ampersand) represents the matched find pattern. - 
\1
 The text which matched the first parenthesis in the Find Pattern. ----- 
Example 1
 
Find Pattern:
 HELLO 
Replace Pattern:
 WORLD Every time the word HELLO is found, it will be replaced with the word WORLD. This operation affects only the selected column. ----- 
Example 2
 
Find Pattern:
 ^(.{4}) 
Replace Pattern:
 &\t Find the first four characters in each line, and replace them with the same text, followed by a tab character. In practice - this will split the first line into two columns. This operation affects only the selected column. ----- 
Extened Regular Expression Syntax
 The select tool searches the data for lines containing or not containing a match to the given pattern. A Regular Expression is a pattern descibing a certain amount of text. - 
( ) { } [ ] . * ? + \ ^ $
 are all special characters. 
\
 can be used to ""escape"" a special character, allowing that special character to be searched for. - 
^
 matches the beginning of a string(but not an internal line). - 
(
 .. 
)
 groups a particular pattern. - 
{
 n or n, or n,m 
}
 specifies an expected number of repetitions of the preceding pattern. - 
{n}
 The preceding item is matched exactly n times. - 
{n,}
 The preceding item ismatched n or more times. - 
{n,m}
 The preceding item is matched at least n times but not more than m times. - 
[
 ... 
]
 creates a character class. Within the brackets, single characters can be placed. A dash (-) may be used to indicate a range such as 
a-z
. - 
.
 Matches any single character except a newline. - 
*
 The preceding item will be matched zero or more times. - 
?
 The preceding item is optional and matched at most once. - 
+
 The preceding item will be matched one or more times. - 
^
 has two meaning: - matches the beginning of a line or string. - indicates negation in a character class. For example, [^...] matches every character except the ones inside brackets. - 
$
 matches the end of a line or string. - 
\|
 Separates alternate possibilities. 
Note
: AWK uses extended regular expression syntax, not Perl syntax. 
\d
, 
\w
, 
\s
 etc. are 
not** supported."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_line/9.5+galaxy3	Replace Text	in entire line	text_manipulation	Text Manipulation	"What it does
 This tool performs find & replace operation on a specified file. .. class:: infomark The 
pattern to find
 uses the 
extended regular
 expression syntax (same as running 'sed -r'). .. class:: infomark 
TIP:
 If you need more complex patterns, use the 
sed
 tool. ----- 
Examples of Find Patterns
 - 
HELLO
 The word 'HELLO' (case sensitive). - 
AG.T
 The letters A,G followed by any single character, followed by the letter T. - 
A{4,}
 Four or more consecutive A's. - 
chr2[012]\t
 The words 'chr20' or 'chr21' or 'chr22' followed by a tab character. - 
hsa-mir-([^ ]+)
 The text 'hsa-mir-' followed by one-or-more non-space characters. When using parenthesis, the matched content of the parenthesis can be accessed with 
\1
 in the 
replace
 pattern. 
Examples of Replace Patterns
 - 
WORLD
 The word 'WORLD' will be placed whereever the find pattern was found. - 
FOO-&-BAR
 Each time the find pattern is found, it will be surrounded with 'FOO-' at the beginning and '-BAR' at the end. 
&
 (ampersand) represents the matched find pattern. - 
\1
 The text which matched the first parenthesis in the Find Pattern. ----- 
Example 1
 
Find Pattern:
 HELLO 
Replace Pattern:
 WORLD Every time the word HELLO is found, it will be replaced with the word WORLD. ----- 
Example 2
 
Find Pattern:
 ^(.{4}) 
Replace Pattern:
 &\t Find the first four characters in each line, and replace them with the same text, followed by a tab character. In practice - this will split the first line into two columns. ----- 
Extended Regular Expression Syntax
 The select tool searches the data for lines containing or not containing a match to the given pattern. A Regular Expression is a pattern descibing a certain amount of text. - 
( ) { } [ ] . * ? + \ ^ $
 are all special characters. 
\
 can be used to ""escape"" a special character, allowing that special character to be searched for. - 
^
 matches the beginning of a string(but not an internal line). - 
(
 .. 
)
 groups a particular pattern. - 
{
 n or n, or n,m 
}
 specifies an expected number of repetitions of the preceding pattern. - 
{n}
 The preceding item is matched exactly n times. - 
{n,}
 The preceding item ismatched n or more times. - 
{n,m}
 The preceding item is matched at least n times but not more than m times. - 
[
 ... 
]
 creates a character class. Within the brackets, single characters can be placed. A dash (-) may be used to indicate a range such as 
a-z
. - 
.
 Matches any single character except a newline. - 
*
 The preceding item will be matched zero or more times. - 
?
 The preceding item is optional and matched at most once. - 
+
 The preceding item will be matched one or more times. - 
^
 has two meaning: - matches the beginning of a line or string. - indicates negation in a character class. For example, [^...] matches every character except the ones inside brackets. - 
$
 matches the end of a line or string. - 
\|
 Separates alternate possibilities. 
Note
: SED uses extended regular expression syntax, not Perl syntax. 
\d
, 
\w
, 
\s
 etc. are 
not** supported. However, you can use SED FAQ to perform commands using special characters. More complex options can look like 
sed -e '$!N;s/foo/bar/;'
. Here, 
$!N;
 is an optional part which you only need to set in very special cases. The 
foo
 part is the search string, and the 
bar
 part is the replacement string. Please read the SED FAQ here: https://www.pement.org/sed/sedfaq3.html#s3.2"
toolshed.g2.bx.psu.edu/repos/earlhaminst/replace_chromosome_names/replace_chromosome_names/0.1	Replace chromosome names	in a tabular dataset using a mapping table	text_manipulation	Text Manipulation	"What it does
 Replace chromosome names in a tabular (e.g. VCF) dataset using a mapping table. Chromosome mapping tables can be downloaded from: https://github.com/dpryan79/ChromosomeMappings/"
toolshed.g2.bx.psu.edu/repos/bgruening/replace_column_by_key_value_file/replace_column_with_key_value_file/0.2	Replace column	by values which are defined in a convert file	text_manipulation	Text Manipulation	"What it does
 This tool replaces the entries of a defined column with entries given by a replacement file. For example the replacement file holds the information of the naming scheme of ensembl annotated chromosomes in the frist column and in the second the UCSC annotation. A file which is having information about chromosomes in ensembl notation in column x can now be converted to a file which holds the same information but in UCSC annotation. A useful repository for ensembl and UCSC chromosomes mapping is: https://github.com/dpryan79/ChromosomeMappings"
toolshed.g2.bx.psu.edu/repos/iuc/sqlite_to_tabular/sqlite_to_tabular/2.0.0	SQLite to tabular	for SQL query	text_manipulation	Text Manipulation	"================= SQLite to Tabular ================= 
Inputs
 An existing SQLite_ data base. 
Outputs
 The results of a SQL query are output to the history as a tabular file. For help in using SQLite_ see: http://www.sqlite.org/docs.html 
NOTE:
 input for SQLite dates input field must be in the format: 
YYYY-MM-DD
 for example: 2015-09-30 See: http://www.sqlite.org/lang_datefunc.html 
Example
 Given 2 tabular datasets: 
customers
 and 
sales
 Dataset 
customers
 Table name: ""customers"" Column names: ""CustomerID,FirstName,LastName,Email,DOB,Phone"" =========== ========== ========== ===================== ========== ============ #CustomerID FirstName LastName Email DOB Phone =========== ========== ========== ===================== ========== ============ 1 John Smith John.Smith@yahoo.com 1968-02-04 626 222-2222 2 Steven Goldfish goldfish@fishhere.net 1974-04-04 323 455-4545 3 Paula Brown pb@herowndomain.org 1978-05-24 416 323-3232 4 James Smith jim@supergig.co.uk 1980-10-20 416 323-8888 =========== ========== ========== ===================== ========== ============ Dataset 
sales
 Table name: ""sales"" Column names: ""CustomerID,Date,SaleAmount"" ============= ============ ============ #CustomerID Date SaleAmount ============= ============ ============ 2 2004-05-06 100.22 1 2004-05-07 99.95 3 2004-05-07 122.95 3 2004-05-13 100.00 4 2004-05-22 555.55 ============= ============ ============ The query :: SELECT FirstName,LastName,sum(SaleAmount) as ""TotalSales"" FROM customers join sales on customers.CustomerID = sales.CustomerID GROUP BY customers.CustomerID ORDER BY TotalSales DESC; Produces this tabular output: ========== ======== ========== #FirstName LastName TotalSales ========== ======== ========== James Smith 555.55 Paula Brown 222.95 Steven Goldfish 100.22 John Smith 99.95 ========== ======== ========== If the optional Table name and Column names inputs are not used, the query would be: :: SELECT t1.c2 as ""FirstName"", t1.c3 as ""LastName"", sum(t2.c3) as ""TotalSales"" FROM t1 join t2 on t1.c1 = t2.c1 GROUP BY t1.c1 ORDER BY TotalSales DESC; You can selectively name columns, e.g. on the customers input you could just name columns 2,3, and 5: Column names: ,FirstName,LastName,,BirthDate Results in the following data base table =========== ========== ========== ===================== ========== ============ #c1 FirstName LastName c4 BirthDate c6 =========== ========== ========== ===================== ========== ============ 1 John Smith John.Smith@yahoo.com 1968-02-04 626 222-2222 2 Steven Goldfish goldfish@fishhere.net 1974-04-04 323 455-4545 3 Paula Brown pb@herowndomain.org 1978-05-24 416 323-3232 4 James Smith jim@supergig.co.uk 1980-10-20 416 323-8888 =========== ========== ========== ===================== ========== ============ Regular_expression_ functions are included for: :: matching: re_match('pattern',column) SELECT t1.FirstName, t1.LastName FROM t1 WHERE re_match('^.*.(net|org)$',c4) Results: =========== ========== #FirstName LastName =========== ========== Steven Goldfish Paula Brown =========== ========== :: searching: re_search('pattern',column) substituting: re_sub('pattern','replacement,column) SELECT t1.FirstName, t1.LastName, re_sub('^\d{2}(\d{2})-(\d\d)-(\d\d)','\3/\2/\1',BirthDate) as ""DOB"" FROM t1 WHERE re_search('[hp]er',c4) Results: =========== ========== ========== #FirstName LastName DOB =========== ========== ========== Steven Goldfish 04/04/74 Paula Brown 24/05/78 James Smith 20/10/80 =========== ========== ========== .. _Regular_expression: https://docs.python.org/release/2.7/library/re.html .. _SQLite: http://www.sqlite.org/index.html .. _SQLite_functions: http://www.sqlite.org/docs.html"
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy3	Search in textfiles	(grep)	text_manipulation	Text Manipulation	"What it does
 This tool runs the unix 
grep
 command on the selected data file. .. class:: infomark 
TIP:
 This tool uses the 
perl
 regular expression syntax (same as running 'grep -P'). This is 
NOT
 the POSIX or POSIX-extended syntax (unlike the awk/sed tools). 
Further reading
 - Wikipedia's Regular Expression page (http://en.wikipedia.org/wiki/Regular_expression) - Regular Expressions cheat-sheet (PDF) (http://www.addedbytes.com/cheat-sheets/download/regular-expressions-cheat-sheet-v2.pdf) - Grep Tutorial (http://www.panix.com/~elflord/unix/grep.html) ----- 
Grep Examples
 - 
AGC.AAT
 would match lines with AGC followed by any character, followed by AAT (e.g. 
AGCQAAT
, 
AGCPAAT
, 
AGCwAAT
) - 
C{2,5}AGC
 would match lines with 2 to 5 consecutive Cs followed by AGC - 
TTT.{4,10}AAA
 would match lines with 3 Ts, followed by 4 to 10 characters (any characeters), followed by 3 As. - 
^chr([0-9A-Za-z])+
 would match lines that begin with chromsomes, such as lines in a BED format file. - 
(ACGT){1,5}
 would match at least 1 ""ACGT"" and at most 5 ""ACGT"" consecutively. - 
hsa|mmu
 would match lines containing ""hsa"" or ""mmu"" (or both). ----- 
Regular Expression Syntax
 The select tool searches the data for lines containing or not containing a match to the given pattern. A Regular Expression is a pattern descibing a certain amount of text. - 
( ) { } [ ] . * ? + \ ^ $
 are all special characters. 
\
 can be used to ""escape"" a special character, allowing that special character to be searched for. - 
^
 matches the beginning of a string(but not an internal line). - 
\d
 matches a digit, same as [0-9]. - 
\D
 matches a non-digit. - 
\s
 matches a whitespace character. - 
\S
 matches anything BUT a whitespace. - 
\t
 matches a tab. - 
\w
 matches an alphanumeric character ( A to Z, 0 to 9 and underscore ) - 
\W
 matches anything but an alphanumeric character. - 
(
 .. 
)
 groups a particular pattern. - 
\Z
 matches the end of a string(but not a internal line). - 
{
 n or n, or n,m 
}
 specifies an expected number of repetitions of the preceding pattern. - 
{n}
 The preceding item is matched exactly n times. - 
{n,}
 The preceding item ismatched n or more times. - 
{n,m}
 The preceding item is matched at least n times but not more than m times. - 
[
 ... 
]
 creates a character class. Within the brackets, single characters can be placed. A dash (-) may be used to indicate a range such as 
a-z
. - 
.
 Matches any single character except a newline. - 
*
 The preceding item will be matched zero or more times. - 
?
 The preceding item is optional and matched at most once. - 
+
 The preceding item will be matched one or more times. - 
^
 has two meaning: - matches the beginning of a line or string. - indicates negation in a character class. For example, [^...] matches every character except the ones inside brackets. - 
$
 matches the end of a line or string. - 
\|** Separates alternate possibilities."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_head_tool/9.5+galaxy3	Select first	lines from a dataset (head)	text_manipulation	Text Manipulation	"What it does
 This tool outputs specified number of lines from the 
beginning
 of a dataset ----- 
Example
 Selecting 2 lines from this:: chr7 56632 56652 D17003_CTCF_R6 310 + chr7 56736 56756 D17003_CTCF_R7 354 + chr7 56761 56781 D17003_CTCF_R4 220 + chr7 56772 56792 D17003_CTCF_R7 372 + chr7 56775 56795 D17003_CTCF_R4 207 + will produce:: chr7 56632 56652 D17003_CTCF_R6 310 + chr7 56736 56756 D17003_CTCF_R7 354 +"
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_tail_tool/9.5+galaxy3	Select last	lines from a dataset (tail)	text_manipulation	Text Manipulation	"What it does
 This tool outputs specified number of lines from the 
end
 of a dataset ----- 
Example
 - Input File:: chr7 57134 57154 D17003_CTCF_R7 356 - chr7 57247 57267 D17003_CTCF_R4 207 + chr7 57314 57334 D17003_CTCF_R5 269 + chr7 57341 57361 D17003_CTCF_R7 375 + chr7 57457 57477 D17003_CTCF_R3 188 + - Show last two lines of above file. The result is:: chr7 57341 57361 D17003_CTCF_R7 375 + chr7 57457 57477 D17003_CTCF_R3 188 +"
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sort_header_tool/9.5+galaxy3	Sort	data in ascending or descending order	text_manipulation	Text Manipulation	"What it does
 This tool sorts an input file. ----- 
Sorting Styles
 * 
Fast Numeric
: sort by numeric values. Handles integer values (e.g. 43, 134) and decimal-point values (e.g. 3.14). 
Does not
 handle scientific notation (e.g. -2.32e2). * 
General Numeric
: sort by numeric values. Handles all numeric notations (including scientific notation). Slower than 
fast numeric
, so use only when necessary. * 
Natural Sort
: Sort in 'natural' order (natural to humans, not to computers). See example below. * 
Alphabetical sort
: Sort in strict alphabetical order. See example below. * 
Human-readable numbers
: Sort human readble numbers (e.g. 1G > 2M > 3K > 400) * 
Random order
: return lines in random order. ------ 
Example - Header line
 
Input file
 (note first line is a header line, should not be sorted):: Fruit Color Price Banana Yellow 4.1 Avocado Green 8.0 Apple Red 3.0 Melon Green 6.1 
Sorting
 by 
numeric order
 on column 
3
, with 
header
, will return:: Fruit Color Price Apple Red 3.0 Banana Yellow 4.1 Melon Green 6.1 Avocado Green 8.0 ----- 
Example - Natural vs. Alphabetical sorting
 Given the following list:: chr4 chr13 chr1 chr10 chr20 chr2 
Alphabetical sort
 would produce the following sorted list:: chr1 chr10 chr13 chr2 chr20 chr4 
Natural Sort
 would produce the following sorted list:: chr1 chr2 chr4 chr10 chr13 chr20 .. class:: infomark If you're planning to use the file with another tool that expected sorted files (such as 
join
), you should use the 
Alphabetical sort
, not the 
Natural Sort
. Natural sort order is easier for humans, but is unnatural for computer programs. ----- 
Example - Sorting based on parts of column values
 The above column of chromosomes, with their constant prefix, could have been sorted in natural order also with the 
Fast numeric sort
 and 
considering its characters from
 character 4 only. In general, sorting based on just a range of characters in a column can be useful for sorting values with internal structure, in a single tool run. Consider, for example, the following column of dates, which is unfortunately not ISO-8601 formatted:: 10/24/2025 09/18/1974 12/16/1998 03/04/2007 You could modify these values with other tools first, but you can achieve correct chronological sort order with a single run of the sort tool like this: - Do a 
Fast numeric sort
 on the column 
considering its characters from
 character 7 (the start of the year). - Resolve ties (using another column selection section) with another 
Fast numeric sort
 on the same column 
considering its characters from
 character 1 
to and including
 character 2 (the month representation). - Resolve remaining ties with a third 
Fast numeric sort
 on again the same column 
considering its characters from
 character 4 
to and including
 character 5 (the day representation). This will result in the ascending chronological order:: 09/18/1974 11/17/1998 11/18/1998 12/16/1998 03/04/2007 10/24/2025 Before relying on in-column character ranges, make extra sure that all values are formatted consistently (in the above example, that all dates use two digits for days and months and the same overall date format)."
toolshed.g2.bx.psu.edu/repos/iuc/column_order_header_sort/column_order_header_sort/0.0.1	Sort Column Order	by heading	text_manipulation	Text Manipulation	Reorders a file's columns by sorted value of header fields. Specify the optional Identifier column parameter to make a column left-most; generally used for a Key column that should not be sorted within the other columns.
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sort_rows/9.5+galaxy3	Sort a row	according to their columns	text_manipulation	Text Manipulation	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 
What it does
 That tool sorts each row in a TAB separated file, according to their columns. In other words: It is a sorted reordering of all columns."
toolshed.g2.bx.psu.edu/repos/iuc/table_compute/table_compute/1.2.4+galaxy2	Table Compute	computes operations on table data	text_manipulation	Text Manipulation	"Table Compute ------------- This tool is a Galaxy wrapper for the 
Pandas Data Analysis Library <https://pandas.pydata.org/>
 in Python, for manipulating and computing expressions upon tabular data and matrices. It can perform functions on the element, row, and column basis, as well as sub-select, duplicate, replace, and perform general and custom expressions on rows, columns, and elements. .. class:: infomark Only a single operation can be performed on the data. Multiple operations can be performed by chaining successive runs of this tool. This is to provide a more transparent workflow for complex operations. Many of the examples given below relate to common research use-cases such as filtering large matrices for specific values, counting unique instances of elements, conditionally manipulating the data, and replacing unwanted values. Full table operations such as normalisation can be easily performed by scaling the data via mean/median/min/max (and many other) metrics, and general expressions can even be computed across multiple tables. Examples ======== Example 1: Sub-selecting from a table ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We have the following table: === === === === . c1 c2 c3 === === === === g1 10 20 30 g2 3 6 9 g3 4 8 12 g4 81 6 3 === === === === and we want to duplicate c1 and remove c2. Also select g1 to g3 and add g2 at the end as well. This would result in the output table: === === === === . c1 c1 c3 === === === === g1 10 10 30 g2 3 3 9 g3 4 4 12 g2 3 3 9 === === === === In Galaxy we would select the following: * 
Input Single or Multiple Tables
 ‚Üí 
Single Table
 * 
Column names on first row?
 ‚Üí 
Yes
 * 
Row names on first column?
 ‚Üí 
Yes
 * 
Type of table operation
 ‚Üí 
Drop, keep or duplicate rows and columns
 * 
List of columns to select
 ‚Üí 
1,1,3
 * 
List of rows to select
 ‚Üí 
1:3,2
 * 
Keep duplicate columns
 ‚Üí 
Yes
 * 
Keep duplicate rows
 ‚Üí 
Yes
 Example 2: Filter for rows with row sums less than 50 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We have the following table: === === === === . c1 c2 c3 === === === === g1 10 20 30 g2 3 6 9 g3 4 8 12 g4 81 6 3 === === === === and we want: === === === === . c1 c2 c3 === === === === g2 3 6 9 g3 4 8 12 === === === === In Galaxy we would select the following: * 
Input Single or Multiple Tables
 ‚Üí 
Single Table
 * 
Column names on first row?
 ‚Üí 
Yes
 * 
Row names on first column?
 ‚Üí 
Yes
 * 
Type of table operation
 ‚Üí 
Filter rows or columns by their properties
 * 
Filter
 ‚Üí 
Rows
 * 
Filter Criterion
 ‚Üí 
Result of function applied to columns/rows
 * 
Keep column/row if its observed
 ‚Üí 
Sum
 * 
is
 ‚Üí 
< (Less Than)
 * 
this value
 ‚Üí 
50
 Example 3: Count the number of values per row smaller than a specified value ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We have the following table: === === === === . c1 c2 c3 === === === === g1 10 20 30 g2 3 6 9 g3 4 8 12 g4 81 6 3 === === === === and we want to count how many elements in each row are smaller than 10, i.e., we want to obtain the following results table: === === . vec === === g1 0 g2 3 g3 2 g4 2 === === In Galaxy we would select the following: * 
Input Single or Multiple Tables
 ‚Üí 
Single Table
 * 
Column names on first row?
 ‚Üí 
Yes
 * 
Row names on first column?
 ‚Üí 
Yes
 * 
Type of table operation
 ‚Üí 
Manipulate selected table elements
 * 
Operation to perform
 ‚Üí 
Custom
 * 
Custom Expression on 'elem'
 ‚Üí 
elem < 10
 * 
Operate on elements
 ‚Üí 
All
 
Note:
 
There are actually simpler ways to achieve our purpose, but here we are demonstrating the use of a custom expression.
 After executing, we would then be presented with a table like so: === ===== ===== ===== . c1 c2 c3 === ===== ===== ===== g1 False False False g2 True True True g3 True True False g4 False True True === ===== ===== ===== To get to our desired table, we would then process this table with the tool again: * 
Input Single or Multiple Tables
 ‚Üí 
Single Table
 * 
Column names on first row?
 ‚Üí 
Yes
 * 
Row names on first column?
 ‚Üí 
Yes
 * 
Type of table operation
 ‚Üí 
Compute Expression across Rows or Columns
 * 
Calculate
 ‚Üí 
Sum
 * 
For each
 ‚Üí 
Row
 Executing this will sum all the 'True' values in each row. Note that the values must have no extra whitespace in them for this to work (e.g. 'True ' or ' True' will not be parsed correctly). Example 4: Perform a scaled log-transformation conditionally ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We want to perform a scaled log transformation on all values greater than 5, and set all other values to 1. We have the following table: === === === === . c1 c2 c3 === === === === g1 0 20 30 g2 3 0 9 g3 4 8 0 g4 81 0 0 === === === === and we want: === ========== ========= ========= . c1 c2 c3 === ========== ========= ========= g1 1.00000000 0.1497866 0.1133732 g2 1.00000000 1.0000000 0.2441361 g3 1.00000000 0.2599302 1.0000000 g4 0.05425246 1.0000000 1.0000000 === ========== ========= ========= In Galaxy we would select the following: * 
Input Single or Multiple Tables
 ‚Üí 
Single Table
 * 
Column names on first row?
 ‚Üí 
Yes
 * 
Row names on first column?
 ‚Üí 
Yes
 * 
Type of table operation
 ‚Üí 
Manipulate selected table elements
 * 
Operation to perform
 ‚Üí 
Custom
 * 
Custom Expression
 ‚Üí 
(math.log(elem) / elem) if (elem > 5) else 1
 * 
Operate on elements
 ‚Üí 
All
 Example 5: Perform a Full table operation ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We have the following table: === === === === . c1 c2 c3 === === === === g1 10 20 30 g2 3 10 9 g3 4 8 10 g4 81 10 10 === === === === and we want to subtract from each column the mean of that column divided by the standard deviation of it to yield: === ========= ========= ========= . c1 c2 c3 === ========= ========= ========= g1 9.351737 17.784353 28.550737 g2 2.351737 7.784353 7.550737 g3 3.351737 5.784353 8.550737 g4 80.351737 7.784353 8.550737 === ========= ========= ========= In Galaxy we would select the following: * 
Input Single or Multiple Tables
 ‚Üí 
Single Table
 * 
Column names on first row?
 ‚Üí 
Yes
 * 
Row names on first column?
 ‚Üí 
Yes
 * 
Type of table operation
 ‚Üí 
Perform a Full Table Operation
 * 
Operation
 ‚Üí 
Custom
 * 
Custom Expression on 'table' along axis (0 or 1)
 ‚Üí 
table - table.mean(0)/table.std(0)
 Example 6: Perform operations on multiple tables ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We have the following three input tables: Table 1 === === === === . c1 c2 c3 === === === === g1 10 20 30 g2 3 10 9 g3 4 8 10 === === === === Table 2 === === === . c1 c2 === === === g1 1 2 g2 3 4 g3 6 5 === === === Table 3 === === === === . c1 c2 c3 === === === === g1 1 2 3 g2 1 2 3 === === === === 
Note that the dimensions of these tables do not match.
 Dimensions: * Table1 [3,3] * Table2 [3,2] * Table3 [2,3] In order to perform simple operations between Tables, they must be of the same dimensions. To add Table2 to Table3 we would have to transpose one of the tables using the in-built 
T
 method:: table2 + table3.T or:: table2.T + table3 We can also perform more general operations using all 3 tables, such as taking the minimum value of the maximum values of Table2 and Table3, and dividing the Table1 values by it:: table1 / min(table2.values.max(), table3.values.max()) To perform these types of operations in Galaxy we would select the following: * 
Input Single or Multiple Tables
 ‚Üí 
Multiple Tables
 * 
(For each inserted table)
 * 
Column names on first row?
 ‚Üí 
Yes
 * 
Row names on first column?
 ‚Üí 
Yes
 * 
Custom Expression
 ‚Üí :: 
 Please note that the last example shown above was chosen to illustrate the limitations of the tool. Nested attributes like 
table2.values.max
 are disallowed in expressions in the tool so the above would have to be replaced with the harder to read workaround:: table1 / min(np.max(np.max(table2)), np.max(np.max(table3))) .. class:: infomark Complex operations (like ones that would benefit from specifying nested attributes) can often be broken into subsequent runs ot the tool, in which the first run generates an intermediate table representing the result of the ""inner"" operation that the second run can then use as input to perform the ""outer"" operation. Also note that, currently 
min()
, 
max()
 and 
sum()
 are the only built-in Python functions that can be used inside expressions. If you want to use additional functions, these have to be qualified functions from the 
math
, 
np
 or 
pd
 libraries. Example 7: Melt ~~~~~~~~~~~~~~~ We have the following table === === === === . A B C === === === === 0 a B 1 1 b B 3 2 c B 5 === === === === and we want: === === ======== ===== . A variable value === === ======== ===== 0 a B B 1 b B B 2 c B B 3 a C 1 4 b C 3 5 c C 5 === === ======== ===== In Galaxy we would select the following: * 
Input Single or Multiple Tables
 ‚Üí 
Single Table
 * 
Column names on first row?
 ‚Üí 
Yes
 * 
Row names on first column?
 ‚Üí 
Yes
 * 
Type of table operation
 ‚Üí 
Perform a Full Table Operation
 * 
Operation
 ‚Üí 
Melt
 * 
Variable IDs
 ‚Üí 
A
 * 
Unpivoted IDs
 ‚Üí 
B,C
 This converts the ""B"" and ""C"" columns into variables. Example 8: Pivot ~~~~~~~~~~~~~~~~ We have the following table === === === === === . foo bar baz zoo === === === === === 0 one A 1 x 1 one B 2 y 2 one C 3 z 3 two A 4 q 4 two B 5 w 5 two C 6 t === === === === === and we want: === === === === . A B C === === === === one 1 2 3 two 4 5 6 === === === === In Galaxy we would select the following: * 
Input Single or Multiple Tables
 ‚Üí 
Single Table
 * 
Column names on first row?
 ‚Üí 
Yes
 * 
Row names on first column?
 ‚Üí 
Yes
 * 
Type of table operation
 ‚Üí 
Perform a Full Table Operation
 * 
Operation
 ‚Üí 
Pivot
 * 
Index
 ‚Üí 
foo
 * 
Column
 ‚Üí 
bar
 * 
Values
 ‚Üí 
baz
 This splits the matrix using ""foo"" and ""bar"" using only the values from ""baz"". Header values may contain extra information. Example 9: Replacing text in specific rows or columns ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We have the following table === === === === . c1 c2 c3 === === === === g1 10 20 30 g2 3 3 9 g3 4 8 12 g4 81 6 3 === === === === and we want to add ""chr"" to the elements in column 2 AND rows 2 and 4: === === ==== === . c1 c2 c3 === === ==== === g1 10 20 30 g2 3 chr3 9 g3 4 8 12 g4 81 chr6 3 === === ==== === In Galaxy we would select the following: * 
Input Single or Multiple Tables
 ‚Üí 
Single Table
 * 
Column names on first row?
 ‚Üí 
Yes
 * 
Row names on first column?
 ‚Üí 
Yes
 * 
Type of table operation
 ‚Üí 
Manipulate selected table elements
 * 
Operation to perform
 ‚Üí 
Replace values
 * 
Replacement value
 ‚Üí 
chr{elem:.0f}
 Here, the placeholder 
{elem}
 lets us refer to each element's current value, while the 
:.0f
 part is a format specifier that makes sure numbers are printed without decimals (for a complete description of the available syntax see the 
Python Format Specification Mini-Language <https://docs.python.org/3/library/string.html#formatspec>
). * 
Operate on elements
 ‚Üí 
Specific Rows and/or Columns
 * 
List of columns to select
 ‚Üí 
2
 * 
List of rows to select
 ‚Üí 
2,4
 * 
Inclusive Selection
 ‚Üí 
No
 If we wanted to instead add ""chr"" to the ALL elements in column 2 and rows 2 and 4, we would repeat the steps above but set the 
Inclusive Selection
 to ""Yes"", to give: === ===== ===== ===== . c1 c2 c3 === ===== ===== ===== g1 10 chr20 30 g2 chr3 chr3 chr9 g3 4 8 12 g4 chr81 chr6 chr3 === ===== ===== ===== Example 10: Pivot Table with unified Aggregator ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ For an input table of: === === ===== === === A B C D E === === ===== === === foo one small 1 2 foo one large 2 4 foo one large 2 5 foo two small 3 5 foo two small 3 6 bar one large 4 6 bar one small 5 8 bar two small 6 9 bar two large 7 9 === === ===== === === we wish to pivot the table with the 'A' column as the new row index and use the values of the column 'C' as the new column indexes, based on the aggregated values of 'D'. By default the aggregator function is the mean, but here we will instead pick the max, to yield: === == == C l s A === == == bar 7 6 foo 2 3 === == == In Galaxy we would select the following: * 
Input Single or Multiple Tables
 ‚Üí 
Single Table
 * 
Column names on first row?
 ‚Üí 
Yes
 * 
Row names on first column?
 ‚Üí 
Yes
 * 
Type of table operation
 ‚Üí 
Perform a Full Table Operation
 * 
Operation
 ‚Üí 
Pivot
 * 
Index
 ‚Üí 
A
 * 
Column
 ‚Üí 
C
 * 
Values
 ‚Üí 
D
 * 
Aggregator Function
 ‚Üí 
max
 Example 11: Pivot Table with custom Aggregrator ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ For an input table of: ==== ========== =========== === ======== Name Position City Age Random ==== ========== =========== === ======== Mary Manager Boston 34 0.678577 Josh Programmer New York 37 0.973168 Jon Manager Chicago 29 0.146668 Lucy Manager Los Angeles 40 0.150120 Jane Programmer Chicago 29 0.112769 Sue Programmer Boston 31 0.185198 ==== ========== =========== === ======== we wish to pivot the table with the 'Position' column as the new index and transform the 'Age' and 'Random' columns to have mean and standard deviation values ========== ========= ======== ======== Position Age Random Random . mean mean std ========== ========= ======== ======== Manager 34.333333 0.325122 0.306106 Programmer 32.333333 0.423712 0.477219 ========== ========= ======== ======== In Galaxy we would select the following: * 
Input Single or Multiple Tables
 ‚Üí 
Single Table
 * 
Column names on first row?
 ‚Üí 
Yes
 * 
Row names on first column?
 ‚Üí 
Yes
 * 
Type of table operation
 ‚Üí 
Perform a Full Table Operation
 * 
Operation
 ‚Üí 
Pivot
 * 
Index
 ‚Üí 
Position
 * 
Column-Function Mapping
 * 
Value Column
 ‚Üí 
Age
 * 
Function
 ‚Üí 
mean
 * 
Value Column
 ‚Üí 
Random
 * 
Function
 ‚Üí 
mean
 * 
Function
 ‚Üí 
std
 This splits the matrix using ""foo"" and ""bar"" using only the values from ""baz"". Header values may contain extra information."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy3	Text reformatting	with awk	text_manipulation	Text Manipulation	"What it does
 This tool runs the unix 
awk
 command on the selected data file. .. class:: infomark 
TIP:
 This tool uses the 
extended regular
 expression syntax (not the perl syntax). 
\d
, 
\w
, 
\s
 etc. are 
not
 supported. 
Further reading
 - Awk by Example (http://www.ibm.com/developerworks/linux/library/l-awk1/index.html) - Long AWK tutorial (http://www.grymoire.com/Unix/Awk.html) ----- 
AWK programs
 Most AWK programs consist of 
patterns
 (i.e. rules that match lines of text) and 
actions
 (i.e. commands to execute when a pattern matches a line). The basic form of AWK program is:: pattern { action 1; action 2; action 3; } 
Variables
 In order to allow parametrization in workflows, the tool allows to specify values for variables that can be used in AWK program. that will be named 
VAR1
, 
VAR2
, ... 
Pattern Examples
 - 
$2 == ""chr3""
 will match lines whose second column is the string 'chr3' - 
$5-$4>23
 will match lines that after subtracting the value of the fourth column from the value of the fifth column, gives value alrger than 23. - 
/AG..AG/
 will match lines that contain the regular expression 
AG..AG
 (meaning the characeters AG followed by any two characeters followed by AG). (This is the way to specify regular expressions on the entire line, similar to GREP.) - 
$7 ~ /A{4}U/
 will match lines whose seventh column contains 4 consecutive A's followed by a U. (This is the way to specify regular expressions on a specific field.) - 
10000 < $4 && $4 < 20000
 will match lines whose fourth column value is larger than 10,000 but smaller than 20,000 - 
BEGIN
 will be executed once only, before the first input record is read. - If no pattern is specified, all lines match (meaning the 
action
 part will be executed on all lines). 
Action Examples
 - 
{ print }
 or 
{ print $0 }
 will print the entire input line (the line that matched in 
pattern
). 
$0
 is a special marker meaning 'the entire line'. - 
{ print $1, $4, $5 }
 will print only the first, fourth and fifth fields of the input line. - 
{ print $4, $5-$4 }
 will print the fourth column and the difference between the fifth and fourth column. (If the fourth column was start-position in the input file, and the fifth column was end-position - the output file will contain the start-position, and the length). - 
{ FS = "","" }
 can be used to change the field separator (delimeter) for parsing the input file. - If no action part is specified (not even the curly brackets) - the default action is to print the entire line. 
AWK's Regular Expression Syntax
 The select tool searches the data for lines containing or not containing a match to the given pattern. A Regular Expression is a pattern descibing a certain amount of text. - 
( ) { } [ ] . * ? + \ ^ $
 are all special characters. 
\
 can be used to ""escape"" a special character, allowing that special character to be searched for. - 
^
 matches the beginning of a string(but not an internal line). - 
(
 .. 
)
 groups a particular pattern. - 
{
 n or n, or n,m 
}
 specifies an expected number of repetitions of the preceding pattern. - 
{n}
 The preceding item is matched exactly n times. - 
{n,}
 The preceding item ismatched n or more times. - 
{n,m}
 The preceding item is matched at least n times but not more than m times. - 
[
 ... 
]
 creates a character class. Within the brackets, single characters can be placed. A dash (-) may be used to indicate a range such as 
a-z
. - 
.
 Matches any single character except a newline. - 
*
 The preceding item will be matched zero or more times. - 
?
 The preceding item is optional and matched at most once. - 
+
 The preceding item will be matched one or more times. - 
^
 has two meaning: - matches the beginning of a line or string. - indicates negation in a character class. For example, [^...] matches every character except the ones inside brackets. - 
$
 matches the end of a line or string. - 
\|** Separates alternate possibilities."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sed_tool/9.5+galaxy3	Text transformation	with sed	text_manipulation	Text Manipulation	"What it does
 This tool runs the unix 
sed
 command on the selected data file. .. class:: infomark 
TIP:
 This tool uses the 
extended regular
 expression syntax (same as running 'sed -r'). 
Further reading
 - Short sed tutorial (http://www.linuxhowtos.org/System/sed_tutorial.htm) - Long sed tutorial (http://www.grymoire.com/Unix/Sed.html) - sed faq with good examples (https://www.pement.org/sed/sedfaq.html) - sed cheat-sheet (http://www.catonmat.net/download/sed.stream.editor.cheat.sheet.pdf) ----- 
Sed commands
 The most useful sed command is 
s
 (substitute). 
Examples
 - 
s/hsa//
 will remove the first instance of 'hsa' in every line. - 
s/hsa//g
 will remove all instances (beacuse of the 
g
) of 'hsa' in every line. - 
s/A{4,}/--&--/g
 will find sequences of 4 or more consecutive A's, and once found, will surround them with two dashes from each side. The 
&
 marker is a place holder for 'whatever matched the regular expression'. - 
s/hsa-mir-([^ ]+)/short name: \1 full name: &/
 will find strings such as 'hsa-mir-43a' (the regular expression is 'hsa-mir-' followed by non-space characters) and will replace it will string such as 'short name: 43a full name: hsa-mir-43a'. The 
\1
 marker is a place holder for 'whatever matched the first parenthesis' (similar to perl's 
$1
) . 
sed's Regular Expression Syntax
 The select tool searches the data for lines containing or not containing a match to the given pattern. A Regular Expression is a pattern descibing a certain amount of text. - 
( ) { } [ ] . * ? + \ ^ $
 are all special characters. 
\
 can be used to ""escape"" a special character, allowing that special character to be searched for. - 
^
 matches the beginning of a string(but not an internal line). - 
(
 .. 
)
 groups a particular pattern. - 
{
 n or n, or n,m 
}
 specifies an expected number of repetitions of the preceding pattern. - 
{n}
 The preceding item is matched exactly n times. - 
{n,}
 The preceding item ismatched n or more times. - 
{n,m}
 The preceding item is matched at least n times but not more than m times. - 
[
 ... 
]
 creates a character class. Within the brackets, single characters can be placed. A dash (-) may be used to indicate a range such as 
a-z
. - 
.
 Matches any single character except a newline. - 
*
 The preceding item will be matched zero or more times. - 
?
 The preceding item is optional and matched at most once. - 
+
 The preceding item will be matched one or more times. - 
^
 has two meaning: - matches the beginning of a line or string. - indicates negation in a character class. For example, [^...] matches every character except the ones inside brackets. - 
$
 matches the end of a line or string. - 
\|
 Separates alternate possibilities. 
Note
: SED uses extended regular expression syntax, not Perl syntax. 
\d
, 
\w
, 
\s
 etc. are 
not** supported."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_unfold_column_tool/9.5+galaxy3	Unfold	columns from a table	text_manipulation	Text Manipulation	"What it does
 This tool will unfold one column of your input dataset. ----- Input Example:: a b 1,2,3,4,5 c Output Example:: a b 1 c a b 2 c a b 3 c a b 4 c a b 5 c"
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sorted_uniq/9.5+galaxy3	Unique	occurrences of each record	text_manipulation	Text Manipulation	".. class:: infomark 
Syntax
 This tool returns all unique lines using the 'sort -u' command. It can be used with unsorted files. If you need additional options, like grouping or counting your unique results, please use the 'Unique lines from sorted file' tool. ----- .. class:: infomark The input file needs to be tab separated. Please convert your file if necessary. ----- 
Example
 - Input file:: chr1 10 100 gene1 chr1 105 200 gene2 chr1 10 100 gene1 chr2 10 100 gene4 chr2 1000 1900 gene5 chr3 15 1656 gene6 chr2 10 100 gene4 - Unique lines will result in:: chr1 10 100 gene1 chr1 105 200 gene2 chr2 10 100 gene4 chr2 1000 1900 gene5 chr3 15 1656 gene6"
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_uniq_tool/9.5+galaxy3	Unique lines	assuming sorted input file	text_manipulation	Text Manipulation	"This tool takes a sorted file and look for lines that are unique. .. class:: warningmark Please make sure your file is sorted, or else this tool will give you an erroneous output. .. class:: infomark You can sort your file using either the ""Sort"" tool in ""Filter and Sort"", or the ""Sort"" tool in ""Unix Tools""."
toolshed.g2.bx.psu.edu/repos/iuc/reshape2_cast/cast/1.4.2	cast	expand combinations of variables:values to columnar format	text_manipulation	Text Manipulation	This tool will apply the dcast function of the reshape2 R package. The input data should be in a 'long' format or molten by the melt tool. The output will be in a wide format. The cast function expands each unique variable:value combination on a single line to columnar format. Documantation on the reshape2 package can be found here: https://cran.r-project.org/web/packages/reshape2/reshape2.pdf
toolshed.g2.bx.psu.edu/repos/bgruening/diff/diff/3.10+galaxy1	diff	analyzes two files and generates an unidiff text file with information about the differences and an optional Html report	text_manipulation	Text Manipulation	".. class:: infomark 
Purpose
 The 
diff
 utility is a data comparison tool that calculates and displays the differences between two files. Unlike edit distance notions used for other purposes, diff is line-oriented rather than character-oriented, but it is like Levenshtein distance in that it tries to determine the smallest set of deletions and insertions to create one file from the other. The diff command displays the changes made in a standard format, such that both humans and machines can understand the changes and apply them: given one file and the changes, the other file can be created. .. class:: infomark 
Input
 Two text files to be checked for differences line by line. .. class:: infomark 
Output
 A 
text file
, either: - containing the lines differences in 
unified format
 (
unidiff
), - or an 
empty
 file if the two input files are the same. An 
optional
 
HTML report
 with a friendlier visual representation of the differences."
toolshed.g2.bx.psu.edu/repos/iuc/reshape2_melt/melt/1.4.2	melt	collapse combinations of variables:values to single lines	text_manipulation	Text Manipulation	This tool will apply the melt function of the reshape2 R package. The melt function summarizes each unique variable:value combination on a single line. An example can be found here: http://www.statmethods.net/management/reshape.html
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_tac/9.5+galaxy3	tac	reverse a file (reverse cat)	text_manipulation	Text Manipulation	"What it does
 tac is a Linux command that allows you to see a file line-by-line backwards. It is named by analogy with cat. Mandatory arguments to long options are mandatory for short options too: -b, --before attach the separator before instead of after -r, --regex interpret the separator as a regular expression -s, --separator=STRING use STRING as the separator instead of newline ----- 
Example
 Input file: 0 1 2 3 4 5 # 6 7 8 9 default settings: 9 8 7 6 # 5 4 3 2 1 0 with option -s 5: # 6 7 8 9 0 1 2 3 4 5 with option -b and -s 5: 5 # 6 7 8 9 0 1 2 3 4"

tool_id	help_text
toolshed.g2.bx.psu.edu/repos/iuc/abricate/abricate/1.0.1	"What it does
 Given a FASTA contig file or a genbank file, ABRicate will perform a mass screening of contigs and identify the presence of antibiotic resistance genes. The user can choose which database to search from a list of available AMR databases. 
Output
 ABRicate will produce a tab-seperated output file with the following outputs: +---------------+------------------------------------------------+ | Column | Description | +===============+================================================+ | FILE | The filename this hit came from | +---------------+------------------------------------------------+ | SEQUENCE | The sequence in the filename | +---------------+------------------------------------------------+ | START | Start coordinate in the sequence | +---------------+------------------------------------------------+ | END | End coordinate | +---------------+------------------------------------------------+ | GENE | ABR gene | +---------------+------------------------------------------------+ | COVERAGE | What proportion of the gene is in our sequence | +---------------+------------------------------------------------+ | COVERAGE_MAP | A visual represenation | +---------------+------------------------------------------------+ | GAPS | Was there any gaps in the | | | alignment - possible pseudogene? | +---------------+------------------------------------------------+ | %COVERAGE | Proportion of gene covered | +---------------+------------------------------------------------+ | %IDENTITY | Proportion of exact nucleotide matches | +---------------+------------------------------------------------+ | DATABASE | The database this sequence comes from | +---------------+------------------------------------------------+ | ACCESSION | The genomic source of the sequence | +---------------+------------------------------------------------+ 
Example Output
 :: #FILE SEQUENCE START END GENE COVERAGE COVERAGE_MAP GAPS %COVERAGE %IDENTITY DATABASE ACCESSION 6159.fna NC_017338.1 39177 41186 mecA_15 1-2010/2010 =============== 0/0 100.00 100.000 resfinder AB505628 6159.fna NC_017338.1 727191 728356 norA_1 1-1166/1167 =============== 0/0 99.91 92.367 resfinder M97169 6159.fna NC_017339.1 10150 10995 blaZ_32 1-846/846 =============== 0/0 100.00 100.000 resfinder AP004832"
toolshed.g2.bx.psu.edu/repos/iuc/abricate/abricate_list/1.0.1	"What it does
 ABRicate List will list all the antibiotic databases used by ABRicate. The database of these genes is built from ResFinder."
toolshed.g2.bx.psu.edu/repos/iuc/abricate/abricate_summary/1.0.1	"What it does
 ABRicate can combine results into a simple matrix of gene presence/absence. An absent gene is denoted by '.' and a present gene is represented by its '%COVERAGE`. This can be individual abricate reports, or a combined one. 
Example Output
 :: #FILE NUM_FOUND AAC3 ANT(4')-Ib ANT3-DPRIME ANT4-PRIME APH3-PRIME ARLR ARLS BLAI BLAR BLAZ BLE DHAP ERMA ErmA FOSB LMRS MECA MECI MEPA MEPB MEPR MGRA NORA NORB PC1_beta-lactamase_(blaZ) RLMH Staphylococcus_aureus_FosB Staphylococcus_aureus_norA Staphylococcys_aureus_LmrS TET38 arlR arlS mecA mecI mecR1 mepA mepR mgrA tet(38) sample1 15 . 100.00 . . . . . . . . . . . 100.00;100.00 . . . . . . . . . . 100.00 . 100.00 99.91 100.00 . 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 sample2 24 100.00 . 100.00;100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00;100.00 . 100.00 100.00 100.00;100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 . 100.00 . . . 100.00 . . . . . . . . ."
toolshed.g2.bx.psu.edu/repos/bgruening/agat/agat/1.4.0+galaxy1	".. class:: infomark 
Purpose
 AGAT a GFF/GTF toolkit allowing you to perform almost everything you might want to achieve ^^ AGAT has the power to check, fix, pad missing information (features/attributes) of any kind of GTF and GFF to create complete, sorted and standardised gff3 format. Over the years it has been enriched by many many tools to perform just about any tasks that is possible related to GTF/GFF format files (sanitizing, conversions, merging, modifying, filtering, FASTA sequence extraction, adding information, etc). Comparing to other methods AGAT is robust to even the most despicable GTF/GFF files."
toolshed.g2.bx.psu.edu/repos/iuc/amrfinderplus/amrfinderplus/3.12.8+galaxy0	"What it does
 AMRFinderPlus is a tool to find acquired antimicrobial resistance genes and point mutations in protein and/or assembled nucleotide sequences. 
Input file analysis
 AMRFinderPlus can check in nucleotide and/or protein fasta file When protein file is provided, sensitivity is higher if you add an annotation file 
Organism options
 With organism informations AMRFinderPlus screens known resistance causing point mutations and blacklisting of common, non-informative genes. The 
mutation_all
 option make a new output file containing all detected variants from the reference sequence. This file allows you to distinguish between called point mutations that were the sensitive variant and the point mutations that could not be called because the sequence was not found. +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Organism option | Point mutations | Blacklisted plus genes | Notes | +=================================+==================+=============================+============================================================+ | Acinetobacter_baumannii | X | | Use for the A. baumannii-calcoaceticus species complex | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Burkholderia_cepacia | X | | Use for the Burkholderia cepacia species complex | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Burkholderia_pseudomallei | X | | Use for the Burkholderia pseudomallei species complex | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Campylobacter | X | | Use for C. coli and C. jejuni | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Citrobacter_freundii | X | | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Clostridioides_difficile | X | | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Enterobacter_cloacae | X | | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Enterococcus_faecalis | X | | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Enterococcus_faecium | X | | Use for E. hirae | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Escherichia | X | X | Use for Shigella and Escherichia | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Klebsiella_oxytoca | X | X | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Klebsiella_pneumoniae | X | X | Use for K. pneumoniae species complex and K. aerogenes | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Neisseria_gonorrhoeae | X | | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Neisseria_meningitidis | X | | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Pseudomonas_aeruginosa | X | | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Salmonella | X | X | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Serratia_marcescens | X | | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Staphylococcus_aureus | X | | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Staphylococcus_pseudintermedius | X | X | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Streptococcus_agalactiae | X | | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Streptococcus_pneumoniae | X | | Use for S. pneumoniae and S. mitis | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Streptococcus_pyogenes | X | | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ | Vibrio_cholerae | X | | | +---------------------------------+------------------+-----------------------------+------------------------------------------------------------+ 
Supplementary options
 - 
nucleotide 5' flanking size
 to extract nucleotide matches and a range of nucleotide in 5' - 
Minimum identity
 to choose threshold in amino acids alignemnt. -1 for a high curation threshold. - 
Minimum coverage
 Coverage to protein alignment. - 
Translation table
 by default AMRFinderPlus work on bacteria but can be modified to some other Phyla. - 
Plus
 option provide more information related to virulence, stress response ... 
Output files
 AMRFinderPlus can generate some differents files : - The AMRFinderPlus report with identified protein, gene and AMR related informations. - A report with all detected variants from the reference sequence. - The protein and/ort nucleotide matched sequences. - Only when nucleotide a nucleotide fasta file could be provided with 5' flanking regions."
toolshed.g2.bx.psu.edu/repos/bgruening/antismash/antismash/6.1.1+galaxy1	"What it does
 AntiSMASH allows the rapid genome-wide identification, annotation and analysis of secondary metabolite biosynthesis gene clusters in bacterial and fungal genomes. It integrates and cross-links with a large number of in silico secondary metabolite analysis tools that have been published earlier. antiSMASH is powered by several open source tools: NCBI BLAST+, HMMer 3, Muscle 3, Glimmer 3, FastTree, TreeGraph 2, Indigo-depict, PySVG and JQuery SVG. 
Input
 The ideal input for antiSMASH is an annotated nucleotide file in Genbank format or EMBL format. You can either upload a GenBank/EMBL file manually, or simply enter the GenBank/RefSeq accession number of your sequence for antiSMASH to upload it. If no annotation is available, we recommend running your sequence through an annotation pipeline like RAST to obtain GBK/EMBL files with high-quality annotations. Alternatively, you can provide a FASTA file containing a single sequence. antiSMASH will generate a preliminary annotation using Prodigal, and use that to run the rest of the analysis. You can also provide gene annotations in GFF3 foramt. Input files should be properly formatted. If you are creating your GBK/EMBL/FASTA file manually, be sure to do so in a plain text editor like Notepad or Emacs, and saving your files as ""All files (.)"", ending with the correct extension (for example "".fasta"", "".gbk"", or "".embl"". There are several optional analyses that may or may not be run on your sequence. Highly recommended is the Gene Cluster Blast Comparative Analysis, which runs BlastP using each amino acid sequence from a detected gene cluster as a query on a large database of predicted protein sequences from secondary metabolite biosynthetic gene clusters, and pools the results to identify the gene clusters that are most homologous to the gene cluster that was detected in your query nucleotide sequence. This analysis is selected by default Also available is the analysis of secondary metabolism gene families (smCOGs). This analysis attempts to allocate each gene in the detected gene clusters to a secondary metabolism-specific gene family using profile hidden Markov models specific for the conserved sequence region characteristic of this family. Additionally, a phylogenetic tree is constructed of each gene together with the (max. 100) sequences of the smCOG seed alignment. This analysis is selected by default 
Ouput
 The output of the antiSMASH analysis pipeline is organized in an interactive HTML page with SVG graphics, and different parts of the analysis are displayed in different panels for every gene cluster In the upper right, a small list of buttons offers further functionality. The house-shaped button will get you back on the antiSMASH start page. The question-mark button will get you to this help page. The exclamation-mark button leads to a page explaining about antiSMASH. The downward-pointing arrow will open a menu offering to download the complete set of results from the antiSMASH run, a summary Excel file and to the summary EMBL/GenBank output file. The EMBL/GenBank file can be viewed in a genome browser such as Artemis."
toolshed.g2.bx.psu.edu/repos/bgruening/augustus/augustus/3.5.0+galaxy0	"What it does
 AUGUSTUS is a gene prediction program for prokaryotes and eukaryotes written by Mario Stanke and Oliver Keller. It can be used as an ab initio program, which means it bases its prediction purely on the sequence. AUGUSTUS may also incorporate hints on the gene structure coming from extrinsic sources such as EST, MS/MS, protein alignments and synthenic genomic alignments. 
Input
 Input data for the gene prediction tool Augustus is a FASTA file with a genomic nucleotide sequence. 
Output
 Augustus produces three output files: a FASTA file with predicted coding sequences, a FASTA file with predicted protein sequences and a gtf/GFF output file if selected. 
Parameters
 Gene Model: partial: allow prediction of incomplete genes at the sequence boundaries (default) intronless: only predict single-exon genes like in prokaryotes and some eukaryotes complete: only predict complete genes atleastone: predict at least one complete gene exactlyone: predict exactly one complete gene 
Example
 Suppose you have the following DNA FASTA sequence: >Seq1 cccgcggagcgggtaccacatcgctgcgcgatgtgcgagcgaacacccgggctgcgcccg ggtgttgcgctcccgctccgcgggagcgctggcgggacgctgcgcgtcccgctcaccaag cccgcttcgcgggcttggtgacgctccgtccgctgcgcttccggagttgcggggcttcgc cccgctaaccctgggcctcgcttcgctccgccttgggcctgcggcgggtccgctgcgctc ccccgcctcaagggcccttccggctgcgcctccaggacccaaccgcttgcgcgggcctgg Running this tool will produce this: # ----- prediction on sequence number 1 (length = 1992969, name = scaffold1|size1992969) ----- # # Constraints/Hints: # (none) # Predicted genes for sequence number 1 on both strands # start gene g1 scaffold1|size1992969 AUGUSTUS gene 17453 19382 0.11 + . g6 scaffold1|size1992969 AUGUSTUS transcript 17453 19382 0.11 + . g6.t1 scaffold1|size1992969 AUGUSTUS start_codon 17453 17455 . + 0 transcript_id ""g6.t1""; gene_id ""g6""; scaffold1|size1992969 AUGUSTUS intron 17615 17660 0.38 + . transcript_id ""g6.t1""; gene_id ""g6""; scaffold1|size1992969 AUGUSTUS intron 17708 17772 0.54 + . transcript_id ""g6.t1""; gene_id ""g6""; scaffold1|size1992969 AUGUSTUS intron 17902 18035 0.58 + . transcript_id ""g6.t1""; gene_id ""g6""; scaffold1|size1992969 AUGUSTUS intron 18313 18367 0.99 + . transcript_id ""g6.t1""; gene_id ""g6""; scaffold1|size1992969 AUGUSTUS intron 19014 19080 0.44 + . transcript_id ""g6.t1""; gene_id ""g6""; scaffold1|size1992969 AUGUSTUS CDS 17453 17614 0.55 + 0 transcript_id ""g6.t1""; gene_id ""g6""; scaffold1|size1992969 AUGUSTUS CDS 17661 17707 0.38 + 0 transcript_id ""g6.t1""; gene_id ""g6""; scaffold1|size1992969 AUGUSTUS CDS 17773 17901 0.54 + 1 transcript_id ""g6.t1""; gene_id ""g6""; scaffold1|size1992969 AUGUSTUS CDS 18036 18312 0.52 + 1 transcript_id ""g6.t1""; gene_id ""g6""; scaffold1|size1992969 AUGUSTUS CDS 18368 19013 0.99 + 0 transcript_id ""g6.t1""; gene_id ""g6""; scaffold1|size1992969 AUGUSTUS CDS 19081 19379 0.31 + 2 transcript_id ""g6.t1""; gene_id ""g6""; scaffold1|size1992969 AUGUSTUS stop_codon 19380 19382 . + 0 transcript_id ""g6.t1""; gene_id ""g6"";"
toolshed.g2.bx.psu.edu/repos/genouest/braker3/braker3/3.0.8+galaxy2	Braker3_ allows for fully automated training of the gene prediction tools GeneMark-EX and AUGUSTUS from RNA-Seq and/or protein homology information, and that integrates the extrinsic evidence from RNA-Seq and protein homology information into the prediction. In contrast to other available methods that rely on protein homology information, BRAKER3 reaches high gene prediction accuracy even in the absence of the annotation of very closely related species and in the absence of RNA-Seq data. .. _Braker3: https://github.com/Gaius-Augustus/BRAKER
toolshed.g2.bx.psu.edu/repos/iuc/bakta/bakta/1.9.4+galaxy1	"What it does
 Bakta is a tool for the rapid & standardized annotation of bacterial genomes and plasmids from both isolates and MAGs. 
Comprehensive & taxonomy-independent database
 Bakta provides a large and taxonomy-independent database using UniProt's entire UniRef protein sequence cluster universe. 
Protein sequence identification
 Bakta exactly identifies known identical protein sequences (IPS) from RefSeq and UniProt allowing the fine-grained annotation of gene alleles (AMR) or closely related but distinct protein families. This is achieved via an alignment-free sequence identification (AFSI) approach using full-length MD5 protein sequence hash digests. 
Small proteins/short open reading frames
 Bakta detects and annotates small proteins/short open reading frames (sORF). 
Expert annotation systems
 To provide high quality annotations for certain proteins of higher interest, e.g. AMR & VF genes, Bakta includes & merges different expert annotation systems. Currently, Bakta uses NCBI's AMRFinderPlus for AMR gene annotations as well as an generalized protein sequence expert system with distinct coverage, identity and priority values for each sequence, currenlty comprising the VFDB as well as NCBI's BlastRules. 
Comprehensive workflow
 Bakta annotates ncRNA cis-regulatory regions, oriC/oriV/oriT and assembly gaps as well as standard feature types: tRNA, tmRNA, rRNA, ncRNA genes, CRISPR, CDS. 
GFF3 & INSDC conform annotations
 Bakta writes GFF3 and INSDC-compliant (Genbank & EMBL) annotation files ready for submission (checked via GenomeTools GFF3Validator, table2asn_GFF and ENA Webin-CLI for GFF3 and EMBL file formats, respectively for representative genomes of all ESKAPE species). 
Bacteria & plasmids
 Bakta was designed to annotate bacteria (isolates & MAGs) and plasmids, only. 
Input options
 1. Choose a genome or assembly in fasta format to use bakta annotations 2. Choose A version of the Bakta database 
Organism options
 You can specify informations about analysed fasta as text input for: - genus - species - strain - plasmid 
Annotation options
 1. You can specify if all sequences (chromosome or plasmids) are complete or not 2. You can add your own prodigal training file for CDS predictionœ 3. The translation table could be modified, default is the 11th for bacteria 4. You can specify if bacteria is gram -/+ or unknonw (default value is unknow) 5. You can keep the name of contig present in the input file 6. You can specify your own replicon table as a TSV/CSV file 7. The compliance option is for ready to submit annotation file to Public database as ENA, Genbank EMBL 8. You can specify a protein sequence file for annotation in GenBank or fasta formats Using the Fasta format, each reference sequence can be provided in a short or long format: # short: >id gene~~~product~~~dbxrefs MAQ... # long: >id min_identity~~~min_query_cov~~~min_subject_cov~~~gene~~~product~~~dbxrefs MAQ... 
Skip steps
 Some steps could be skiped: - skip-trna Skip tRNA detection & annotation - skip-tmrna Skip tmRNA detection & annotation - skip-rrna Skip rRNA detection & annotation - skip-ncrna Skip ncRNA detection & annotation - skip-ncrna-region Skip ncRNA region detection & annotation - skip-crispr Skip CRISPR array detection & annotation - skip-cds Skip CDS detection & annotation - skip-pseudo Skip pseudogene detection & annotation - skip-sorf Skip sORF detection & annotation - skip-gap Skip gap detection & annotation - skip-ori Skip oriC/oriT detection & annotation 
Output options
 Bakta produce numbers of output files, you can select what type of file you want: - Summary of the annotation - Annotated files - Sequence files for nucleotide and/or amino acid"
toolshed.g2.bx.psu.edu/repos/iuc/blastxml_to_gapped_gff3/blastxml_to_gapped_gff3/1.1	"What it does
 Convert BlastXML results into GFF3 format. 
Options
 The trimming option captures an important feature provided in this tool that isn't provided in most other BlastXML visualization tools: the fact that blast captures complete alignment location information. This means that when most blast visualization tools produce output which looks like this: .. image:: $PATH_TO_IMAGES/blast2html.png This tool produces output which shows where the real subject sequence starts and ends relative to your sequence: .. image:: $PATH_TO_IMAGES/blast-extended.png This can be a useful feature for examining alternate start locations that are used by sequences found from your blast query. The green bars on the very top row of the picture indicate start sites, as you can see the blast hits and the genome in the visualization share an upstream start site. You don't lose the information present in your blastxml data. So, to the end of useful functionality, this options is controllable: - you can trim neither end, see where the real protein alignments are. - you can trim both ends, like blasts HTML reports - you can trim just the end of the sequence, as upstream is generally more interesting, and having long tails can result in poor visualizations."
toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.8.0+galaxy2	"BUSCO: Assessing genome assembly and annotation completeness with Benchmarking Universal Single-Copy Orthologs -------------------------------------------------------------------------------------------------------------- Interpreting the results ^^^^^^^^^^^^^^^^^^^^^^^^ BUSCO_ attempts to provide a quantitative assessment of the completeness in terms of the expected gene content of a genome assembly, transcriptome, or annotated gene set. The results are simplified into categories of Complete and single-copy, Complete and duplicated, Fragmented, or Missing BUSCOs. BUSCO completeness results make sense only in the context of the biology of your organism. You have to understand whether missing or duplicated genes are of biological or technical origin. For instance, a high level of duplication may be explained by a recent whole duplication event (biological) or a chimeric assembly of haplotypes (technical). Transcriptomes and protein sets that are not filtered for isoforms will lead to a high proportion of duplicates. Therefore you should filter them before a BUSCO analysis. Finally, focusing on specific tissues or specific life stages and conditions in a transcriptomic experiment is unlikely to produce a BUSCO-complete transcriptome. In this case, consistency across your samples is what you will be aiming for. For more information please refer to the Busco_ 
user guide &lt;https://busco.ezlab.org /busco_userguide.html#interpreting-the-results&gt;
_ . .. _BUSCO: http://busco.ezlab.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/cpat/cpat/3.0.5+galaxy1	".. class:: infomark 
Purpose
 CPAT is a bioinformatics tool to predict RNAs coding probability based on the RNA sequence characteristics. To achieve this goal, CPAT calculates scores of these 4 linguistic features from a set of known protein-coding genes and another set of non-coding genes. - ORF size - ORF coverage - Fickett TESTCODE - Hexamer usage bias CPAT will then builds a logistic regression model using these 4 features as predictor variables and the “protein-coding status” as the response variable. After evaluating the performance and determining the probability cutoff, the model can be used to predict new RNA sequences."
toolshed.g2.bx.psu.edu/repos/iuc/cooc_mutbamscan/cooc_mutbamscan/0.9.2+galaxy0	"What it does ============ The cojac package comprises a set of command-line tools to analyse co-occurrence of mutations on amplicons. It is useful, for example, for early detection of viral variants of concern (e.g. Alpha, Delta, Omicron) in environmental samples, and has been designed to scan for multiple SARS-CoV-2 variants in wastewater samples. Information about 
cojac cooc-mutbamscan
 =========================================== The tool scans an alignment BAM/CRAM/SAM file for mutation co-occurrences. It can report its findings in json, yaml and/or tabular format."
toolshed.g2.bx.psu.edu/repos/iuc/cooc_tabmut/cooc_tabmut/0.9.2+galaxy0	"What it does ============ The cojac package comprises a set of command-line tools to analyse co-occurrence of mutations on amplicons. It is useful, for example, for early detection of viral variants of concern (e.g. Alpha, Delta, Omicron) in environmental samples, and has been designed to scan for multiple SARS-CoV-2 variants in wastewater samples. Information about 
cojac cooc_tabmut
 method ============================================== The method exports a JSON or YAML file as a CSV/TSV table for downstream analysis (e.g.: RStudio)."
toolshed.g2.bx.psu.edu/repos/iuc/progressivemauve/xmfa2gff3/2015_02_13.1	"What it does ============ XMFA Alignments are great, but now you need a way to visualize this data! This tool provides a conversion step to GFF3 formatted output consiting of 
match
es and 
match_part
s with scores indicating percent identity over that chunk of sequence. A rendering of the output GFF3 file in JBrowse is shown below: .. image:: $PATH_TO_IMAGES/xmfa2gff3.png"
toolshed.g2.bx.psu.edu/repos/iuc/coreprofiler_allele_calling/coreprofiler_allele_calling/2.0.0+galaxy2	"CoreProfiler allele_calling =========================== 
CoreProfiler allele_calling
 performs cgMLST profiling by identifying alleles present in a bacterial genome assembly based on a reference allele scheme. It works in two steps: 1. 
Autotag
 (exact matches on reference database): This step runs BLASTn with 100% identity, no gaps, and parameters optimized for speed. The output is parsed, and only results with 100% coverage are kept. 2. 
Scannew
 (detection of new alleles): In this step, loci with no exact match from the autotag step are selected. A BLAST database is created for those loci files, and a looser BLASTn search is performed. The best hit is kept for each locus and classified as follows: * If identity > 90% and coverage > 90%, the locus is considered a new allele. * If identity > 90% and coverage between 70% and 90%, the allele is present but incomplete (""X""). * If identity < 90% or coverage < 70%, the allele is considered missing (""-""). Usage ----- 1. Select your genome contigs (in FASTA format). 2. Select a pre-installed reference allele scheme. This requires you to specify the specific organism you are using. 3. Change the parameters (or not) to perform allele matching and detection. * 
autotag
: Finds exact allele matches using BLASTn with 100% identity and coverage. * 
scannew
: Detects potential new or incomplete alleles through relaxed BLASTn searches. 4. Run the tool. Input ----- Genomes Assembly 
``````````````` The input genome assembly must be in FASTA format. This is the sequence that will be scanned for matching alleles. Reference Database `````````````````` The reference database is a a pre-installed scheme of known alleles and loci used to identify alleles in the provided genome. Ensure you specify the appropriate reference database for the organism or group under study. If the desired scheme is not listed, please contact your Galaxy administrator. Output ------ The
allele_calling
command produces a tabular profile listing the identified alleles for each locus, including new or missing alleles when applicable. Supplementary output files `````````````````````````` The **scannew** step in CoreProfiler provides three additional output files to offer more detailed insights into the analysis: *
--outfa
: FASTA file containing the sequences of new alleles detected during the analysis. *
--profiles_w_tmp_alleles
: JSON file providing detailed information about files containing temporary alleles. *
--num_alleles_per_locus`: TSV file listing the number of alleles detected for each locus in a given scheme."
toolshed.g2.bx.psu.edu/repos/iuc/deeparg_short_reads/deeparg_short_reads/1.0.4+galaxy1	"The DeepARG Short Reads Pipeline is a specialized tool for detecting antibiotic resistance genes (ARGs) directly from 
paired-end short read sequencing data
 It provides detailed output files summarizing ARG detection, merged results, subtype and type classifications, potential ARGs, and all detected hits If you want to annotate ARGs from assembled contigs, use the DeepARG Predict tool"
toolshed.g2.bx.psu.edu/repos/iuc/exonerate/exonerate/2.4.0+galaxy2	Exonerate is a generic tool for pairwise sequence comparison. It allows you to align sequences using a many alignment models, using either exhaustive dynamic programming, or a variety of heuristics. .. _Exonerate website: https://www.ebi.ac.uk/about/vertebrate-genomics/software/exonerate
toolshed.g2.bx.psu.edu/repos/rnateam/sortmerna/bg_sortmerna/4.3.6+galaxy0	"What it does
 SortMeRNA_ is a software designed to rapidly filter ribosomal RNA fragments from metatransriptomic data produced by next-generation sequencers. It is capable of handling large RNA databases and sorting out all fragments matching to the database with high accuracy and specificity. .. 
SortMeRNA: http://bioinfo.lifl.fr/RNA/sortmerna/ 
Input
 The input is one file of reads in FASTA or FASTQ format and any number of rRNA databases to search against. If the user has two foward-reverse paired-sequencing reads files, they may use the script ""merge_paired_reads.sh"" to interleave the reads into one file, preserving their order. If the sequencing type for the reads is paired-ended, the user has two options under ""Sequencing type"" to filter the reads and preserve their order in the file. For a further example of each option, please refer to Section 4.2.3 in the 
SortMeRNA User Manual
. .. _sortmerna user manual: http://bioinfo.lifl.fr/RNA/sortmerna/code/SortMeRNA-user-manual-v1.7.pdf 
Output
 The output will follow the same format (FASTA or FASTQ) as the reads. Optionally, a statistic file for the rRNA content of reads, as well as rRNA subunit distribution can be generated. 
rRNA databases
 SortMeRNA is distributed with 8 representative rRNA databases, which were all constructed from the SILVA SSU,LSU (version 111) and the RFAM 5/5.8S (version 11.0) databases using the tool UCLUST. +--------------------------+------+-------------+-------------------+------------------------+-------------------+ | Representative database | id % | average id% | # seq (clustered) | Origin | # seq (original) | +==========================+======+=============+===================+========================+===================+ | SILVA 16S bacteria | 85 | 91.6 | 8174 | SILVA SSU Ref NR v.111 | 244077 | +--------------------------+------+-------------+-------------------+------------------------+-------------------+ | SILVA 16S archaea | 95 | 96.7 | 3845 | SILVA SSU Ref NR v.111 | 10919 | +--------------------------+------+-------------+-------------------+------------------------+-------------------+ | SILVA 18S eukarya | 95 | 96.7 | 4512 | SILVA SSU Ref NR v.111 | 31862 | +--------------------------+------+-------------+-------------------+------------------------+-------------------+ | SILVA 23S bacteria | 98 | 99.4 | 3055 | SILVA LSU Ref v.111 | 19580 | +--------------------------+------+-------------+-------------------+------------------------+-------------------+ | SILVA 23s archaea | 98 | 99.5 | 164 | SILVA LSU Ref v.111 | 405 | +--------------------------+------+-------------+-------------------+------------------------+-------------------+ | SILVA 28S eukarya | 98 | 99.1 | 4578 | SILVA LSU Ref v.111 | 9321 | +--------------------------+------+-------------+-------------------+------------------------+-------------------+ | Rfam 5S archaea/bacteria | 98 | 99.2 | 59513 | RFAM | 116760 | +--------------------------+------+-------------+-------------------+------------------------+-------------------+ | Rfam 5.8S eukarya | 98 | 98.9 | 13034 | RFAM | 225185 | +--------------------------+------+-------------+-------------------+------------------------+-------------------+ id %: members of the cluster must have identity at least 'id %' identity with the representative sequence average id %: average identity of a cluster member to the representative sequence The user may also choose to use their own rRNA databases. .. class:: warningmark Note that your personal databases are indexed each time. The public ribosomal databases are indexed when added, but they can be re-indexed with non-default indexing parameters. The indexing may take some time depending on the size of the given database."
toolshed.g2.bx.psu.edu/repos/iuc/find_nested_alt_orfs/find_nested_alt_orfs/0.1.2	Find Nested Alternate Open Reading Frames (nAlt-ORFs). Using a BED12 file containing the location of genes, and a matching reference genome, this tool searches in the alternate reading frames of the provided canonical coding sequence, and outputs nested alternate ORFs which match the provided thresholds. Translation table identifiers are based upon NCBI standards (https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi).
toolshed.g2.bx.psu.edu/repos/iuc/freyja_aggregate_plot/freyja_aggregate_plot/2.0.1+galaxy0	"What it does ============ Freyja is a tool to recover relative lineage abundances from mixed SARS-CoV-2 samples from a sequencing dataset (BAM aligned to the Hu-1 reference). General information =================== Freyja is a tool to recover relative lineage abundances from mixed SARS-CoV-2 samples from a sequencing dataset (BAM aligned to the Hu-1 reference). The method uses lineage-determining mutational ""barcodes"" derived from the UShER global phylogenetic tree as a basis set to solve the constrained (unit sum, non-negative) de-mixing problem. Freyja is intended as a post-processing step after primer trimming and variant calling in iVar (Grubaugh and Gangavaparu et al., 2019). From measurements of SNV freqency and sequencing depth at each position in the genome, Freyja returns an estimate of the true lineage abundances in the sample. Information about 
freyja aggregate
 method ============================================= Method for manipulating the ""demixed"" output files. Outputs ------- This resulting aggregated data can analyzed directly as a tsv file, or can be visualized using 
freyja plot
 and 
freyja dash
. Information about 
freyja plot
 method ======================================== Method provides a fractional abundance estimate for all aggregated samples. A 
time(s) metadata CSV file
 should have 
Sample,sample_collection_datetime
 form: 
sample_0.tsv,03/01/21
 
sample_1.tsv,03/03/21
 
sample_2.tsv,03/08/21
 
sample_3.tsv,03/10/21
 or 
Sample,sample_collection_datetime,viral_load
 form: 
sample_0.tsv,03/01/21,460326
 
sample_1.tsv,03/03/21,176645.1
 
sample_2.tsv,03/08/21,449891.7
 
sample_3.tsv,03/10/21,361699.5
 Note: sample_collection_datetime can have either 
MM/DD/YY
 or 
YYYY-MM-DD
 format. Information about 
freyja dash
 method ======================================== Functionality to rapidly prepare a dashboard web page, directly from aggregated freyja output. A 
sample(s) metadata CSV file
 should have this form: 
Sample,sample_collection_datetime,viral_load
 
sample_0.tsv,03/01/21,460326
 
sample_1.tsv,03/03/21,176645.1
 
sample_2.tsv,03/08/21,449891.7
 
sample_3.tsv,03/10/21,361699.5
 Note: sample_collection_datetime can have either 
MM/DD/YY
 or 
YYYY-MM-DD
 format."
toolshed.g2.bx.psu.edu/repos/iuc/freyja_demix/freyja_demix/2.0.1+galaxy0	"What it does ============ Freyja is a tool to recover relative lineage abundances from mixed SARS-CoV-2 samples from a sequencing dataset (BAM aligned to the Hu-1 reference). General information =================== Freyja is a tool to recover relative lineage abundances from mixed SARS-CoV-2 samples from a sequencing dataset (BAM aligned to the Hu-1 reference). The method uses lineage-determining mutational ""barcodes"" derived from the UShER global phylogenetic tree as a basis set to solve the constrained (unit sum, non-negative) de-mixing problem. Freyja is intended as a post-processing step after primer trimming and variant calling in iVar (Grubaugh and Gangavaparu et al., 2019). From measurements of SNV freqency and sequencing depth at each position in the genome, Freyja returns an estimate of the true lineage abundances in the sample. 
Freyja demix
 estimates lineage abundances in a potentially multi-lineage input sample. Inputs ====== The tool requires as input a dataset with called variants and a dataset with genome-wide sequencing depth information. Both types of data can be produced with 
Freyja call
, but the tool accepts variant calls also in VCF format. Note ---- For single samples it is recommended to select ""Specify sample name explicitly"" under ""Set sample name"". To use this tool on multiple samples in parallel, please provide two collections in the same sample sort order - one with the variant calls, the other one with the sequencing depths - and select ""Autodetect sample name"", which will use collection element identifiers as the names of the samples. This will produce a new collection of demixing reports that can be passed to 
Freyja: Aggregate and visualize
 with sample names preserved. Selection of multiple regular called variants and depth datasets is discouraged since proper dataset pairing cannot be guaranteed! Outputs ======= The tool produces tabular output that includes the lineages detected in the sample, their corresponding abundances, and a lineage summary by constellation. Example output: ========== =================================================== filename summarized [('Delta', 0.65), ('Other', 0.25), ('Alpha', 0.1')] lineages ['B.1.617.2' 'B.1.2' 'AY.6' 'Q.3'] abundances ""[0.5 0.25 0.15 0.1]"" resid 3.14159 coverage 95.8 ========== =================================================== Where 
summarized
 denotes a sum of all lineage abundances in a particular WHO designation (i.e. B.1.617.2 and AY.6 abundances are summed in the above example), otherwise they are grouped into ""Other"". The 
lineage
 array lists the identified lineages in descending order, and 
abundances
 contains the corresponding abundances estimates. The value of 
resid
 corresponds to the residual of the weighted least absolute devation problem used to estimate lineage abundances. The 
coverage
 value provides the 10x coverage estimate (percent of sites with 10 or greater reads- 10 is the default but can be modfied using the 
--covcut
 option)."
toolshed.g2.bx.psu.edu/repos/iuc/funannotate_compare/funannotate_compare/1.8.15+galaxy5	Funannotate_ compare -------------------- Funannotate_ is a pipeline for genome annotation (built specifically for fungi, but will also work with higher eukaryotes). This script does light-weight comparative genomics between funannotated genomes. Output is graphs, phylogeny, CSV files, etc --> visualized in web-browser. .. _Funannotate: http://funannotate.readthedocs.io
toolshed.g2.bx.psu.edu/repos/iuc/funannotate_annotate/funannotate_annotate/1.8.17+galaxy5	Funannotate_ annotate --------------------- Funannotate_ is a pipeline for genome annotation (built specifically for fungi, but will also work with higher eukaryotes). This script functionally annotates the results from funannotate predict. It pulls annotation from PFAM, InterPro, EggNog, UniProtKB, MEROPS, CAZyme, and GO ontology. .. _Funannotate: http://funannotate.readthedocs.io
toolshed.g2.bx.psu.edu/repos/iuc/funannotate_predict/funannotate_predict/1.8.17+galaxy0	Funannotate_ predict -------------------- Funannotate_ is a pipeline for genome annotation (built specifically for fungi, but will also work with higher eukaryotes). Script takes genome multi-fasta file and a variety of inputs to do a comprehensive whole genome gene prediction. Uses AUGUSTUS, GeneMark, Snap, GlimmerHMM, BUSCO, EVidence Modeler, tbl2asn, tRNAScan-SE, Exonerate, minimap2. .. _Funannotate: http://funannotate.readthedocs.io
toolshed.g2.bx.psu.edu/repos/iuc/goenrichment/goenrichment/2.0.1	.. class:: infomark GOEnrichment is a Java application that can be used to analyze gene product sets (e.g., from microarray or RNAseq experiments) for enriched GO terms. ----- .. class:: infomark GOEnrichment requires: - A Gene Ontology file in either OBO or OWL format (see http://geneontology.org/docs/download-ontology). - A tabular annotation file in GAF (http://geneontology.org/docs/download-go-annotations) format, BLAST2GO format, or a simple two-column table (e.g. from BioMart) with gene product ids in the first column and GO terms in the second one. - A list of gene products comprising the study set (a flat text file with one gene product per line). - Optionally, a list of gene products comprising the population set (if none is submitted, the population set will be the set of gene products listed in the annotation file). ----- .. class:: infomark GOEnrichment produces a tabular result file and a graph file for each GO type (MF - Molecular Function, BP - Biological Process and CC - Cellular Component): - The result file is a tabular list of all GO terms present in the study set and their respective p-values. - The graph file can be either a png image, an svg image, or a text file for importing into cytoscape (together with the result file). ----- .. class:: infomark The graph is colored by p-value: terms with p-value above cut-off appear in white; and the color gets darker as the p-value decreases .. image:: https://github.com/DanFaria/GOEnrichment/raw/master/Scale.png :width: 600 :height: 315 (see the scale at https://github.com/DanFaria/GOEnrichment/blob/master/Scale.png). In addition to the name of each GO term, the graph shows its frequency in the study set. Dashed edges indicate that one or more intermediate terms were ommited from the graph. ----- .. class:: warningmark Gene products listed in either the study or population set files that are not present in the annotation file will be ignored.
toolshed.g2.bx.psu.edu/repos/iuc/goslimmer/goslimmer/1.0.1	.. class:: infomark GOSlimmer is a Java application that converts a set of annotations from the full GO version to a given GOSlim version. It requires: - A full Gene Ontology file in either OBO or OWL format (see http://geneontology.org/docs/download-ontology). - A slim Gene Ontology file in either OBO or OWL format (see http://geneontology.org/page/go-subset-guide). - A tabular annotation file in GAF (http://geneontology.org/docs/download-go-annotations) format, BLAST2GO format, or a simple two-column table (e.g. from BioMart) with gene product ids in the first column and GO terms in the second one. ----- .. class:: infomark GOSlimmer can also be applied to other pairs of OWL/OBO ontologies where one is a subset of the other and you want to convert annotations from the larger to the smaller one.
toolshed.g2.bx.psu.edu/repos/iuc/bp_genbank2gff3/bp_genbank2gff3/1.1	"What it does
: This tool uses Bio::SeqFeature::Tools::Unflattener and Bio::Tools::GFF to convert GenBank flatfiles to GFF3 with gene containment hierarchies mapped for optimal display in gbrowse. The input files are assumed to be gzipped GenBank flatfiles for refseq contigs. The files may contain multiple GenBank records. 
Designed for RefSeq
 This script is designed for RefSeq genomic sequence entries. It may work for third party annotations but this has not been tested. But see below, Uniprot/Swissprot works, EMBL and possibly EMBL/Ensembl if you don't mind some gene model unflattener errors (dgg). 
G-R-P-E Gene Model
 Don Gilbert worked this over with needs to produce GFF3 suited to loading to GMOD Chado databases. This writes GFF with an alternate, but useful Gene model, instead of the consensus model for GFF3 [ gene > mRNA> (exon,CDS,UTR) ] This alternate is gene > mRNA > polypeptide > exon means the only feature with dna bases is the exon. The others specify only location ranges on a genome. Exon of course is a child of mRNA and protein/peptide. The protein/polypeptide feature is an important one, having all the annotations of the GenBank CDS feature, protein ID, translation, GO terms, Dbxrefs to other proteins. UTRs, introns, CDS-exons are all inferred from the primary exon bases inside/outside appropriate higher feature ranges. Other special gene model features remain the same. 
Authors
 Sheldon McKay (mckays@cshl.edu) Copyright (c) 2004 Cold Spring Harbor Laboratory. 
Author of hacks for GFF2Chado loading
 Don Gilbert (gilbertd@indiana.edu)"
toolshed.g2.bx.psu.edu/repos/iuc/jcvi_gff_stats/jcvi_gff_stats/0.8.4	Based on JCVI Python utility libraries on genome assembly, annotation and comparative genomics. .. _JCVI Python utilities: https://github.com/tanghaibao/jcvi
toolshed.g2.bx.psu.edu/repos/iuc/bicodon_counts_from_fasta/bicodon_counts_from_fasta/0.1.2	Calculate Codon and Bicodon (Codon Pair) usage frequency from FASTA files. Output is designed to mimic that from CoCoPUTs (https://dnahive.fda.gov/dna.cgi?cmd=codon_usage&id=537&mode=cocoputs; e.g. https://dnahive.fda.gov/dna.cgi?cmd=objFile&ids=537&filename=Refseq_Bicod.tsv&raw=1). Translation table identifiers are based upon NCBI standards (https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi). Example of CoCoPUTs style codon output, the majority of codons have been removed in this example, for brevity: +-----------+-----------+-------+--------------+-----------+-------------------+-------+----------+-----+-----+-----+-----+-----+ | Division | Assembly | Taxid | Species | Organelle | Translation Table | # CDS | # Codons | aaa | aac | aag | aat | ... | +===========+===========+=======+==============+===========+===================+=======+==========+=====+=====+=====+=====+=====+ | custom | hg38 | 9606 | Homo sapiens | genomic | 1 | 4 | 859 | 14 | 13 | 29 | 8 | ... | +-----------+-----------+-------+--------------+-----------+-------------------+-------+----------+-----+-----+-----+-----+-----+ Example of CoCoPUTs style bicodon output, the majority of bicodons (codon pairs) have been removed in this example, for brevity: +-----------+-----------+-------+--------------+-----------+-------------------+-------+---------------+--------+--------+--------+--------+-----+ | Division | Assembly | Taxid | Species | Organelle | Translation Table | # CDS | # Codon Pairs | aaaaaa | aaaaac | aaaaag | aaaaat | ... | +===========+===========+=======+==============+===========+===================+=======+===============+========+========+========+========+=====+ | custom | hg38 | 9606 | Homo sapiens | genomic | 1 | 4 | 859 | 0 | 0 | 0 | 1 | ... | +-----------+-----------+-------+--------------+-----------+-------------------+-------+---------------+--------+--------+--------+--------+-----+
toolshed.g2.bx.psu.edu/repos/iuc/codon_freq_from_bicodons/codon_freq_from_bicodons/0.1.2	Get Codon frequency from bicodons. Input format should match that provided by CoCoPUTs (https://dnahive.fda.gov/dna.cgi?cmd=codon_usage&id=537&mode=cocoputs; e.g. https://dnahive.fda.gov/dna.cgi?cmd=objFile&ids=537&filename=Refseq_Bicod.tsv&raw=1). Input row of interest is selected by the combination of the provided taxid and organelle. Translation table identifiers are based upon NCBI standards (https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi). Example of CoCoPUTs style bicodon input, the majority of bicodons (codon pairs) have been removed in this example, for brevity: +-----------+-----------+-------+--------------+-----------+-------------------+-------+---------------+--------+--------+--------+--------+-----+ | Division | Assembly | Taxid | Species | Organelle | Translation Table | # CDS | # Codon Pairs | aaaaaa | aaaaac | aaaaag | aaaaat | ... | +===========+===========+=======+==============+===========+===================+=======+===============+========+========+========+========+=====+ | custom | hg38 | 9606 | Homo sapiens | genomic | 1 | 4 | 859 | 0 | 0 | 0 | 1 | ... | +-----------+-----------+-------+--------------+-----------+-------------------+-------+---------------+--------+--------+--------+--------+-----+
toolshed.g2.bx.psu.edu/repos/iuc/groot/groot/1.1.2+galaxy2	"What it does
 GROOT is a tool to profile Antibiotic Resistance Genes (ARGs) in metagenomic samples. The method combines variation graph representation of gene sets with an LSH Forest indexing scheme to allow for fast classification of metagenomic reads using similarity-search queries. Subsequent hierarchical local alignment of classified reads against graph traversals facilitates accurate reconstruction of full-length gene sequences using a scoring scheme. The main advantages of GROOT over existing tools are: - quick classification of reads to candidate ARGs - accurate annotation of full-length ARGs - can run on a laptop in minutes GROOT aligns reads to ARG variation graphs, producing an alignment file that contains the graph traversals possible for each query read. The alignment file is then used to generate a simple resistome profile report. 
Output
 GROOT will output an ARG alignment file (in BAM format) that contains the graph traversals possible for each query read; the alignment file is then used by GROOT to generate a resistome profile."
toolshed.g2.bx.psu.edu/repos/iuc/gubbins/gubbins/3.2.1+galaxy0	"Gubbins
 Since the introduction of high-throughput, second-generation DNA sequencing technologies, there has been an enormous increase in the size of datasets being used for estimating bacterial population phylodynamics. Although many phylogenetic techniques are scalable to hundreds of bacterial genomes, methods which have been used for mitigating the effect of mechanisms of horizontal sequence transfer on phylogenetic reconstructions cannot cope with these new datasets. Gubbins (Genealogies Unbiased By recomBinations In Nucleotide Sequences) is an algorithm that iteratively identifies loci containing elevated densities of base substitutions while concurrently constructing a phylogeny based on the putative point mutations outside of these regions. Simulations demonstrate the algorithm generates highly accurate reconstructions under realistic models of short-term bacterial evolution, and can be run in only a few hours on alignments of hundreds of bacterial genome sequences. 
Running Gubbins
 To run Gubbins with default settings: Supply a ""fasta"" genome alignment file and press execute. 
Other options
 
Advanced
 
Iterations
, (-i) The maximum number of iterations to perform; the algorithm will stop earlier than this if it converges on the same tree in two successive iterations. Default is 5. 
Converge_method
 (-z) Criteria to use to know when to halt iterations [weighted_robinson_foulds|robinson_foulds|recombination]. Default is weighted_robinson_foulds. 
Outgroup
, (-o) The name of a sequence in the alignment on which to root the tree 
Really Advanced
 These options are here for completeness and you shouldn't need to change them from their defaults. You really need to know what you are doing before you use these. 
Tree builder
, (-t) The algorithm to use in the construction of phylogenies in the analysis; can be ‘raxml’, to use RAxML, ‘fasttree’, to use Fasttree, or ‘hybrid’, to use Fasttree for the first iteration and RAxML in all subsequent iterations. Default is raxml Filter Percentage, (-f) Filter out taxa with more than this percentage of missing data. Default is 25% 
Minimum snps
, (-m) The minimum number of base substitutions required to identify a recombination. Default is 3. And others... 
Output files
 * Recombination predictions in EMBL tab file format. * Recombination predictions in GFF3 format * Base substitution reconstruction in EMBL tab format. * VCF file summarising the distribution of SNPs * Per branch reporting of the base substitutions inside and outside recombinations events. * FASTA format alignment of filtered polymorphic sites used to generate the phylogeny in the final iteration. * Phylip format alignment of filtered polymorphic sites used to generate the phylogeny in the final iteration. * Final phylogenetic tree in newick format."
toolshed.g2.bx.psu.edu/repos/genouest/helixer/helixer/0.3.3+galaxy1	Helixer_: Gene calling with Deep Neural Networks. .. _Helixer: https://github.com/weberlab-hhu/Helixer
toolshed.g2.bx.psu.edu/repos/iuc/icescreen/icescreen/1.3.3+galaxy0	".. class:: warningmark ICEscreen requires input files in genbank format. Multigenbank files (i.e. gbff files featuring multiple genome records back to back) are supported. Each Genbank record must include the nucleotide sequence. ----- 
What it does
 ICEscreen is a bioinformatic pipeline for the detection and annotation of ICEs (Integrative and Conjugative Elements) and IMEs (Integrative and Mobilizable Elements) in Bacillota genomes. The full documentation is available at https://icescreen.migale.inrae.fr. The forge of the project is accessible at https://forgemia.inra.fr/ices_imes_analysis/icescreen. 
Main features of ICEscreen
 - Detection of signature proteins (SPs) of ICEs/IMEs by using blastP on a curated resource. BlastP allows for an accurate assignment of hits to a given ICE/IME superfamily or family. The curated resource was derived from an analysis of over 120 ICEs and IMEs in Streptococcus genomes by the DINAMIC lab. - Detection of distant homologs of SPs by using HMM profiles of ICEs/IMEs protein families. The HMM profiles have been either imported from trusted resources or created and curated when needed. - Detection of the ICE/IME structures: ICEScreen groups together SPs that belong to the same ICE/IME structures to the best of its ability. - Delimitation of the elements at the gene or nucleotide level is not yet implemented and still needs manual curation. 
Output files
 There are 3 main output results files generated by ICEscreen: - Detected Signature Proteins table (
*_detected_SP_withMEIds.csv
): list of the signature proteins detected by the tool and their possible assignment to an ICE or IME element. Each line represents a signature protein detected by ICEscreen. - Detected ICEs/IMEs table (
*_detected_ME.tsv
): list of the ICEs and IMEs elements detected by the tool, including information about the signature proteins they contain. Information in this file is complementary to the _withICEIMEIds.csv file above with each line representing an ICE / IME structures instead of a signature protein. - Results summary (
*_detected_ME.summary
): this file summarizes the main parameters and statistics regarding the ICE / IME structures and the SPs. Other optional and additional output files generated by ICEscreen: - Genbank annotation file (full): annotations of the original Genbank file as well as annotations of ICEscreen on ICEs, IMEs, and the detected signature proteins. - EMBL and GFF3 annotation files (ICEs and IMEs only): annotations of ICEscreen on ICEs, IMEs, and the detected signature proteins. - Intermediate files (tar.gz): You can download this file locally and uncompress it to have the intermediate files generated by the tool. See https://icescreen.migale.inrae.fr for details. - Log file (
*_detected_ME.log
): this file contains the log of each step of the ICEscreen pipeline. - Param conf file (
param.conf.gz
): this file contains the configuration parameters related to the ICEscreen run."
toolshed.g2.bx.psu.edu/repos/iuc/isescan/isescan/1.7.3+galaxy0	"What it does
 ISEScan is a python pipeline to identify Insertion Sequence elements (both complete and incomplete IS elements) in genome. If you want isescan to report only complete IS elements, you need to set command removeShortIS. 
Input data
 ISEScan accept fasta file format as input 
Parameters
 You can remove the short partial IS elements which include IS element with length < 400 or single copy IS element without perfect TIR/>"
toolshed.g2.bx.psu.edu/repos/iuc/integron_finder/integron_finder/2.0.5+galaxy0	"How does it work ? ================== - First, IntegronFinder annotates the DNA sequence's CDS with Prodigal. - Second, IntegronFinder detects independently integron integrase and 
attC
 recombination sites. The Integron integrase is detected by using the intersection of two HMM profiles: - one specific of tyrosine-recombinase (PF00589) - one specific of the integron integrase, near the patch III domain of tyrosine recombinases. The 
attC
 recombination site is detected with a covariance model (CM), which models the secondary structure in addition to the few conserved sequence positions. - Third, the results are integrated, and IntegronFinder distinguishes 3 types of elements: - complete integron Integron with integron integrase nearby 
attC
 site(s) - In0 element Integron integrase only, without any 
attC
 site nearby - CALIN element Cluster of 
attC
 sites Lacking INtegrase nearby. A rule of thumb to avoid false positive is to filter out singleton of 
attC
 site. IntegronFinder can also annotate gene cassettes (CDS nearby 
attC
 sites) using Resfams, a database of HMM profiles aiming at annotating antibiotic resistance genes. This database is provided but the user can add any other HMM profiles database of its own interest. When available, IntegronFinder annotates the promoters and attI sites by pattern matching."
toolshed.g2.bx.psu.edu/repos/bgruening/interproscan/interproscan/5.59-91.0+galaxy3	"What it does
 Interproscan is a batch tool to query the InterPro database. It provides annotations based on multiple searches of profile and other functional databases. Phobius (licensed software), SignalP, SMART (licensed components) and TMHMM use licensed code and data provided by third parties. If you wish to run these analyses it will be necessary for you to obtain a licence from the vendor and configure the Galaxy server InterProScan installation to use them. 
Input
 Required is a FASTA file containing protein or nucleotide sequences. 
Output
 In this version of InterProScan, you can retrieve output in any of the following five formats: * TSV: tab-separated values format * XML: XML format * GFF: The GFF 3.0 format * JSON: A JSON representation of the protein matches that can be visualised on https://www.ebi.ac.uk/interpro/result/InterProScan/ 
Example Output
 :: P51587 14086411a2cdf1c4cba63020e1622579 3418 Pfam PF09103 BRCA2, oligonucleotide/oligosaccharide-binding, domain 1 2670 2799 7.9E-43 T 15-03-2013 P51587 14086411a2cdf1c4cba63020e1622579 3418 ProSiteProfiles PS50138 BRCA2 repeat profile. 1002 1036 0.0 T 18-03-2013 IPR002093 BRCA2 repeat GO:0005515|GO:0006302 P51587 14086411a2cdf1c4cba63020e1622579 3418 Gene3D G3DSA:2.40.50.140 2966 3051 3.1E-52 T 15-03-2013 ... The TSV format presents the match data in columns as follows: - Protein Accession (e.g. P51587) - Sequence MD5 digest (e.g. 14086411a2cdf1c4cba63020e1622579) - Sequence Length (e.g. 3418) - Analysis (e.g. Pfam / PRINTS / Gene3D) - Signature Accession (e.g. PF09103 / G3DSA:2.40.50.140) - Signature Description (e.g. BRCA2 repeat profile) - Start location - Stop location - Score - is the e-value of the match reported by member database method (e.g. 3.1E-52) - Status - is the status of the match (T: true) - Date - is the date of the run - (InterProScan annotations - accession (e.g. IPR002093) - optional column; only displayed if -iprscan option is switched on) - (InterProScan annotations - description (e.g. BRCA2 repeat) - optional column; only displayed if -iprscan option is switched on) - (GO annotations (e.g. GO:0005515) - optional column; only displayed if --goterms option is switched on) - (Pathways annotations (e.g. REACT_71) - optional column; only displayed if --pathways option is switched on) 
Extensible Markup Language (XML)
 XML representation of the matches - this is the richest form of the data. The XML Schema Definition (XSD) is available [http://www.ebi.ac.uk/interpro/resources/schemas/interproscan5 here]. 
Generic Feature Format Version 3 (GFF3)
 The GFF3 format is a flat tab-delimited file, which is much richer then the TSV output format. It allows you to trace back from matches to predicted proteins and to nucleic acid sequences. It also contains a FASTA format representation of the predicted protein sequences and their matches. You will find a documentation of all the columns and attributes used on [https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md]. 
Example Output
 :: ##gff-version 3 ##feature-ontology http://song.cvs.sourceforge.net/viewvc/song/ontology/sofa.obo?revision=1.269 ##sequence-region AACH01000027 1 1347 ##seqid|source|type|start|end|score|strand|phase|attributes AACH01000027 provided_by_user nucleic_acid 1 1347 . + . Name=AACH01000027;md5=b2a7416cb92565c004becb7510f46840;ID=AACH01000027 AACH01000027 getorf ORF 1 1347 . + . Name=AACH01000027.2_21;Target=pep_AACH01000027_1_1347 1 449;md5=b2a7416cb92565c004becb7510f46840;ID=orf_AACH01000027_1_1347 AACH01000027 getorf polypeptide 1 449 . + . md5=fd0743a673ac69fb6e5c67a48f264dd5;ID=pep_AACH01000027_1_1347 AACH01000027 Pfam protein_match 84 314 1.2E-45 + . Name=PF00696;signature_desc=Amino acid kinase family;Target=null 84 314;status=T;ID=match$8_84_314;Ontology_term=""GO:0008652"";date=15-04-2013;Dbxref=""InterPro:IPR001048"",""Reactome:REACT_13"" ##sequence-region 2 ... >pep_AACH01000027_1_1347 LVLLAAFDCIDDTKLVKQIIISEIINSLPNIVNDKYGRKVLLYLLSPRDPAHTVREIIEV LQKGDGNAHSKKDTEIRRREMKYKRIVFKVGTSSLTNEDGSLSRSKVKDITQQLAMLHEA GHELILVSSGAIAAGFGALGFKKRPTKIADKQASAAVGQGLLLEEYTTNLLLRQIVSAQI LLTQDDFVDKRRYKNAHQALSVLLNRGAIPIINENDSVVIDELKVGDNDTLSAQVAAMVQ ADLLVFLTDVDGLYTGNPNSDPRAKRLERIETINREIIDMAGGAGSSNGTGGMLTKIKAA TIATESGVPVYICSSLKSDSMIEAAEETEDGSYFVAQEKGLRTQKQWLAFYAQSQGSIWV DKGAAEALSQYGKSLLLSGIVEAEGVFSYGDIVTVFDKESGKSLGKGRVQFGASALEDML RSQKAKGVLIYRDDWISITPEIQLLFTEF ... >match$8_84_314 KRIVFKVGTSSLTNEDGSLSRSKVKDITQQLAMLHEAGHELILVSSGAIAAGFGALGFKK RPTKIADKQASAAVGQGLLLEEYTTNLLLRQIVSAQILLTQDDFVDKRRYKNAHQALSVL LNRGAIPIINENDSVVIDELKVGDNDTLSAQVAAMVQADLLVFLTDVDGLYTGNPNSDPR AKRLERIETINREIIDMAGGAGSSNGTGGMLTKIKAATIATESGVPVYICS"
toolshed.g2.bx.psu.edu/repos/iuc/kobas/kobas_annotate/2.1.1	"KOBAS
 KOBAS is a KEGG Orthology Based Annotation System. Its purpose is to identify statistically enriched pathways, diseases, and GO terms for a set of genes or proteins, using pathway, disease, and GO knowledge from multiple famous databases. The necessary KOBAS databases for multiple species can be downloaded from http://kobas.cbi.pku.edu.cn/download.php 
KOBAS Annotate
 Annotates an input set of genes with putative pathways and disease relationships based on mapping to genes with known annotations. It allows for both ID mapping and cross-species sequence similarity mapping. KOBAS can be accessed at http://kobas.cbi.pku.edu.cn"
toolshed.g2.bx.psu.edu/repos/iuc/kobas/kobas_identify/2.1.1	"KOBAS
 KOBAS is a KEGG Orthology Based Annotation System. Its purpose is to identify statistically enriched pathways, diseases, and GO terms for a set of genes or proteins, using pathway, disease, and GO knowledge from multiple famous databases. The necessary KOBAS databases for multiple species can be downloaded from http://kobas.cbi.pku.edu.cn/download.php 
KOBAS Identify
 Performs statistical tests to identify significantly enriched pathways and diseases. KOBAS can be accessed at http://kobas.cbi.pku.edu.cn"
toolshed.g2.bx.psu.edu/repos/iuc/spaln/list_spaln_tables/2.4.9+galaxy0	Spaln has a number of pre-computed settings files to tune its predictions for different species. These are listed in a file named gnm2tab_ in the package. This tool uses the NCBI Taxonomy database to search that able for a suitable (i.e. taxonomically close) set of settings for optimising spaln's alignment predictions. Input is the scientific name of a species (as reflected in the NCBI Taxonomy DB), output is information from the gnm2tab file sorted by taxonomic distance from the query species. .. _gnm2tab: https://github.com/ogotoh/spaln/blob/master/table/gnm2tab
toolshed.g2.bx.psu.edu/repos/iuc/mitos2/mitos2/2.1.10+galaxy0	"MITOS2 does a de-novo annotation of a given metazoan mitochondrial sequence. 
Inputs
 - A fasta formatted sequence - MITOS2 processes only FASTA files containing a single sequence. If you have a FASTA file with multiple sequences you may use the tool 
Split Fasta files into a collection
. The resulting collection can then used as input to MITOS2. - The correct genetic code needs to be selected - Reference data needs to be selected (can be provided by an administrator using the MITOS data manager) 
Outputs
 By default the annotation in BED format. Several other outputs can be enabled: - The annotation can be given as well in GFF, SEQ, or mito (a custom tabular format) - zipped raw data (all files computed during the run, e.g. BLAST and cmsearch outputs) - protein and ncRNA prediction plots visualize the predicted genes and their qualities - ncRNA structure plots (secondary structures of the predicted ncRNAs) See http://mitos.bioinf.uni-leipzig.de/help.py Contact mitos (at) bioinf (dash) uni (dash) leipzig (dot) de in case of problems. 
Advanced options
 - Feature types Select the feature types that should be annotated. By default this is protein coding genes, tRNA and rRNA which is useful for metazoan mitogenomes. In addition also the replication origins of the light (OL) and heavy (OH) strand and introns can be annotated. The annotation of the replication origins is most useful for chordate mitogenomes. Introns are usually only found in mitogenomes of non-metazoans and basal Metazoa. - Final overlap (nt) Maximum number of nucleotides by which genes of different types may overlap. Applies to merging of the final predictions. - Annotate only the best copy of each feature If there are copies of the same feature type only the one with the lowest e-value (for ncRNAs and OL) or highest quality score (protein coding genes and OH) - Fragment overlap Maximum fraction (of the shorter feature) allowed that two hits overlap in the query to be counted as fragments. - Fragment quality factor Maximum factor by which fragments may differ in their quality scores. Higher values allow that parts of a gene can differ more in their quality. 
Advanced options for protein coding gene prediction
 - BLAST E-value Exponent The statistical significance threshold for considering matches in the BLASTX search. The value entered here is the negation of the exponent of the E-value threshold that should be used by BLAST, i.e. a value X gives an E-value of 10^(-X). - Quality cutoff Minimum allowed quality value (in percent) of the maximum quality value per reading frame. A higher values correspond to shorter protein prediction and therefore reduced risk for conflicts with other features - Clipping factor Clipping is started if overlapping prediction of hits with the same name differ by less than a factor X in their quality value. - use start/stop codons as in NCBI (default: learned start/stop codons) Instead of the codon probabilities derived from the protein coding genes annotated in RefSeq the codons listed at NCBI taxonomy are used with equal probabilities (https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi) - Use the hmmer based method of Al Arab et al. 2016. This will consider the evalue, ncbicode, fragovl, fragfac parameters Note: 1) this only works for Metazoa RefSeq release 63 reference data set. 2) This will only predict the protein coding genes that are typical for metazoan mitochondrial genomes. - Use the old start/stop prediction method of MITOS1 The search for start and stop codons just takes the closest to the initial start / stop positions within 6aa (i.e. the method used in MITOS1) 
Advanced options for ncRNA gene prediction
 - Run mitfi in glocal and local mode (default: local only) By default mitfi uses infernal's cmsearch in local search mode only. By enabling this option mitfi will invoke cmserach also in glocal mode if a feature is missing. - e-value to use for inferal fast mode The e-value passed to the first pass of cmsearch in the second pass (the sensitive search) an e-value of 0.1 is used. - Use infernal's sensitive mode only By default mitfi searches for ncRNAs using cmsearch's default fast mode first. If a ncRNA type is missing it is searched using the sensitive mode. This can be useful if low scoring copies are expected which might be missed when searching in the two stage mode. - Allow tRNA/rRNA overlap of up to X nt for mitfi Allow that a tRNA/rRNA overlaps with another feature by this number of nucleotides."
toolshed.g2.bx.psu.edu/repos/iuc/mlst/mlst/2.22.0	"What it does
 Given a genome file in FASTA or Genbank format, MLST will scan the file against PubMLST typing schemes. 
Output
 MLST will produce a tab-seperated output file which contains: - the filename - the closest PubMLST scheme name - the ST (sequence type) - the allele IDs 
Example Output
 :: genomes/6008.fna saureus 239 arcc(2) aroe(3) glpf(1) gmk_(1) pta_(4) tpi_(4) yqil(3) 
Without auto-detection
 If you choose to manually set the MLST scheme, it will print a fixed tabular output with a heading containing allele names specific to that scheme (adjustable with 
Include allele header
). To view a list of schemes, use the MLST List tool. :: FILE SCHEME ST abcZ adk aroE fumC gdh pdhC pgm NM003.fa neisseria 11 2 3 4 3 8 4 6 
Missing data
 MLST does not just look for exact matches to full length alleles. It attempts to tell you as much as possible about what it found using the notation below: +--------+---------------------------------------+ | Symbol | Meaning | +========+=======================================+ | n | Exact intact allele | +--------+---------------------------------------+ | ~n | Novel full length allele similar to n | +--------+---------------------------------------+ | n? | Partial match to known allele | +--------+---------------------------------------+ | n,m | Multiple alleles | +--------+---------------------------------------+ | 
-
 | Allele missing | +--------+---------------------------------------+ Setting 
Output novel alleles
 to true will produce an additional 
novel_alleles.fasta
 file containing the novel alleles. Galaxy wrapper maintained by Simon Gladman."
toolshed.g2.bx.psu.edu/repos/iuc/mlst/mlst_list/2.22.0	"What it does
 MLST List will list all the scheme names used by MLST. Selecting the long list option will also provide the alleles for all MLST schemes."
toolshed.g2.bx.psu.edu/repos/iuc/mmseqs2_taxonomy_assignment/mmseqs2_taxonomy_assignment/17-b804f+galaxy0	"MMseqs2: ultra fast and sensitive sequence search and clustering suite
 MMseqs2 (Many-against-Many sequence searching) is a software suite to search and cluster huge protein and nucleotide sequence sets. MMseqs2 is open source GPL-licensed software implemented in C++ for Linux, MacOS, and (as beta version, via cygwin) Windows. The software is designed to run on multiple cores and servers and exhibits very good scalability. MMseqs2 can run 10000 times faster than BLAST. At 100 times its speed it achieves almost the same sensitivity. It can perform profile searches with the same sensitivity as PSI-BLAST at over 400 times its speed. 
Usage
 * Convert FASTA/Q file(s) to MMseqs sequence DB format 
mmseqs createdb <i:fastaFile1[.gz|.bz2]> ... <i:fastaFileN[.gz|.bz2]>|<i:stdin> <o:sequenceDB> [options]
 * Filter taxonomy sequence database 
mmseqs filtertaxseqdb <i:taxSeqDB> <o:taxSeqDB> [options]
 * Taxonomy assignment by computing the lowest common ancestor of homologs 
mmseqs taxonomy <i:queryDB> <i:targetDB> <o:taxaDB> <tmpDir> [options]
 * Convert result DB to tab-separated flat file 
mmseqs createtsv <i:queryDB> [<i:targetDB>] <i:resultDB> <o:tsvFile> [options]
 * Create a taxonomy report in Kraken or Krona format 
mmseqs taxonomyreport <i:seqTaxDB> <i:taxResultDB/resultDB/sequenceDB> <o:taxonomyReport> [options]
 https://github.com/soedinglab/MMseqs2"
toolshed.g2.bx.psu.edu/repos/iuc/maker/maker/2.31.11+galaxy2	MAKER is a portable and easily configurable genome annotation pipeline. Its purpose is to allow smaller eukaryotic and prokaryotic genome projects to independently annotate their genomes and to create genome databases. MAKER identifies repeats, aligns ESTs and proteins to a genome, produces ab-initio gene predictions and automatically synthesizes these data into gene annotations having evidence-based quality values. MAKER is also easily trainable: outputs of preliminary runs can be used to automatically retrain its gene prediction algorithm, producing higher quality gene-models on seusequent runs. MAKER's inputs are minimal and its ouputs can be directly loaded into a GMOD database. They can also be viewed in the Apollo genome browser; this feature of MAKER provides an easy means to annotate, view and edit individual contigs and BACs without the overhead of a database. MAKER should prove especially useful for emerging model organism projects with minimal bioinformatics expertise and computer resources. .. _Maker: http://www.yandell-lab.org/software/maker.html
toolshed.g2.bx.psu.edu/repos/iuc/maker_map_ids/maker_map_ids/2.31.11	MAKER is a portable and easily configurable genome annotation pipeline. Its purpose is to allow smaller eukaryotic and prokaryotic genome projects to independently annotate their genomes and to create genome databases. MAKER identifies repeats, aligns ESTs and proteins to a genome, produces ab-initio gene predictions and automatically synthesizes these data into gene annotations having evidence-based quality values. MAKER is also easily trainable: outputs of preliminary runs can be used to automatically retrain its gene prediction algorithm, producing higher quality gene-models on seusequent runs. MAKER's inputs are minimal and its ouputs can be directly loaded into a GMOD database. They can also be viewed in the Apollo genome browser; this feature of MAKER provides an easy means to annotate, view and edit individual contigs and BACs without the overhead of a database. MAKER should prove especially useful for emerging model organism projects with minimal bioinformatics expertise and computer resources. This tool will automatically assign new ids to a Maker annotation respecting a specified pattern. .. _Maker: http://www.yandell-lab.org/software/maker.html
toolshed.g2.bx.psu.edu/repos/iuc/miniprot/miniprot/0.18+galaxy0	"miniprot_ rapidly aligns a protein sequence against a genome with affine gap penalty, splicing and frameshift. It is primarily intended for annotating protein-coding genes in a new species using known genes from other species. While an index of the genome to be mapped to can be built ""on the fly"", the Miniprot index tool can pre-index a genome and will result in faster performance if the genome index is reused multiple times. For details of the algorithm and some insight into how parameters can be tuned see this overview_. .. _miniprot: https://github.com/lh3/miniprot .. _overview: https://github.com/lh3/miniprot#algorithm-overview"
toolshed.g2.bx.psu.edu/repos/iuc/miniprot_index/miniprot_index/0.18+galaxy1	Pre-index genomic sequences (contigs, scaffolds or chromosomes) for use in the miniprot_ protein to genome alignment tool. .. _miniprot: https://github.com/lh3/miniprot
toolshed.g2.bx.psu.edu/repos/richard-burhans/ncbi_egapx/ncbi_egapx/0.4.1+galaxy3	"Galaxy tool wrapping the Eukaryotic Genome Annotation Pipeline (EGAPx) ================================================================================================= .. class:: warningmark 
Proof of concept: a hack to run a NF workflow inside a specialised Galaxy tool wrapper
 EGAPx is a big, complicated Nextflow workflow, challenging and costly to re-implement 
properly
, requiring dozens of new tools and replicating a lot of complicated 
groovy
 workflow logic. It is also very new and in rapid development. Investing developer effort and keeping updated as EGAPx changes rapidly may be 
inefficient of developer resources
. This wrapper is designed to allow measuring how 
inefficient
 it is in terms of computing resource utilisation, in comparison to the developer effort required to convert Nextflow DDL into tools and WF logic. Balancing these competing requirements is a fundamental Galaxy challenge. EGAPx requires very substantial resources to run with real data. 
132GB and 32 cores
 are the minimum requirement; 
256GB and 64 cores
 are recommended. A special minimal example that can be run in 6GB with 4 cores is provided as a yaml configuration and is used for the tool test. In this implementation, the user must supply a yaml configuration file as initial proof of concept. History inputs and even a yaml editor might be provided in future. The NF workflow to tool model tested here may be applicable to other NF workflows that take a single configuration yaml. .. class:: warningmark The computational resource cost of typing the wrong SRA identifiers into a tool form is potentially enormous with this tool! Sample yaml configurations =========================== YAML sample configurations can be uploaded into your Galaxy history from the 
EGAPx github repository &lt;https://github.com/ncbi/egapx/tree/main/examples/&gt;
. The simplest possible example is shown below - can be cut/paste into a history dataset in the upload tool. 
./examples/input_D_farinae_small.yaml
 is shown below and can be cut and pasted into the upload form to create a yaml file. RNA-seq data is provided as URI to the reads FASTA files. input_D_farinae_small.yaml :: genome: https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/020/809/275/GCF_020809275.1_ASM2080927v1/GCF_020809275.1_ASM2080927v1_genomic.fna.gz taxid: 6954 short_reads: - https://ftp.ncbi.nlm.nih.gov/genomes/TOOLS/EGAP/sample_data/Dermatophagoides_farinae_small/SRR8506572.1 - https://ftp.ncbi.nlm.nih.gov/genomes/TOOLS/EGAP/sample_data/Dermatophagoides_farinae_small/SRR8506572.2 - https://ftp.ncbi.nlm.nih.gov/genomes/TOOLS/EGAP/sample_data/Dermatophagoides_farinae_small/SRR9005248.1 - https://ftp.ncbi.nlm.nih.gov/genomes/TOOLS/EGAP/sample_data/Dermatophagoides_farinae_small/SRR9005248.2 input_Gavia_stellata.yaml :: genome: https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/030/936/135/GCF_030936135.1_bGavSte3.hap2/GCF_030936135.1_bGavSte3.hap2_genomic.fna.gz short_reads: txid37040[Organism] AND biomol_transcript[properties] NOT SRS024887[Accession] taxid: 37040 input_C_longicornis.yaml :: genome: https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/029//603/195/GCF_029603195.1_ASM2960319v2/GCF_029603195.1_ASM2960319v2_genomic.fna.gz short_reads: txid2530218[Organism] AND biomol_transcript[properties] NOT SRS024887[Accession] taxid: 2530218 Purpose ======== 
This is not intended for production
 Just a proof of concept. It is possibly too inefficient to be useful although it may turn out not to be a problem if run on a dedicated workstation. At least the efficiency can now be more easily estimated. This tool is not recommended for public deployment because of the resource demands. EGAPx Overview =============== .. image:: $PATH_TO_IMAGES/Pipeline_sm_ncRNA_CAGE_80pct.png 
Warning:
 The current version is an alpha release with limited features and organism scope to collect initial feedback on execution. Outputs are not yet complete and not intended for production use. Please open a GitHub 
Issue
 if you encounter any problems with EGAPx. You can also write to cgr@nlm.nih.gov to give us your feedback or if you have any questions. EGAPx is the publicly accessible version of the updated NCBI 
Eukaryotic Genome Annotation Pipeline
. EGAPx takes an assembly FASTA file, a taxid of the organism, and RNA-seq data. Based on the taxid, EGAPx will pick protein sets and HMM models. The pipeline runs 
miniprot
 to align protein sequences, and 
STAR
 to align RNA-seq to the assembly. Protein alignments and RNA-seq read alignments are then passed to 
Gnomon
 for gene prediction. In the first step of 
Gnomon
, the short alignments are chained together into putative gene models. In the second step, these predictions are further supplemented by 
ab-initio
 predictions based on HMM models. The final annotation for the input assembly is produced as a 
gff
 file. 
Security Notice:
 EGAPx has dependencies in and outside of its execution path that include several thousand files from the 
NCBI C++ toolkit
, and more than a million total lines of code. Static Application Security Testing has shown a small number of verified buffer overrun security vulnerabilities. Users should consult with their organizational security team on risk and if there is concern, consider mitigating options like running via VM or cloud instance. 
To specify an array of NCBI SRA datasets in yaml
 :: short_reads: - SRR8506572 - SRR9005248 
To specify an SRA entrez query
 :: short_reads: 'txid6954[Organism] AND biomol_transcript[properties] NOT SRS024887[Accession] AND (SRR8506572[Accession] OR SRR9005248[Accession] )' 
Note:
 Both the above examples will have more RNA-seq data than the 
input_D_farinae_small.yaml
 example. To make sure the entrez query does not produce a large number of SRA runs, please run it first at the 
NCBI SRA page
. If there are too many SRA runs, then select a few of them and list it in the input yaml. Output ======= EGAPx output will appear as a collection in the user history. The main annotation file is called 
complete.genomic.gff
. :: complete.genomic.gff annot_builder_output nextflow.log run.report.html run.timeline.html run.trace.txt run_params.yaml The 
nextflow.log
 is the log file that captures all the process information and their work directories. 
run_params.yaml
 has all the parameters that were used in the EGAPx run. More information about the process time and resources can be found in the other run* files. ## Intermediate files In the log, each line denotes the process that completed in the workflow. The first column (_e.g.
 
[96/621c4b]
) is the subdirectory where the intermediate output files and logs are found for the process in the same line, 
i.e.
, 
egapx:miniprot:run_miniprot
. To see the intermediate files for that process, you can go to the work directory path that you had supplied and traverse to the subdirectory 
96/621c4b
: :: $ aws s3 ls s3://temp_datapath/D_farinae/96/ PRE 06834b76c8d7ceb8c97d2ccf75cda4/ PRE 621c4ba4e6e87a4d869c696fe50034/ $ aws s3 ls s3://temp_datapath/D_farinae/96/621c4ba4e6e87a4d869c696fe50034/ PRE output/ 2024-03-27 11:19:18 0 2024-03-27 11:19:28 6 .command.begin 2024-03-27 11:20:24 762 .command.err 2024-03-27 11:20:26 762 .command.log 2024-03-27 11:20:23 0 .command.out 2024-03-27 11:19:18 13103 .command.run 2024-03-27 11:19:18 129 .command.sh 2024-03-27 11:20:24 276 .command.trace 2024-03-27 11:20:25 1 .exitcode $ aws s3 ls s3://temp_datapath/D_farinae/96/621c4ba4e6e87a4d869c696fe50034/output/ 2024-03-27 11:20:24 17127134 aligns.paf"
toolshed.g2.bx.psu.edu/repos/peterjc/clinod/clinod/0.1.1	"What it does
 This calls the command line version of the NoD tool from the Barton Group for prediction of nucleolar localization sequences (NoLSs). The NoD tool uses an artificial neural network trained on a set of human NoLSs. The nucleolus is a sub-compartmentof the nucleus, thus an NoLS can be regarded as a special nuclear localization sequence (NLS). The input is a FASTA file of protein sequences, and the output is tabular with four columns (multiple rows per protein): ====== =================== Column Description ------ ------------------- 1 Sequence identifier 2 Start of NoLS 3 End of NoLS 4 NoLS sequence ====== =================== If a sequence has no predicted NoLS, then there is no line in the output file for it. ----- 
References
 If you use this Galaxy tool in work leading to a scientific publication please cite the following papers: Peter J.A. Cock, Björn A. Grüning, Konrad Paszkiewicz and Leighton Pritchard (2013). Galaxy tools and workflows for sequence analysis with applications in molecular plant pathology. PeerJ 1:e167 https://doi.org/10.7717/peerj.167 M. S. Scott, F. M. Boisvert, M. D. McDowall, A. I. Lamond and G. J. Barton (2010). Characterization and prediction of protein nucleolar localization sequences. Nucleic Acids Research 38(21), 7388-7399. https://doi.org/10.1093/nar/gkq653 M. S. Scott, P. V. Troshin and G. J. Barton (2011). NoD: a Nucleolar localization sequence detector for eukaryotic and viral proteins. BMC Bioinformatics, 12:317. https://doi.org/10.1186/1471-2105-12-317 See also http://www.compbio.dundee.ac.uk/www-nod/ This wrapper is available to install into other Galaxy Instances via the Galaxy Tool Shed at http://toolshed.g2.bx.psu.edu/view/peterjc/clinod"
toolshed.g2.bx.psu.edu/repos/iuc/omark/omark/0.3.1+galaxy1	OMark_: is a software for proteome quality assessment. It provides measures of proteome completeness, characterizes the consistency of all protein coding genes with regard to their homologs, and identifies the presence of contamination from other species. OMArk relies on the OMA orthology database. For more information, please refer to the OMamer_ documentaion. .. _OMark: https://github.com/DessimozLab/OMArk .. _OMamer: https://github.com/DessimozLab/omamer
toolshed.g2.bx.psu.edu/repos/iuc/orfipy/orfipy/0.0.4+galaxy2	"What it does
 Orfipy is a tool for finding open reading frames (ORFs) in sequences (FASTA files)."
toolshed.g2.bx.psu.edu/repos/iuc/optitype/optitype/1.3.5+galaxy0	"In paired-end mode one might want to use reads with just one mapped end (e.g., the other end falls outside the reference region). This setting allows the user to keep them with an optionally reduced weight. A value of 0 means they are discarded for typing, 0.2 means single reads are ""worth"" 20% of paired reads, and a value of 1 means they are treated as valuable as properly mapped read pairs. Note: unpaired reads will be reported on the result coverage plots for completeness, regardless of this setting."
toolshed.g2.bx.psu.edu/repos/bgruening/pfamscan/pfamscan/1.6+galaxy0	".. class:: infomark 
Purpose
 Search one or more sequences for matching Pfam domains. Depending on the user options, the script can also process the results such that overlaps between families belonging to the same clan are resolved and can predict active sites. ---- .. class:: infomark 
Required files
 To run PfamScan you will need to download the following files from the Pfam ftp site: - Pfam-A HMMs in an HMM library searchable with the hmmscan program: 
Pfam-A.hmm.gz &lt;https://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz&gt;
 - Pfam-A HMM Stockholm file associated with each HMM required for PfamScan: 
Pfam-A.hmm.dat.gz &lt;https://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.dat.gz&gt;
 - Active sites: 
active_sites.dat.gz &lt;ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/active_site.dat.gz&gt;
_"
toolshed.g2.bx.psu.edu/repos/iuc/plasmidfinder/plasmidfinder/2.1.6+galaxy1	"What it does
 PlasmidFinder characterize plasmid sequences into whole genome sequencing. It is based on the 
plasmidfinder database
 with hundreds sequences. 
Input
 PlasmidFinder takes raw data (with a k-mer analysisi) as reads or genome assembly (blastn analysis) to search plasmids. 
Output
 Some output files are availables - A fasta file with all available sequences detected in the genome - A fasta file with all plasmid sequences from the database - A summary of the analysis in tabular format - A Raw result file in text format - A JSON file could be use for other boinformatic analysis - A log file with analysis parameters"
toolshed.g2.bx.psu.edu/repos/iuc/prodigal/prodigal/2.6.3+galaxy0	Fast, reliable protein-coding gene prediction for prokaryotic genomes. Features: * Predicts protein-coding genes: Prodigal provides fast, accurate protein-coding gene predictions in GFF3, Genbank, or Sequin table format. * Handles draft genomes and metagenomes: Prodigal runs smoothly on finished genomes, draft genomes, and metagenomes. * Runs quickly: Prodigal analyzes the E. coli K-12 genome in 10 seconds on a modern MacBook Pro. * Runs unsupervised: Prodigal is an unsupervised machine learning algorithm. It does not need to be provided with any training data, and instead automatically learns the properties of the genome from the sequence itself, including RBS motif usage, start codon usage, and coding statistics. * Handles gaps and partial genes: The user can specify if Prodigal should build genes across runs of N's as well as how to handle genes at the edges of contigs. * Identifies translation initiation sites: Prodigal predicts the correct translation initiation site for most genes, and can output information about every potential start site in the genome, including confidence score, RBS motif, and much more.
toolshed.g2.bx.psu.edu/repos/crs4/prokka/prokka/1.14.6+galaxy1	"What it does
 Prokka_ is a software tool to rapidly annotate bacterial, archaeal and viral genomes, and produce output files that require only minor tweaking to submit to GenBank/ENA/DDBJ. .. 
Prokka: http://github.com/tseemann/prokka 
Output files
 Prokka creates several output files, which are described in the 
Additional outputs
 section above. 
License and citation
 This Galaxy tool is Copyright © 2013 Lionel Guy, © 2013-2014 
CRS4 Srl.
, © 2015-2016 
Earlham Institute
, 2018 
Galaxy IUC
 and is released under the 
MIT license
. .. 
CRS4 Srl.: http://www.crs4.it/ .. _Earlham Institute: http://earlham.ac.uk/ .. _MIT license: https://opensource.org/licenses/MIT You can use this tool only if you agree to the license terms of: 
Prokka
. .. _Prokka: http://github.com/tseemann/prokka"
toolshed.g2.bx.psu.edu/repos/iuc/read_it_and_keep/read_it_and_keep/0.2.2+galaxy0	"ReadItAndKeep ------------- ReadItAndKeep is a tool for filtering viral sequence data to remove host reads, developed for cleaning SARS-CoV-2 sequencing data. It maps reads against the SARS-CoV-2 viral genome (with the poly-A tail removed) and only keeps those that map well. 
Note
: If the reference genome supplied contains a poly-A tail, reads that contain part of a poly-A tail will map to the refence, no matter what species they originate from. If you are not sure if the reference you are using has had trailing A's trimmed, enable the 
Trim trailing As
 option. Input can be either Illumina or Oxford Nanopore reads."
toolshed.g2.bx.psu.edu/repos/iuc/red/red/2018.09.10+galaxy1	This is Red (REpeat Detector) designed and developed by Hani Zakaria Girgis, PhD. An intelligent, rapid, accurate tool for detecting repeats de-novo on the genomic scale.
toolshed.g2.bx.psu.edu/repos/bgruening/repeat_masker/repeatmasker_wrapper/4.1.5+galaxy0	RepeatMasker is a program that screens DNA for interspersed repeats and low complexity DNA sequences. The database of repeats to screen for can be provided as a FASTA file or downloaded from RepBase_. If the RepBase option is chosen the RepBaseRepeatMaskerEdition file should be downloaded and unpacked, and the enclosed EMBL format file ('RMRBSeqs.embl') should be uploaded to Galaxy for use with this tool. Further documentation is available on the RepeatMasker homepage_. .. _RepBase: http://www.girinst.org/repbase/ .. _homepage: http://www.repeatmasker.org/webrepeatmaskerhelp.html
toolshed.g2.bx.psu.edu/repos/csbl/repeatmodeler/repeatmodeler/2.0.5+galaxy0	RepeatModeler is a de novo transposable element (TE) family identification and modeling package. At the heart of RepeatModeler are three de-novo repeat finding programs ( RECON, RepeatScout and LtrHarvest/Ltr_retriever ) which employ complementary computational methods for identifying repeat element boundaries and family relationships from sequence data. RepeatModeler assists in automating the runs of the various algorithms given a genomic database, clustering redundant results, refining and classifying the families and producing a high quality library of TE families suitable for use with RepeatMasker and ultimately for submission to the Dfam database (http://dfam.org).
toolshed.g2.bx.psu.edu/repos/iuc/roary/roary/3.13.0+galaxy3	"Roary
 Roary is a high speed stand alone pan genome pipeline, which takes annotated assemblies in GFF3 format (produced by Prokka) and calculates the pan genome. Using a standard desktop PC, it can analyse datasets with thousands of samples, something which is computationally infeasible with existing methods, without compromising the quality of the results. 128 samples can be analysed in under 1 hour using 1 GB of RAM and a single processor. To perform this analysis using existing methods would take weeks and hundreds of GB of RAM. To use Roary, select two or more gff3 files OR a collection of gff3 files 
Options
: - Minimum percentage identity for blastp - an integer, default is [95] - Percentage of isolates a gene must be in to be core - a float, default is [99.0] 
Advanced Options
: - Maximum number of clusters - integer, default is [50000] - Don't split paralogs - check box - Translation table - which translation table to use, an integer, default is [11] - Change the MCL inflation value - a float, default is [1.5] For further info see: http://sanger-pathogens.github.io/Roary/"
toolshed.g2.bx.psu.edu/repos/iuc/microsatbed/microsatbed/1.3.3+galaxy1	"Convert short repetitive sequences to bed features or windowed density bigwigs
 Microsatellites are usually defined as repeated short DNA patterns in an unbroken sequence. A microsatellite pattern or 
motif
 can be any combination nucleotides, typically from 1 to 6nt in length. This tool allows microsatellite and related features to be selected from a fasta sequence input file, and output into a track, suitable for viewing in a genome browser such as JBrowse2. All motifs of selected lengths can be reported as individual features in the output bed file, or specific motifs can be provided and all others will be ignored. In all cases, a minimum required number of repeats can be specified. For example, requiring 2 or more repeats of the trimer 
ACG
 will report every sequence of 
ACGACG
 or 
ACGACGACG
 or 
ACGACGACGACG
 and so on, as individual bed features. Similarly, requiring 3 repeats of any trimer will report every distinct 3 nucleotide pattern, including 
ACGACGACG
 as well as every other unique 3 nucleotide pattern with 3 sequential repeats or more such, as ""CTCCTCCTC
. For other output formats, the pytrf native command line 
findstr
 can be used to produce a gff, csv or tsv output containing all exact short tandem repeats, as described at the end of https://pytrf.readthedocs.io/en/latest A fasta file must be supplied for processing. A built in genome can be selected, or a fasta file of any kind can be selected from the current history. Note that all symbols are treated as valid nucleotides by pytrf, so extraneous characters such as 
-
 or 
N
 in the input fasta may appear as unexpected bed features. Lower case fasta symbols will be converted to uppercase, to prevent them being reported as distinct motifs. Output can be bed format, or for two kinds of operation, a bigwig track showing bases covered by selected features over a configurable window size with a default of 128nt. 
Select motifs by length - for bed or windowed density bigwig
 The default tool form setting is to select all dimer motif patterns. Any combination of motif lengths from 1 to 6nt can be selected in the multiple-select drop-down list. All features will be returned in a single bed file. For each selected motif length, the minimum number of repeats required for reporting can be adjusted. 
Tandem repeats
 are defined as at least 2 of any pattern. This tool allows singleton dimer motifs to be reported, so is not restricted to short tandem repeats (STR) This mode of operation can produce a bed file with every STR as a separate feature. These can be very large and a bigwig containing the sum of STR bases over a selectable window size (default 128) may be more useful and much faster to load. 
Select motifs by pattern - for bed or windowed density bigwig
 This option allows a motif pattern to be specified as a text string such as 
CG
 or 
ATC
. Multiple motifs can be specified as a comma separated string such as 
CG,ATC
. All features will be returned as a single bed file. The minimum number of repeats for all motifs can be set to match specific requirements. For example, technical sequencing read bias may be influenced by the density of specific dimers, whether they are repeated or not such as in https://github.com/arangrhie/T2T-Polish/tree/master/pattern This mode of operation can produce a bed file with every STR as a separate feature. These can be very large and a bigwig containing the sum of STR bases over a selectable window size (default 128) may be more useful and much faster to load. 
Select all perfect STR using pytrf findstr in csv, tsv or gff output format
 This selection runs the pytrf 
findstr
 option to create gff/csv/tsv outputs as described at the end of https://pytrf.readthedocs.io/en/latest/. Quoted here: 
A Tandem repeat (TR) in genomic sequence is a set of adjacent short DNA sequence repeated consecutively. The core sequence or repeat unit is generally called motif. According to the motif length, tandem repeats can be classified as microsatellites and minisatellites. Microsatellites are also known as simple sequence repeats (SSRs) or short tandem repeats (STRs) with motif length of 1-6 bp. Minisatellites are also sometimes referred to as variable number of tandem repeats (VNTRs) has longer motif length than microsatellites. Pytrf is a lightweight Python C extension for identification of tandem repeats. The pytrf enables to fastly identify both exact or perfect SSRs. It also can find generic tandem repeats with any size of motif, such as with maximum motif length of 100 bp. Additionally, it has capability of finding approximate or imperfect tandem repeats*"
toolshed.g2.bx.psu.edu/repos/iuc/spaln/spaln/2.4.9+galaxy0	"Spaln_ (space-efficient spliced alignment) is a stand-alone program that maps and aligns a set of cDNA or protein sequences onto a whole genomic sequence in a single job. This Galaxy wrapper currently only supports the default (i.e. 
-O3
) algorithm for Spaln. This algorithm takes FASTA format query and genome sequence and finds an alignment of the query (either cDNA or protein) against the genome. Spaln optionally takes a species name to use for parameter setting (the ""-T"" parameter). The ""List spaln parameter tables"" (list_spaln_tables) can be used to find a parameter file that is close (in terms of taxonomic distance) to your species of interest. Use of this setting is recommended. .. _Spaln: https://github.com/ogotoh/spaln"
toolshed.g2.bx.psu.edu/repos/bgruening/tapscan/tapscan_classify/4.76+galaxy0	"What it does
 TAPscan is a comprehensive tool for annotating TAPs with a special focus on species belonging to the Archaeplastida. In general, the detection of TAPs is based on the detection of highly conserved protein domains. During the first step, each sequence out of a species protein set is scanned for protein domains (stored as profile Hidden Markov Models) using hmmsearch. The domains list consists of 154 profile HMMs and functions as the domain reference during the hmmsearch command. Afterwards, by running TAPscan, specialized rules are applied to finally assign the protein sequences to TAP families based on the detected domains in the previous step. With the latest TAPscan v4, a protein set can be scanned for 137 different TAP families with high accuracy through applying GA-thresholds and coverage values. 
Output Files
 TAPscan provides the user with three different output files. Each output file is tab-separated. - 
Output 1: ""Detected TAPs""
 - contains the detected domains and finally assigned TAP family for each gene ID. If domains are assigned to a sequence but not all rules are fulfilled, the sequence is assigned to “0_no_family_found”. - 
Output 2: ""Family Counts""
 is a summary of the number of members for each TAP family. - 
Output 3: ""Detected TAPs Extra""
 - is similar to output 1 but contains additional information about subfamilies."
toolshed.g2.bx.psu.edu/repos/iuc/tb_variant_filter/tb_variant_filter/0.4.0+galaxy0	"This tool offers multiple options for filtering variants (in VCF files, relative to M. tuberculosis H37Rv). It currently has 5 main modes: 1. Filter by region. Mask out variants in certain regions. Region lists available as: 1. Refined Low Confidence (RLC) regions from 
Marin et al 2022 &lt;https://doi.org/10.1093/bioinformatics/btac023&gt;
 2. Refined Low Confidence (RLC) and Low Mappability regions from 
Marin et al 2022 &lt;https://doi.org/10.1093/bioinformatics/btac023&gt;
 3. PE/PPE genes from 
Fishbein et al 2015 &lt;https://web.archive.org/web/20210929032252/https://onlinelibrary.wiley.com/doi/10.1111/mmi.12981&gt;
 4. 
TBProfiler &lt;http://tbdr.lshtm.ac.uk/&gt;
 list of antibiotic resistant genes 5. 
MTBseq &lt;https://github.com/ngs-fzb/MTBseq_source&gt;
 list of antibiotic resistant genes 6. 
UVP &lt;https://github.com/CPTR-ReSeqTB/UVP&gt;
 list of repetitive loci in M. tuberculosis genome 2. Filter by window around indels. Masks out variants within a certain distance (by default 5 bases) of an insertion or deletion site. 3. Filter by percentage of alternate allele bases. Mask out variants with less than a minimum percentage (by default 90%) alternative alleles. 4. Filter by depth of aligned reads. 5. Filter out all variants that are not SNV (single nucleotide variants). For region filtering, the default choice is to use the RLC regions. These are based on 
Marin et al 2022 &lt;https://doi.org/10.1093/bioinformatics/btac023&gt;
_, a study of regions of the M. tuberculosis H37Rv genome where Illumina reads don't map well. If you are using reads shower than 100 base pairs or single-ended reads, you should use the RLC and Low Mappability region filter. The PE/PPE and UVP region filters are retained for backward compatibility but the afore-mentioned paper has shown that they exclude too much of the genome from analysis. When used together the effects of the filters are added (i.e. a variant is masked out if it is masked by any of the filters)."
toolshed.g2.bx.psu.edu/repos/iuc/tbvcfreport/tbvcfreport/1.0.1+galaxy0	"tbvcfreport - 1.0.1
 
tbvcfreport
 takes SnpEff annotated M.tuberculosis VCF file(s) and generates an HTML-based report with data from Combat-TB-NeoDB and links to Combat-TB-eXplorer (https://explorer.sanbi.ac.za/). 
tbvcfreport
 will generate an HTML-based Drug Resistance report if provided with a TBProfiler json report. Drug resistance predictions are for 
Research Purposes Only
 and are produced by TBProfiler (https://github.com/jodyphelan/TBProfiler) 
Inputs
 - SnpEff annotated M.tuberculosis VCF file(s) - vcf - 
required
 - TBProfiler Drug Resistance JSON Report - json - 
optional
 
Outputs
 - Variant Report in HTML and TXT format - Drug Resistance Report in HTML and TXT format if provided with a TBProfiler json report 
Advanced options
: - database_uri - String - Use an on-premise COMBAT-TB-NeoDB (default 'neodb.sanbi.ac.za') - 
optional
 
Further information
 For more on the 
COMBAT-TB Project
, see https://combattb.org. For more on 
tbvcfreport
, see https://github.com/COMBAT-TB/tbvcfreport. For more on 
COMBAT-TB-NeoDB
, see https://github.com/COMBAT-TB/combat-tb-neodb."
toolshed.g2.bx.psu.edu/repos/iuc/tbprofiler/tb_profiler_profile/6.6.4+galaxy0	"Summary ======= The pipeline aligns reads to the H37Rv reference using BWA, bowtie2 or minimap2 and then calls variants (using bcftools, GATK4 or freebayes). These variants are then compared to a drug-resistance database. TB-Profiler also predicts the number of reads supporting drug resistance variants as an insight into hetero-resistance (not applicable for MinION data). Produces a JSON output file by default. In the Advanced options, you can select the mapper and variant caller to use, as well as set a number of filtering parameters. Each of these typically has a ""hard"" and a ""soft"" cutoff. If a variant value is below the ""hard"" cutoff it is discarded, if it is between the ""hard"" and ""soft"" values it is removed from the final predictions but added to the ""qc_fail_variants"" section of the output. One of the advanced options is to use the SUSPECT tool suite to predict resistance to 
rifampicin (RIF)
, 
pyrazinamide (PZA)
 and 
bedaquiline (BDQ)
_. .. _rifampicin (RIF): https://www.nature.com/articles/s41598-020-74648-y .. _pyrazinamide (PZA): https://www.nature.com/articles/s41598-020-58635-x .. _bedaquiline (BDQ): https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0217169"
toolshed.g2.bx.psu.edu/repos/iuc/tetyper/tetyper/1.1+galaxy2	"What it does
 TETyper is designed for typing a specific transposable element (TE) of interest from paired-end sequencing data. It determines single nucleotide variants (SNVs) and deletions within the TE, as well as flanking sequences surrounding the TE. 
Input
 
SNP Profiles
: A tab-delimited file with the following columns: 1. Profile ID 2. Homozygous SNPs 3. Heterozygous SNPs SNPs are represented in the format [REF][POSITION][ALT], and separated by pipe (
|
) characters. SNPs should be ordered by position. Valid alt-bases for heterozygous SNPs are: 
M,R,W,S,Y,K
 For example: :: 1 none none 2 C8015T none 3 C8015T|T9621C none 4 T7199A|C8015T|T9621C none 6 C7509G|T7917G none N2 none C8015Y N4 none A5178R N5 none C8015Y|T9663Y 
Structural Variant Profiles
: A tab-delimited file with the following columns: 1. Profile ID 2. Structural Variants Structural Variants are represented in the format [START-POSITION]-[END-POSITION], and separated by pipe (
|
) characters. For example: :: Tn4401b none Tn4401a 7020-7118 Tn4401h 6919-7106 Tn4401_truncC 1-7127|9198-10006 
Output
 TETyper will produce a tab-seperated output file with the following outputs: +--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Column | Description | +==========================+==================================================================================================================================================================================================================================================+ | Deletions | A list of sequence ranges corresponding to regions of the reference classified as deletions for this sample, or ""none"" for no deletions. | +--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Structural_variant | If --struct_profiles is specified and the pattern of deletions above corresponds to one of these profiles, then the profile name is given, otherwise ""unknown"". | +--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | SNPs_homozygous | A list of homozygous SNPs identified, or ""none"". | +--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | SNPs_heterozygous | A list of heterozygous SNPs identified, or ""none"". | +--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Heterozygous_SNP_counts | For each heterozygous SNP, the number of reads supporting the reference and alternative calls, or ""none"" if there are no heterozygous SNPs. | +--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | SNP_variant | If --snp_profiles is specified and the pattern of homozygous and heterozygous SNPs corresponds to one of these profiles, then the profile name is given. Otherwise ""unknown"". | +--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Combined_variant | Single name combining Structural_variant and SNP_variant, separated by ""-"". | +--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Left_flanks | A list of distinct sequences passing quality filters that flank the start position of the reference. | +--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Right_flanks | A list of distinct sequences passing quality filters that flank the end position of the reference. | +--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Left_flank_counts | The number of high quality reads supporting each of the left flanking sequences. | +--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Right_flank_counts | The number of high quality reads supporting each of the right flanking sequences. | +--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | X_Y_presence | If --show_region is specified as --show_region X-Y, this column shows 1 if the entirety of that region is classified as present (i.e. no overlap with deleted regions), or 0 otherwise. If --show_region is unspecified, this column is omitted. | +--------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
toolshed.g2.bx.psu.edu/repos/iuc/tbl2gff3/tbl2gff3/1.2	"What it does
 Convert a table into GFF3. This is not a very complex or advanced tool, it won't produce GFF with hierarchy, just flat gff3 which may be sufficient for visualisation purposes."
toolshed.g2.bx.psu.edu/repos/bgruening/augustus_training/augustus_training/3.5.0+galaxy0	This tool allows to train Augustus (the ab-initio gene predictor) based on a previous prediction (e.g. made with Maker).
toolshed.g2.bx.psu.edu/repos/iuc/snap_training/snap_training/2013_11_29+galaxy1	This tool allows to train SNAP (an ab-initio gene predictor) based on a previous prediction made with Maker.
toolshed.g2.bx.psu.edu/repos/iuc/transtermhp/transtermhp/2.09.1	"What it does
 Finds rho-independent transcription terminators in bacterial genomes."
toolshed.g2.bx.psu.edu/repos/iuc/windowmasker/windowmasker_mkcounts/1.0	"What it does
 This tool runs 
stage 1 &lt;https://www.ncbi.nlm.nih.gov/IEB/ToolBox/CPP_DOC/lxr/source/src/app/winmasker/&gt;
_ of the WindowMasker analysis to produce a unit counts file for a genome assembly."
toolshed.g2.bx.psu.edu/repos/iuc/windowmasker/windowmasker_ustat/1.0	"What it does
 This tool runs 
stage 2 &lt;https://www.ncbi.nlm.nih.gov/IEB/ToolBox/CPP_DOC/lxr/source/src/app/winmasker/&gt;
 of the WindowMasker analysis to identify repeats within the input sequences. .. class:: infomark 
Output formats:
 * Use the 
binary or text maskinfo ASN.1
 output formats to generate the mask file for the 
NCBI BLAST+ makeblastdb tool &lt;https://www.ncbi.nlm.nih.gov/books/NBK279681/#_cookbook_Create_BLAST_database_with_the_&gt;
 * Use the BED output format to generate a list of masked regions .. class:: infomark 
Advanced options:
 * See the 
WindowMasker README file &lt;https://www.ncbi.nlm.nih.gov/IEB/ToolBox/CPP_DOC/lxr/source/src/app/winmasker/README&gt;
_ for additional details on the WindowMasker repeat masking options"
toolshed.g2.bx.psu.edu/repos/iuc/argnorm/argnorm/1.0.0+galaxy0	argNorm is a tool to normalize antibiotic resistance genes (ARGs) by mapping them to the antibiotic resistance ontology (ARO) created by the CARD database. argNorm also enhances antibiotic resistance gene annotations by providing drug categorization of the drugs that antibiotic resistance genes confer resistance to.
toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/0.2.6+galaxy3	compleasm_ assesses genome completeness based on genome assembly. .. _compleasm: https://github.com/huangnengCSU/compleasm
toolshed.g2.bx.psu.edu/repos/iuc/fargene/fargene/0.1+galaxy1	fARGene (Fragmented Antibiotic Resistance Gene iENntifiEr ) is a tool that takes either fragmented metagenomic data or longer sequences as input and predicts and delivers full-length antiobiotic resistance genes as output. The tool includes developed and optimized models for a number or resistance gene types, and the functionality to create and optimize models of your own choice of resistance genes. The current version of the tool includes developed and optimized models for identification of the following resistance genes - Class A beta-lactamases - Subclass B1 and B2 beta-lactamases - Subclass B3 beta-lactamases - Class C beta-lactamases - Class D beta-lactamases - qnr
toolshed.g2.bx.psu.edu/repos/iuc/fgsea/fgsea/1.8.0+galaxy1	"fgsea_ is a Bioconductor package for fast preranked gene set enrichment analysis (GSEA). The performance is achieved by using an algorithm for cumulative GSEA-statistic calculation. This allows to reuse samples between different gene set sizes. See the preprint_ for algorithmic details. ----- 
Inputs
 
Ranked Genes
 A two-column file containing a ranked list of genes is required. The first column must contain the gene identifiers and the second column the statistic used to rank. Gene identifiers must be unique (not repeated) within the file and must be the same type as the identifiers in the Gene Sets file. Example: ========= ============ Symbol Ranked Stat ========= ============ VDR 67.198 IL20RA 65.963 MPHOSPH10 51.353 RCAN1 50.269 HILPDA 50.015 TSC22D3 47.496 FAM107B 45.926 ========= ============ 
Gene Sets
 A Gene Sets file is required. This can be a tabular file in Gene Matrix Transposed (GMT) format. In GMT format, each row represents a gene set, with the set name in the first column, a description in the second, then the identifiers of the genes in the set in the following columns, see the example below. GMT files with any identifiers (e.g. Entrez IDs, Symbols) can be used but the same type of identifiers must be present in the Ranked Genes file. More information on 
GMT format
 can be found at the Broad website. GMT files for human gene sets can be obtained from the Broad's MSigDB
 collections. Example: ================== ================================================================== ====== ===== ==== HALLMARK_APOPTOSIS http://www.broadinstitute.org/gsea/msigdb/cards/HALLMARK_APOPTOSIS CASP3 CASP9 ... HALLMARK_HYPOXIA http://www.broadinstitute.org/gsea/msigdb/cards/HALLMARK_HYPOXIA PGK1 PDK1 ... ================== ================================================================== ====== ===== ==== Alternatively, an RData file containing a collection of gene sets can be input, like the ones provided here_ containing mouse versions of the MSigDB collections. ----- 
Outputs
 * A Tabular file of gene set rankings * A PDF with plots of top pathways (optional) ----- Wrapper released under MIT License. Copyright (c) 2017 Mark Dunning .. 
fgsea: https://bioconductor.org/packages/release/bioc/html/fgsea.html .. _preprint: http://biorxiv.org/content/early/2016/06/20/060012 .. _GMT format: https://software.broadinstitute.org/cancer/software/gsea/wiki/index.php/Data_formats#GMT:_Gene_Matrix_Transposed_file_format
.28.2A.gmt.29 .. _MSigDB: http://software.broadinstitute.org/gsea/msigdb/collections.jsp .. _here: http://bioinf.wehi.edu.au/software/MSigDB/index.html"
toolshed.g2.bx.psu.edu/repos/iuc/gprofiler_convert/gprofiler_convert/0.1.7+galaxy11	"What it does
 This tool wraps gprofiler2 R package which performs a request to g:Profiler g:Convert tool through its API. g:Profiler g:Convert uses the information in Ensembl databases to handle hundreds of types of identifiers for genes, proteins, transcripts, microarray probesets, etc, for many species, experimental platforms and biological databases. The input is flexible: it accepts a mixed list of IDs and recognises their types automatically. It can also serve as a service to get all genes belonging to a particular functional category. ----- This tool is part of the 
g:Profiler
 from the 
University of Tartu
. .. 
g:Profiler: https://biit.cs.ut.ee/gprofiler/ .. _University of Tartu: https://ut.ee/en/ 
Resources
 * 
A complete list of suppоrted organism IDs
 * 
A complete list of supported namespaces
_ .. _A complete list of suppоrted organism IDs: https://biit.cs.ut.ee/gprofiler/page/organism-list .. _A complete list of supported namespaces: https://biit.cs.ut.ee/gprofiler/page/namespaces-list 
gprofiler2 R tool
 * https://CRAN.R-project.org/package=gprofiler2 — as CRAN package. * https://anaconda.org/conda-forge/r-gprofiler2 — as conda-forge package. 
Contact
 * Contact us at biit.support@ut.ee for further help. .. class:: warningmark By default the tool makes requests to APIs of the latest g:Profiler instance with the most recent data release. Please, use ’Tool Settings → Base URL’ to modify it in case you want to make use of older data versions. Also, if the results should be pinned for reproducibility, we suggest to modify the base url to the last archived data version."
toolshed.g2.bx.psu.edu/repos/iuc/gprofiler_gost/gprofiler_gost/0.1.7+galaxy11	"What it does
 This tool wraps gprofiler2 R package which performs a request to g:Profiler g:GOSt tool through its API. g:Profiler g:GOSt performs functional enrichment analysis of gene lists. The output is a table with the results and (optionally) a publication ready plot similar to the one shown in the g:GOSt web tool. ----- This tool is part of the 
g:Profiler
 from the 
University of Tartu
. .. 
g:Profiler: https://biit.cs.ut.ee/gprofiler/ .. _University of Tartu: https://ut.ee/en/ 
Resources
 * 
A complete list of suppоrted organism IDs
 * 
A complete list of supported namespaces
_ .. _A complete list of suppоrted organism IDs: https://biit.cs.ut.ee/gprofiler/page/organism-list .. _A complete list of supported namespaces: https://biit.cs.ut.ee/gprofiler/page/namespaces-list 
gprofiler2 R tool
 * https://CRAN.R-project.org/package=gprofiler2 — as CRAN package. * https://anaconda.org/conda-forge/r-gprofiler2 — as conda-forge package. 
Contact
 * Contact us at biit.support@ut.ee for further help. .. class:: warningmark By default the tool makes requests to APIs of the latest g:Profiler instance with the most recent data release. Please, use ’Tool Settings → Base URL’ to modify it in case you want to make use of older data versions. Also, if the results should be pinned for reproducibility, we suggest to modify the base url to the last archived data version."
toolshed.g2.bx.psu.edu/repos/iuc/gprofiler_orth/gprofiler_orth/0.1.7+galaxy11	"What it does
 This tool wraps gprofiler2 R package which performs a request to g:Profiler g:Orth tool through its API. g:Profiler g:Orth retrieves the genes of the target organism that are similar in sequence to the source organism genes in the input. ----- This tool is part of the 
g:Profiler
 from the 
University of Tartu
. .. 
g:Profiler: https://biit.cs.ut.ee/gprofiler/ .. _University of Tartu: https://ut.ee/en/ 
Resources
 * 
A complete list of suppоrted organism IDs
 * 
A complete list of supported namespaces
_ .. _A complete list of suppоrted organism IDs: https://biit.cs.ut.ee/gprofiler/page/organism-list .. _A complete list of supported namespaces: https://biit.cs.ut.ee/gprofiler/page/namespaces-list 
gprofiler2 R tool
 * https://CRAN.R-project.org/package=gprofiler2 — as CRAN package. * https://anaconda.org/conda-forge/r-gprofiler2 — as conda-forge package. 
Contact
 * Contact us at biit.support@ut.ee for further help. .. class:: warningmark By default the tool makes requests to APIs of the latest g:Profiler instance with the most recent data release. Please, use ’Tool Settings → Base URL’ to modify it in case you want to make use of older data versions. Also, if the results should be pinned for reproducibility, we suggest to modify the base url to the last archived data version."
toolshed.g2.bx.psu.edu/repos/iuc/gprofiler_random/gprofiler_random/0.1.7+galaxy11	"What it does
 This tool performs a request to g:Profiler API to fetch a set of pseudorandom gene IDs. Gene IDs are selected in a way that g:GOSt output with high probability will contain a small number of results with low enough p-values to be considered as significant. ----- This tool is part of the 
g:Profiler
 from the 
University of Tartu
. .. 
g:Profiler: https://biit.cs.ut.ee/gprofiler/ .. _University of Tartu: https://ut.ee/en/ 
Resources
 * 
A complete list of suppоrted organism IDs
 * 
A complete list of supported namespaces
_ .. _A complete list of suppоrted organism IDs: https://biit.cs.ut.ee/gprofiler/page/organism-list .. _A complete list of supported namespaces: https://biit.cs.ut.ee/gprofiler/page/namespaces-list 
gprofiler2 R tool
 * https://CRAN.R-project.org/package=gprofiler2 — as CRAN package. * https://anaconda.org/conda-forge/r-gprofiler2 — as conda-forge package. 
Contact
 * Contact us at biit.support@ut.ee for further help. .. class:: warningmark By default the tool makes requests to APIs of the latest g:Profiler instance with the most recent data release. Please, use ’Tool Settings → Base URL’ to modify it in case you want to make use of older data versions. Also, if the results should be pinned for reproducibility, we suggest to modify the base url to the last archived data version."
toolshed.g2.bx.psu.edu/repos/iuc/gprofiler_snpense/gprofiler_snpense/0.1.7+galaxy11	"What it does
 This tool wraps gprofiler2 R package which performs a request to g:Profiler g:SNPense tool through its API. g:Profiler g:SNPense maps SNP rs identifiers to chromosome positions, genes and variant effects. Available only for human SNPs. ----- This tool is part of the 
g:Profiler
 from the 
University of Tartu
. .. 
g:Profiler: https://biit.cs.ut.ee/gprofiler/ .. _University of Tartu: https://ut.ee/en/ 
Resources
 * 
A complete list of suppоrted organism IDs
 * 
A complete list of supported namespaces
_ .. _A complete list of suppоrted organism IDs: https://biit.cs.ut.ee/gprofiler/page/organism-list .. _A complete list of supported namespaces: https://biit.cs.ut.ee/gprofiler/page/namespaces-list 
gprofiler2 R tool
 * https://CRAN.R-project.org/package=gprofiler2 — as CRAN package. * https://anaconda.org/conda-forge/r-gprofiler2 — as conda-forge package. 
Contact
 * Contact us at biit.support@ut.ee for further help. .. class:: warningmark By default the tool makes requests to APIs of the latest g:Profiler instance with the most recent data release. Please, use ’Tool Settings → Base URL’ to modify it in case you want to make use of older data versions. Also, if the results should be pinned for reproducibility, we suggest to modify the base url to the last archived data version."
toolshed.g2.bx.psu.edu/repos/iuc/progressivemauve/progressivemauve/2015_02_13.1	"What it does ============ Mauve is a system for efficiently constructing multiple genome alignments in the presence of large-scale evolutionary events such as rearrangement and inversion. Multiple genome alignment provides a basis for research into comparative genomics and the study of evolutionary dynamics. Aligning whole genomes is a fundamentally different problem than aligning short sequences. Mauve has been developed with the idea that a multiple genome aligner should require only modest computational resources. It employs algorithmic techniques that scale well in the amount of sequence being aligned. For example, a pair of Y. pestis genomes can be aligned in under a minute, while a group of 9 divergent Enterobacterial genomes can be aligned in a few hours. progressiveMauve XMFA alignment visualized with the Mauve tool: .. image:: $PATH_TO_IMAGES/hemolysin.jpg Example Usage ============= +-----------------------------------+-------------+ | Usage | Notes | +===================================+=============+ | Align genomes |Simply | | |select as | | |many fasta | | |files with | | |one or more | | |sequences as | | |necessary | +-----------------------------------+-------------+ | Align genomes but also save |Use the | | the guide tree and produce a |
Output | | backbone file |Guide Tree
 | | |and 
Output | | |Backbone
 | | |options | +-----------------------------------+-------------+ | Align genomes, but do not |Use the | | detect forced alignment of |
Disable | | unrelated sequences |backbone
 | | |option | +-----------------------------------+-------------+ | Detect forced alignment of |Use the | | unrelated sequence in the |
Apply | | alignment produced |Backbone
 | | in previous example, use |option and | | custom Homology HMM transition |specify the | | parameters. |XMFA file | | |produced | | |in the | | |previous | | |example | +-----------------------------------+-------------+ | Compute ungapped |Use the | | local-multiple alignments among |
MUMs
 | | the input sequences |option | +-----------------------------------+-------------+ | Compute an alignment of the |Set the | | same genomes, using previously |
Match | | computed local-multiple |Input
 to | | alignments |the tabular | | |MUMs file | | |produced in | | |the previous | | |example | +-----------------------------------+-------------+ | Set a minimum scaled |Use the | | breakpoint penalty to cope with |
Min Scaled | | the case where most genomes |Penalty
 and| | are aligned correctly, but manual |set to a | | inspection reveals that |value like | | a divergent genome has too |5000 | | many predicted rearrangements. | | +-----------------------------------+-------------+ | Globally align a set of |Use the | | collinear virus |
Colinear
,| | genomes, using seed families |
Seed | | to improve anchoring sensitivity |Family
 | | in regions below 70% sequence |options | | identity. | | +-----------------------------------+-------------+ The progressiveMauve algorithm: addressing limitations of the original algorithm ================================================================================ Comparative genomics has revealed that closely-related bacteria often have highly divergent gene content. While the original Mauve algorithm could align regions conserved among all organisms, the portion of the genome conserved among all taxa (the core genome) shrinks as more taxa are added to the analysis. As such, the original Mauve algorithm did not scale well to large numbers of taxa because it could not align regions conserved among subsets of the genomes under study. progressiveMauve employs a different algorithmic approach to scoring alignments that allows alignment of segments conserved among subsets of taxa. The progressiveMauve algorithm has been described in Aaron Darling's Ph.D. Thesis, and is also the subject of a manuscript published in PLoS ONE. A brief overview is given here. Finding initial local multiple alignments ----------------------------------------- progressiveMauve elaborates on the original algorithm for finding local multiple alignments. Instead of using a single seed pattern for match filtration, progressiveMauve uses a combination of three seed patterns for improved sensitivity. The palindromic seed patterns have been described in Darling et al. 2006 ""Procrastination leads to efficient filtration for local multiple alignment"" Seed matches which represent a unique subsequence shared by two or more input genomes are subjected to ungapped extension until the seed pattern no longer matches. The result is an ungapped local multiple alignment with at most one component from each of the input genome sequences. Computing a pairwise genome content distance matrix and guide tree ------------------------------------------------------------------ progressiveMauve builds up genome alignments progressively according to a guide tree. The guide tree is computed based on an estimate of the shared gene content among each pair of input genomes. For a pair of input genomes, g.x and g.y, shared gene content is estimated by counting the number of nucleotides in gx and gy aligned to each other in the initial set of local multiple alignments. The count is normalized to a similarity value between 0 and 1 by dividing by the average size of gx and gy. The similarity value is subtracted from 1 to arrive at a distance estimate. Neighbor joining is then applied to the matrix of distance estimates to yield a guide tree topology. Note that the guide tree is not intended to be a phylogeny indicative of the genealogy of input genomes. It is merely a computational crutch for progressive genome alignment. Also note that alignments are later refined independently of a single guide tree toplogy to avoid biasing later phylogenetic inference. Computing a pairwise breakpoint distance matrix ----------------------------------------------- Prior to alignment, progressiveMauve attempts to compute a conservative estimate of the number of rearrangement breakpoints among any pair of genomes. For each pair of genomes, pairwise alignments are created from the local-multiple alignments and the pairwise alignments are subjected to greedy breakpoint elimination. The breakpoint penalty used for greedy breakpoint elimination is set high for closely related genomes and scaled downward according to the estimate of genomic content distance. Because the breakpoint penalty is high, the resulting set of locally collinear blocks represent robustly supported segmental homology, and a conservative estimate of the breakpoint distance can be made on this basis. The conservative estimate of breakpoint distance is used later during progressive alignment to scale breakpoint penalties. Progressive genome alignment ---------------------------- A genome alignment is progressively built up according to the guide tree. At each step of the progressive genome alignment, alignment anchors are selected from the initial set of local multiple alignments. Anchors are selected so that they maximize a Sum-of-pairs scoring scheme which applies a penalty for predicting breakpoints among any pair of genomes. Because rates of genomic rearrangement are highly variable, especially in some bacterial pathogens, some genomes may be expected to exhibit greater rearrangement than others. As such, a single choice of scoring penalty is unlikely to yield accurate alignments for all genomes. To cope with this phenomenon, progressiveMauve scales the breakpoint penalty according to the expected level of sequence divergence and the number of well-supported genomic rearrangements among the pair of input genomes. These scaling values are taken from the distance matrices computed earlier in the algorithm. Anchored alignment ------------------ Once anchors have been computed at a node in the guide tree, a global alignment is computed on the basis of the anchors. Given a set of anchors among two genomes, a genome and an alignment, or a pair of alignments, a modified MUSCLE global alignment algorithm is applied to compute an anchored profile-profile alignment. MUSCLE is then used to perform tree-independent iterative refinement on the global genome alignment. Rejecting alignment of unrelated sequence ----------------------------------------- Although we compute a global alignment among sequences, genomes often contain lineage-specific sequence and are thus not globally related. The global alignment will often contain forced alignment of unrelated sequence. A simple hidden Markov model structure is used to detect forced alignment of unrelated sequence, which are then removed from the alignment. Strengths of the progressiveMauve algorithm ------------------------------------------- - It can be applied to a much larger number of genomes than the original Mauve algorithm - It can align more divergent genomes than the original algorithm. Genomes with as little as 50% nucleotide identity can be alignable - Manual adjustment of the alignment scoring parameters is usually not necessary - It aligns the pan-genome, e.g. regions conserved among subsets of the input genomes - It is more accurate than the previous Mauve algorithm Notes on Reproducibility ------------------------ The command line programme progressiveMauve seems to behave differently when:: --max-breakpoint-distance-scale=0.5 --conservation-distance-scale=0.5 are passed to the tool, compared to when those options are not passed. This means that if you wish to precisely replicate the results you see in Galaxy at the command line, you'll need to pass these flags with their ""default"" values. @ATTRIBUTION@"
toolshed.g2.bx.psu.edu/repos/iuc/seq2hla/seq2hla/2.3+galaxy0	"seq2HLA
 
HLA typing from RNA-Seq sequence reads
 Release: 2.2 seq2HLA_ is an in-silico method, written in python and R, which takes standard RNA-Seq sequence reads in fastq format as input, uses a bowtie index comprising all HLA alleles and outputs the most likely HLA class I and class II genotypes (in 4 digit resolution), a p-value for each call, and the expression of each class 
Inputs
 Paired read fastq files with illumina style IDs. 
Outputs
 1. <prefix>-ClassI.HLAgenotype2digits => 2 digit result of Class I 2. <prefix>-ClassII.HLAgenotype2digits => 2 digit result of Class II 3. <prefix>-ClassI.HLAgenotype4digits => 4 digit result of Class I 4. <prefix>-ClassII.HLAgenotype4digits => 4 digit result of Class II 5. <prefix>.ambiguity => reports typing ambuigities (more than one solution for an allele possible) 6. <prefix>-ClassI.expression => expression of Class I alleles 7. <prefix>-ClassII.expression => expression of Class II alleles ClassI.HLAgenotype4digits ======= ======== =========== ======== ============ #Locus Allele 1 Confidence Allele 2 Confidence ======= ======== =========== ======== ============ A A
03:01 0.000510333 A
02:01' 0.0005975604 B B
50:01 0.001271273 B
58:02 3.52561e-05 C C
04:01 0.06362723 C
06:02 0.04725865 ======= ======== =========== ======== ============ ClassI.expression ======= ====== #Locus RPKM ======= ====== A 89.59 B 139.66 C 184.42 ======= ====== .. _seq2HLA: https://github.com/TRON-Bioinformatics/seq2HLA"
toolshed.g2.bx.psu.edu/repos/iuc/socru/socru/2.1.7	"Socru allows you to easily identify and communicate the order and orientation of complete genomes around ribosomal operons. These large scale structural variants have real impacts on the phenotype of the organism, and with the advent of long read sequencing, we can now start to delve into the mechanisms at work. Documentation can be found at 
&lt;https://github.com/quadram-institute-bioscience/socru&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/staramr/staramr_search/0.11.1+galaxy0	"staramr ======= staramr_ scans bacterial genome contigs against both the ResFinder_, PlasmidFinder_ and PointFinder_ databases (used by their respective webservices_) and compiles a summary report of detected antimicrobial resistance genes. Usage ----- 1. Select your genome contigs (in FASTA format). 2. Select whether or not you wish to scan your genome for point mutations giving antimicrobial resistance using the PointFinder database. This requires you to specify the specific organism you are scanning. 3. Run the tool. Input ----- Genomes 
`````` staramr_ takes as input one or more assembled genomes (in FASTA format) to search for AMR genes. Exclude genes file `````````````````` Setting **Provide a custom list of AMR genes to exclude** in the **Advanced options** allows you to pass a file containing a list of genes to exclude from the results. The file must start with a line **#gene_id** and the gene names must correspond to the sequence IDs in the ResFinder/PointFinder databases. For example: :: #gene_id aac(6')-Iaa_1_NC_003197 ColpVC_1__JX133088 Complex mutations file `````````````````````` Complex mutations describe multiple point mutations that must be simultaneously present in order to confer resistance. Complex mutations may be specified by the user using a TSV-formatted file with the following format: :: positions mandatory phenotype mutation(s) mutation(s) phenotype Where ""positions"" are all the point mutations to group into the complex mutation (optional and mandatory), ""mandatory"" are all the point mutations that must be present for the complex mutation to be reported (mandatory is a subset of positions), and phenotype is the phenotype that is conferred when this set of mutations is present. Output ------ There are 6 different output files produced by
staramr
as well as a collection of additional files. mlst.tsv ```````````````````` A tabular file of each multi-locus sequence type (MLST) and it's corresponding locus/alleles, one genome per line. * Isolate ID: The id of the isolate/genome file(s) passed to staramr. * Scheme: The scheme that MLST has identified. * Sequence Type: The sequence type that's assigned when combining all allele types * Locus #: A particular locus in the specified MLST scheme. Example : ```````````````````` +------------+---------------------+---------------+---------+---------+---------+---------+---------+---------+----------+ | Isolate ID | Scheme | Sequence Type | Locus 1 | Locus 2 | Locus 3 | Locus 4 | Locus 5 | Locus 6 | Locus 7 | +============+=====================+===============+=========+=========+=========+=========+=========+=========+==========+ | SRR1952908 | senterica_achtman_2 | 11 | aroC(5) | dnaN(2) | hemD(3) | hisD(7) | purE(6) | sucA(6) | thrA(11) | | SRR1952926 | senterica_achtman_2 | 11 | aroC(5) | dnaN(2) | hemD(3) | hisD(7) | purE(6) | sucA(6) | thrA(11) | +------------+---------------------+---------------+---------+---------+---------+---------+---------+---------+----------+ summary.tsv ``````````` The summary.tsv output file generated by staramr contains the following columns: * Isolate ID: The id of the isolate/genome file(s) passed to staramr. * Quality Module: The isolate/genome file(s) pass/fail result(s) for the quality metrics * Genotype: The AMR genotype of the isolate. * Predicted Phenotype: The predicted AMR phenotype (drug resistances) for the isolate. * CGE Predicted Phenotype: The CGE-predicted AMR phenotype (drug resistances) for the isolate. * Plasmid: Plasmid types that were found for the isolate. * Scheme: The MLST scheme used * Sequence Type: The sequence type that's assigned when combining all allele types * Genome Length: The isolate/genome file(s) genome length(s) * N50 value: The isolate/genome file(s) N50 value(s) * Number of Contigs Greater Than Or Equal To 300 bp: The number of contigs greater or equal to 300 base pair in the isolate/genome file(s) * Quality Module Feedback: The isolate/genome file(s) detailed feedback for the quality metrics Example : ``````````` +------------+----------------+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------+---------------------+---------------+---------------+-----------+---------------------------------------------------+-------------------------+ | Isolate ID | Quality Module | Genotype | Predicted Phenotype | CGE Predicted Phenotype | Plasmid | Scheme | Sequence Type | Genome Length | N50 value | Number of Contigs Greater Than Or Equal To 300 bp | Quality Module Feedback | +============+================+===========================================================+===========================================================================================================+===========================================================================================================================================================================================+==============================================+=====================+===============+===============+===========+===================================================+=========================+ | SRR1952908 | Passed | aadA1, aadA2, blaTEM-57, cmlA1, gyrA (S83Y), sul3, tet(A) | streptomycin, ampicillin, chloramphenicol, ciprofloxacin I/R, nalidixic acid, sulfisoxazole, tetracycline | Spectinomycin, Streptomycin, Amoxicillin, Ampicillin, Cephalothin, Piperacillin, Ticarcillin, Chloramphenicol, Nalidixic acid, Ciprofloxacin, Sulfamethoxazole, Doxycycline, Tetracycline | ColpVC, IncFIB(S), IncFII(S), IncI1-I(Alpha) | senterica_achtman_2 | 11 | 4785500 | 250423 | 41 | | | SRR1952926 | Passed | blaTEM-57, gyrA (S83Y), tet(A) | ampicillin, ciprofloxacin I/R, nalidixic acid, tetracycline | Amoxicillin, Ampicillin, Cephalothin, Piperacillin, Ticarcillin, Nalidixic acid, Ciprofloxacin, Doxycycline, Tetracycline | ColpVC, IncFIB(S), IncFII(S), IncI1-I(Alpha) | senterica_achtman_2 | 11 | 4785451 | 228311 | 40 | | +------------+----------------+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------+---------------------+---------------+---------------+-----------+---------------------------------------------------+-------------------------+ detailed_summary.tsv ```````````````````` A detailed summary of all detected AMR genes/mutations/plasmids in each genome/type, one gene/type per line. * Isolate ID: The id of the isolate/genome file(s) passed to staramr. * Data: The particular gene detected from ResFinder, PlasmidFinder, PointFinder, or the sequence type. * Data Type: The type of gene (Resistance or Plasmid), or MLST. * Predicted Phenotype: The predicted AMR phenotype (drug resistances) found in ResFinder/PointFinder. Plasmids will be left blank by default. * CGE Predicted Phenotype: The CGE-predicted AMR phenotype (drug resistances) found in ResFinder/PointFinder. Plasmids will be left blank by default. * %Identity: The % identity of the top BLAST HSP to the gene. * %Overlap: THe % overlap of the top BLAST HSP to the gene (calculated as hsp length/total length * 100). * HSP Length/Total Length The top BLAST HSP length over the gene total length (nucleotides). * Contig: The contig id containing this gene. * Start: The start of the gene (will be greater than End if on minus strand). * End: The end of the gene. * Accession: The accession of the gene from either ResFinder or PlasmidFinder database. Example : ```````````````````` +------------+----------------------------+------------+---------------------+-----------------------------+-----------+----------+-------------------------+-------------+-------+------+-----------+ | Isolate ID | Data | Data Type | Predicted Phenotype | CGE Predicted Phenotype | %Identity | %Overlap | HSP Length/Total Length | Contig | Start | End | Accession | +============+============================+============+=====================+=============================+===========+==========+=========================+=============+=======+======+===========+ | SRR1952908 | ST11 (senterica_achtman_2) | MLST | | | | | | | | | | | SRR1952908 | ColpVC | Plasmid | | | 98.96 | 100.0 | 193/193 | contig00038 | 1618 | 1426 | JX133088 | | SRR1952908 | aadA1 | Resistance | streptomycin | Spectinomycin, Streptomycin | 100.0 | 100.0 | 792/792 | contig00030 | 5355 | 4564 | JQ414041 | +------------+----------------------------+------------+---------------------+-----------------------------+-----------+----------+-------------------------+-------------+-------+------+-----------+ resfinder.tsv ````````````` A tabular file of each AMR gene and additional BLAST information from the **ResFinder** database, one gene per line. * Isolate ID: The id of the isolate/genome file(s) passed to staramr. * Gene: The particular AMR gene detected. * Predicted Phenotype: The predicted AMR phenotype (drug resistances) for this gene. * CGE Predicted Phenotype: The CGE-predicted AMR phenotype (drug resistances) for this gene. * %Identity: The % identity of the top BLAST HSP to the AMR gene. * %Overlap: THe % overlap of the top BLAST HSP to the AMR gene (calculated as hsp length/total length * 100). * HSP Length/Total Length The top BLAST HSP length over the AMR gene total length (nucleotides). * Contig: The contig id containing this AMR gene. * Start: The start of the AMR gene (will be greater than End if on minus strand). * End: The end of the AMR gene. * Accession: The accession of the AMR gene in the ResFinder database. * Sequence: The AMR Gene sequence * CGE Notes: Any CGE notes associated with the prediction. Example : ````````````` +------------+--------+---------------------+---------------------------+-----------+----------+-------------------------+-------------+-------+------+-----------+-----------+-----------+ | Isolate ID | Gene | Predicted Phenotype | CGE Predicted Phenotype | %Identity | %Overlap | HSP Length/Total Length | Contig | Start | End | Accession | Sequence | CGE Notes | +============+========+=====================+===========================+===========+==========+=========================+=============+=======+======+===========+===========+===========+ | SRR1952908 | sul3 | sulfisoxazole | Sulfamethoxazole | 100.00 | 100.00 | 792/792 | contig00030 | 2091 | 2882 | AJ459418 | ATGA[...] | | | SRR1952908 | tet(A) | tetracycline | Doxycycline, Tetracycline | 99.92 | 97.80 | 1247/1275 | contig00032 | 1476 | 2722 | AF534183 | ATGT[...] | | +------------+--------+---------------------+---------------------------+-----------+----------+-------------------------+-------------+-------+------+-----------+-----------+-----------+ plasmidfinder.tsv ````````````````` A tabular file of each AMR plasmid type and additional BLAST information from the **PlasmidFinder** database, one plasmid type per line. * Isolate ID: The id of the isolate/genome file(s) passed to staramr. * Plasmid: The particular plasmid type detected. * %Identity: The % identity of the top BLAST HSP to the plasmid type. * %Overlap: The % overlap of the top BLAST HSP to the plasmid type (calculated as hsp length/total length * 100). * HSP Length/Total Length The top BLAST HSP length over the plasmid type total length (nucleotides). * Contig: The contig id containing this plasmid type. * Start: The start of the plasmid type (will be greater than End if on minus strand). * End: The end of the plasmid type. * Accession: The accession of the plasmid type in the PlasmidFinder database. Example : ````````````````` +------------+-----------+-----------+----------+-------------------------+-------------+-------+------+-----------+ | Isolate ID | Plasmid | %Identity | %Overlap | HSP Length/Total Length | Contig | Start | End | Accession | +============+===========+===========+==========+=========================+=============+=======+======+===========+ | SRR1952908 | ColpVC | 98.96 | 100 | 193/193 | contig00038 | 1618 | 1426 | JX133088 | | SRR1952908 | IncFIB(S) | 98.91 | 100 | 643/643 | contig00024 | 10302 | 9660 | FN432031 | +------------+-----------+-----------+----------+-------------------------+-------------+-------+------+-----------+ pointfinder.tsv ``````````````` A tabular file of each AMR point mutation and additional BLAST information from the **PointFinder** database, one gene per line. * Isolate ID: The id of the isolate/genome file(s) passed to staramr. * Gene: The particular AMR gene detected, with the point mutation within. * Predicted Phenotype: The predicted AMR phenotype (drug resistances) for this gene. * CGE Predicted Phenotype: The CGE-predicted AMR phenotype (drug resistances) for this gene. * Type: The type of this mutation from PointFinder (either codon or nucleotide). * Position: The position of the mutation. For codon type, the position is the codon number in the gene, for nucleotide type it is the nucleotide number. * Mutation: The particular mutation. For codon type lists the codon mutation, for nucleotide type lists the single nucleotide mutation. * %Identity: The % identity of the top BLAST HSP to the AMR gene. * %Overlap: The % overlap of the top BLAST HSP to the AMR gene (calculated as hsp length/total length * 100). * HSP Length/Total Length The top BLAST HSP length over the AMR gene total length (nucleotides). * Contig: The contig id containing this AMR gene. * Start: The start of the AMR gene (will be greater than End if on minus strand). * End: The end of the AMR gene. * Pointfinder Position: The Pointfinder-adjusted position, which may be off by one from the sequence position in the case of some indels. * CGE Notes: Any CGE notes associated with the prediction. * CGE Required Mutation: Any additional mutations that CGE predicts are required to confer the CGE predicted phenotype. * CGE Mechanism: The CGE-reported mechanism. * CGE PMID: The PMID ID associated with the CGE prediction. Example : ``````````````` +------------+-------------+-----------------------------------+------------------------------+-------+----------+---------------------+-----------+----------+-------------------------+-------------+--------+--------+----------------------+-----------+-----------------------+---------------------+------------------+ | Isolate ID | Gene | Predicted Phenotype | CGE Predicted Phenotype | Type | Position | Mutation | %Identity | %Overlap | HSP Length/Total Length | Contig | Start | End | Pointfinder Position | CGE Notes | CGE Required Mutation | CGE Mechanism | CGE PMID | +============+=============+===================================+==============================+=======+==========+=====================+===========+==========+=========================+=============+========+========+======================+===========+=======================+=====================+==================+ | SRR1952908 | gyrA (S83Y) | ciprofloxacin I/R, nalidixic acid | Nalidixic acid,Ciprofloxacin | codon | 83 | TCC -&gt; TAC (S -&gt; Y) | 99.96 | 100.00 | 2637/2637 | contig00008 | 22801 | 20165 | S83Y | | | Target modification | 7492118,10471553 | | SRR1952926 | gyrA (S83Y) | ciprofloxacin I/R, nalidixic acid | Nalidixic acid,Ciprofloxacin | codon | 83 | TCC -&gt; TAC (S -&gt; Y) | 99.96 | 100.00 | 2637/2637 | contig00011 | 157768 | 160404 | S83Y | | | Target modification | 7492118,10471553 | +------------+-------------+-----------------------------------+------------------------------+-------+----------+---------------------+-----------+----------+-------------------------+-------------+--------+--------+----------------------+-----------+-----------------------+---------------------+------------------+ settings.txt ```````````` The command-line, database versions, and other settings used to run
staramr`. :: command_line = staramr search --pointfinder-organism enterococcus_faecium -o out pbp5.fa version = 0.10.0 start_time = 2023-08-23 13:37:41 end_time = 2023-08-23 13:37:42 total_minutes = 0.02 resfinder_db_dir = staramr/databases/data/dist/resfinder resfinder_db_url = https://bitbucket.org/genomicepidemiology/resfinder_db.git resfinder_db_commit = fa32d9a3cf0c12ec70ca4e90c45c0d590ee810bd resfinder_db_date = Tue, 24 May 2022 06:51 pointfinder_db_dir = staramr/databases/data/dist/pointfinder pointfinder_db_url = https://bitbucket.org/genomicepidemiology/pointfinder_db.git pointfinder_db_commit = 8c694b9f336153e6d618b897b3b4930961521eb8 pointfinder_db_date = Mon, 01 Feb 2021 15:46 pointfinder_organisms_all = campylobacter, enterococcus_faecalis, enterococcus_faecium, escherichia_coli, helicobacter_pylori, klebsiella, mycobacterium_tuberculosis, neisseria_gonorrhoeae, plasmodium_falciparum, salmonella, staphylococcus_aureus pointfinder_organisms_valid = campylobacter, enterococcus_faecalis, enterococcus_faecium, escherichia_coli, helicobacter_pylori, salmonella plasmidfinder_db_dir = staramr/databases/data/dist/plasmidfinder plasmidfinder_db_url = https://bitbucket.org/genomicepidemiology/plasmidfinder_db.git plasmidfinder_db_commit = c18e08c17a5988d4f075fc1171636e47546a323d plasmidfinder_db_date = Wed, 18 Jan 2023 09:45 mlst_version = 2.23.0 pointfinder_organism = enterococcus_faecium pointfinder_gene_drug_version = 072621.2 resfinder_gene_drug_version = 072621 results.xlsx 
`` An Excel spreadsheet containing the previous 5 files as separate worksheets. BLAST Hits
 The dataset collection 
hits
 stores fasta files of the specific blast hits. .. _staramr: https://github.com/phac-nml/staramr .. _ResFinder: https://bitbucket.org/genomicepidemiology/resfinder_db .. _PlasmidFinder: https://bitbucket.org/genomicepidemiology/plasmidfinder_db .. _PointFinder: https://bitbucket.org/genomicepidemiology/pointfinder_db .. _webservices: http://www.genomicepidemiology.org/services/"
toolshed.g2.bx.psu.edu/repos/iuc/aegean_parseval/aegean_parseval/0.16.0+galaxy1	".. class:: infomark 
Purpose
 ParsEval is a program for comparing two sets of gene annotations for the same sequence. The most common use cases for ParsEval are as follows. - You are annotating a newly assembled genome. The optimal parameter settings for annotation are not clear initially, so you do some exploratory data analysis and try several different parameter settings. You can use ParsEval to identify the similarities and differences between the different annotations you have produced. - You are doing a genome-wide analysis of genes in your favorite organism. There is a gene annotation available from the consortium that sequenced and assembled the genome, but there is a different annotation available at NCBI. Again, ParsEval is the best way to compare these two annotations to quickly identify their similarities and differences. ----- .. class:: infomark 
Input
 Input for ParsEval is two sets of annotations in GFF3 format. ParsEval uses the GenomeTools GFF3 parser, which strictly enforces syntax rules laid out in the GFF3 specification. ParsEval itself does some additional checks on the data to make sure valid comparisons are possible. Any features not directly related to protein-coding genes are ignored. ParsEval will infer features implicitly encoded in the data. For example, if a gene annotation declares 6 exon features but no intron features, ParsEval will infer the 5 corresponding intron features from the exon boundaries. However, if ParsEval sees any intron features in a gene model it will assume all introns are declared explicitly. Violations of that assumption will likely elicit a program error. ParsEval is pretty flexible in handling various common conventions for encoding gene structure: exons + start/stop codons, exons + CDS, CDS + UTRs, etc. Any subset of features that completely captures the gene’s exon/intron structure, CDS(s), and UTRs should be handled correctly. ParsEval requires that gene isoforms be encoded using the feature type mRNA (as opposed to transcript, primary_transcript, or other valid SO terms). For mRNA features lacking an explicitly declared gene parent, ParsEval will create one. Note, however, that ParsEval will treat all such transcripts as belonging to separate distinct genes, which will erroneously inflate summary statistics reported by ParsEval. ----- .. class:: infomark 
Output
 ParsEval output includes a variety of similarity statistics that measure the agreement between the two annotations. Our use of agreement here instead of accuracy is intentional: except in a very few rare cases, you will not be comparing a prediction to a true high-quality “gold standard.” It is much more common to compare two annotation sets whose relative quality is unknown. ParsEval uses the terms reference and prediction only to distinguish the two sets: it makes no assumptions as to their relative quality."
toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_image/2022.09+galaxy4	"THE NEW BANDAGE: As of version 2022.09, the Bandage tool is now built on 
Bandage-NG &lt;https://github.com/asl/BandageNG&gt;
_. This fork of the original Bandage is better maintained with additional features and performance upgrades.
 
Bandage Overview
 Bandage is a GUI program that allows users to interact with the assembly graphs made by de novo assemblers such as Velvet, SPAdes, MEGAHIT and others. De novo assembly graphs contain not only assembled contigs but also the connections between those contigs, which were previously not easily accessible. Bandage visualises assembly graphs, with connections, using graph layout algorithms. Nodes in the drawn graph, which represent contigs, can be automatically labelled with their ID, length or depth. Users can interact with the graph by moving, labelling and colouring nodes. Sequence information can also be extracted directly from the graph viewer. By displaying connections between contigs, Bandage opens up new possibilities for analysing and improving de novo assemblies that are not possible by looking at contigs alone. Bandage works with Graphical Fragment Assembly (GFA) files. For more information about this file format, see here_ .. _here: https://gfa-spec.github.io/GFA-spec/GFA2.html 
Command Documentation
 
Bandage image
 will generate an image file of the graph visualisation. .. image:: $PATH_TO_IMAGES/bandage_graph.png :alt: example bandage plot"
toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_info/2022.09+galaxy2	"THE NEW BANDAGE: As of version 2022.09, the Bandage tool is now built on 
Bandage-NG &lt;https://github.com/asl/BandageNG&gt;
_. This fork of the original Bandage is better maintained with additional features and performance upgrades.
 
Bandage Overview
 Bandage is a GUI program that allows users to interact with the assembly graphs made by de novo assemblers such as Velvet, SPAdes, MEGAHIT and others. De novo assembly graphs contain not only assembled contigs but also the connections between those contigs, which were previously not easily accessible. Bandage visualises assembly graphs, with connections, using graph layout algorithms. Nodes in the drawn graph, which represent contigs, can be automatically labelled with their ID, length or depth. Users can interact with the graph by moving, labelling and colouring nodes. Sequence information can also be extracted directly from the graph viewer. By displaying connections between contigs, Bandage opens up new possibilities for analysing and improving de novo assemblies that are not possible by looking at contigs alone. Bandage works with Graphical Fragment Assembly (GFA) files. For more information about this file format, see here_ .. _here: https://gfa-spec.github.io/GFA-spec/GFA2.html 
Command Documentation
 
Bandage info
 takes a graph file (GFA) as input and outputs the following statistics about the graph: - 
Node count:
 The number of nodes in the graph. Only positive nodes are counted (i.e. each complementary pair counts as one). - 
Edge count:
 The number of edges in the graph. Only one edge in each complementary pair is counted. - 
Total length:
 The total number of base pairs in the graph. - 
Dead ends:
 The number of instances where an end of a node does not connect to any other nodes. - 
Percentage dead ends:
 The proportion of possible dead ends. The maximum number of dead ends is twice the number of nodes (occurs when there are no edges), so this value is the number of dead ends divided by twice the node count. - 
Connected components:
 The number of regions of the graph which are disconnected from each other. - 
Largest component:
 The total number of base pairs in the largest connected component. - 
N50:
 Nodes that are this length or greater will collectively add up to at least half of the total length. - 
Shortest node:
 The length of the shortest node in the graph. - 
Lower quartile node:
 The median node length for the shorter half of the nodes. - 
Median node:
 The median node length for the graph. - 
Upper quartile node:
 The median node length for the longer half of the nodes. - 
Longest node:
 The length of the longest node in the graph. Example output:: Node count: 561 Edge count: 734 Total length (bp): 4878380 Dead ends: 33 Percentage dead ends: 2.94118% Connected components: 19 Largest component (bp): 4821329 N50 (bp): 90360 Shortest node (bp): 1 Lower quartile node (bp): 17 Median node (bp): 87 Upper quartile node (bp): 404 Longest node (bp): 205425"
toolshed.g2.bx.psu.edu/repos/bgruening/bionano_scaffold/bionano_scaffold/3.7.0+galaxy3	".. class:: infomark 
Purpose
 The Hybrid Scaffold pipeline automates the comprehensive scaffolding process and is consisted of five major steps: 1) generate in silico maps for sequence assembly; 2) align in silico sequence maps against Bionano genome maps to identify and resolve potential conflicts in either data set; 3) merge the non-conflicting maps into hybrid scaffolds; 4) align sequence maps to the hybrid scaffolds; and 5) generate AGP and FASTA files for the scaffolds. ---- .. class:: infomark 
Coverage
 For Hybrid Scaffold, we recommend using as input a minimum of 80X effective molecule coverage in order to build an accurate and contiguous consensus genome map assembly for each enzyme. When using nickases, using more coverage does not significantly improve map contiguity. When using a DLS enzyme such as DLE-1, effective coverage up to and beyond 100X has shown improved map contiguities for some plants and animals. ---- .. class:: infomark 
Input Bionano assembly
 When running the de novo assembly pipeline for hybrid scaffolding applications, users are recommended to use assembly parameters for non-haplotype-aware assembly. The current Hybrid Scaffold pipeline does not explicitly handle haplotype information and assumes there is only one genome map or NGS sequence contig covering a given genomic region. If multiple haplotypes are present, the pipeline may make false positive conflict cuts and incorrectly mix haplotypes in the final scaffolds. We understand that haplotype information is important in many applications, and a fully haplotype-aware Hybrid Scaffold pipeline is in our roadmap for a future release. ---- .. class:: infomark Bionano Genomics has agreed to provide the licensed Bionano Solve software to enable the VGP to package the software in a container. Bionanno Solve as shipped in this container may not be the latest and may not run efficiently on non-validated hardware. Hence, it is not supported by Bionano Genomics. Any questions concerning this version should be passed to the VGP (https://github.com/VGP/vgp-assembly/issues) or Galaxy community (https://help.galaxyproject.org). For the latest supported Bionano software, visit Bionano Support https://bionanogenomics.com/support/"
toolshed.g2.bx.psu.edu/repos/bgruening/blobtoolkit/blobtoolkit/4.0.7+galaxy2	"BlobToolKit is a software suite to aid researchers in identifying and isolating non-target data in draft and publicly available genome assemblies. It can be used to process assembly, read and analysis files for fully reproducible interactive exploration in the browser-based Viewer. BlobToolKit can be used during assembly to filter non-target DNA, helping researchers produce assemblies with high biological credibility. .. class:: infomark 
NCBI taxdump directory
 The taxdump database, provided by NCBI, includes the taxonomic lineage of taxa, information on type strains and material, and host information. The file 
new_taxdump.tar.gz
 can be downloaded from the taxonomy directory on the 
FTP site &lt;https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/0.5.1+galaxy0	"Unicycler
 Unicycler is a hybrid assembly pipeline for bacterial genomes. It uses both Illumina reads and long reads (PacBio or Nanopore) to produce complete and accurate assemblies. It is written by 
Ryan Wick
 at the University of Melbourne's Centre for Systems Genomics. Much of the description below is lifted from Unicycler's 
github page
. .. 
Ryan Wick
: https://github.com/rrwick .. 
github page
: https://github.com/rrwick/Unicycler ----- 
Input data
 Unicycler accepts inputs short (Illumina) reads in FASTQ format. Galaxy places additional requirement of having these in FASTQ format with 
Sanger encoding
 of quality scores. Long reads (from Oxford Nanopore or PacBio) can be either in FASTQ of FASTA form. .. 
Sanger encoding
: https://en.wikipedia.org/wiki/FASTQ_format#Quality The input options are:: -1 SHORT1, --short1 SHORT1 FASTQ file of short reads (first reads in each pair) -2 SHORT2, --short2 SHORT2 FASTQ file of short reads (second reads in each pair) -s SHORT_UNPAIRED, --short_unpaired SHORT_UNPAIRED FASTQ file of unpaired short reads -l LONG, --long LONG FASTQ or FASTA file of long reads, if all reads are available at start. ----- 
Bridging mode
 Unicycler can be run in three modes: conservative, normal (the default) and bold, set with the --mode option. Conservative mode is least likely to produce a complete assembly but has a very low risk of misassembly. Bold mode is most likely to produce a complete assembly but carries greater risk of misassembly. Normal mode is intermediate regarding both completeness and misassembly risk. See 
description of modes
 for more information. .. 
description of modes
: https://github.com/rrwick/Unicycler#conservative-normal-and-bold The available modes are:: --mode {conservative,normal,bold} Bridging mode (default: normal) conservative = smaller contigs, lowest misassembly rate normal = moderate contig size and misassembly rate bold = longest contigs, higher misassembly rate ---- 
Skip SPAdes error correction step
 Sequencing data contains a substantial number of sequencing errors that manifest themselves as deviations (bulges and non-connected components) within the assembly graph. One of the ways to improve the graph even constructing it is to minimize the amount sequencing errors by performing error correction. SPAdes, which is used by Unicycler for error correction and assembly, uses 
BayesHammer
 to correct the reads. Here is a brief summary of what it does: 1. SPAdes (or rather BayesHammer) counts 
k
-mers in reads and computed 
k
-mer statistics that takes into account base quality values. 2. 
Hamming graph
 is constructed for 
k
-mers is which 
k
-mers are nodes. In this graph edges connect nodes (
k
-mers) is they differ from each other by a number of nucleotides up to a certain threshold (the 
Hamming distance
). The graph is central to the error correction algorithm. 3. At this step Bayesian subclustering of the graph produced in the previous step. For each 
k
-mer we now know the center of its subcluster. 4. Solid 
k
-mers are derived from cluster centers and are assumed to be 
error free
. 5. Solid 
k
-mers are mapped back to the reads and used to correct them. This step takes considerable time, so if one need to quickly evaluate assemblies this step can be skipped. However, this is not recommended if one if trying to produce a final high quality assembly. .. 
BayesHammer
: https://goo.gl/1iGkMe .. 
Hamming graph
: https://en.wikipedia.org/wiki/Hamming_graph .. 
Hamming distance
: https://en.wikipedia.org/wiki/Hamming_distance ----- 
Do not rotate completed replicons to start at a standard gene
 Unicycler uses TBLASTN to search for dnaA or repA alleles in each completed replicon. If one is found, the sequence is rotated and/or flipped so that it begins with that gene encoded on the forward strand. This provides consistently oriented assemblies and reduces the risk that a gene will be split across the start and end of the sequence. The following option turns rotation on and off:: --no_rotate Do not rotate completed replicons to start at a standard gene (default: completed replicons are rotated) 
Do not use Pilon to polish the final assembly
 
Pilon
 is a tool for improving overall quality of draft assemblies and finding variation among strains. Unicycler uses it for assembly 
polishing
. The following option turns pilon part of Unicycler pipeline on and off:: --no_pilon Do not use Pilon to polish the final assembly (default: Pilon is used) .. 
Pilon
: https://github.com/broadinstitute/pilon/wiki ------ 
Expected number of linear sequences
 If you expect your sample to contain linear (non circular) sequences, set this option:: --linear_seqs EXPECTED_LINEAR_SEQS The expected number of linear (i.e. non-circular) sequences in the underlying sequence ---- 
SPAdes options
 This section provides control of SPAdes options:: --min_kmer_frac MIN_KMER_FRAC Lowest k-mer size for SPAdes assembly, expressed as a fraction of the read length (default: 0.2) --max_kmer_frac MAX_KMER_FRAC Highest k-mer size for SPAdes assembly, expressed as a fraction of the read length (default: 0.95) --kmer_count KMER_COUNT Number of k-mer steps to use in SPAdes assembly (default: 10) --depth_filter DEPTH_FILTER Filter out contigs lower than this fraction of the chromosomal depth, if doing so does not result in graph dead ends (default: 0.25) ---- 
Rotation options
 Unicycler attempts to rotate circular assemblies to make sure that they begin at a consistent starting gene. The following parameters control assembly rotation:: --start_genes START_GENES FASTA file of genes for start point of rotated replicons (default: start_genes.fasta) --start_gene_id START_GENE_ID The minimum required BLAST percent identity for a start gene search (default: 90.0) --start_gene_cov START_GENE_COV The minimum required BLAST percent coverage for a start gene search (default: 95.0) ----- 
Graph cleaning options
 These options control the removal of small leftover sequences after bridging is complete:: --min_component_size MIN_COMPONENT_SIZE Unbridged graph components smaller than this size (bp) will be removed from the final graph (default: 1000) --min_dead_end_size MIN_DEAD_END_SIZE Graph dead ends smaller than this size (bp) will be removed from the final graph (default: 1000) ----- 
Long read alignment options
 These options control the alignment of long reads to the assembly graph:: --contamination CONTAMINATION FASTA file of known contamination in long reads --scores SCORES Comma-delimited string of alignment scores: match, mismatch, gap open, gap extend (default: 3,-6,-5,-2) --low_score LOW_SCORE Score threshold - alignments below this are considered poor (default: set threshold automatically) ----- 
Outputs
 Galaxy's wrapped for Unicycler produces two outputs: * final assembly in FASTA format * final assembly grapth in graph format While most will likely be interested in the FASTA dataset, the graph dataset is also quite useful and can be visualized using tools such as 
Bandage
. .. 
Bandage
: https://github.com/rrwick/Bandage"
toolshed.g2.bx.psu.edu/repos/iuc/disco/disco/1.2.1	DISCO is a multi threaded and multiprocess distributed memory overlap-layout-consensus (OLC) metagenome assembler. Disco was developed as a scalable assembler to assemble large metagenomes from billions of Illumina sequencing reads of complex microbial communities. Disco was parallelized for computer clusters in a hybrid architecture that integrated shared-memory multi-threading, point-to-point message passing, and remote direct memory access. The assembly and scaffolding were performed using an iterative overlap graph approach. The detailed user manual of the assembler and how to use it to acheive best results is provided here: http://disco.omicsbio.org/user-manual. This is a quick start guide generally for developers and testers. Users with limited experience with genome assembly are advised to use the user manual.
toolshed.g2.bx.psu.edu/repos/iuc/fastk_fastk/fastk_fastk/1.1.0+galaxy2	FastK is a k‑mer counter that is optimized for processing high quality DNA assembly data sets such as those produced with an Illumina instrument or a PacBio run in HiFi mode. The input data can be in CRAM, BAM, SAM, fasta, or fastq files. FastK produces the following outputs: 1. FastK hist: file in binary format containing histogram information detailing the frequency of occurrence for each k‑mer within the dataset. 2. A Tabex txt file comprising a table of k‑mer/count pairs, sorted lexicographically on the k‑mer sequence, followingthe order a < c < g < t 3. A tar file comprising of hidden .ktab files that can be used for downstream FASTK tools.
toolshed.g2.bx.psu.edu/repos/iuc/fastk_histex/fastk_histex/1.1.0+galaxy3	"FastK Histex
 Histex is a tool, part of FASTK suite that performs reading and displaying a kmer histogram produced by FastK. Histex requires the .hist file generated by the FASTK tool to view the histogram of k‑mer counts."
toolshed.g2.bx.psu.edu/repos/iuc/fastk_logex/fastk_logex/1.1.0+galaxy5	Logex tool can perform operations such as AND, OR, XOR, Minus and Unionsum on the input given input Ktab files. The tool requires two input Ktab files and their associated files (Generated in the tarball of FASTK tool with .1 extension) as inputs to perform various binary operations. The tool also supports Ktab file collections as input. The user can select the operation to be performed on the input Ktab files.
toolshed.g2.bx.psu.edu/repos/iuc/bellerophon/bellerophon/1.0+galaxy1	Filter mapped reads where the mapping spans a junction, retaining the 5-prime read. This is usually needed when dealing with data from Arima Genomics.
toolshed.g2.bx.psu.edu/repos/iuc/genomescope/genomescope/2.1.0+galaxy0	"GenomeScope 2.0: Reference-free profiling of polyploid genomes ============================================================== GenomeScope 2.0 applies classical insights from combinatorial theory to establish a detailed mathematical model of how k-mer frequencies will be distributed in heterozygous and polyploid genomes.It employs a polyploid-aware mixture model that, within seconds, accurately infers genome properties from unassembled sequencing data. GenomeScope 2.0 uses the k-mer count distribution, e.g. from KMC or Jellyfish, and produces a report and several informative plots describing the genome properties. We validate the approach on simulated polyploid data created using a generative model with parameters for genome size, heterozygosity, repetitiveness, ploidy, and sequencing coverage, and find GenomeScope 2.0 retains accuracy across a broad range of realistic and extreme parameter values. ----- .. class:: infomark 
Topological relationships
 In the field of phylogenetics, the evolutionary relationships between species are often depicted in a branching diagram known as a phylogenetic tree. In this setting, the topology of the tree refers to the branching structure of the tree. We may also depict the similarities between homologous chromosomes in a branching diagram. In this case, a topology refers to the similarities between distinct homologues. For ploidies of 4 and greater, there are multiple possible topologies. For example, the two tetraploid topologies are: :: AAAA → AAAB → AABC → ABCD AAAA → AABB → AABC → ABCD"
toolshed.g2.bx.psu.edu/repos/iuc/halfdeep/halfdeep/0.1.0+galaxy2	"HalfDeep identifies genomic regions with half-depth coverage based on sequencing read mappings. These regions may reveal insights into heterogametic sex chromosomes, haplotype-specific variation, or potential assembly errors such as heterotypic duplications. Given the following inputs: 1. A genome assembly in FASTA format. 2. Reads in FASTQ format. 3. Mapped reads in BAM format (optional) HalfDeep automates the following tasks: 1. Mapping reads and merging individual mapping files. 2. Calculating per-base read depth. 3. Smoothing read coverage using a defined window with genodsp. 4. Determining the percentile of read coverage. 5. Identifying genomic regions with half-depth coverage based on a specified percentile threshold (e.g., 40–60%) and exporting them in BED file format HalfDeep produces the following output: 1. HalfDeep: BED file containing regions of the genome assembly that are ""covered at half depth"""
toolshed.g2.bx.psu.edu/repos/bgruening/hifiasm/hifiasm/0.25.0+galaxy1	".. class:: infomark 
HiFiASM - a fast de novo assembler
 Hifiasm is a fast haplotype-resolved 
de novo
 assembler for PacBio Hifi reads. It can assemble a human genome in several hours and works with the California redwood genome, one of the most complex genomes sequenced so far. Hifiasm can produce primary/alternate assemblies of quality competitive with the best assemblers. It also introduces a new graph binning algorithm and achieves the best haplotype-resolved assembly given trio data. ---- .. class:: infomark 
Assembly mode
 - 
Standard
: Standard assembly can be run in pseudohaplotype mode, or with Hi-C phasing using Hi-C reads from the same individual. - 
ONT
: Enables direct, haplotype-resolved assembly of Oxford Nanopore simplex R10 reads with integrated error correction, optimizing for contiguity without requiring pre-corrected input. - 
Trio
: When parental short reads are available, hifiasm can generate a pair of haplotype-resolved assemblies with trio binning. ---- .. class:: infomark 
Outputs
 Non-Trio assembly: - Haplotype-resolved raw unitig graph: This graph keeps all haplotype information, including somatic mutations and recurrent sequencing errors. - Haplotype-resolved processed unitig graph without small bubbles: This graph 'pops' small bubbles in the raw unitig graph; small bubbles might be caused by somatic mutations or noise in data, which are not the real haplotype information. - Primary assembly contig graph: This graph includes a complete assembly with long stretches of phased blocks, though there may be some haplotype collapse. - Alternate assembly contig graph: This graph consists of all contigs that are discarded from the primary contig graph. - [hap1]/[hap2] contig graph: Each graph consists of phased contigs (output only with Hi-C phasing enabled). Trio assembly: - Haplotype-resolved raw unitig graph in GFA format . This graph keeps all haplotype information. - Phased paternal/haplotype1 contig graph. This graph keeps the phased paternal/haplotype1 assembly. - Phased maternal/haplotype2 contig graph. This graph keeps the phased maternal/haplotype2 assembly."
toolshed.g2.bx.psu.edu/repos/iuc/idba_hybrid/idba_hybrid/1.1.3	"IDBA is an iterative De Bruijn Graph De Novo Assembler for sequence assembly. Most assemblers based on de Bruijn graph build a de Bruijn graph with a specific k-mer size to perform the assembling task. For all of them, it is very crucial to find a specific value of k. If k is too large, there will be a lot of gap problems in the graph. If k is too small, there will a lot of branch problems. IDBA uses not only one specific k but a range of k values to build the iterative de Bruijn graph. It can keep all the information in graphs with different k values. IDBA-Hybrid is an iterative De Bruijn Graph De Novo Assembler for hybrid sequencing. It is an extension of IDBA-UD algorithm. It aims at using a closed related reference genome to help de novo assembly, especially when sequencing depth is low. IDBA-Hybrid does alignment between reads and reference first to extract similar regions in the reference genome, and then it correct the similar regions based on the alignment results and apply local assembly technique to resolve potential structure virations. Finally, it groups all the reads and the contigs got from those similar regions to do de novo assembly. The expriments showed it outperforms all existing de novo or hybrid assembly algorithms, especilly when the sequencing depth is low and the reference genome is similar to the target genome. Input: IDBA-
 take interleaved paired end data in the FASTA format as input, i.e. paired-end reads need to be stored in the same FASTA file such that a pair of reads should be in two consecutive lines. In Galaxy paired reads in separate FASTQ files can be converted into interleaved FASTA using the tools: * 
FASTQ interlacer on paired end read &lt;https://toolshed.g2.bx.psu.edu/view/devteam/fastq_paired_end_interlacer&gt;
 * 
Samtools extract FASTA or FASTQ from a SAM file &lt;https://toolshed.g2.bx.psu.edu/view/devteam/fastq_to_fasta&gt;
 Note that, IDBA-
 assumes that the paired-end reads are in order (->,<-). If your data is in reverse order (<-,->), please convert it by yourself."
toolshed.g2.bx.psu.edu/repos/iuc/idba_tran/idba_tran/1.1.3+galaxy0	"IDBA is an iterative De Bruijn Graph De Novo Assembler for sequence assembly. Most assemblers based on de Bruijn graph build a de Bruijn graph with a specific k-mer size to perform the assembling task. For all of them, it is very crucial to find a specific value of k. If k is too large, there will be a lot of gap problems in the graph. If k is too small, there will a lot of branch problems. IDBA uses not only one specific k but a range of k values to build the iterative de Bruijn graph. It can keep all the information in graphs with different k values. IDBA-Tran is an iterative De Bruijn Graph De Novo short read assembler for transcriptome. It is purely de novo assembler based on only RNA sequencing reads. IDBA-Tran uses local assembly to reconstructing missing k-mers in low-expressed transcripts and then employs progressive cutoff on contigs to seperate the graph into components. Each component corresponds to one gene in most cases and contains not many transcripts. A heuristic algorithm based on pair-end reads is then used to find the isoforms. Input: IDBA-
 take interleaved paired end data in the FASTA format as input, i.e. paired-end reads need to be stored in the same FASTA file such that a pair of reads should be in two consecutive lines. In Galaxy paired reads in separate FASTQ files can be converted into interleaved FASTA using the tools: * 
FASTQ interlacer on paired end read &lt;https://toolshed.g2.bx.psu.edu/view/devteam/fastq_paired_end_interlacer&gt;
 * 
Samtools extract FASTA or FASTQ from a SAM file &lt;https://toolshed.g2.bx.psu.edu/view/devteam/fastq_to_fasta&gt;
 Note that, IDBA-
 assumes that the paired-end reads are in order (->,<-). If your data is in reverse order (<-,->), please convert it by yourself."
toolshed.g2.bx.psu.edu/repos/iuc/idba_ud/idba_ud/1.1.3+galaxy1	"IDBA is an iterative De Bruijn Graph De Novo Assembler for sequence assembly. Most assemblers based on de Bruijn graph build a de Bruijn graph with a specific k-mer size to perform the assembling task. For all of them, it is very crucial to find a specific value of k. If k is too large, there will be a lot of gap problems in the graph. If k is too small, there will a lot of branch problems. IDBA uses not only one specific k but a range of k values to build the iterative de Bruijn graph. It can keep all the information in graphs with different k values. IDBA-UD is an extension of IDBA algorithm for Short Reads Sequencing data with Highly Uneven Sequencing Depth. IDBA-UD also iterates from small k to a large k. In each iteration, short and low-depth contigs are removed iteratively with cutoff threshold from low to high to reduce the errors in low-depth and high-depth regions. Paired-end reads are aligned to contigs and assembled locally to generate some missing k-mers in low-depth regions. With these technologies, IDBA-UD can iterate k value of de Bruijn graph to a very large value with less gaps and less branches to form long contigs in both low-depth and high-depth regions. Input: IDBA-
 take interleaved paired end data in the FASTA format as input, i.e. paired-end reads need to be stored in the same FASTA file such that a pair of reads should be in two consecutive lines. In Galaxy paired reads in separate FASTQ files can be converted into interleaved FASTA using the tools: * 
FASTQ interlacer on paired end read &lt;https://toolshed.g2.bx.psu.edu/view/devteam/fastq_paired_end_interlacer&gt;
 * 
Samtools extract FASTA or FASTQ from a SAM file &lt;https://toolshed.g2.bx.psu.edu/view/devteam/fastq_to_fasta&gt;
 Note that, IDBA-
 assumes that the paired-end reads are in order (->,<-). If your data is in reverse order (<-,->), please convert it by yourself."
toolshed.g2.bx.psu.edu/repos/iuc/megahit/megahit/1.2.9+galaxy2	"What it does
 MEGAHIT is a single node assembler for large and complex metagenomics NGS reads, such as soil. It makes use of succinct de Bruijn graph (SdBG) to achieve low memory assembly. MEGAHIT can optionally utilize a CUDA-enabled GPU to accelerate its SdBG contstruction. The GPU-accelerated version of MEGAHIT has been tested on NVIDIA GTX680 (4G memory) and Tesla K40c (12G memory) with CUDA 5.5, 6.0 and 6.5. -------- 
Project links:
 https://github.com/voutcn/megahit"
toolshed.g2.bx.psu.edu/repos/iuc/merqury/merqury/1.3+galaxy4	".. class:: infomark 
Purpose
 Merqury allows a reference-free assembly evaluation based on efficient k-mer set operations. By comparing k-mers in a de novo assembly to those found in unassembled high-accuracy reads, Merqury estimates base-level accuracy and completeness. For trios, Merqury can also evaluate haplotype-specific accuracy, completeness, phase block continuity, and switch errors. ---- .. class:: infomark 
Input
 Merqury requires two types of inputs: meryldbs and the genome assemblies. There is no need to run merqury per-assemblies. If two assemblies areprovided, Merqury generates stats for each and combined. ---- .. class:: infomark 
Output
 The generated metrics include consensus quality and k-mer completeness, and when parental genomic sequences are available (either assembled or unassembled), Merqury can output haplotype completeness, phase block statistics, switch error rates, and visual representations of phase consistency for the child’s genome. This includes TDF (or BED) features that can be displayed in a genome browser for visualizing the presence of k-mer classes across a genome (e.g., the k-mers inherited from a parental genome). You can add column headers to the 
completeness
 and 
qv
 outputs by checking the option ""Add column header on tabular outputs""."
toolshed.g2.bx.psu.edu/repos/iuc/merqury/merquryplot/1.3+galaxy4	".. class:: infomark 
Purpose
 Make spectra-cn plots. Line, filled, and stacked spectra-cn plots will be generated. Inputs : Histogram files generated by merqury."
toolshed.g2.bx.psu.edu/repos/iuc/meryl/meryl/1.3+galaxy6	".. class:: infomark 
Purpose
 Meryl is the k-mer counter. It is built into the Celera assembler and is also available as a stand-alone application. Meryl uses a sorting-based approach that sorts the k-mers in lexicographical order. In addition of generating count-databases, meryl can perform simple operations on it. ----- .. class:: infomark 
Basic functions
 The functions that meryl includes are described below: :: COUNT OPERATIONS - Count: count the occurrences of canonical k-mers - Count-forward: count the occurreces of forward k-mers - Count-reverse: count the occurreces of reverse k-mers FILTERING OPERATIONS - Less-than: return k-mers that occur fewer than N times in the input - Greater-than: return k-mers that occur more than N times in the input - Equal-to: return k-mers that occur exactly N times in the input - Not-equal-to: return k-mers that do not occur exactly N times in the input ARITHMETIC OPERATIONS - Increase: add x to the count of each k-mer - Decrease: subsctract x from the count of each k-mer - Multiply: multiply the count of each k-mer by x - Divide: divide the count of each k-mer by x - Divide-round: divide the count of each k-mer by x and round th results - Modulo: set the count of each k-mer to the remainder of the count divided by x OPERATIONS ON SETS - Union-min: return k-mers that occur in any input, set the count to the minimum count - Union-max: return k-mers that occur in any input, set the count to the maximum count - Union-sum: return k-mers that occur in any input, set the count to the sum of the counts - Intersect: return k-mers that occur in all inputs, set the count to the count in the first input - Intersect-min: return k-mers that occur in all inputs, set the count to the minimum count - Intersect-max: return k-mers that occur in all inputs, set the count to the maximum count - Intersect-sum: return k-mers that occur in all inputs, set the count to the sum of the counts - Subtract: return k-mers that occur in the first input, subtracting counts from the other inputs - Difference: return k-mers that occur in the first input, but none of the other inputs - Symmetric-difference: return k-mers that occur in exactly one input ----- .. class:: infomark 
Additional function: build hap-mers dbs for trios
 In addition of the basic operations, this wrapper allows to build the hap-mers databases for trios, in accordance with 
merqury's recommended guidelines. &lt;https://github.com/marbl/merqury/wiki/1.-Prepare-meryl-dbs#3-build-hap-mer-dbs-for-trios&gt;
_"
toolshed.g2.bx.psu.edu/repos/iuc/meryl_arithmetic_kmers/meryl_arithmetic_kmers/1.4.1+galaxy0	".. class:: infomark 
Purpose
 Meryl is the k-mer counter. This tool applies arithmetic operations on k-mer counts: - Increase: add x to the count of each k-mer - Decrease: subsctract x from the count of each k-mer - Multiply: multiply the count of each k-mer by x - Divide: divide the count of each k-mer by x - Divide-round: divide the count of each k-mer by x and round th results - Modulo: set the count of each k-mer to the remainder of the count divided by x"
toolshed.g2.bx.psu.edu/repos/iuc/meryl_count_kmers/meryl_count_kmers/1.4.1+galaxy0	".. class:: infomark 
Purpose
 Meryl is the k-mer counter. This tool can be used to count kmers. - Count: count the occurrences of canonical k-mers - Count-forward: count the occurreces of forward k-mers - Count-reverse: count the occurreces of reverse k-mers"
toolshed.g2.bx.psu.edu/repos/iuc/meryl_filter_kmers/meryl_filter_kmers/1.4.1+galaxy0	".. class:: infomark 
Purpose
 Meryl is the k-mer counter. This tool can be used to filter k-mers. - Less-than: return k-mers that occur fewer than N times in the input - Greater-than: return k-mers that occur more than N times in the input - Equal-to: return k-mers that occur exactly N times in the input - Not-equal-to: return k-mers that do not occur exactly N times in the input"
toolshed.g2.bx.psu.edu/repos/iuc/meryl_groups_kmers/meryl_groups_kmers/1.4.1+galaxy0	".. class:: infomark 
Purpose
 Meryl is the k-mer counter. Apply operations on multiple k-mer databases. - Union-min: return k-mers that occur in any input, set the count to the minimum count - Union-max: return k-mers that occur in any input, set the count to the maximum count - Union-sum: return k-mers that occur in any input, set the count to the sum of the counts - Intersect: return k-mers that occur in all inputs, set the count to the count in the first input - Intersect-min: return k-mers that occur in all inputs, set the count to the minimum count - Intersect-max: return k-mers that occur in all inputs, set the count to the maximum count - Intersect-sum: return k-mers that occur in all inputs, set the count to the sum of the counts - Subtract: return k-mers that occur in the first input, subtracting counts from the other inputs - Difference: return k-mers that occur in the first input, but none of the other inputs - Symmetric-difference: return k-mers that occur in exactly one input"
toolshed.g2.bx.psu.edu/repos/iuc/meryl_histogram_kmers/meryl_histogram_kmers/1.4.1+galaxy0	".. class:: infomark 
Purpose
 Meryl is the k-mer counter. This tool determines a k-mer frequency histogram."
toolshed.g2.bx.psu.edu/repos/iuc/meryl_print/meryl_print/1.4.1+galaxy0	".. class:: infomark 
Purpose
 Meryl is the k-mer counter. This tool gets the k-mer counts as a table."
toolshed.g2.bx.psu.edu/repos/iuc/meryl_trio_mode/meryl_trio_mode/1.4.1+galaxy0	".. class:: infomark 
Purpose
 Meryl is the k-mer counter. This tool builds hap-mer databases for trios, in accordance with 
merqury's recommended guidelines. &lt;https://github.com/marbl/merqury/wiki/1.-Prepare-meryl-dbs#3-build-hap-mer-dbs-for-trios&gt;
_"
toolshed.g2.bx.psu.edu/repos/bgruening/mitohifi/mitohifi/3.2.3+galaxy0	".. class:: infomark 
Purpose
 The dissemination of high-quality long reads (such as PacBio HiFi) makes the assembly of high-quality mitogenome straight forward. Because of the circular nature of the molecule, however, the mitocontig is usually assembled redundantly resulting in multiple-copy mitogenome-contigs. This pipeline was developed to finalise the assembly and annotation of the mitogenome. It will also dected different variants of the mitogenome present in your sample. At the end you are going to have all the variants assembled and annotated, and MitoHiFi.v2 is going to choose a final consensus sequence. In addtion, you will find an aligment of all the variants to facilitate your analysis of mitochondria heteroplasmy. .. class:: infomark 
Important parameter to change and test (-p)
 Mitohifi is going to pull possible mito contigs by blasting your contigs with the close-related mito. The Default parameter -p is going to chose any contig which has 50% or more of its length in the blast match. This is the default because with invertebrate taxa from the Darwin Tree of Life we have been seeing that the repetitive portion of the mitogenomes is not very conserved between some taxa. In these cases, a more stringent -p ends up excluding real mito sequences. Nevertheless, if you are working with more conserved taxa - such as mammals and other vertebrates - use higher -p (such as 80 or 90) for better results."
toolshed.g2.bx.psu.edu/repos/bgruening/nextdenovo/nextdenovo/2.5.0+galaxy0	"NextDenovo is a string graph-based de novo assembler for long reads (CLR, HiFi and ONT). It uses a ""correct-then-assemble"" strategy similar to canu (no correction step for PacBio HiFi reads), but requires significantly less computing resources and storages."
toolshed.g2.bx.psu.edu/repos/iuc/pairtools_stats/pairtools_stats/1.1.3+galaxy6	Calculate pairs statistics. By default, uses 4dn pairs or 4dn pairsam file to calculate statistics. Setting merge to true will merge multiple input stats files instead of calculating statistics of .pairs/.pairsam file.
toolshed.g2.bx.psu.edu/repos/iuc/pairtools_dedup/pairtools_dedup/1.1.3+galaxy6	"Pairtools dedup
 Find PCR/optical duplicates in an upper-triangular flipped sorted pairs/pairsam file. Allow for a +/-N bp mismatch at each side of duplicated molecules."
toolshed.g2.bx.psu.edu/repos/iuc/pairtools_parse/pairtools_parse/1.1.3+galaxy6	"Pairtools parse
 Detects ligation events in the aligned sequences of DNA molecules formed in Hi-C experiments and reports them in the .pairs/.pairsam format. sam_path : an input .sam/.bam (unsorted/name-sorted) file with paired-end sequence alignments of Hi-C molecules. By default, the generated .pair/.pairsam output is sorted by piping it through pairtools sort. You can disable this behavior by unchecking the “Generate sorted output file” checkbox."
toolshed.g2.bx.psu.edu/repos/iuc/pairtools_sort/pairtools_sort/1.1.3+galaxy6	"Pairtools sort
 Sort pairs in the lexicographic order along chrom1 and chrom2, in the numeric order along pos1 and pos2 and in the lexicographic order along pair_type."
toolshed.g2.bx.psu.edu/repos/iuc/pairtools_split/pairtools_split/1.1.3+galaxy6	"Pairtools split
 Restore a SAM/BAM file from SAM1 and SAM2 fields of a 
pairsam
 file. Create a 
pairs
 file without SAM1/SAM2 fields."
toolshed.g2.bx.psu.edu/repos/iuc/pretext_snapshot/pretext_snapshot/0.0.5+galaxy1	"Color map:
 +---------------------------------------+-------------------------------+-------------------------------+ |0. Inferno |11. Blue 4 |22. Orange 2 | +---------------------------------------+-------------------------------+-------------------------------+ |1. Black Body |12. Blue-Orange Divergent |23. Orange-Green-Blue-Gray | +---------------------------------------+-------------------------------+-------------------------------+ |2. Kindlmann |13. Brown 1 |24. Purple 1 | +---------------------------------------+-------------------------------+-------------------------------+ |3. Extended Kindlmann |14. Brown 2 |25. Purple 2 | +---------------------------------------+-------------------------------+-------------------------------+ |4. Three Wave Yellow-Grey-Blue |15. Brown 3 |26. Red 1 | +---------------------------------------+-------------------------------+-------------------------------+ |5. Three Wave Blue-Green-Yellow |16. Green 1 |27. Red 2 | +---------------------------------------+-------------------------------+-------------------------------+ |6. Four Wave Grey-Red-Green |17. Green 2 |28. Red 3 | +---------------------------------------+-------------------------------+-------------------------------+ |7. Five Wave Yellow-Brown-Blue |18. Green 3 |29. Yellow 1 | +---------------------------------------+-------------------------------+-------------------------------+ |8. Blue 1 |19. Green 4 |30. Yellow 2 | +---------------------------------------+-------------------------------+-------------------------------+ |9. Blue 2 |20. Mellow Rainbow | | +---------------------------------------+-------------------------------+-------------------------------+ |10. Blue 3 |21. Orange 1 | | +---------------------------------------+-------------------------------+-------------------------------+ 
Sequence strings:
 * --sequences ""=full"" * Creates a single image of the full contact map. * --sequences ""=full, =all"" * Creates an image of the full contact map, plus an image of each sequence. * --sequences ""seq_0"" * Creates a single image of the sequence named ""seq_0"" * --sequences ""seq_0, seq_1 > seq_2"" * Creates an image of seq_0, and an image from the start of seq_1 to the end of seq_2 * --sequences ""seq_0[1000000] > seq_3"" * Creates an image starting 1Mbp into seq_0, ending at the end of seq_3 * --sequences ""{seq_0, seq_1}"" * Create an single image of the off-diagonal region between seq_0 and seq_1, * i.e. the inter-sequence map of seq_0 and seq_1. * --sequences ""{seq_2[2000000] > seq_4[1000000] , seq_0[3000000] > seq_1[4000000]} , seq_0 > seq_10"" * Creates two images. The first is the off-diagonal region between 2Mbp into seq_2 and 1Mbp into seq_4 along the x-dimension, and 3Mbp into seq_0 and 4Mbp into seq_1 along the y-dimension. * The second image is the diagonal region between the start of seq_0 and the end of seq_10."
toolshed.g2.bx.psu.edu/repos/iuc/pretext_map/pretext_map/0.1.9+galaxy1	"What is does
 PretextMap (Paired REad TEXTure Mapper) converts SAM/BAM/CRAM or 
pairs files &lt;https://github.com/4dn-dcic/pairix/blob/master/pairs_format_specification.md&gt;
 into 
genome contact maps &lt;https://genome.ucsc.edu/goldenPath/help/hic.html&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/pretext_graph/pretext_graph/0.0.9+galaxy0	Adds additional data onto existing pretextmap files. Input Pretext file acts as a foundation for additional data to be added onto. Sequence names in the Pretext file must match sequence names in the bedgraph data; although relative sort order is unimportant. The chosen name acts as a lable on the graph.
toolshed.g2.bx.psu.edu/repos/iuc/purge_dups/purge_dups/1.2.6+galaxy1	".. class:: infomark 
Purpose
 The purge_dups tools are designed to remove haplotigs and contig overlaps in a de novo assembly based on read depth. purge_dups can significantly improve genome assemblies by removing overlaps and haplotigs caused by sequence divergence in heterozygous regions. This both removes false duplications in primary draft assemblies while retaining completeness and sequence integrity, and can improve scaffolding. ---- .. class:: infomark 
Pipeline Guide
 Given a primary assembly, and an alternative assembly (optional, if you have one), follow the steps shown below to build your own purge_dups pipeline, steps with same number can be run simultaneously. Among all the steps, although step 5 is optional, we highly recommend our users to do so, because assemblers may produce overrepresented sequences. In such a case, the final step 5 can be applied to remove those seqeuences. - Step 1: Calculate the coverage cutoffs and base coverages. - Step 2: Split an assembly with the 
split_fasfa
 function and do a self-self alignment by using minimap2. - Step 3: Purge haplotigs and overlaps with the 
purge_dups
 function. - Step 4: Get purged primary and haplotig sequences from the draft assembly with the 
get_seqs
 function. - Step 5: Merge hap.fa file, generated in the previous step, and the alternate assembly, and redo the above steps to get a decent haplotig set. ---- .. class:: infomark 
Limitations
 - Read depth cutoffs calculation: the coverage cutoffs can be larger for a low heterozygosity species, which causes the purged assembly size smaller than expected. In such a case, please use script/hist_plot.py to make the histogram plot and set coverage cutoffs manually. - Repeats: purge_dups has a limited ability to process repeats. ---- .. class:: infomark 
Purged assembly validation
 There are many ways to validate the purged assembly. One way is to make a coverage plot for it, the 2nd way is to run 
BUSCO &lt;https://busco.ezlab.org/&gt;
. A thid option is to use 
Merqury &lt;https://github.com/marbl/merqury&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/quast/quast/5.3.0+galaxy1	"What it does
 QUAST = QUality ASsessment Tool. The tool evaluates genome assemblies by computing various metrics. If you have one or multiple genome assemblies, you can assess their quality with Quast. It works with or without reference genome. If you are new to Quast, start by reading its 
manual page &lt;http://quast.sourceforge.net/docs/manual.html&gt;
. 
Using Quast without reference
 Without reference Quast can calculate a number of assembly related-metrics but cannot provide any information about potential misassemblies, inversions, translocations, etc. Suppose you have three assemblies produced by Unicycler corresponding to three different antibiotic treatments 
car
, 
pit
, and 
cef
 (these stand for carbenicillin, piperacillin, and cefsulodin, respectively). Evaluating them without reference will produce the following Quast outputs: * Quast report in HTML format * 
Contig viewer &lt;http://quast.sourceforge.net/docs/manual.html#sec3.4&gt;
 (an HTML file) * 
Quast report &lt;http://quast.sourceforge.net/docs/manual.html#sec3.1.1&gt;
 in Tab-delimited format * Quast log (a file technical information about Quast tool execution) The 
tab delimited Quast report
 will contain the following information:: Assembly pit_fna cef_fna car_fna # contigs (>= 0 bp) 100 91 94 # contigs (>= 1000 bp) 62 58 61 Total length (>= 0 bp) 6480635 6481216 6480271 Total length (>= 1000 bp) 6466917 6468946 6467103 # contigs 71 66 70 Largest contig 848753 848766 662053 Total length 6473173 6474698 6473810 GC (%) 66.33 66.33 66.33 N50 270269 289027 254671 N75 136321 136321 146521 L50 7 7 8 L75 15 15 16 # N's per 100 kbp 0.00 0.00 0.00 where values are defined as specified in 
Quast manual &lt;http://quast.sourceforge.net/docs/manual.html#sec3.1.1&gt;
 
Quast report in HTML format
 contains graphs in addition to the above metrics, while 
Contig viewer
 draws contigs ordered from longest to shortest. This ordering is suitable for comparing only largest contigs or number of contigs longer than a specific threshold. The viewer shows N50 and N75 with color and textual indication. If the reference genome is available or at least approximate genome length is known (see 
--est-ref-size
), NG50 and NG75 are also shown. You can also tone down contigs shorter than a specified threshold using Icarus control panel: .. image:: $PATH_TO_IMAGES/contig_view_noR.png :width: 558 :height: 412 Also see 
Plot description &lt;http://quast.sourceforge.net/docs/manual.html#sec2&gt;
 section of the manual. 
Using Quast with reference
 Car, pit, and cef are in fact assemblies of 
Pseudomonas aeruginosa
 UCBPP-PA14, so we can use its genome as a reference (by supplying a Fasta file containing 
P. aeruginosa
 pa14 genome to 
Reference genome
 input box). The following outputs will be produced (note the alignment viewer): * Quast report in HTML format * 
Contig viewer &lt;http://quast.sourceforge.net/docs/manual.html#sec3.4&gt;
 (an HTML file) * 
Alignment viewer &lt;http://quast.sourceforge.net/docs/manual.html#sec3.4&gt;
 (an HTML file) * 
Quast report &lt;http://quast.sourceforge.net/docs/manual.html#sec3.1.1&gt;
 in Tab-delimited format * Summary of 
misassemblies &lt;http://quast.sourceforge.net/docs/manual.html#sec3.1.2&gt;
 * Summary of 
unaligned contigs &lt;http://quast.sourceforge.net/docs/manual.html#sec3.1.3&gt;
 * Quast log (a file technical information about Quast tool execution) With the reference Quast produces a much more comprehensive set of results:: Assembly pit_fna cef_fna car_fna # contigs (>= 0 bp) 100 91 94 # contigs (>= 1000 bp) 62 58 61 Total length (>= 0 bp) 6480635 6481216 6480271 Total length (>= 1000 bp) 6466917 6468946 6467103 # contigs 71 66 70 Largest contig 848753 848766 662053 Total length 6473173 6474698 6473810 Reference length 6537648 6537648 6537648 GC (%) 66.33 66.33 66.33 Reference GC (%) 66.29 66.29 66.29 N50 270269 289027 254671 NG50 270269 289027 254671 N75 136321 136321 146521 NG75 136321 136321 136321 L50 7 7 8 LG50 7 7 8 L75 15 15 16 LG75 15 15 17 # misassemblies 0 0 0 # misassembled contigs 0 0 0 Misassembled contigs length 0 0 0 # local misassemblies 1 1 2 # unaligned mis. contigs 0 0 0 # unaligned contigs 0 + 0 0 + 0 0 + 0 part part part Unaligned length 0 0 0 Genome fraction (%) 99.015 99.038 99.025 Duplication ratio 1.000 1.000 1.000 # N's per 100 kbp 0.00 0.00 0.00 # mismatches per 100 kbp 3.82 3.63 3.49 # indels per 100 kbp 1.19 1.13 1.13 Largest alignment 848753 848766 662053 Total aligned length 6473163 6474660 6473792 NA50 270269 289027 254671 NGA50 270269 289027 254671 NA75 136321 136321 146521 NGA75 136321 136321 136321 LA50 7 7 8 LGA50 7 7 8 LA75 15 15 16 LGA75 15 15 17 where, again, values are defined as specified in 
Quast manual &lt;http://quast.sourceforge.net/docs/manual.html#sec3.1.1&gt;
. You can see that this report includes a variety of data that can only be computer against a reference assembly. Using reference also produces an 
Alignment viewer
: .. image:: $PATH_TO_IMAGES/Align_view.png :width: 515 :height: 395 Alignment viewer highlights regions of interest as, in this case, missassemblies that can potentially point to genome rearrangements (see more 
here &lt;http://quast.sourceforge.net/docs/manual.html#sec3.4&gt;
)."
toolshed.g2.bx.psu.edu/repos/bgruening/racon/racon/1.5.0+galaxy1	"What it does
 Consensus module for raw de novo DNA assembly of long uncorrected reads. Racon is intended as a standalone consensus module to correct raw contigs generated by rapid assembly methods which do not include a consensus step. The goal of Racon is to generate genomic consensus which is of similar or better quality compared to the output generated by assembly methods which employ both error correction and consensus steps, while providing a speedup of several times compared to those methods. It supports data produced by both Pacific Biosciences and Oxford Nanopore Technologies. Racon can be used as a polishing tool after the assembly with either Illumina data or data produced by third generation of sequencing. The type of data inputed is automatically detected. Racon takes as input only three files: contigs in FASTA/FASTQ format, reads in FASTA/FASTQ format and overlaps/alignments between the reads and the contigs in SAM format. Output is a set of polished contigs in FASTA format printed to stdout. Racon can also be used as a read error-correction tool. In this scenario, the SAM file needs to contain pairwise overlaps between reads including dual overlaps."
toolshed.g2.bx.psu.edu/repos/iuc/ragtag/ragtag/2.1.0+galaxy1	".. class:: infomark 
Purpose
 RagTag is a collection of software tools for scaffolding and improving modern genome assemblies. Tasks include: - Homology-based misassembly correction - Homology-based assembly scaffolding and patching - Scaffold merging ---- .. class:: infomark 
Correct mode
 RagTag offers a correction module that uses a reference genome to identify and correct potential misassemblies in a query assembly. RagTag also provides the option to verify putative misassemblies by aligning reads (from the same genotype) to the query assembly and observing read coverage near misassembly break points. In all cases, sequence is never added or subtracted. Query sequences are only broken at points of putative misassembly. 
Misassemblies vs true variation
 Reference-guided misassembly signatures are sometimes caused by true biological structural variation if the reference and query assemblies represent distinct genotypes (or haplotypes). The read validation feature should help to avoid some of these misassembly false positives, and the validation sensitivity can be tuned with command line parameters. However, it is ultimately up to the discretion of the user to decide if misassembly correction is appropriate. One should validate all RagTag results with independent data (usually physical, optical, or genetic maps), when possible. ---- .. class:: infomark 
Scaffold mode
 Scaffolding is the process of ordering and orienting draft assembly (query) sequences into longer sequences. Gaps (stretches of ""N"" characters) are placed between adjacent query sequences to indicate the presence of unknown sequence. RagTag uses whole-genome alignments to a reference assembly to scaffold query sequences. RagTag does not alter input query sequence in any way and only orders and orients sequences, joining them with gaps. ---- .. class:: infomark 
Patch mode
 This mode uses one genome assembly to 
patch
 another genome assembly. We define two types of patches: - Fills are patches that fill assembly gaps. This process is like traditional gap-filling, though it uses an assembly instead of WGS sequencing reads. - Joins are patches that join distinct contigs. This is essentially scaffolding and gap-filling in a single step. ---- .. class:: infomark 
Merge mode
 Draft genome assemblies are often scaffolded multiple times using different approaches. For example, one might scaffold an assembly using different genome maps (physical, linkage, Hi-C, etc.), different methods, or different method parameters. RagTag merge is a tool to merge and reconcile different scaffoldings of the same assembly. In this way, one can leverage the advantages of multiple techniques to synergistically improve scaffolding. Most tools write scaffolding results in the AGP file format, which encodes adjacency and gap information in a plain text file. To run RagTag merge, one must supply the assembly in FASTA format and at least two AGP files that define a scaffolding of the assembly. Each AGP file can optionally be assigned a weight, allowing users to assign the relative influence of each AGP on the final result. If available, users can supply Hi-C alignments to the draft assembly to resolve conflicts in the merging graph. In this scenario, the input AGP files are used to build the initial graph, but then Hi-C alignments are used to re-weight the graph before computing the scaffolding solution. 
List of accepted restriction enzymes
 List of all accepted restriction enzymes and their restriction sites: - HindIII: AAGCTT - Sau3AI: GATC - MboI: GATC - DpnII: GATC - HinfI: GA[ATCG]TC - DdeI: CT[ATCG]AG - MseI: TTAA For RagTag, use a comma separated list of enzymes or sites (or a mix). For example: - Arima Hi-C v1.0: 
Sau3AI,HinfI
 or 
GATC,GA[ATCG]TC
 - Arima Hi-C v2.0: 
Sau3AI,HinfI,DdeI,MseI
 or 
GATC,GA[ATCG]TC,CT[ATCG]AG,TTAA
 Note that for restriction sites, wildcards are represented with python regex syntax, not IUPAC ambiguity codes. e.g. '[ATCG]' instead of 'N'. Restriction enzymes are not necessarily the enzyme used for sample prep. Each is only a enzyme that cuts at the corresponding restriction site."
toolshed.g2.bx.psu.edu/repos/nml/refseq_masher/refseq_masher_contains/0.1.2	"RefSeq Masher - Containment =========================== Find what NCBI RefSeq genomes are contained within your sequence data using Mash_ with a Mash sketch database of 54,925 NCBI RefSeq Genomes. Source code available on Github at github.com/phac-nml/refseq_masher 
contains
 - find what NCBI RefSeq Genomes are contained in your input sequences -------------------------------------------------------------------------------- If you have a metagenomic sample or maybe a sample with some contamination, you may be interested in seeing what's in your sample. You can do this with 
refseq_masher contains &lt;INPUT&gt;
.:: Usage: refseq_masher contains [OPTIONS] INPUT... Find the NCBI RefSeq genomes contained in your sequence files using Mash Screen Input is expected to be one or more FASTA/FASTQ files or one or more directories containing FASTA/FASTQ files. Files can be Gzipped. Options: --mash-bin TEXT Mash binary path (default=""mash"") -o, --output PATH Output file path (default=""-""/stdout) --output-type [tab|csv] Output file type (tab|csv) -n, --top-n-results INTEGER Output top N results sorted by identity in ascending order (default=0/all) -i, --min-identity FLOAT Mash screen min identity to report (default=0.9) -v, --max-pvalue FLOAT Mash screen max p-value to report (default=0.01) -p, --parallelism INTEGER Mash screen parallelism; number of threads to spawn (default=1) -h, --help Show this message and exit. Example - metagenomic a sample SAMEA1877340_ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ For this example, we're going to see what RefSeq genomes are contained within sample SAMEA1877340_ from BioProject PRJEB1775_. Description from BioProject PRJEB1775_: .. epigraph:: Design, Setting and Patients Forty-five samples were selected from a set of fecal specimens obtained from patients with diarrhea during the 2011 outbreak of STEC O104:H4 in Germany. Samples were chosen to represent STEC-positive patients with a range of clinical conditions and colony counts together with a small number of patients with other infections (Campylobacter jejnuni, Clostridium difficile and Salmonella enterica). Samples were subjected to high-throughput sequencing on the Illumina MiSeq and HiSeq 2500, followed by bioinformatics analysis. We're going to download the FASTQ files for ERR260489_:: wget ftp.sra.ebi.ac.uk/vol1/fastq/ERR260/ERR260489/ERR260489_1.fastq.gz wget ftp.sra.ebi.ac.uk/vol1/fastq/ERR260/ERR260489/ERR260489_2.fastq.gz We're going to run 
refseq_masher
 against these FASTQ files:: refseq_masher -vv contains --top-n-results 50 -p 12 -o containment-ERR260489.tab ERR260489_1.fastq.gz ERR260489_2.fastq.gz 
Log
:: 2018-01-29 10:59:25,849 INFO: Grouped 2 fastqs into 1 groups [in ...refseq_masher/refseq_masher/utils.py:174] 2018-01-29 10:59:25,849 INFO: Collected 0 FASTA inputs and 1 read sets [in ...refseq_masher/refseq_masher/utils.py:185] 2018-01-29 10:59:25,849 INFO: Running Mash Screen with NCBI RefSeq sketch database against sample ""ERR260489"" with inputs: ['../ERR260489_1.fastq.gz', '../ERR260489_2.fastq.gz'] [in ...refseq_masher/refseq_masher/mash/screen.py:44] Loading ...refseq_masher/refseq_masher/data/RefSeqSketches.msh... 4669418 distinct hashes. Streaming from 2 inputs... Estimated distinct k-mers in pool: 206836855 Summing shared... Computing coverage medians... Writing output... 2018-01-29 11:00:19,665 INFO: Ran Mash Screen on all input. Merging NCBI taxonomic information into results output. [in ...refseq_masher/refseq_masher/cli.py:134] 2018-01-29 11:00:19,666 INFO: Fetching all taxonomy info for 23 unique NCBI Taxonomy UIDs [in ...refseq_masher/refseq_masher/taxonomy.py:35] 2018-01-29 11:00:19,669 INFO: Dropping columns with all NA values (ncol=32) [in ...refseq_masher/refseq_masher/taxonomy.py:38] 2018-01-29 11:00:19,671 INFO: Columns with all NA values dropped (ncol=12) [in ...refseq_masher/refseq_masher/taxonomy.py:40] 2018-01-29 11:00:19,671 INFO: Merging Mash results with relevant taxonomic information [in ...refseq_masher/refseq_masher/taxonomy.py:41] 2018-01-29 11:00:19,674 INFO: Merged Mash results with taxonomy info [in ...refseq_masher/refseq_masher/taxonomy.py:43] 2018-01-29 11:00:19,674 INFO: Merged taxonomic information into results output [in ...refseq_masher/refseq_masher/cli.py:136] 2018-01-29 11:00:19,674 INFO: Reordering output columns [in ...refseq_masher/refseq_masher/cli.py:137] 2018-01-29 11:00:19,677 INFO: Wrote output to ""containment-ERR260489.tab"" [in ...refseq_masher/refseq_masher/writers.py:20] 
Output
 +-----------+--------------------------------------+----------+----------------+----------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+------------------------------+------------------+--------------------+------------------+---------------------+-------------------+-------------------------+------------+---------+----------+------------+-----------+---------+---------------------+----------------------------------------------------------------------------------------------+--------------------------+----------------+ | sample | top_taxonomy_name | identity | shared_hashes | median_multiplicity | pvalue | full_taxonomy | taxonomic_subspecies | taxonomic_species | taxonomic_genus | taxonomic_family | taxonomic_order | taxonomic_class | taxonomic_phylum | taxonomic_superkingdom | subspecies | serovar | plasmid | bioproject | biosample | taxid | assembly_accession | match_id | taxonomic_species group | match_comment | +===========+======================================+==========+================+======================+========+==================================================================================================================================================+=======================+==============================+==================+====================+==================+=====================+===================+=========================+============+=========+==========+============+===========+=========+=====================+==============================================================================================+==========================+================+ | ERR260489 | Bacteroides fragilis | 1.0 | 400/400 | 786 | 0.0 | Bacteria; FCB group; Bacteroidetes/Chlorobi group; Bacteroidetes; Bacteroidia; Bacteroidales; Bacteroidaceae; Bacteroides; fragilis | | Bacteroides fragilis | Bacteroides | Bacteroidaceae | Bacteroidales | Bacteroidia | Bacteroidetes | Bacteria | | | pLV22a | | | 817 | | ./rcn/refseq-NG-817-.-.-.-pLV22a-Bacteroides_fragilis.fna | | | +-----------+--------------------------------------+----------+----------------+----------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+------------------------------+------------------+--------------------+------------------+---------------------+-------------------+-------------------------+------------+---------+----------+------------+-----------+---------+---------------------+----------------------------------------------------------------------------------------------+--------------------------+----------------+ | [1 row] | | | | | | | | | | | | | | | | | | | | | | | | | +-----------+--------------------------------------+----------+----------------+----------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+------------------------------+------------------+--------------------+------------------+---------------------+-------------------+-------------------------+------------+---------+----------+------------+-----------+---------+---------------------+----------------------------------------------------------------------------------------------+--------------------------+----------------+ | ERR260489 | Escherichia coli O104:H4 str. E92/11 | 1.0 | 400/400 | 48 | 0.0 | Bacteria; Proteobacteria; Gammaproteobacteria; Enterobacterales; Enterobacteriaceae; Escherichia; coli; O104:H4; str. E92/11 | | Escherichia coli | Escherichia | Enterobacteriaceae | Enterobacterales | Gammaproteobacteria | Proteobacteria | Bacteria | | | pE9211p3 | | | 1090927 | NZ_AHAU | ./rcn/refseq-NZ-1090927-.-.-NZ_AHAU-pE9211p3-Escherichia_coli_O104_H4_str._E92_11.fna | | | +-----------+--------------------------------------+----------+----------------+----------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+------------------------------+------------------+--------------------+------------------+---------------------+-------------------+-------------------------+------------+---------+----------+------------+-----------+---------+---------------------+----------------------------------------------------------------------------------------------+--------------------------+----------------+ | [3 rows] | | | | | | | | | | | | | | | | | | | | | | | | | +-----------+--------------------------------------+----------+----------------+----------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+------------------------------+------------------+--------------------+------------------+---------------------+-------------------+-------------------------+------------+---------+----------+------------+-----------+---------+---------------------+----------------------------------------------------------------------------------------------+--------------------------+----------------+ | ERR260489 | Kingella kingae KKC2005004457 | 1.0 | 400/400 | 5 | 0.0 | Bacteria; Proteobacteria; Betaproteobacteria; Neisseriales; Neisseriaceae; Kingella; kingae; KKC2005004457 | | Kingella kingae | Kingella | Neisseriaceae | Neisseriales | Betaproteobacteria | Proteobacteria | Bacteria | | | unnamed | | | 1229911 | | ./rcn/refseq-NG-1229911-.-.-.-unnamed-Kingella_kingae_KKC2005004457.fna | | | +-----------+--------------------------------------+----------+----------------+----------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+------------------------------+------------------+--------------------+------------------+---------------------+-------------------+-------------------------+------------+---------+----------+------------+-----------+---------+---------------------+----------------------------------------------------------------------------------------------+--------------------------+----------------+ | ERR260489 | Bacteroides cellulosilyticus WH2 | 0.99984 | 399/400 | 772 | 0.0 | Bacteria; FCB group; Bacteroidetes/Chlorobi group; Bacteroidetes; Bacteroidia; Bacteroidales; Bacteroidaceae; Bacteroides; cellulosilyticus; WH2 | | Bacteroides cellulosilyticus | Bacteroides | Bacteroidaceae | Bacteroidales | Bacteroidia | Bacteroidetes | Bacteria | | | pBWH2B | | | 1268240 | NZ_ATFI | ./rcn/refseq-NZ-1268240-.-.-NZ_ATFI-pBWH2B-Bacteroides_cellulosilyticus_WH2.fna | | | +-----------+--------------------------------------+----------+----------------+----------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+------------------------------+------------------+--------------------+------------------+---------------------+-------------------+-------------------------+------------+---------+----------+------------+-----------+---------+---------------------+----------------------------------------------------------------------------------------------+--------------------------+----------------+ | [1 row] | | | | | | | | | | | | | | | | | | | | | | | | | +-----------+--------------------------------------+----------+----------------+----------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+------------------------------+------------------+--------------------+------------------+---------------------+-------------------+-------------------------+------------+---------+----------+------------+-----------+---------+---------------------+----------------------------------------------------------------------------------------------+--------------------------+----------------+ | ERR260489 | Klebsiella pneumoniae | 0.99984 | 399/400 | 4 | 0.0 | Bacteria; Proteobacteria; Gammaproteobacteria; Enterobacterales; Enterobacteriaceae; Klebsiella; pneumoniae | | Klebsiella pneumoniae | Klebsiella | Enterobacteriaceae | Enterobacterales | Gammaproteobacteria | Proteobacteria | Bacteria | | | pMRC151 | | | 573 | | ./rcn/refseq-NG-573-.-.-.-pMRC151-Klebsiella_pneumoniae.fna | | | +-----------+--------------------------------------+----------+----------------+----------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+------------------------------+------------------+--------------------+------------------+---------------------+-------------------+-------------------------+------------+---------+----------+------------+-----------+---------+---------------------+----------------------------------------------------------------------------------------------+--------------------------+----------------+ | [37 rows] | | | | | | | | | | | | | | | | | | | | | | | | | +-----------+--------------------------------------+----------+----------------+----------------------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+------------------------------+------------------+--------------------+------------------+---------------------+-------------------+-------------------------+------------+---------+----------+------------+-----------+---------+---------------------+----------------------------------------------------------------------------------------------+--------------------------+----------------+ Some of the top genomes contained in this sample are sorted by identity and median multiplicity are: - 
Bacteroides fragilis
 - fully contained (400/400) and high multiplicity (768) - 
Escherichia coli
 O104:H4 - fully contained (400/400) and median multiplicity of 48 - 
Kingella kingae
 - fully contained (400/400) and median multiplicity of 5 - 
Klebsiella pneumoniae
 - 399/400 sketches contained with median multiplicity of 4 So with Mash we are able to find that the sample contained the expected genomic data (especially 
E. coli
 O104:H4). Legal ----- Copyright Government of Canada 2017 Written by: National Microbiology Laboratory, Public Health Agency of Canada Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this work except in compliance with the License. You may obtain a copy of the License at: www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contact ------- 
Gary van Domselaar
: gary.vandomselaar@phac-aspc.gc.ca .. _Mash: genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0997-x .. _SAMEA1877340: www.ebi.ac.uk/ena/data/view/SAMEA1877340 .. _PRJEB1775: www.ebi.ac.uk/ena/data/view/PRJEB1775 .. _ERR260489: www.ebi.ac.uk/ena/data/view/ERR260489&display=html"
toolshed.g2.bx.psu.edu/repos/nml/refseq_masher/refseq_masher_matches/0.1.2	"RefSeq Masher - Genomic Distance ================================ Find what NCBI RefSeq genomes most closely match your sequence data using Mash_ with a Mash sketch database of 54,925 NCBI RefSeq Genomes. Source code available on Github at github.com/phac-nml/refseq_masher 
matches
 - find the closest matching NCBI RefSeq Genomes in your input sequences --------------------------------------------------------------------------------- Command-line usage information:: Usage: refseq_masher matches [OPTIONS] INPUT... Find NCBI RefSeq genome matches for an input genome fasta file Input is expected to be one or more FASTA/FASTQ files or one or more directories containing FASTA/FASTQ files. Files can be Gzipped. Options: --mash-bin TEXT Mash binary path (default=""mash"") -o, --output PATH Output file path (default=""-""/stdout) --output-type [tab|csv] Output file type (tab|csv) -n, --top-n-results INTEGER Output top N results sorted by distance in ascending order (default=5) -m, --min-kmer-threshold INTEGER Mash sketch of reads: ""Minimum copies of each k-mer required to pass noise filter for reads"" (default=8) -h, --help Show this message and exit. Example ~~~~~~~ With the FNA.GZ_ file for 
Salmonella enterica
 subsp. enterica serovar Enteritidis str. CHS44_:: # download sequence file wget ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/329/025/GCF_000329025.1_ASM32902v1/GCF_000329025.1_ASM32902v1_genomic.fna.gz # find RefSeq matches refseq_masher -vv matches GCF_000329025.1_ASM32902v1_genomic.fna.gz 
Log
:: 2018-01-29 11:02:13,786 INFO: Collected 1 FASTA inputs and 0 read sets [in ...refseq_masher/refseq_masher/utils.py:185] 2018-01-29 11:02:13,786 INFO: Creating Mash sketch file for ...refseq_masher/GCF_000329025.1_ASM32902v1_genomic.fna.gz [in ...refseq_masher/refseq_masher/mash/sketch.py:24] 2018-01-29 11:02:14,055 INFO: Created Mash sketch file at ""/tmp/GCF_000329025.1_ASM32902v1_genomic.msh"" [in ...refseq_masher/refseq_masher/mash/sketch.py:40] 2018-01-29 11:02:14,613 INFO: Ran Mash dist successfully (output length=11647035). Parsing Mash dist output [in ...refseq_masher/refseq_masher/mash/dist.py:64] 2018-01-29 11:02:15,320 INFO: Parsed Mash dist output into Pandas DataFrame with 54924 rows [in ...refseq_masher/refseq_masher/mash/dist.py:67] 2018-01-29 11:02:15,321 INFO: Deleting temporary sketch file ""/tmp/GCF_000329025.1_ASM32902v1_genomic.msh"" [in ...refseq_masher/refseq_masher/mash/dist.py:72] 2018-01-29 11:02:15,321 INFO: Sketch file ""/tmp/GCF_000329025.1_ASM32902v1_genomic.msh"" deleted! [in ...refseq_masher/refseq_masher/mash/dist.py:74] 2018-01-29 11:02:15,322 INFO: Ran Mash dist on all input. Merging NCBI taxonomic information into results output. [in ...refseq_masher/refseq_masher/cli.py:88] 2018-01-29 11:02:15,323 INFO: Fetching all taxonomy info for 5 unique NCBI Taxonomy UIDs [in ...refseq_masher/refseq_masher/taxonomy.py:35] 2018-01-29 11:02:15,325 INFO: Dropping columns with all NA values (ncol=32) [in ...refseq_masher/refseq_masher/taxonomy.py:38] 2018-01-29 11:02:15,327 INFO: Columns with all NA values dropped (ncol=11) [in ...refseq_masher/refseq_masher/taxonomy.py:40] 2018-01-29 11:02:15,327 INFO: Merging Mash results with relevant taxonomic information [in ...refseq_masher/refseq_masher/taxonomy.py:41] 2018-01-29 11:02:15,329 INFO: Merged Mash results with taxonomy info [in ...refseq_masher/refseq_masher/taxonomy.py:43] 2018-01-29 11:02:15,329 INFO: Merged taxonomic info into results output [in ...refseq_masher/refseq_masher/cli.py:90] 2018-01-29 11:02:15,329 INFO: Reordering output columns [in ...refseq_masher/refseq_masher/cli.py:91] 2018-01-29 11:02:15,331 INFO: Writing output to stdout [in ...refseq_masher/refseq_masher/writers.py:16] 
Output
 +---------------------------------------+--------------------------------------------------------------------+----------+--------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------+---------------------+------------------+--------------------+------------------+---------------------+-------------------+-------------------------+------------+-------------+---------+-------------+--------------+--------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------+ | sample | top_taxonomy_name | distance | pvalue | matching | full_taxonomy | taxonomic_subspecies | taxonomic_species | taxonomic_genus | taxonomic_family | taxonomic_order | taxonomic_class | taxonomic_phylum | taxonomic_superkingdom | subspecies | serovar | plasmid | bioproject | biosample | taxid | assembly_accession | match_id | +=======================================+====================================================================+==========+========+==========+=============================================================================================================================================================+=====================================+=====================+==================+====================+==================+=====================+===================+=========================+============+=============+=========+=============+==============+========+=====================+==========================================================================================================================================+ | GCF_000329025.1_ASM32902v1_genomic | Salmonella enterica subsp. enterica serovar Enteritidis str. CHS44 | 0.0 | 0.0 | 400/400 | Bacteria; Proteobacteria; Gammaproteobacteria; Enterobacterales; Enterobacteriaceae; Salmonella; enterica; subsp. enterica; serovar Enteritidis; str. CHS44 | Salmonella enterica subsp. enterica | Salmonella enterica | Salmonella | Enterobacteriaceae | Enterobacterales | Gammaproteobacteria | Proteobacteria | Bacteria | enterica | Enteritidis | | PRJNA185053 | SAMN01041154 | 702979 | NZ_ALFF | ./rcn/refseq-NZ-702979-PRJNA185053-SAMN01041154-NZ_ALFF-.-Salmonella_enterica_subsp.
enterica_serovar_Enteritidis_str._CHS44.fna | +---------------------------------------+--------------------------------------------------------------------+----------+--------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------+---------------------+------------------+--------------------+------------------+---------------------+-------------------+-------------------------+------------+-------------+---------+-------------+--------------+--------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------+ The top match is 
Salmonella enterica
 subsp. enterica serovar Enteritidis str. CHS44
 with a distance of 0.0 and 400/400 sketches matching, which is what we expected. There's other taxonomic information available in the results table that may be useful. Legal ----- Copyright Government of Canada 2017 Written by: National Microbiology Laboratory, Public Health Agency of Canada Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this work except in compliance with the License. You may obtain a copy of the License at: www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contact ------- 
Gary van Domselaar
: gary.vandomselaar@phac-aspc.gc.ca .. _Mash: genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0997-x .. _FNA.GZ: ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/329/025/GCF_000329025.1_ASM32902v1/GCF_000329025.1_ASM32902v1_genomic.fna.gz .. _CHS44: ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/329/025/GCF_000329025.1_ASM32902v1/"
toolshed.g2.bx.psu.edu/repos/iuc/salsa/salsa/2.3+galaxy5	".. class:: infomark 
Purpose
 SALSA (Simple AssembLy ScAffolder) is a scaffolding tool based on a computational method that exploits the genomic proximity information in Hi-C data sets for long range scaffolding of de novo genome assemblies. ---- .. class:: infomark 
Mapping reads
 To start the scaffolding, first step is to map reads to the assembly. We recommend using 
BWA &lt;https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.17.2&gt;
 or 
BOWTIE2 &lt;https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.4.2+galaxy0&gt;
 aligner to map reads. The read mapping generates a bam file. SALSA requires BED file as the input. This can be done using the bamToBed command from the 
Bedtools package &lt;http://bedtools.readthedocs.io/en/latest/&gt;
. Also, SALSA requires BED files to be sorted by the read name, rather than the alignment coordinates. Once you have bam file, you can run following commands to get the bam file needed as an input to SALSA. Since Hi-C reads and alignments contain experimental artifacts, the alignments needs some postprocessing. To align and postprocess the alignments, you can use the pipeline released by Arima Genomics which can be found in the 
GitHub repository &lt;https://github.com/ArimaGenomics&gt;
. Additional information on how to generate/filter the bam 
here &lt;https://github.com/marbl/SALSA#mapping-reads&gt;
_."
toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0	".. class:: infomark 
What it does
 SPAdes - St. Petersburg genome assembler - is an assembly toolkit containing various assembly pipelines. 
Input
 SPAdes takes as input paired-end reads, mate-pairs and single (unpaired) reads in FASTA and FASTQ. For IonTorrent data SPAdes also supports unpaired reads in unmapped BAM format (like the one produced by Torrent Server). However, in order to run read error correction, reads should be in FASTQ or BAM format. Sanger, Oxford Nanopore and PacBio CLR reads can be provided in both formats since SPAdes does not run error correction for these types of data. To run SPAdes 3.15.3 you need at least one library of the following types: - Illumina paired-end/high-quality mate-pairs/unpaired reads - IonTorrent paired-end/high-quality mate-pairs/unpaired reads - PacBio CCS reads - Illumina and IonTorrent libraries should not be assembled together. All other types of input data are compatible. SPAdes should not be used if only PacBio CLR, Oxford Nanopore, Sanger reads or additional contigs are available. SPAdes supports mate-pair only assembly. However, we recommend to use only high-quality mate-pair libraries in this case (e.g. that do not have a paired-end part). We tested mate-pair only pipeline using Illumina Nextera mate-pairs. Notes: - It is strongly suggested to provide multiple paired-end and mate-pair libraries according to their insert size (from smallest to longest). - It is not recommended to run SPAdes on PacBio reads with low coverage (less than 5). - We suggest not to run SPAdes on PacBio reads for large genomes. - SPAdes accepts gzip-compressed files. A detailed description can be found in the 
input section &lt;https://github.com/ablab/spades/#sec3.1&gt;
 of the manual. 
Output
 - Assembly graph - Assembly graph with scaffolds - Contigs - Contigs paths in the assembly graph - Corrected reads by BayesHammer - Contigs stats - Log file - Scaffolds (recommended for use as resulting sequences) - Scaffolds paths in the assembly graph - Scaffolds stats ------------------- .. class:: infomark 
IonTorrent data
 The selection of k-mer length is non-trivial for IonTorrent. If the dataset is more or less conventional (good coverage, not high GC, etc), then use our 
recommendation for long reads &lt;https://github.com/ablab/spades#sec3.4&gt;
 (e.g. assemble using k-mer lengths 21,33,55,77,99,127). However, due to increased error rate some changes of k-mer lengths (e.g. selection of shorter ones) may be required. For example, if you ran SPAdes with k-mer lengths 21,33,55,77 and then decided to assemble the same data set using more iterations and larger values of K, you can run SPAdes once again specifying the same output folder and the following options: --restart-from k77 -k 21,33,55,77,99,127 --mismatch-correction -o <previous_output_dir>. Do not forget to copy contigs and scaffolds from the previous run. We're planning to tackle issue of selecting k-mer lengths for IonTorrent reads in next versions. You may need no error correction for Hi-Q enzyme at all. However, we suggest trying to assemble your data with and without error correction and select the best variant. For non-trivial datasets (e.g. with high GC, low or uneven coverage) we suggest to enable single-cell mode (setting --sc option) and use k-mer lengths of 21,33,55. ------------------- .. class:: infomark 
References
 More information are available on 
github &lt;https://github.com/ablab/spades&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/shovill/shovill/1.4.2+galaxy0	"Synopsis: Faster de novo assembly pipeline for Illumina paired end reads based around Spades Details and options: - Takes paired end Illumina fastq reads - Trim reads: Use Trimmomatic to remove common adaptors first (default: OFF) - Output log file: If set to ""Yes"", tool will return Shovill's log file as part of the output - Assembler: Which assembler should shovill use from: Skesa, Megahit, Velvet or Spades. Spades is the default. - Plasmid mode: Use plasmid mode if available. This works only when Spades assembler is used (default: OFF) Advanced options: - Name format: Format of output contig FASTA IDs in 'printf' style (default: 'contig%05d') - Depth: Sub-sample the reads to this depth. Disable with 
Depth: 0
 (default: 100) - Estimated genomesize: An estimate of the final genome size, it will autodetect if this is blank. (default: '') - List of kmers to use: List of K-mer sizes to use in SPAdes. Blank is AUTO. (default: '') - Extra SPAdes options: Extra SPAdes options eg. --plasmid --sc ... (default: '') - Disable post-assembly correction: Disable post assembly correction with pilon (default: ON) - Keep the bam files : Enable to keep mapped files from bwa in the post assembly correction (default: OFF) - Minimum contig length: Minimum length of contig to be output. 0 is AUTO (default: 0) - Minimum contig coverage: Minimum coverage to call part of a contig. 0 is AUTO (default: 2) - Spades result to correct: Spades result to correct: before_rr, contigs or scaffolds (default: 'contigs') Documentation can be found at Torsten Seemann 
site &lt;https://github.com/tseemann/shovill&gt;
_."
toolshed.g2.bx.psu.edu/repos/galaxy-australia/smudgeplot/smudgeplot/0.2.5+galaxy3	".. class:: infomark 
What it does
 This tool extracts heterozygous kmer pairs from kmer count databases and performs gymnastics with them. We are able to disentangle genome structure by comparing the sum of kmer pair coverages (CovA + CovB) to their relative coverage (CovB / (CovA + CovB)). Such an approach also allows us to analyze obscure genomes with duplications, various ploidy levels, etc. Smudgeplots are computed from raw or even better from trimmed reads and show the haplotype structure using heterozygous kmer pairs. For example: .. image:: $PATH_TO_IMAGES/smudge.png :height: 520 :alt: Example smudgeplot graph Every haplotype structure has a unique smudge on the graph and the heat of the smudge indicates how frequently the haplotype structure is represented in the genome compared to the other structures. The image above is an ideal case, where the sequencing coverage is sufficient to beautifully separate all the smudges, providing very strong and clear evidence of triploidy. Please see 
Smudgeplot on GitHub &lt;https://github.com/KamilSJaron/smudgeplot&gt;
 for further documentation and tutorials. 
Inputs
 You have two choices when running Smudgeplot in Galaxy: 1. Input reads file(s) for default kmer-counting with Jellyfish This should be at least one file which providing coverage of your genome of interest. The tool accepts compressed (.gz) inputs. If choosing this option, you can (optionally) specify manual cutoff values for the kmer dump step. The Smudgeplot docs suggest that you can use GenomeScope on a kmer histogram in order to choose reasonable lower and upper cutoff values. 2. Input your own kmer dump file for more control of kmer counting parameters This file would be created by running 
jellyfish count
 and then 
jellyfish dump
 - the process is well described 
on GitHub &lt;https://github.com/KamilSJaron/smudgeplot&gt;
. 
Outputs
 - 
smudgeplot.png
 smudgeplot image - 
smudgeplot_log10.png
 smudgeplot with log scale - 
my_genome_summary.tsv
 summarized genome statistics - 
my_genome_verbose.txt
 detailed genome statistics - 
my_genome_warnings.txt
 warnings emitted from the Smudgeplot tool 
Default operation
 If choosing reads as the input, a default kmer counting procedure will be used to create a kmer dump. This default process is summarized as follows: - 
jellyfish count -m 21 &gt; counts.jf
 - 
jellyfish histo counts.jf &gt; counts.hist
 - 
smudgeplot.py cutoff counts.hist
 to get kmer cutoff values (U & L) - 
jellyfish dump -c -L &lt;L&gt; -U &lt;U&gt; counts.jf &gt; dump.jf
 The kmer dump file is then used to create a smudgeplot: - 
smudgeplot.py hetkmers -o kmer_pairs dump.jf
 - 
smudgeplot.py plot kmer_pairs_coverages.tsv -o my_genome"
toolshed.g2.bx.psu.edu/repos/iuc/teloscope/teloscope/0.1.3+galaxy1	Description: Teloscope is a tool for telomere annotation in genome assemblies. It scans for user-specified telomeric repeat patterns across assembly paths, contigs and windows. Teloscope annotates terminal and interstitial telomeres, canonical/noncanonical matches and genome-wide metrics such as GC content, Shannon entropy, and repeat counts. It generates a detailed telomere summary report for paths, telomere statistics, and chromosome labels to assess telomere completeness. Teloscope can be used for both complete and fragmented assemblies, providing valuable information for genome manual curation and analysis. Usage: Default (ultra-fast) scans terminal regions and reports terminal telomeres + a summary report. * ${input_sequence.name}_terminal_telomeres.bed * ${input_sequence.name}.telo.report Enabling window/match options (-g -e -r -m -i) performs a genome-wide scan and produces: * ${input_sequence.name}_terminal_telomeres.bed * ${input_sequence.name}_interstitial_telomeres.bed * ${input_sequence.name}_canonical_matches.bed * ${input_sequence.name}_noncanonical_matches.bed * ${input_sequence.name}_window_metrics.bedgraph * ${input_sequence.name}.telo.report Key parameters: - -c / --canonical: Canonical repeat (default TTAGGG). This is the vertebrate telomeric motif found at chromosome ends that binds to shelterin complex to form a telomere. - -p / --patterns: Variant patterns (comma-separated). These are additional telomeric repeat motifs to search for, besides the canonical repeat, it includes other variants that can be part of telomeres. - -w / -s: window size / step (defaults 1000/500). - -u / --ultra-fast: terminal scan only (default true); disabled automatically when -g/-e/-r/-m/-i are used. - -x / --edit-distance: Edit Hamming distance for pattern matching (0–2). Useful for identifying degenerate telomeric repeats (default 0).
toolshed.g2.bx.psu.edu/repos/iuc/vgp_chromosome_assignment/vgp_chromosome_assignment/1.1+galaxy0	"What it does
 chromosome_assignment substitutes scaffold identifiers with chromosome assignments, generating chromosome-level sequences and mapping tables. The tool processes AGP metadata to: 1. Identify sex chromosomes (X, Y, W, Z) and regular chromosomes 2. Filter autosomal scaffolds and assign sequential 
SUPER_
 identifiers 3. Rename sex-linked scaffolds with 
SUPER_X/Y/W/Z
 prefixes 4. Handle unlocalized contigs by replacing parent scaffold names with chromosomal assignments 5. Generate documentation mapping original names to new names 
Inputs
 - 
Haplotype AGP file
: Tab-delimited AGP file with chromosome assignment metadata (typically hap.unlocs.no_hapdups.agp from split_agp) - 
Sorted FASTA file
: Sorted sequence file containing scaffolds/contigs (typically sorted using gfastats) 
Outputs
 - 
Chromosome Mapping Table (inter_chr.tsv)
: Tab-separated file documenting all scaffold-to-chromosome name transformations - 
Chromosome-level FASTA
: FASTA file with sequences renamed to chromosome-level assignments 
Workflow Context
 This tool is typically run twice in the VGP curation pipeline, once for each haplotype: 1. Run on Haplotype 1: Use Hap1 AGP and Hap1 sorted FASTA 2. Run on Haplotype 2: Use Hap2 AGP and Hap2 sorted FASTA 
Input Preparation
 Before running this tool: 1. Run split_agp to split haplotypes and correct AGP files 2. Use gfastats to sort each haplotype with its corresponding AGP file: - gfastats hap1.fa -a hap1_unlocs_no_hapdups.agp -o hap1.sorted.fa - gfastats hap2.fa -a hap2_unlocs_no_hapdups.agp -o hap2.sorted.fa 
Next Steps
 After running chromosome_assignment on both haplotypes: 1. Run MashMap to align the two chromosome-level haplotypes 2. Use sak_generation with the two inter_chr.tsv files and MashMap output to generate SAK instructions .. class:: infomark 
More Information
 This tool is part of the VGP ProcessCuration pipeline for preparing curated genome assemblies for submission. <expand macro=""help_common""/>"
toolshed.g2.bx.psu.edu/repos/iuc/vgp_sak_generation/vgp_sak_generation/1.1+galaxy0	"What it does
 sak_generation processes MashMap alignment output to identify scaffold pairs between haplotypes, generating instruction files for sequence reversal and renaming operations using gfastats SAK (Swiss Army Knife) format. The tool analyzes alignments between two haplotypes and produces instructions to: 1. Reverse-complement sequences that are inverted relative to the reference haplotype 2. Rename FASTA headers to ensure consistent naming between haplotypes 3. Document orientation relationships between paired scaffolds 4. Identify curated pairs that are missing from the alignments 
Inputs
 - 
Haplotype 1 chromosome mapping
: inter_chr.tsv file from chromosome_assignment for Hap1 - 
Haplotype 2 chromosome mapping
: inter_chr.tsv file from chromosome_assignment for Hap2 - 
Corrected AGP file
: Corrected AGP from split_agp containing both haplotype tags - 
MashMap output
: Alignment file from MashMap comparing the two chromosome-level haplotypes - 
Query haplotype
: Which haplotype was used as query in MashMap (default: Hap_2) - 
Reference haplotype
: Which haplotype was used as reference in MashMap (default: Hap_1) 
Outputs
 - 
SAK Instructions (reversing_renaming.sak)
: Tab-separated file containing sequential commands for gfastats: - RVCP commands: Reverse-complement specified sequences - RENAME commands: Rename FASTA headers - 
Orientation Table (orientation.tsv)
: Shows the most frequent alignment orientation for each scaffold pair - 
Missing Pairs (missing.tsv)
: Lists curated scaffold pairs that were not found in MashMap alignments (for quality control) 
SAK File Format
 The SAK file contains tab-separated commands processed sequentially by gfastats: - 
RVCP &lt;sequence_name&gt;
 - Reverse-complements a sequence in place - 
RENAME &lt;old_name&gt; &lt;new_name&gt;
 - Renames a FASTA header Example SAK file:: RVCP SUPER_5 RENAME SUPER_5 chr5 RVCP SUPER_X RENAME SUPER_X chrX 
Workflow Context
 This tool is used after running chromosome_assignment on both haplotypes: 1. Run split_agp to separate haplotypes 2. Sort each haplotype with gfastats 3. Run chromosome_assignment on both haplotypes (produces inter_chr.tsv files) 4. Align haplotypes with MashMap 5. 
Run sak_generation
 (this tool) to create SAK instructions 6. Apply SAK instructions with gfastats to finalize the query haplotype 
Applying SAK Instructions
 After generating the SAK file, use gfastats to apply the instructions:: gfastats input.fasta -k reversing_renaming.sak -o output.fasta This will reverse-complement and rename sequences as specified in the SAK file. .. class:: infomark 
More Information
 This tool is part of the VGP ProcessCuration pipeline for preparing curated genome assemblies for submission. <expand macro=""help_common""/>"
toolshed.g2.bx.psu.edu/repos/iuc/vgp_split_agp/vgp_split_agp/1.1+galaxy0	"What it does
 split_agp corrects AGP files for sequence length discrepancies, splits haplotypes into separate files, assigns unlocalized sequences, and removes haplotig duplications. This tool performs three sequential operations: 1. 
AGPcorrect
: Validates sequence lengths from FASTA against AGP coordinates, adjusting start/end positions 2. 
hap_split
: Segregates lines containing haplotype markers into separate directories (Hap_1 and Hap_2) 3. 
unloc
: Processes metadata tags, renames unloc scaffolds with sequential numbering, removes haplotigs 
Inputs
 - 
Assembly FASTA file
: FASTA file containing both haplotypes (may be gzip-compressed) - 
Curated AGP file
: Tab-delimited AGP file with haplotype markers (Hap_1/H1 or Hap_2/H2) 
Outputs
 - 
Corrected AGP
: Length-validated AGP file with corrected coordinates - 
Hap1 AGP
: Haplotype 1 AGP file - 
Hap1 Unlocs No Hapdups
: Haplotype 1 AGP with unlocalized sequences assigned and haplotigs removed - 
Hap1 Haplotigs
: Removed duplicate haplotigs from Haplotype 1 - 
Hap2 AGP
: Haplotype 2 AGP file - 
Hap2 Unlocs No Hapdups
: Haplotype 2 AGP with unlocalized sequences assigned and haplotigs removed - 
Hap2 Haplotigs
: Removed duplicate haplotigs from Haplotype 2 
Input Naming Requirements
 ⚠️ 
Important
: Scaffolds in the input FASTA must follow this naming convention: - Haplotype 1: H1.scaffold_X - Haplotype 2: H2.scaffold_X This naming pattern is required before manual curation in PretextView. 
Next Steps
 After running split_agp, use the gfastats tool to sort each haplotype: 1. Sort Haplotype 1: gfastats with Hap1 Unlocs No Hapdups AGP 2. Sort Haplotype 2: gfastats with Hap2 Unlocs No Hapdups AGP Then proceed to the chromosome_assignment tool to assign chromosome-level names. .. class:: infomark 
More Information
 This tool is part of the VGP ProcessCuration pipeline for preparing curated genome assemblies for submission. <expand macro=""help_common""/>"
toolshed.g2.bx.psu.edu/repos/simon-gladman/velvetoptimiser/velvetoptimiser/2.2.6	"Velvet Optimiser Overview
 Velvet_ is a de novo genomic assembler specially designed for short read sequencing technologies, such as Solexa or 454, developed by Daniel Zerbino and Ewan Birney at the European Bioinformatics Institute (EMBL-EBI), near Cambridge, in the United Kingdom. Velvet currently takes in short read sequences, removes errors then produces high quality unique contigs. It then uses paired-end read and long read information, when available, to retrieve the repeated areas between contigs. Read the Velvet 
documentation
 for details on using the Vevlet Assembler. .. _Velvet: http://www.ebi.ac.uk/~zerbino/velvet/ .. 
: http://www.ebi.ac.uk/~zerbino/velvet/Manual.pdf 
VelvetOptimiser
 VelvetOptimiser_ was written by Simon Gladman of CSIRO/Melbourne University. VelvetOptimiser performs a number of velveth and velvetg steps to try and optimise an assembly based on the metrics provided below. .. _VelvetOptimiser: http://github.com/Slugger70/VelvetOptimiser Galaxy tool wrapper for newer versions (2.5.5) of Velvet Optimiser. Written by Simon Gladman of Melbourne University. 
Outputs
 
Contigs
 The 
contigs.fa
 file. This fasta file contains the sequences of the contigs longer than 2k, where k is the word-length used in velveth. If you have specified a min contig lgth threshold, then the contigs shorter than that value are omitted. Note that the length and coverage information provided in the header of each contig should therefore be understood in k-mers and in k-mer coverage (cf. 5.1) respectively. The N's in the sequence correspond to gaps between scaffolded contigs. The number of N's corresponds to the estimated length of the gap. For reasons of compatibility with the archives, any gap shorter than 10bp is represented by a sequence of 10 N's. 
Stats
 The 
stats.txt
 file. This file is a simple tabbed-delimited description of the nodes. The column names are pretty much self-explanatory. Note however that node lengths are given in k-mers. To obtain the length in nucleotides of each node you simply need to add k - 1, where k is the word-length used in velveth. The in and out columns correspond to the number of arcs on the 5' and 3' ends of the contig respectively. The coverages in columns short1 cov, short1 Ocov, short2 cov, and short2 Ocov are provided in k-mer coverage (5.1). Also, the difference between # cov and # Ocov is the way these values are computed. In the first count, slightly divergent sequences are added to the coverage tally. However, in the second, stricter count, only the sequences which map perfectly onto the consensus sequence are taken into account. 
LastGraph
 The 
LastGraph
 file. This file describes in its entirety the graph produced by Velvet. This file is hidden by default. 
Logfile
 The Velvet Optimiser's logfile. This file is hidden by default 
STDERR
 The Standard Error output of the Optimiser for error messages etc. This file is hidden by default. 
Advanced options
 Verbose : Include verbose velvet output in log file. Good for debugging when things don't work. Other Velvetg Options : Extra velvetg options to pass through. eg. -long_mult_cutoff -max_coverage etc (default '') See below for details. Minimum coverage cutoff : The minimum cov_cutoff to be used. (default '0'). Maximum coverage cutoff : The maximum coverage cutoff to consider as a multiplier of the expected coverage. (default '0.8'). K-mer optimisation function : The optimisation function used for k-mer choice. (default 'n50'). Coverage cutoff optimisation function : The optimisation function used for cov_cutoff optimisation. (default 'Lbp'). Velvet optimiser 
assembly optimisation functions
 can be built from the following variables. 
LNbp
 = The total number of Ns in large contigs 
Lbp
 = The total number of base pairs in large contigs 
Lcon
 = The number of large contigs 
max
 = The length of the longest contig 
n50
 = The n50 
ncon
 = The total number of contigs 
tbp
 = The total number of basepairs in contigs Examples are: 'Lbp' = Just the total basepairs in contigs longer than 1kb 'n50
Lcon' = The n50 times the number of long contigs. 'n50
Lcon/tbp+log(Lbp)' = The n50 times the number of long contigs divided by the total bases in all contigs plus the log of the number of bases in long contigs (as an example only.) Defaults are: 
n50
 for k-mer length optimisation & 
Lbp
 for coverage cutoff 
Hash Length
 The hash length, also known as k-mer length, corresponds to the length, in base pairs, of the words being hashed. The hash length is the length of the k-mers being entered in the hash table. Firstly, you must observe three technical constraints:: - it must be an odd number, to avoid palindromes. If you put in an even number, Velvet will just decrement it and proceed. - it must be below or equal to MAXKMERHASH length (cf. 2.3.3, by default 31bp), because it is stored on 64 bits - it must be strictly inferior to read length, otherwise you simply will not observe any overlaps between reads, for obvious reasons. Now you still have quite a lot of possibilities. As is often the case, it's a trade-off between specificity and sensitivity. Longer kmers bring you more specificity (i.e. less spurious overlaps) but lowers coverage (cf. below)... so there's a sweet spot to be found with time and experience. We like to think in terms of ""k-mer coverage"", i.e. how many times has a k-mer been seen among the reads. The relation between k-mer coverage Ck and standard (nucleotide-wise) coverage C is Ck = C * (L - k + 1)/L where k is your hash length, and L you read length. Experience shows that this kmer coverage should be above 10 to start getting decent results. If Ck is above 20, you might be ""wasting"" coverage. Experience also shows that empirical tests with different values for k are not that costly to run! VelvetOptimiser automates these tests for you. 
Velvetg options
 -scaffolding yes|no : scaffolding of contigs used paired end information (default: on) -max_branch_length integer : maximum length in base pair of bubble (default: 100) -max_divergence floating-point : maximum divergence rate between two branches in a bubble (default: 0.2) -max_gap_count integer : maximum number of gaps allowed in the alignment of the two branches of a bubble (default: 3) -min_pair_count integer : minimum number of paired end connections to justify the scaffolding of two long contigs (default: 5) -max_coverage floating point : removal of high coverage nodes AFTER tour bus (default: no removal) -coverage_mask integer : minimum coverage required for confident regions of contigs (default: 1) -long_mult_cutoff integer : minimum number of long reads required to merge contigs (default: 2) -paired_exp_fraction double : remove all the paired end connections which less than the specified fraction of the expected count (default: 0.1) -conserveLong yes|no : preserve sequences with long reads in them (default no) 
Input Files
 Velvet works mainly with fasta and fastq formats. For paired-end reads, the assumption is that each read is next to its mate read. In other words, if the reads are indexed from 0, then reads 0 and 1 are paired, 2 and 3, 4 and 5, etc. Supported file formats are:: fasta fastq bam Read categories are:: short (default) shortPaired long (for Sanger, 454 or even reference sequences) longPaired reference (for pre-mapped sam or bam files - see Velvet manual for details on how to use this option)"
toolshed.g2.bx.psu.edu/repos/iuc/yahs/yahs/1.2a.2+galaxy3	YaHS is scaffolding tool using Hi-C data. It relies on a new algorithm for contig joining detection which considers the topological distribution of Hi-C signals aiming to distingush real interaction signals from mapping nosies. YaHS has been tested in a wide range of genome assemblies. Compared to other Hi-C scaffolding tools, it usually generates more contiguous scaffolds - especially with a higher N90 and L90 statistics. It is also super fast - takes less than 5 minutes to reconstruct the human genome from an assembly of 5,483 contigs with ~45X Hi-C data.
toolshed.g2.bx.psu.edu/repos/iuc/spades_biosyntheticspades/spades_biosyntheticspades/4.2.0+galaxy0	".. class:: infomark 
What it does
 SPAdes - St. Petersburg genome assembler - is an assembly toolkit containing various assembly pipelines. biosyntheticSPAdes is a subtool for biosynthetic gene cluster assembly with paired-end reads. 
Input
 biosyntheticSPAdes works with Illumina or IonTorrent reads in a single paired-end library and is capable of providing hybrid assemblies using PacBio, Oxford Nanopore and TSLR reads. Input data can be provided as interlaced, forward and reverse, merged and unpaired files. A detailed description can be found in the 
input section &lt;https://github.com/ablab/spades/#sec3.1&gt;
 of the manual. 
Output
 biosyntheticSPAdes outputs four files of interest: - Scaffolds: contains DNA sequences from putative biosynthetic gene clusters (BGC). Since each sample may contain multiple BGCs and biosyntheticSPAdes can output several putative DNA sequences for eash cluster, for each contig name we append suffix _cluster_X_candidate_Y, where X is the id of the BGC and Y is the id of the candidate from the BGC. - Raw_scaffolds: SPAdes scaffolds generated without domain-graph related algorithms. Very close to regular scaffolds.fasta file. - HMM statistics: contains statistics about BGC composition in the sample. First, it outputs number of domain hits in the sample. Then, for each BGC candidate we output domain order with positions on the corresponding DNA sequence from scaffolds.fasta. - Domain graphs: contains domain graph structure, that can be used to assess complexity of the sample and structure of BGCs. A detailed description can be found in the 
output section &lt;https://github.com/ablab/spades/#bgc&gt;
 of the manual. .. class:: infomark 
References
 More information can be found on 
github &lt;https://github.com/ablab/spades&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/spades_coronaspades/spades_coronaspades/4.2.0+galaxy0	".. class:: infomark 
What it does
 SPAdes - St. Petersburg genome assembler - is an assembly toolkit containing various assembly pipelines. Due to an increased interest in coronavirus research, a coronavirus assembly mode for SPAdes assembler has been developed (a.k.a coronaSPAdes). It enables assembly of full-length coronaviridae genomes from the transcriptomic and metatranscriptomic data. Algorithmically, coronaSPAdes is a HMM-guided assembler and it is built upon biosyntheticSPAdes idea of using profile HMMs specific for gene/organism to enhance assembly. 
Input
 coronasSpades works with Illumina or IonTorrent reads in a single paired-end library and is capable of providing hybrid assemblies using PacBio, Oxford Nanopore and TSLR reads. Input data can be provided as interlaced, forward and reverse, merged and unpaired files. 
Output
 - Scaffolds: set of putative virus sequences derived from HMM matches. - Raw scaffolds: full set of scaffolds from input data (derived without HMMs) - Domain statistics: various information about HMM alignments, including the coordinates of the matches, their order, etc. - HMM statistics: contains statistics about BGC composition in the sample. First, it outputs number of domain hits in the sample. Then, for each BGC candidate we output domain order with positions on the corresponding DNA sequence from scaffolds.fasta. .. class:: infomark 
References
 More information can be found on 
github &lt;https://github.com/ablab/spades&gt;
_."
toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy1	".. class:: infomark 
Purpose
 gfastats is a single fast and exhaustive tool for summary statistics and simultaneous genome assembly file manipulation. gfastats also allows seamless format conversion. .. class:: infomark 
Metrics details
 Typical fast
 metrics include: - Scaffold, contig and gap size - Number of scaffolds, contigs and gaps - Total length of scaffolds, contigs and gaps - Scaffold, contig, gap N50 and statistics (full N
/NG
 statistics with the --nstar-report flag) - Area under the curve (AuN/AuNG) values for scaffolds, contigs and gaps - Average scaffold, contig, gap size - Largest scaffold, contig and gap - Base composition and GC content - Soft-masked base counts (lower case bases) Typical gfa metrics include: - Number of nodes and edges - Average degree - Number of connected components, and length of the largets connected component - Number of dead ends - Number of disconnected components, and their total length .. class:: infomark 
Assembly manipulation
 gfastats allows extensive assembly manipulation at the sequence level. Manipulation is achieved using a set of instructions provided as an ordered list in a file to the option 
swiss army knife
*. See the 
instruction wiki &lt;https://github.com/vgl-hub/gfastats/tree/main/instructions&gt;
_ for a full list of instructions."
toolshed.g2.bx.psu.edu/repos/nml/metaspades/metaspades/4.2.0+galaxy0	".. class:: infomark 
What it does
 SPAdes - St. Petersburg genome assembler - is an assembly toolkit containing various assembly pipelines. metaSPAdes is a subtool for assembling metagenomic data sets. 
Input
 SPAdes takes as input paired-end reads, mate-pairs and single (unpaired) reads in FASTA and FASTQ. For IonTorrent data SPAdes also supports unpaired reads in unmapped BAM format (like the one produced by Torrent Server). However, in order to run read error correction, reads should be in FASTQ or BAM format. Sanger, Oxford Nanopore and PacBio CLR reads can be provided in both formats since SPAdes does not run error correction for these types of data. To run SPAdes 3.15.3 you need at least one library of the following types: - Illumina paired-end/high-quality mate-pairs/unpaired reads - IonTorrent paired-end/high-quality mate-pairs/unpaired reads - PacBio CCS reads - Illumina and IonTorrent libraries should not be assembled together. All other types of input data are compatible. SPAdes should not be used if only PacBio CLR, Oxford Nanopore, Sanger reads or additional contigs are available. SPAdes supports mate-pair only assembly. However, we recommend to use only high-quality mate-pair libraries in this case (e.g. that do not have a paired-end part). We tested mate-pair only pipeline using Illumina Nextera mate-pairs. Notes: - It is strongly suggested to provide multiple paired-end and mate-pair libraries according to their insert size (from smallest to longest). - It is not recommended to run SPAdes on PacBio reads with low coverage (less than 5). - We suggest not to run SPAdes on PacBio reads for large genomes. - SPAdes accepts gzip-compressed files. A detailed description can be found in the 
input section &lt;https://github.com/ablab/spades/#sec3.1&gt;
 of the manual. 
Output
 - Assembly graph - Assembly graph with scaffolds - Contigs - Contigs paths in the assembly graph - Corrected reads by BayesHammer - Contigs stats - Log file - Scaffolds (recommended for use as resulting sequences) - Scaffolds paths in the assembly graph - Scaffolds stats 
References
 More information can be found on 
github &lt;https://github.com/ablab/spades&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/spades_metaplasmidspades/spades_metaplasmidspades/4.2.0+galaxy0	".. class:: infomark 
What it does
 SPAdes - St. Petersburg genome assembler - is an assembly toolkit containing various assembly pipelines. metaplasmidSPAdes is a subtool for assembling plasmids from metagenomic data sets. 
Input
 SPAdes takes as input paired-end reads, mate-pairs and single (unpaired) reads in FASTA and FASTQ. For IonTorrent data SPAdes also supports unpaired reads in unmapped BAM format (like the one produced by Torrent Server). However, in order to run read error correction, reads should be in FASTQ or BAM format. Sanger, Oxford Nanopore and PacBio CLR reads can be provided in both formats since SPAdes does not run error correction for these types of data. To run SPAdes 3.15.3 you need at least one library of the following types: - Illumina paired-end/high-quality mate-pairs/unpaired reads - IonTorrent paired-end/high-quality mate-pairs/unpaired reads - PacBio CCS reads - Illumina and IonTorrent libraries should not be assembled together. All other types of input data are compatible. SPAdes should not be used if only PacBio CLR, Oxford Nanopore, Sanger reads or additional contigs are available. SPAdes supports mate-pair only assembly. However, we recommend to use only high-quality mate-pair libraries in this case (e.g. that do not have a paired-end part). We tested mate-pair only pipeline using Illumina Nextera mate-pairs. Notes: - It is strongly suggested to provide multiple paired-end and mate-pair libraries according to their insert size (from smallest to longest). - It is not recommended to run SPAdes on PacBio reads with low coverage (less than 5). - We suggest not to run SPAdes on PacBio reads for large genomes. - SPAdes accepts gzip-compressed files. A detailed description can be found in the 
input section &lt;https://github.com/ablab/spades/#sec3.1&gt;
 of the manual. 
Output
 - Contigs - Contigs stats - Corrected reads by BayesHammer - Log file - Scaffolds (recommended for use as resulting sequences) - Scaffolds stats 
References
 More information can be found on 
github &lt;https://github.com/ablab/spades&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/spades_metaviralspades/spades_metaviralspades/4.2.0+galaxy0	".. class:: infomark 
What it does
 SPAdes - St. Petersburg genome assembler - is an assembly toolkit containing various assembly pipelines. metaplasmidSPAdes is a subtool for assembling plasmids from metagenomic data sets. 
Input
 SPAdes takes as input paired-end reads, mate-pairs and single (unpaired) reads in FASTA and FASTQ. For IonTorrent data SPAdes also supports unpaired reads in unmapped BAM format (like the one produced by Torrent Server). However, in order to run read error correction, reads should be in FASTQ or BAM format. Sanger, Oxford Nanopore and PacBio CLR reads can be provided in both formats since SPAdes does not run error correction for these types of data. To run SPAdes 3.15.3 you need at least one library of the following types: - Illumina paired-end/high-quality mate-pairs/unpaired reads - IonTorrent paired-end/high-quality mate-pairs/unpaired reads - PacBio CCS reads - Illumina and IonTorrent libraries should not be assembled together. All other types of input data are compatible. SPAdes should not be used if only PacBio CLR, Oxford Nanopore, Sanger reads or additional contigs are available. SPAdes supports mate-pair only assembly. However, we recommend to use only high-quality mate-pair libraries in this case (e.g. that do not have a paired-end part). We tested mate-pair only pipeline using Illumina Nextera mate-pairs. Notes: - It is strongly suggested to provide multiple paired-end and mate-pair libraries according to their insert size (from smallest to longest). - It is not recommended to run SPAdes on PacBio reads with low coverage (less than 5). - We suggest not to run SPAdes on PacBio reads for large genomes. - SPAdes accepts gzip-compressed files. A detailed description can be found in the 
input section &lt;https://github.com/ablab/spades/#sec3.1&gt;
 of the manual. 
Output
 - Contigs - Contigs stats - Corrected reads by BayesHammer - Log file - Scaffolds (recommended for use as resulting sequences) - Scaffolds stats 
References
 More information can be found on 
github &lt;https://github.com/ablab/spades&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/miniasm/miniasm/0.3_r179+galaxy1	Miniasm is a very fast OLC-based de novo assembler for noisy long reads. It takes all-vs-all read self-mappings (typically by minimap) as input and outputs an assembly graph in the GFA format. Different from mainstream assemblers, miniasm does not have a consensus step. It simply concatenates pieces of read sequences to generate the final unitig sequences. Thus the per-base error rate is similar to the raw input reads. NOTE: This tool may take a long time depending on the size and characteristics of your dataset.
toolshed.g2.bx.psu.edu/repos/iuc/pilon/pilon/1.20.1	"Pilon is a software tool which can be used to: * Automatically improve draft assemblies * Find variation among strains, including large event detection Pilon requires as input a FASTA file of the genome along with one or more BAM files of reads aligned to the input FASTA file. Pilon uses read alignment analysis to identify inconsistencies between the input genome and the evidence in the reads. It then attempts to make improvements to the input genome, including: * Single base differences * Small indels * Larger indel or block substitution events * Gap filling * Identification of local misassemblies, including optional opening of new gaps Pilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome. To aid manual inspection and improvement by an analyst, Pilon can optionally produce tracks that can be displayed in genome viewers such as IGV and GenomeView, and it reports other events (such as possible large collapsed repeat regions) in its standard output. Note on 
mindepth
: Variants (snps and indels) will only be called if there is coverage of good pairs at the value set for 
mindepth
 depth or more; if this value is >= 1, it is an absolute depth, if it is a fraction < 1, then minimum depth is computed by multiplying this value by the mean coverage for the region, with a minumum value of 5 (default 0.1: min depth to call is 10% of mean coverage or 5, whichever is greater). Note on 
stray read filtering
 By default a pass is made through the input BAM files to identify stray pairs, that is, those pairs in which both reads are aligned but not marked valid because they have inconsistent orientation or separation. Identifying stray pairs can help fill gaps and assemble larger insertions, especially of repeat content. However, doing so sometimes consumes considerable memory."
toolshed.g2.bx.psu.edu/repos/iuc/spades_plasmidspades/spades_plasmidspades/4.2.0+galaxy0	".. class:: infomark 
What it does
 SPAdes - St. Petersburg genome assembler - is an assembly toolkit containing various assembly pipelines. plasmidSPAdes is a subtool for assembling plasmid data sets. 
Input
 SPAdes takes as input paired-end reads, mate-pairs and single (unpaired) reads in FASTA and FASTQ. For IonTorrent data SPAdes also supports unpaired reads in unmapped BAM format (like the one produced by Torrent Server). However, in order to run read error correction, reads should be in FASTQ or BAM format. Sanger, Oxford Nanopore and PacBio CLR reads can be provided in both formats since SPAdes does not run error correction for these types of data. To run SPAdes 3.15.3 you need at least one library of the following types: - Illumina paired-end/high-quality mate-pairs/unpaired reads - IonTorrent paired-end/high-quality mate-pairs/unpaired reads - PacBio CCS reads - Illumina and IonTorrent libraries should not be assembled together. All other types of input data are compatible. SPAdes should not be used if only PacBio CLR, Oxford Nanopore, Sanger reads or additional contigs are available. SPAdes supports mate-pair only assembly. However, we recommend to use only high-quality mate-pair libraries in this case (e.g. that do not have a paired-end part). We tested mate-pair only pipeline using Illumina Nextera mate-pairs. Notes: - It is strongly suggested to provide multiple paired-end and mate-pair libraries according to their insert size (from smallest to longest). - It is not recommended to run SPAdes on PacBio reads with low coverage (less than 5). - We suggest not to run SPAdes on PacBio reads for large genomes. - SPAdes accepts gzip-compressed files. A detailed description can be found in the 
input section &lt;https://github.com/ablab/spades/#sec3.1&gt;
 of the manual. 
Output
 - Assembly graph - Assembly graph with scaffolds - Contigs - Contigs paths in the assembly graph - Corrected reads by BayesHammer - Contigs stats - Log file - Scaffolds (recommended for use as resulting sequences) - Scaffolds paths in the assembly graph - Scaffolds stats 
References
 More information can be found on 
github &lt;https://github.com/ablab/spades&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/rnaquast/rna_quast/2.3.0+galaxy1	".. class:: infomark 
Purpose
 rnaQUAST is a tool for evaluating RNA-Seq assemblies using reference genome and gene database. In addition, rnaQUAST is also capable of estimating gene database coverage by raw reads and de novo quality assessment. .. class:: infomark 
rnaQUAST pipeline
 To evaluate quality of the assembled transcripts, rnaQUAST takes a reference genome in FASTA format and optionally its gene database in GFF/GTF format. A user can provide either a FASTA file with transcripts, which will be aligned to the given reference genome using GMAP or BLAT. The alignments are analyzed to calculate simple metrics and then are matched against the isoforms from the gene database in order to obtain statistics that represent completeness and correctness levels of the assembly. In addition, rnaQUAST is capable of estimating gene database coverage by raw reads using STAR or TopHat2. For de novo quality assessment when reference genome and gene database are unavailable, the transcripts are analyzed using BUSCO. .. class:: infomark 
Metrics and alignment analysis
 rnaQUAST calculates various metrics without using alignment information, e.g. length distribution and N50 of the assembled transcripts. Additionally, rnaQUAST computes the following statistics for the gene database: the total number of genes and isoforms, isoform and exon length distribution, average number of exons per gene, etc. To analyze transcripts' alignments, rnaQUAST firstly filters out short partial alignments (shorter than a user-defined threshold, default value is 50 bp). Such short alignments are typically caused by genomic repeats and thus are ignored. Afterwards, rnaQUAST selects the best-scored spliced alignment for each transcript. If a transcript has more than one alignment with the highest score, it is reported as multiply aligned. Otherwise, it is considered to be uniquely aligned. If the best-scored alignment is discordant (e.g. the transcript has partial alignments that are either mapped to different strands or to different chromosomes) the transcript is classified as misassembled. Transcripts without misassemblies are analyzed to calculate such metrics as average transcript alignment fraction and mismatch rate. For the simplicity of explanation, transcript is further referred to as a sequence generated by the assembler and isoform denotes a sequence from the gene database. rnaQUAST matches best-scored alignments of non-misassembled transcripts to the isoforms' coordinates and analyzes them to estimate how well the isoforms are covered by the assembly. rnaQUAST computes such metrics as database coverage (the total number of covered bases of all isoforms divided by the total length of all isoforms) and the number of 50%/95%-assembled isoforms. An isoform is considered to be x%-assembled if it has at least x% covered by a single transcript. Vice versa, to evaluate how well the assembled transcripts are covered by the isoforms, rnaQUAST estimates the number of unannotated transcripts (that align to the genome, but do not match to any isoform) and the number of 50%/95%-matched transcripts (that have corresponding fraction mapped to an isoform). Indeed, the thresholds described above (50% and 95%) can be varied by the user."
toolshed.g2.bx.psu.edu/repos/iuc/rnaspades/rnaspades/4.2.0+galaxy0	".. class:: infomark 
What it does
 SPAdes - St. Petersburg genome assembler - is an assembly toolkit containing various assembly pipelines. rnaSPAdes is a subtool for de novo transcriptome assembly from RNA-Seq data and is suitable for all kinds of organisms. 
Input
 rnaSPAdes take as an input at least one paired-end or single-end library. For hybrid assembly you can use PacBio or Oxford Nanopore reads. In case you have sequenced several RNA-Seq libraries using the same protocol from different tissues / conditions, and the goal as to assemble a total transcriptome, we suggest to provide all files as a single library. Note, that sequencing using the same protocol implies that the resulting reads have the same length, insert size and strand-specificity. Transcript quantification for each sample can be done afterwards by separately mapping reads from each library to the assembled transcripts. 
Output
 - Assembly graph - Assembly graph with scaffolds - Corrected reads by BayesHammer - Hard filtered transcripts includes only long and reliable transcripts with rather high expression - Log file - Soft filtered transcripts includes short and low-expressed transcipts, likely to contain junk sequences - Transcripts - Transcripts paths .. class:: infomark 
References
 More information can be found on on 
github &lt;https://github.com/ablab/spades&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/spades_rnaviralspades/spades_rnaviralspades/4.2.0+galaxy0	".. class:: infomark 
What it does
 SPAdes - St. Petersburg genome assembler - is an assembly toolkit containing various assembly pipelines. rnaviralSPAdes is a pipeline specially designed for de novo assembler tailored for RNA viral datasets (transcriptome, metatranscriptome and metavirome). 
Input
 SPAdes takes as input paired-end reads, mate-pairs and single (unpaired) reads in FASTA and FASTQ. For IonTorrent data SPAdes also supports unpaired reads in unmapped BAM format (like the one produced by Torrent Server). However, in order to run read error correction, reads should be in FASTQ or BAM format. Sanger, Oxford Nanopore and PacBio CLR reads can be provided in both formats since SPAdes does not run error correction for these types of data. To run SPAdes 3.15.3 you need at least one library of the following types: - Illumina paired-end/high-quality mate-pairs/unpaired reads - IonTorrent paired-end/high-quality mate-pairs/unpaired reads - PacBio CCS reads - Illumina and IonTorrent libraries should not be assembled together. All other types of input data are compatible. SPAdes should not be used if only PacBio CLR, Oxford Nanopore, Sanger reads or additional contigs are available. SPAdes supports mate-pair only assembly. However, we recommend to use only high-quality mate-pair libraries in this case (e.g. that do not have a paired-end part). We tested mate-pair only pipeline using Illumina Nextera mate-pairs. Notes: - It is strongly suggested to provide multiple paired-end and mate-pair libraries according to their insert size (from smallest to longest). - It is not recommended to run SPAdes on PacBio reads with low coverage (less than 5). - We suggest not to run SPAdes on PacBio reads for large genomes. - SPAdes accepts gzip-compressed files. A detailed description can be found in the 
input section &lt;https://github.com/ablab/spades/#sec3.1&gt;
 of the manual. 
Output
 - Assembly graph - Assembly graph with scaffolds - Contigs - Contigs paths in the assembly graph - Corrected reads by BayesHammer - Contigs stats - Log file - Scaffolds (recommended for use as resulting sequences) - Scaffolds paths in the assembly graph - Scaffolds stats ------------------- .. class:: infomark 
IonTorrent data
 The selection of k-mer length is non-trivial for IonTorrent. If the dataset is more or less conventional (good coverage, not high GC, etc), then use our 
recommendation for long reads &lt;https://github.com/ablab/spades#sec3.4&gt;
 (e.g. assemble using k-mer lengths 21,33,55,77,99,127). However, due to increased error rate some changes of k-mer lengths (e.g. selection of shorter ones) may be required. For example, if you ran SPAdes with k-mer lengths 21,33,55,77 and then decided to assemble the same data set using more iterations and larger values of K, you can run SPAdes once again specifying the same output folder and the following options: --restart-from k77 -k 21,33,55,77,99,127 --mismatch-correction -o <previous_output_dir>. Do not forget to copy contigs and scaffolds from the previous run. We're planning to tackle issue of selecting k-mer lengths for IonTorrent reads in next versions. You may need no error correction for Hi-Q enzyme at all. However, we suggest trying to assemble your data with and without error correction and select the best variant. For non-trivial datasets (e.g. with high GC, low or uneven coverage) we suggest to enable single-cell mode (setting --sc option) and use k-mer lengths of 21,33,55. ------------------- .. class:: infomark 
References
 More information are available on 
github &lt;https://github.com/ablab/spades&gt;
_."
toolshed.g2.bx.psu.edu/repos/devteam/velvet/velvetg/1.2.10.2	"What it does
 Velvet_ is a de novo genomic assembler specially designed for short read sequencing technologies, such as Solexa or 454, developed by Daniel Zerbino and Ewan Birney at the European Bioinformatics Institute (EMBL-EBI), near Cambridge, in the United Kingdom. Velvet currently takes in short read sequences, removes errors then produces high quality unique contigs. It then uses paired-end read and long read information, when available, to retrieve the repeated areas between contigs. Read the Velvet 
documentation
 for details on using the Velvet Assembler. .. _Velvet: http://www.ebi.ac.uk/~zerbino/velvet/ .. 
: http://www.ebi.ac.uk/~zerbino/velvet/Manual.pdf ------ 
Input formats
 Velvet can input sequence files in the following formats: fasta fastq fasta.gz fastq.gz eland gerald The input files are prepared for the velvet assembler using 
velveth
. ------ 
Outputs
 
Contigs
 The 
contigs.fa
 file. This fasta file contains the sequences of the contigs longer than 2k, where k is the word-length used in velveth. If you have specified a min contig length threshold, then the contigs shorter than that value are omitted. Note that the length and coverage information provided in the header of each contig should therefore be understood in k-mers and in k-mer coverage (cf. 5.1) respectively. The N's in the sequence correspond to gaps between scaffolded contigs. The number of N's corresponds to the estimated length of the gap. For reasons of compatibility with the archives, any gap shorter than 10bp is represented by a sequence of 10 N's. 
Stats
 The 
stats.txt
 file. This file is a simple tabbed-delimited description of the nodes. The column names are pretty much self-explanatory. Note however that node lengths are given in k-mers. To obtain the length in nucleotides of each node you simply need to add k - 1, where k is the word-length used in velveth. The in and out columns correspond to the number of arcs on the 5' and 3' ends of the contig respectively. The coverages in columns short1 cov, short1 Ocov, short2 cov, and short2 Ocov are provided in k-mer coverage (5.1). Also, the difference between # cov and # Ocov is the way these values are computed. In the first count, slightly divergent sequences are added to the coverage tally. However, in the second, stricter count, only the sequences which map perfectly onto the consensus sequence are taken into account. 
LastGraph
 The 
LastGraph
 file. This file describes in its entirety the graph produced by Velvet. 
AMOS.afg
 The 
velvet_asm.afg
 file. This file is mainly designed to be read by the open-source AMOS genome assembly package. Nonetheless, a number of programs are available to transform this kind of file into other assembly file formats (namely ACE, TIGR, Arachne and Celera). See http://amos.sourceforge.net/ for more information. The file describes all the contigs contained in the contigs.fa file (cf 4.2.1). ------ 
Velvet parameter list
 This is a list of implemented Velvetg options:: Standard options: -cov_cutoff floating-point|auto : removal of low coverage nodes AFTER tour bus or allow the system to infer it (default: no removal) -ins_length integer : expected distance between two paired end reads (default: no read pairing) -read_trkg yes|no : tracking of short read positions in assembly (default: no tracking) -min_contig_lgth integer : minimum contig length exported to contigs.fa file (default: hash length * 2) -amos_file yes|no : export assembly to AMOS file (default: no export) -exp_cov floating point|auto : expected coverage of unique regions or allow the system to infer it (default: no long or paired-end read resolution) Advanced options: -ins_length2 integer : expected distance between two paired-end reads in the second short-read dataset (default: no read pairing) -ins_length_long integer : expected distance between two long paired-end reads (default: no read pairing) -ins_length
_sd integer : est. standard deviation of respective dataset (default: 10% of corresponding length) [replace '
' by nothing, '2' or '_long' as necessary] -scaffolding yes|no : scaffolding of contigs used paired end information (default: on) -max_branch_length integer : maximum length in base pair of bubble (default: 100) -max_divergence floating-point : maximum divergence rate between two branches in a bubble (default: 0.2) -max_gap_count integer : maximum number of gaps allowed in the alignment of the two branches of a bubble (default: 3) -min_pair_count integer : minimum number of paired end connections to justify the scaffolding of two long contigs (default: 10) -max_coverage floating point : removal of high coverage nodes AFTER tour bus (default: no removal) -long_mult_cutoff int : minimum number of long reads required to merge contigs (default: 2) -unused_reads yes|no : export unused reads in UnusedReads.fa file (default: no) Output: directory/contigs.fa : fasta file of contigs longer than twice hash length directory/stats.txt : stats file (tab-spaced) useful for determining appropriate coverage cutoff directory/LastGraph : special formatted file with all the information on the final graph directory/velvet_asm.afg : (if requested) AMOS compatible assembly file"
toolshed.g2.bx.psu.edu/repos/devteam/velvet/velveth/1.2.10.3	"What it does
 Velvet_ is a de novo genomic assembler specially designed for short read sequencing technologies, such as Solexa or 454, developed by Daniel Zerbino and Ewan Birney at the European Bioinformatics Institute (EMBL-EBI), near Cambridge, in the United Kingdom. Velvet currently takes in short read sequences, removes errors then produces high quality unique contigs. It then uses paired-end read and long read information, when available, to retrieve the repeated areas between contigs. Read the Velvet 
documentation
 for details on using the Velvet Assembler. .. _Velvet: http://www.ebi.ac.uk/~zerbino/velvet/ .. 
: http://www.ebi.ac.uk/~zerbino/velvet/Manual.pdf ------ 
Velveth
 Velveth takes in a number of sequence files, produces a hashtable, then outputs two files in an output directory (creating it if necessary), Sequences and Roadmaps, which are necessary to velvetg. ------ 
Hash Length
 The hash length, also known as k-mer length, corresponds to the length, in base pairs, of the words being hashed. The hash length is the length of the k-mers being entered in the hash table. Firstly, you must observe three technical constraints:: # it must be an odd number, to avoid palindromes. If you put in an even number, Velvet will just decrement it and proceed. # it must be below or equal to MAXKMERHASH length (cf. 2.3.3, by default 31bp), because it is stored on 64 bits # it must be strictly inferior to read length, otherwise you simply will not observe any overlaps between reads, for obvious reasons. Now you still have quite a lot of possibilities. As is often the case, it's a trade- off between specificity and sensitivity. Longer kmers bring you more specificity (i.e. less spurious overlaps) but lowers coverage (cf. below). . . so there's a sweet spot to be found with time and experience. We like to think in terms of ""k-mer coverage"", i.e. how many times has a k-mer been seen among the reads. The relation between k-mer coverage Ck and standard (nucleotide-wise) coverage C is Ck = C # (L - k + 1)/L where k is your hash length, and L you read length. Experience shows that this kmer coverage should be above 10 to start getting decent results. If Ck is above 20, you might be ""wasting"" coverage. Experience also shows that empirical tests with different values for k are not that costly to run! 
Input Files
 Velvet works mainly with fasta and fastq formats. For paired-end reads, the assumption is that each read is next to its mate read. In other words, if the reads are indexed from 0, then reads 0 and 1 are paired, 2 and 3, 4 and 5, etc. Supported file formats are:: fasta fastq fasta.gz fastq.gz eland gerald Read categories are:: short (default) shortPaired short2 (same as short, but for a separate insert-size library) shortPaired2 (see above) long (for Sanger, 454 or even reference sequences) longPaired"
toolshed.g2.bx.psu.edu/repos/iuc/bbtools_bbmap/bbtools_bbmap/39.08+galaxy3	"What it does
 BBMap is a splice-aware global aligner for DNA and RNA sequencing reads. It is fast and extremely accurate, particularly with highly mutated genomes or reads with long indels, even whole-gene deletions over 100kbp long. It has no upper limit to genome size or number of contigs and has been successfully used for mapping to an 85 gigabase soil metagenome with over 200 million contigs. the indexing phase is very fast compared to other aligners. BBMap can output many different statistics files; an empirical read quality histogram, insert-size distribution, and genome coverage with or without generating a sam file. It is useful in quality control of libraries and sequencing runs or evaluating new sequencing platforms. 
Options
 
Bam sorting mode
 - the generated bam files can be sorted according to three criteria: coordinates, names and input order. * Sort by chromosomal coordinates - the file is sorted by coordinates (i.e., the reads from the beginning of the first chromosome are first in the file. * Sort by read names - the file is sorted by the reference ID (i.e., the QNAME field). * Not sorted (sorted as input) - the file is sorted in the order of the reads in the input file."
toolshed.g2.bx.psu.edu/repos/iuc/bbtools_bbmerge/bbtools_bbmerge/39.08+galaxy3	"What it does
 BBMerge merges two overlapping paired reads into a single read. A 2x100nt read pair, for instance can be merged into a single read of length 150nt if the last 50nt of the first read mate and the last 50nt of the second read map overlap. The accuracy of the base calling can also improve as a result of such a reconciliation between the read pairs. BBMerge is also capable of error-correcting the overlapping portion of reads without merging them, as well as merging nonoverlapping reads, if enough coverage is available. ----- 
A Martian PE sequencing result is expected to be processed as follows:
 input_R1.fastq:: @read_header_1/1 AAAAATTTTTAAAAACCCCCGGGGG + FFFFFFFFFFFFFFFEFFFFFF,FF @read_header_2/1 AAAATTTTAAAACCCCCGGGGG + FFFFFFFFFFFFFFFEFFFFFF input_R2.fastq:: @read_header_1/2 TTAATTAATTCCCCCGGGGG + FFFFFFFFFFFFFFFFFFFF @read_header_2/2 TTTAAATTTAAACCCCCGGGGG + FFFFFFFFFFFFFFFFFFFFEF output.fastq:: @read_header_1 AAAAATTTTTAAAAACCCCCGGGGGAATTAATTAA + FFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFF @read_header_2 AAAATTTTAAAACCCCCGGGGGTTTAAATTTAAA + FFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFF"
toolshed.g2.bx.psu.edu/repos/iuc/bbtools_bbnorm/bbtools_bbnorm/39.08+galaxy3	"What it does
 BBNorm downsamples a provided sequencing output, while paying attention to potential heteregeneities in sequencing depth obtained from the wet-lab workflow. The reads corresponding to regions with low coverage will be kept as is, whereas some of the reads contributing to an above-threshold coverage depth will be subsampled. The resulting data set is expected to be smaller in size, whereas the genome regions with low coverage levels will still be represented in the subsampled dataset. This provides a more uniform coverage depth against all genomic coordinates while the computational resources needed for subsequent steps such as assembly can be substantially reduced without losing coverage anywhere. ----- 
If the target sequencing depth is 2X, a Martian genome sequencing result is expected to be down-sampled as follows:
 input.fastq:: @read_header_1 AAAAATTTTTCCCCCGGGGGAAATTT + FFFFFFFFFFFFFFFEFFFFFF,FFE @read_header_2 TTTTTCCCCCGGGGGAAATTTCCCGGG + FFFFFFFFFFFFFFFEFFFFFFEFFDD @read_header_3 TTTTTCCCCCGGGGGAAATTTCCCGGG + FFFFFFFFFFFCEFFEFFFFFFEFFEE @read_header_4 TTTTTCCCCCGGGGGAAATTTCCCGGG + FFFFFDDFFFFFFFFEFFFFFFEFFEF @read_header_5 TTTTTCCCCCGGGGGAAATTTCCCGGG + FFFFFEFFFFEEFFFEFFFFFFDFFFF @read_header_6 AAAAATTTTTCCCCCGGGGGAAATTT + FFFFFFFFFFFFFFFEFFFFFFEFFD output.fastq:: @read_header_1 AAAAATTTTTCCCCCGGGGGAAATTT + FFFFFFFFFFFFFFFEFFFFFF,FFE @read_header_2 TTTTTCCCCCGGGGGAAATTTCCCGGG + FFFFFFFFFFFFFFFEFFFFFFEFFDD @read_header_3 TTTTTCCCCCGGGGGAAATTTCCCGGG + FFFFFFFFFFFCEFFEFFFFFFEFFEE @read_header_6 AAAAATTTTTCCCCCGGGGGAAATTT + FFFFFFFFFFFFFFFEFFFFFFEFFD 
Indications
 BBNorm is mainly intended for use in assembly pipelines out of short reads. It might be useful when there is too much data that increases computation time or a highly skewed coverage distribution by subsampling from the existing data. As opposed to keeping a randomly selected subset of reads, such as retaining the first n-many elements, this is a weighted resampling that tries to reduce coverage around coordinates of very high sequencing depth. 
Contraindications
 * The data already has a roughly uniform coverage that does not need to be normalised further. * You do not have any excess data to discard: BBnorm does not increase data quantity by imputation or by repeatedly sampling with replacement. * Your pipeline reports results that rely on quantification of abundance (ex: differential expression profiling or ChIP-Seq) * You want to do variant discovery. Reduction of sequencing depth might bias significance levels, or even obscure the existence of rare variants altogether. * The sequencing platform has a very high error rate (ex: ONT) that might mislead this algorithm."
toolshed.g2.bx.psu.edu/repos/iuc/bbtools_bbduk/bbtools_bbduk/39.08+galaxy3	"What it does
 BBDuk was developed to combine most common data-quality-related trimming, filtering, and masking operations into a single high-performance tool. It is capable of quality-trimming and filtering, adapter-trimming, contaminant-filtering via kmer matching, sequence masking, GC-filtering, length filtering, entropy-filtering, format conversion, histogram generation, subsampling, quality-score recalibration, kmer cardinality estimation, and various other operations in a single pass. Specifically, any combination of operations is possible in a single pass with the exception of kmer-based operations (kmer trimming, kmer masking, or kmer filtering). At most 1 kmer-based operation can be done in a single pass. 
Options
 * 
Reference
 - if a reference is specified, BBDuk will operate on kmers in one of 4 modes: right-trimming, left-trimming, masking, or filtering. The default is filtering - any read matching a reference kmer will be discarded. * 
Trim reads to remove bases matching reference kmers
 - When trimming to the right, once a reference kmer is matched in a read, that kmer and all the bases to the right will be trimmed, leaving only the bases to the left. When trimming to the left, trimming will be done to the left instead. 
Outputs
 * 
Unmatched
 - All the reads that pass all filtering criteria. Reads will be at least as long as 
Minimum read length
 after any trimming operations and reads will not match any reference kmer if kmer-filtering is being performed. A read’s average quality will be at least as high as the specified 
Minimum average quality
. * 
Matched
 - Reads failing any filter criteria (such as matching a reference kmer). By default, if either read in a pair fails, both will be included in 
Matched
. * 
Single
 - Singleton reads whose mate was trimmed shorter than the value of 
Minimum read length
."
toolshed.g2.bx.psu.edu/repos/iuc/bbtools_tadpole/bbtools_tadpole/39.08+galaxy3	"What it does
 Tadpole is a kmer-based assembler, with additional capabilities of error-correcting and extending reads. It does not do any complicated graph analysis or scaffolding, and therefore, is not particularly good for diploid organisms. However, compared to most other assemblers, it is incredibly fast, has a very low misassembly rate, and is very adept at handling extremely irregular or super high coverage distributions. It does not have any annoying side-effects of generating temp files and directories. Also, it can selectively assemble a coverage ‘band’ from a dataset (for example, just areas with a depth between 1000x and 1500x). These features make it a good choice for microbial single-cell data, viruses, organelles, and preliminary assemblies for use in binning, quality recalibration, insert-size estimation, and so forth. Tadpole has no upper limit on kmer length."
toolshed.g2.bx.psu.edu/repos/iuc/bbtools_callvariants/bbtools_callvariants/39.08+galaxy3	"What it does
 CallVariants is a high-speed, multithreaded variant caller that accepts bam files, and output VCF files. It is capable of indel realignment, multi-sample variant-calling, and processing samples with arbitrary ploidy. BBMap is the recommended mapping program for CallVariants, but output from any aligner is acceptable. Reads can be realigned with the ""realign"" flag. This is slower, but is highly recommended if the input is from any mapping tool other than BBMap. Output from BBMap should not be realigned. 
Options
 * 
Ploidy
 - Arbitrary ploidy is supported - the default is 1 which works for haploid organisms. Allele fractions lower than those expected for the ploidy (for example, anything below 0.5 for a diploid, or 0.25 for a tetraploid) will incur a score penalty. When calling variants on non-haploid organisms, it is crucial to set the ploidy."
toolshed.g2.bx.psu.edu/repos/iuc/bedops_sortbed/bedops-sort-bed/2.4.42	What this tool does =================== The sort-bed utility sorts BED files of any size, even larger than system memory. BED files that are in lexicographic-chromosome order allow BEDOPS utilities to work efficiently with data from any species without software modifications. Further, sorted files can be traversed very quickly. Sorted BED order is defined first by lexicographic chromosome order, then ascending integer start coordinate order, and finally by ascending integer end coordinate order. To make the sort order unambiguous, a lexicographical sort is applied on fourth and subsequent columns, where present in the input BED dataset. Input ===== The sort-bed utility requires one or more three-column BED file(s). Support for common headers (such as UCSC BED track headers) is included, although headers will be stripped from the output. Output ====== Sort order is defined by a lexicographical sort on chromosome name, a numerical sort on start coordinates, a numerical sort on stop coordinates where there are start matches, and finally a lexicographical sort on the remainder of the BED element (if additional columns are present). Additional options may be specified to print only unique or duplicate elements.
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_annotatebed/2.31.1	"What it does
 bedtools annotate, well, annotates one BED/bedGraph/GFF/VCF/EncodePeak file with the coverage and number of overlaps observed from multiple other BED/bedGraph/GFF/VCF/EncodePeak files. In this way, it allows one to ask to what degree one feature coincides with multiple other feature types with a single command. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bamtobed/2.31.1+galaxy0	"What it does
 bedtools bamtobed is a conversion utility that converts sequence alignments in BAM format into BED, BED12, and/or BEDPE records. .. class:: infomark The ""Report spliced BAM alignment..."" option breaks BAM alignments with the ""N"" (splice) operator into distinct BED entries. For example, using this option on a CIGAR such as 50M1000N50M would, by default, produce a single BED record that spans 1100bp. However, using this option, it would create two separate BED records that are each 50bp in size and are separated by 1000bp (the size of the N operation). This is important for RNA-seq and structural variation experiments. .. class:: warningmark If using a custom BAM alignment TAG as the BED score, note that this must be a numeric tag (e.g., type ""i"" as in NM:i:0). ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bedtobam/2.31.1+galaxy0	"What it does
 bedToBam converts features in a feature file to BAM format. This is useful as an efficient means of storing large genome annotations in a compact, indexed format for visualization purposes. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bedtoigv/2.31.1	"What it does
 Creates a batch script to create IGV images at each interval defined in a BED/bedGraph/GFF/VCF/EncodePeak file. 
Notes
 (1) The resulting script is meant to be run from within IGV. (2) It is assumed that prior to running the script, you've loaded the proper genome and tracks. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bed12tobed6/2.31.1	"What it does
 bed12ToBed6 is a convenience tool that converts BED features in BED12 (a.k.a. “blocked” BED features such as genes) to discrete BED6 features. For example, in the case of a gene with six exons, bed12ToBed6 would create six separate BED6 features (i.e., one for each exon). ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bedpetobam/2.31.1+galaxy0	"What it does
 Converts feature records to BAM format. .. class:: warningmark BED files must be at least BED4 to create BAM (needs name field). ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_closestbed/2.31.1+galaxy1	"What it does
 Similar to intersectBed, closestBed searches for overlapping features in A and B. In the event that no feature in B overlaps the current feature in A, closestBed will report the closest (that is, least genomic distance from the start or end of A) feature in B. For example, one might want to find which is the closest gene to a significant GWAS polymorphism. Note that closestBed will report an overlapping feature as the closest—that is, it does not restrict to closest non-overlapping feature. .. image:: $PATH_TO_IMAGES/closest-glyph.png ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_clusterbed/2.31.1	"What it does
 Similar to merge, cluster report each set of overlapping or “book-ended” features in an interval file. In contrast to merge, cluster does not flatten the cluster of intervals into a new meta-interval; instead, it assigns an unique cluster ID to each record in each cluster. This is useful for having fine control over how sets of overlapping intervals in a single interval file are combined. .. image:: $PATH_TO_IMAGES/cluster-glyph.png .. class:: warningmark bedtools cluster requires that you presort your data by chromosome and then by start position (e.g., sort -k1,1 -k2,2n in.bed > in.sorted.bed for BED files). ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_complementbed/2.31.1+galaxy0	"What it does
 bedtools complement returns all intervals in a genome that are not covered by at least one interval in the input BED/bedGraph/GFF/VCF/EncodePeak file. .. image:: $PATH_TO_IMAGES/complement-glyph.png ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_coveragebed/2.31.1+galaxy0	"What it does
 
bedtools coverage
 computes both the 
depth
 and 
breadth
 of coverage of features in file B on the features in file A. For example, 
bedtools coverage
 can compute the coverage of sequence alignments (file B) across 1 kilobase (arbitrary) windows (file A) tiling a genome of interest. One advantage that 
bedtools coverage
 offers is that it not only 
counts
 the number of features that overlap an interval in file A, it also computes the fraction of bases in the interval in A that were overlapped by one or more features. Thus, 
bedtools coverage
 also computes the 
breadth
 of coverage for each interval in A. .. _bedtools coverage: http://bedtools.readthedocs.org/en/latest/content/tools/coverage.html .. class:: infomark The lines in the output will be comprised of each interval in A, followed by: 1. The number of features in B that overlapped (by at least one base pair) the A interval. 2. The number of bases in A that had non-zero coverage from features in B. 3. The length of the entry in A. 4. The fraction of bases in A that had non-zero coverage from features in B. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. _bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bamtofastq/2.27.1	"What it does
 bedtools bamtofastq is a conversion utility for extracting FASTQ records from sequence alignments in BAM format. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_expandbed/2.31.1	"What it does
 Replicate lines in a file based on columns of comma-separated values. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_fisher/2.31.1+galaxy0	"What it does
 Perform fisher’s exact test on the number of overlaps/unique intervals between 2 files. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_flankbed/2.31.1+galaxy0	"What it does
 bedtools flank will optionally create flanking intervals whose size is user-specified fraction of the original interval. .. image:: $PATH_TO_IMAGES/flank-glyph.png .. class:: warningmark In order to prevent creating intervals that violate chromosome boundaries, bedTools flank requires a bedTool genome file defining the length of each chromosome or contig. . This should be a two column tabular file with the chromosome name in the first column and the END coordinate of the chromosome in the second column. If you need this data for any genome that is at UCSC (http://genome.ucsc.edu), it can be extracted from the Table Browser with the ""Get Data: UCSC Main"" tool. Set ""group"" to ""All Tables"", ""table"" to ""chromInfo"", and ""output format"" to ""all fields from selected table"". ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_genomecoveragebed/2.31.1	"What it does
 This tool calculates the genome-wide coverage of intervals defined in a BAM or BED file and reports them in BedGraph format. .. image:: $PATH_TO_IMAGES/genomecov-glyph.png .. class:: warningmark The input BED or BAM file must be sorted by chromosome name (but doesn't necessarily have to be sorted by start position). ----- 
Example 1
 Input (BED format)- Overlapping, un-sorted intervals:: chr1 140 176 chr1 100 130 chr1 120 147 Output (BedGraph format)- Sorted, non-overlapping intervals, with coverage value on the 4th column:: chr1 100 120 1 chr1 120 130 2 chr1 130 140 1 chr1 140 147 2 chr1 147 176 1 ----- 
Example 2 - with ZERO-Regions selected (assuming hg19)
 Input (BED format)- Overlapping, un-sorted intervals:: chr1 140 176 chr1 100 130 chr1 120 147 BedGraph output will contain five columns: * 1. Chromosome name (or 'genome' for whole-genome coverage) * 2. Coverage depth * 3. The number of bases on chromosome (or genome) with depth equal to column 2. * 4. The size of chromosome (or entire genome) in base pairs * 5. The fraction of bases on chromosome (or entire genome) with depth equal to column 2. 
Example Output
: chr2L 0 1379895 23011544 0.0599653 chr2L 1 837250 23011544 0.0363839 chr2L 2 904442 23011544 0.0393038 chr2L 3 913723 23011544 0.0397072 chr2L 4 952166 23011544 0.0413778 chr2L 5 967763 23011544 0.0420555 chr2L 6 986331 23011544 0.0428624 chr2L 7 998244 23011544 0.0433801 chr2L 8 995791 23011544 0.0432735 chr2L 9 996398 23011544 0.0432999 ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_groupbybed/2.31.1+galaxy0	"What it does
 Replicate lines in a file based on columns of comma-separated values. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.31.1+galaxy0	"What it does
 By far, the most common question asked of two sets of genomic features is whether or not any of the features in the two sets “overlap” with one another. This is known as feature intersection. bedtools intersect allows one to screen for overlaps between two sets of genomic features. Moreover, it allows one to have fine control as to how the intersections are reported. bedtools intersect works with both BED/bedGraph/GFF/VCF/EncodePeak and BAM files as input. .. image:: $PATH_TO_IMAGES/intersect-glyph.png .. class:: infomark Note that each BAM alignment is treated individually. Therefore, if one end of a paired-end alignment overlaps an interval in the BED file, yet the other end does not, the output file will only include the overlapping end. .. class:: infomark Note that a BAM alignment will be sent to the output file 
once
 even if it overlaps more than one interval in the BED file. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_jaccard/2.31.1	"What it does
 By default, bedtools jaccard reports the length of the intersection, the length of the union (minus the intersection), the final Jaccard statistic reflecting the similarity of the two sets, as well as the number of intersections. Whereas the bedtools intersect tool enumerates each an every intersection between two sets of genomic intervals, one often needs a single statistic reflecting the similarity of the two sets based on the intersections between them. The Jaccard statistic is used in set theory to represent the ratio of the intersection of two sets to the union of the two sets. Similarly, Favorov et al [1] reported the use of the Jaccard statistic for genome intervals: specifically, it measures the ratio of the number of intersecting base pairs between two sets to the number of base pairs in the union of the two sets. The bedtools jaccard tool implements this statistic, yet modifies the statistic such that the length of the intersection is subtracted from the length of the union. As a result, the final statistic ranges from 0.0 to 1.0, where 0.0 represents no overlap and 1.0 represent complete overlap. .. image:: $PATH_TO_IMAGES/jaccard-glyph.png .. class:: warningmark The jaccard tool requires that your data is pre-sorted by chromosome and then by start position (e.g., sort -k1,1 -k2,2n in.bed > in.sorted.bed for BED files). ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_links/2.31.1	"What it does
 Creates an HTML file with links to an instance of the UCSC Genome Browser for all features / intervals in a file. This is useful for cases when one wants to manually inspect through a large set of annotations or features. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_makewindowsbed/2.31.1+galaxy0	"What it does
 Makes adjacent or sliding windows across a genome or BED file. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_map/2.31.1.3	"What it does
 bedtools map allows one to map overlapping features in a B file onto features in an A file and apply statistics and/or summary operations on those features. .. image:: $PATH_TO_IMAGES/map-glyph.png .. class:: infomark bedtools map requires each input file to be sorted by genome coordinate. For BED files, this can be done with sort -k1,1 -k2,2n. Other sorting criteria are allowed if a genome file (-g) is provides that specifies the expected chromosome order. .. class:: infomark The map tool is substantially faster in versions 2.19.0 and later. The plot below demonstrates the increased speed when, for example, counting the number of exome alignments that align to each exon. The bedtools times are compared to the bedops bedmap utility as a point of reference. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_maskfastabed/2.31.1	"What it does
 bedtools maskfasta masks sequences in a FASTA file based on intervals defined in a feature file. The headers in the input FASTA file must exactly match the chromosome column in the feature file. This may be useful fro creating your own masked genome file based on custom annotations or for masking all but your target regions when aligning sequence data from a targeted capture experiment. .. image:: $PATH_TO_IMAGES/maskfasta-glyph.png ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_unionbedgraph/2.31.1	"What it does
 This tool merges multiple BedGraph files, allowing direct and fine-scale coverage comparisons among many samples/files. The BedGraph files need not represent the same intervals; the tool will identify both common and file-specific intervals. In addition, the BedGraph values need not be numeric: one can use any text as the BedGraph value and the tool will compare the values from multiple files. .. class:: warningmark This tool requires that each BedGraph file is reference-sorted (chrom, then start) and contains non-overlapping intervals (within a given file). ------ 
Example input
:: # 1.bedgraph chr1 1000 1500 10 chr1 2000 2100 20 # 2.bedgraph chr1 900 1600 60 chr1 1700 2050 50 # 3.bedgraph chr1 1980 2070 80 chr1 2090 2100 20 ------ 
Examples using the Zero Coverage checkbox
 Output example (
without
 checking ""Report regions with zero coverage""):: chr1 900 1000 0 60 0 chr1 1000 1500 10 60 0 chr1 1500 1600 0 60 0 chr1 1700 1980 0 50 0 chr1 1980 2000 0 50 80 chr1 2000 2050 20 50 80 chr1 2050 2070 20 0 80 chr1 2070 2090 20 0 0 chr1 2090 2100 20 0 20 Output example (
with
 checking ""Report regions with zero coverage""). The lines marked with (
) are not covered in any input file, but are still reported (The asterisk marking does not appear in the file).:: chr1 0 900 0 0 0 (
) chr1 900 1000 0 60 0 chr1 1000 1500 10 60 0 chr1 1500 1600 0 60 0 chr1 1600 1700 0 0 0 (
) chr1 1700 1980 0 50 0 chr1 1980 2000 0 50 80 chr1 2000 2050 20 50 80 chr1 2050 2070 20 0 80 chr1 2070 2090 20 0 0 chr1 2090 2100 20 0 20 chr1 2100 247249719 0 0 0 (
) ------ 
Examples adjusting the ""Filler value"" for no-covered intervals
 The default value is '0', but you can use any other value. Output example with 
filler = N/A
:: chr1 900 1000 N/A 60 N/A chr1 1000 1500 10 60 N/A chr1 1500 1600 N/A 60 N/A chr1 1600 1700 N/A N/A N/A chr1 1700 1980 N/A 50 N/A chr1 1980 2000 N/A 50 80 chr1 2000 2050 20 50 80 chr1 2050 2070 20 N/A 80 chr1 2070 2090 20 N/A N/A chr1 2090 2100 20 N/A 20 ------ 
Examples using the ""sample name"" labels
:: chrom start end WT-1 WT-2 KO-1 chr1 900 1000 N/A 60 N/A chr1 1000 1500 10 60 N/A chr1 1500 1600 N/A 60 N/A chr1 1600 1700 N/A N/A N/A chr1 1700 1980 N/A 50 N/A chr1 1980 2000 N/A 50 80 chr1 2000 2050 20 50 80 chr1 2050 2070 20 N/A 80 chr1 2070 2090 20 N/A N/A chr1 2090 2100 20 N/A 20 ------ 
Non-numeric values
 The input BedGraph files can contain any kind of value in the fourth column, not necessarily a numeric value. Input Example:: File-1 File-2 chr1 200 300 Sample1 chr1 100 240 0.75 chr1 400 450 Sample1 chr1 250 700 0.43 chr1 530 600 Sample2 Output Example:: chr1 100 200 0 0.75 chr1 200 240 Sample1 0.75 chr1 240 250 Sample1 0 chr1 250 300 Sample1 0.43 chr1 300 400 0 0.43 chr1 400 450 Sample1 0.43 chr1 450 530 0 0.43 chr1 530 600 Sample2 0.43 chr1 600 700 0 0.43 ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_mergebed/2.31.1+galaxy2	"What it does
 bedtools merge combines overlapping or ""book-ended"" features in an interval file into a single feature which spans all of the combined features. .. image:: $PATH_TO_IMAGES/merge-glyph.png .. class:: warningmark bedtools merge requires that you presort your data by chromosome and then by start position. ========================================================================== Default behavior ========================================================================== By default, 
bedtools merge
 combines overlapping (by at least 1 bp) and/or bookended intervals into a single, ""flattened"" or ""merged"" interval. :: $ cat A.bed chr1 100 200 chr1 180 250 chr1 250 500 chr1 501 1000 $ bedtools merge -i A.bed chr1 100 500 chr1 501 1000 ========================================================================== 
-s
 Enforcing ""strandedness"" ========================================================================== The 
-s
 option will only merge intervals that are overlapping/bookended 
and
 are on the same strand. :: $ cat A.bed chr1 100 200 a1 1 + chr1 180 250 a2 2 + chr1 250 500 a3 3 - chr1 501 1000 a4 4 + $ bedtools merge -i A.bed -s chr1 100 250 + chr1 501 1000 + chr1 250 500 - ========================================================================== 
-d
 Controlling how close two features must be in order to merge ========================================================================== By default, only overlapping or book-ended features are combined into a new feature. However, one can force 
merge
 to combine more distant features with the 
-d
 option. For example, were one to set 
-d
 to 1000, any features that overlap or are within 1000 base pairs of one another will be combined. :: $ cat A.bed chr1 100 200 chr1 501 1000 $ bedtools merge -i A.bed chr1 100 200 chr1 501 1000 $ bedtools merge -i A.bed -d 1000 chr1 100 200 1000 ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_multicovtbed/2.31.1	"What it does
 bedtools multicov, reports the count of alignments from multiple position-sorted and indexed BAM files that overlap intervals in a BED file. Specifically, for each BED interval provided, it reports a separate count of overlapping alignments from each BAM file. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_multiintersectbed/2.31.1	"What it does
 This tool identifies common intervals among multiple, sorted BED files. Intervals can be common among 0 to N of the N input BED files. .. class:: warningmark This tool requires that each BED file is reference-sorted (chrom, then start). .. class:: infomark The output file will contain five fixed columns, plus additional columns for each BED file: * 1. Chromosome name (or 'genome' for whole-genome coverage). * 2. The zero-based start position of the interval. * 3. The one-based end position of the interval. * 4. The number of input files that had at least one feature overlapping this interval. * 5. A list of input files or labels that had at least one feature overlapping this interval. * 6. For each input file, an indication (1 = Yes, 0 = No) of whether or not the file had at least one feature overlapping this interval. ------ 
Example input
:: # a.bed chr1 6 12bed chr1 10 20 chr1 22 27 chr1 24 30 # b.bed chr1 12 32 chr1 14 30 # c.bed chr1 8 15 chr1 10 14 chr1 32 34 ------ 
Example without a header and without reporting intervals with zero coverage
:: chr1 6 8 1 1 1 0 0 chr1 8 12 2 1,3 1 0 1 chr1 12 15 3 1,2,3 1 1 1 chr1 15 20 2 1,2 1 1 0 chr1 20 22 1 2 0 1 0 chr1 22 30 2 1,2 1 1 0 chr1 30 32 1 2 0 1 0 chr1 32 34 1 3 0 0 1 
Example adding a header line
:: chrom start end num list a.bed b.bed c.bed chr1 6 8 1 1 1 0 0 chr1 8 12 2 1,3 1 0 1 chr1 12 15 3 1,2,3 1 1 1 chr1 15 20 2 1,2 1 1 0 chr1 20 22 1 2 0 1 0 chr1 22 30 2 1,2 1 1 0 chr1 30 32 1 2 0 1 0 chr1 32 34 1 3 0 0 1 
Example adding a header line and custom file labels
:: chrom start end num list joe bob sue chr1 6 8 1 joe 1 0 0 chr1 8 12 2 joe,sue 1 0 1 chr1 12 15 3 joe,bob,sue 1 1 1 chr1 15 20 2 joe,bob 1 1 0 chr1 20 22 1 bob 0 1 0 chr1 22 30 2 joe,bob 1 1 0 chr1 30 32 1 bob 0 1 0 chr1 32 34 1 sue 0 0 1 ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_nucbed/2.31.1	"What it does
 Profiles the nucleotide content of intervals in a fasta file. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_overlapbed/2.31.1	"What it does
 overlap computes the amount of overlap (in the case of positive values) or distance (in the case of negative values) between feature coordinates occurring on the same input line and reports the result at the end of the same line. In this way, it is a useful method for computing custom overlap scores from the output of other BEDTools. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_randombed/2.31.1+galaxy1	"What it does
 bedtools random will generate a random set of intervals in BED6 format. One can specify both the number (-n) and the size (-l) of the intervals that should be generated. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_reldistbed/2.31.1	"What it does
 Traditional approaches to summarizing the similarity between two sets of genomic intervals are based upon the number or proportion of intersecting intervals. However, such measures are largely blind to spatial correlations between the two sets where, dpesite consistent spacing or proximity, intersections are rare (for example, enhancers and transcription start sites rarely overlap, yet they are much closer to one another than two sets of random intervals). Favorov et al proposed a relative distance metric that describes distribution of relative distances between each interval in one set nd the two closest intervals in another set (see figure above). If there is no spatial correlation between the two sets, one would expect the relative distances to be uniformaly distributed among the relative distances ranging from 0 to 0.5. If, however, the intervals tend to be much closer than expected by chance, the distribution of observed relative distances would be shifted towards low relative distance values (e.g., the figure below). .. image:: $PATH_TO_IMAGES/reldist-glyph.png .. class:: infomark ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_shufflebed/2.31.1+galaxy1	"What it does
 bedtools shuffle will randomly permute the genomic locations of a feature file among a genome defined in a genome file. One can also provide an “exclusions” BED/bedGraph/GFF/VCF/EncodePeak file that lists regions where you do not want the permuted features to be placed. For example, one might want to prevent features from being placed in known genome gaps. shuffle is useful as a null basis against which to test the significance of associations of one feature with another. .. image:: $PATH_TO_IMAGES/shuffle-glyph.png ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_slopbed/2.31.1+galaxy0	"What it does
 bedtools slop will increase the size of each feature in a feature file by a user-defined number of bases. While something like this could be done with an awk '{OFS=""\t"" print $1,$2-&lt;slop>,$3+&lt;slop>}', bedtools slop will restrict the resizing to the size of the chromosome (i.e. no start &lt; 0 and no end > chromosome size). .. image:: $PATH_TO_IMAGES/slop-glyph.png .. class:: warningmark In order to prevent the extension of intervals beyond chromosome boundaries, bedtools slop requires a genome file defining the length of each chromosome or contig. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_sortbed/2.31.1+galaxy0	"What it does
 Sorts a feature file by chromosome and other criteria. .. class:: warningmark It should be noted that sortBed is merely a convenience utility, as the UNIX sort utility will sort BED files more quickly while using less memory. For example, UNIX sort will sort a BED file by chromosome then by start position in the following manner: sort -k 1,1 -k2,2 -n a.bed ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_spacingbed/2.31.1	"What it does
 Report the spacing between intervals in a file. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_subtractbed/2.31.1	"What it does
 bedtools subtract searches for features in B that overlap A. If an overlapping feature is found in B, the overlapping portion is removed from A and the remaining portion of A is reported. If a feature in B overlaps all of a feature in A, the A feature will not be reported. .. image:: $PATH_TO_IMAGES/subtract-glyph.png ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_tagbed/2.31.1	"What it does
 Annotates a BAM file based on overlaps with multiple BED/bedGraph/GFF/VCF/EncodePeak files on the intervals in an input bam file ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_windowbed/2.31.1	"What it does
 Similar to bedtools intersect, window searches for overlapping features in A and B. However, window adds a specified number (1000, by default) of base pairs upstream and downstream of each feature in A. In effect, this allows features in B that are “near” features in A to be detected. .. image:: $PATH_TO_IMAGES/window-glyph.png ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_getfastabed/2.31.1+galaxy0	"What it does
 bedtools getfasta will extract the sequence defined by the coordinates in a BED interval and create a new FASTA entry in the output file for each extracted sequence. By default, the FASTA header for each extracted sequence will be formatted as follows: “>chrom>:&lt;start>-&lt;end>”. .. image:: $PATH_TO_IMAGES/getfasta-glyph.png .. class:: warningmark 1. The headers in the input FASTA file must exactly match the chromosome column in the BED file. 2. You can use the UNIX fold command to set the line width of the FASTA output. For example, fold -w 60 will make each line of the FASTA file have at most 60 nucleotides for easy viewing. ------ This tool is part of the 
bedtools package
 from the 
Quinlan laboratory
. .. 
bedtools package: https://github.com/arq5x/bedtools2 .. _Quinlan laboratory: http://quinlanlab.org 
Citation
 If you use this tool in Galaxy, please cite: Bjoern A. Gruening (2014), 
Galaxy wrapper &lt;https://github.com/bgruening/galaxytools&gt;"
toolshed.g2.bx.psu.edu/repos/ecology/obisindicators/obisindicators/0.0.2	"================================================ Calculates biodiversity indicators and maps them ================================================ 
What it does
 This tool calculates the number of records, species richness, Simpson index, Shannon index, Hurlbert index (n = 50), and Hill numbers for each cell. 
Input description
 A tabular file with observation data. Must at least contain four columns longitude, latitude, taxon (any taxon level : species, family, ...) and records. +----------+-----------+---------+------------+ | latitude | longitude | taxon | records | +==========+===========+=========+============+ | lat |long | taxonID | 4 | +----------+-----------+---------+------------+ | ... | ... | ... | ... | +----------+-----------+---------+------------+ 
Output
 - A text output that summarizes in a tabular the results of all the indicators. - Multiple PNG files representing the indicators on a map (according to the choice of map made)."
toolshed.g2.bx.psu.edu/repos/iuc/ngsutils_bam_filter/ngsutils_bam_filter/0.5.9	"Removes reads from a BAM file based on criteria. Given a BAM file, this tool will discard reads that did not meet the selected filtering criteria The output is another BAM file with the reads not matching the criteria removed. Note: this does not adjust tag values reflecting any filtering. (for example: if a read mapped to two locations (IH:i:2), and one was removed by filtering, the IH:i tag would still read IH:i:2). Currently, the available filters are: +--------------------------------+-------------------------------------------------+ | Agument | Description | +================================+=================================================+ | -minlen val | Remove reads that are smaller than {val} | +--------------------------------+-------------------------------------------------+ | -maxlen val | Remove reads that are larger than {val} | +--------------------------------+-------------------------------------------------+ | -mapped | Keep only mapped reads | +--------------------------------+-------------------------------------------------+ | -unmapped | Keep only unmapped reads | +--------------------------------+-------------------------------------------------+ | -properpair | Keep only properly paired reads (both mapped, | | | correct orientation, flag set in BAM) | +--------------------------------+-------------------------------------------------+ | -noproperpair | Keep only not-properly paired reads | +--------------------------------+-------------------------------------------------+ | -mask bitmask | Remove reads that match the mask (e.g. 0x400) | +--------------------------------+-------------------------------------------------+ | -uniq {length} | Remove reads that are have the same sequence | | | Note: BAM file should be sorted | | | (up to an optional length) | +--------------------------------+-------------------------------------------------+ | -uniq_start | Remove reads that start at the same position | | | Note: BAM file should be sorted | | | (Use only for low-coverage samples) | +--------------------------------+-------------------------------------------------+ |-mismatch num | Number of mismatches or indels | | | indel always counts as 1 regardless of length | | | (requires NM tag in reads) | +--------------------------------+-------------------------------------------------+ |-nosecondary | Remove reads that have the 0x100 flag set | +--------------------------------+-------------------------------------------------+ |-noqcfail | Remove reads that have the 0x200 flag set | +--------------------------------+-------------------------------------------------+ |-nopcrdup | Remove reads that have the 0x400 flag set | +--------------------------------+-------------------------------------------------+ |-excludebed file.bed {nostrand} | Remove reads that are in any of the regions | | | from the given BED file. If 'nostrand' is given,| | | strand information from the BED file is ignored.| +--------------------------------+-------------------------------------------------+ |-includebed file.bed {nostrand} | Remove reads that are NOT any of the regions | | | from the given BED file. If 'nostrand' is given,| | | strand information from the BED file is ignored.| | | Note: If this is a large dataset, use | | | ""bamutils extract"" instead. | +--------------------------------+-------------------------------------------------+ | -includeref refname | Exclude reads NOT mapped to a reference | +--------------------------------+-------------------------------------------------+ | -excluderef refname | Exclude reads mapped to a particular reference | | | (e.g. chrM, or _dup chromosomes) | +--------------------------------+-------------------------------------------------+"
toolshed.g2.bx.psu.edu/repos/iuc/cwpair2/cwpair2/1.1.1	"What it does
 CWPair accepts one or more gff files as input and takes the peak location to be the midpoint between the exclusion zone start and end coordinate (columns D and E). CWPair starts with the highest peak (primary peak) in the dataset, and then looks on the opposite strand for another peak located within the distance defined by a combination of the tool's 
Distance upstream from a peak to allow a pair
 (the distance upstream or 5’ to the primary peak) and 
Distance downstream from a peak to allow a pair
 (the distance downstream or 3’ to the primary peak) parameters. So ""upstream"" value 30 ""downstream"" value 20 makes the tool look 30 bp upstream and 20 bp downstream (inclusive). Consequently, the search space would be 51 bp, since it includes the primary peak coordinate. The use of a negative number changes the direction of the search limits. So, ""upstream"" -30 and ""downstream"" 20 produces an 11 bp downstream search window (20-30 bp downstream, inclusive). .. image:: $PATH_TO_IMAGES/cwpair2.png When encountering multiple candidate peaks within the search window, CWPair uses the resolution method defined by the tool's 
Method of finding a match
 parameter as follows: * 
mode
 - This is an iterative process in which all peak-pair distances within the search window are determined, and the mode calculated. The pair whose distance apart is closest to the mode is then selected. * 
closest
 - Pairs the peak that has the closest absolute distance from the primary peak. * 
largest
 - Pairs the peak that has the highest tag count. * 
all
 - Runs all three methods, producing separate outputs for each. When considering the candidate peaks for pairing to a primary peak, a tag-count threshold may also be set using the tool's 
Filter using relative/absolute threshold
 parameter. A relative threshold determines the tag counts at the 95th percentile of peak occupancy (i.e. top 5% in terms of tag counts), then uses a tag count threshold at the specified percentage of this 95th percentile. So if the peak at the 95th percentile has 200 tags, and ""relative threshold"" 50 is used, then it will not consider any peak having less than 100 tags. ----- 
Options
 * 
Method of finding match
 - Method of finding matched pair, mode, closest, largest, or all (run with each method). * 
Distance upstream from a peak to allow a pair
 - The maximum distance (inclusive) upstream on the opposite strand from the primary peak to locate another peak, resulting in a pair. * 
Distance downstream from a peak to allow a pair
 - The maximum distance (inclusive) downstream on the opposite strand from the primary peak to locate another peak, resulting in a pair. * 
Percentage of the 95 percentile value to filter below
 - Percentage of the 95 percentile value below which to filter when using a relative threshold. * 
Absolute value to filter below
 - Absolute value below which to filter when using an absolute threshold. * 
Output files
 - Restrict output dataset collections to matched pairs only or one of several combinations of collection types. ----- 
Output Data Files
 * 
closest/largest/mode MP
 - gff file containing the Matched Pairs and includes the peak-pair midpoint coordinate (column D) and the coordinate +1 (column E). The tag count sum is reported in column F, along with the C-W distance in bp in column I. * 
closest/largest/mode O
 - tabular file containing the Orphans (all peaks that are not in pairs). * 
closest/largest/mode D
 - tabular file containing the Details, which lists + and – strand information separately. The start and end represent the lower and higher coordinates of the exclusion zone from GeneTrack, and “Value” is the tag count sum within the exclusion zone. The peak pair midpoint is calculated along with the distance between the two paired peaks (midpoint-to-midpoint or C-W distance). 
Output Statistics Files
 * 
closest/largest/mode C
 - pdf file that provides the frequency distribution of peak pair distances. * 
closest/largest/mode P
 - pdf file that provides the preview plots graph (the initial iteration of the process for finding the mode). * 
closest/largest/mode F
 - pdf file that provides the final plots graph. * 
Statistics Table
 - provides the number of peaks in pairs (dividing this by 2 provides the number of peak-pairs)."
toolshed.g2.bx.psu.edu/repos/rnateam/chipseeker/chipseeker/1.28.3+galaxy0	".. class:: infomark 
What it does
 ChIPseeker_ is a Bioconductor package for annotating ChIP-seq data analysis. Peak Annotation is performed by the annotatePeak function. The position and strand information of nearest genes are reported, in addition to the distance from the peak to the TSS of its nearest gene. Users can define the TSS (transcription start site) region under 
Advanced Options
, by default the TSS region is defined from -3kb to +3kb. The genomic region of the peak is reported in the annotation column. Since some annotations may overlap for a peak, ChIPseeker adopts the following priority in genomic annotation: * Promoter * 5’ UTR * 3’ UTR * Exon * Intron * Downstream * Intergenic ChIPseeker also produces plots to help users visualise the overlaps in annotation for peaks, for example, the vennpie and upsetplot. See the 
ChIPseeker vignette
 for more information. ----- 
Inputs
 A peaks file in BED, Interval or Tabular format e.g from MACS2 or DiffBind. Note that there is an option to specify if the input peaks file has a header row. No header row is assumed by default, which is usually the case for BED format e.g. MACS narrowpeak, however other formats e.g. MACS tabular format, may contain a header row. Example: ===== ====== ====== ======== ===== ====== Chrom Start End Name Score Strand ===== ====== ====== ======== ===== ====== 18 394599 396513 DiffBind 0 . 18 111566 112005 DiffBind 0 . 18 346463 347342 DiffBind 0 . 18 399013 400382 DiffBind 0 . 18 371109 372102 DiffBind 0 . ===== ====== ====== ======== ===== ====== A GTF file for annotation. The GTF file must have fields called ""gene_id"" and gene_name"". ----- 
Outputs
 This tool outputs * a file of annotated peaks in Interval or Tabular format * a PDF of plots (plotAnnoPie, plotAnnoBar, vennpie, upsetplot, plotDistToTSS) Optionally, you can choose to output * the R script used by this tool * an RData file 
Annotated peaks
 Annotation similar to below will be added to the input file. Example - 
Interval format
: ===== ====== ====== ===================================================================================================================================================== Chrom Start End Comment ===== ====== ====== ===================================================================================================================================================== 18 394599 396513 DiffBind|0|.|Intron (ENST00000400256/ENSG00000158270, intron 1 of 1)|1|346465|400382|53918|2|ENST00000400256| 3869|COLEC12|ENSG00000158270 18 346463 347342 DiffBind|0|.|Exon (ENST00000400256/ENSG00000158270, exon 1 of 1)|1|346465|400382|53918|2|ENST00000400256|53040|COLEC12|ENSG00000158270 18 399013 400382 DiffBind|0|.|Promoter (<=1kb)|1|346465|400382|53918|2|ENST00000400256| 0|COLEC12|ENSG00000158270 18 371109 372102 DiffBind|0|.|Intron (ENST00000400256/ENSG00000158270, intron 1 of 1)|1|346465|400382|53918|2|ENST00000400256|28280|COLEC12|ENSG00000158270 18 111566 112005 DiffBind|0|.|Promoter (<=1kb)|1|111568|112005| 438|1|ENST00000608049| 0|ROCK1P1|ENSG00000263006 ===== ====== ====== ===================================================================================================================================================== Columns contain the following data: * 
Chrom
: Chromosome name * 
Start
: Start position of site * 
End
: End position of site * 
Comment
: The pipe (""|"") separated values in this column correspond to: * 
<Any additional input columns>
 * 
annotation
 (Promoter, 5’ UTR, 3’ UTR, Exon, Intron, Downstream, Intergenic) * 
geneChr
 * 
geneStart
 * 
geneEnd
 * 
geneLength
 * 
geneStrand
 * 
transcriptId
 * 
distanceToTSS
 * 
geneName
 * 
geneId
 Example - 
Tabular format
: ===== ====== ====== ======== ====== ====== =========================================== ======================================================= ======= ========= ======= ========== ========== =============== ============= ======== =============== Chrom Start End Name Score Strand Comment annotation geneChr geneStart geneEnd geneLength geneStrand transcriptId distanceToTSS geneName geneId ===== ====== ====== ======== ====== ====== =========================================== ======================================================= ======= ========= ======= ========== ========== =============== ============= ======== =============== 18 394599 396513 DiffBind 0 . 1914|7.15|5.55|7.89|-2.35|7.06e-24|9.84e-21 Intron (ENST00000400256/ENSG00000158270, intron 1 of 1) 1 346465 400382 53918 2 ENST00000400256 3869 COLEC12 ENSG00000158270 18 346463 347342 DiffBind 0 . 879|5|5.77|3.24|2.52|6.51e-06|0.00303 Exon (ENST00000400256/ENSG00000158270, exon 1 of 1) 1 346465 400382 53918 2 ENST00000400256 53040 COLEC12 ENSG00000158270 18 399013 400382 DiffBind 0 . 1369|7.62|7|8.05|-1.04|1.04e-05|0.00364 Promoter (<=1kb) 1 346465 400382 53918 2 ENST00000400256 0 COLEC12 ENSG00000158270 18 371109 372102 DiffBind 0 . 993|4.63|3.07|5.36|-2.3|8.1e-05|0.0226 Intron (ENST00000400256/ENSG00000158270, intron 1 of 1) 1 346465 400382 53918 2 ENST00000400256 28280 COLEC12 ENSG00000158270 18 111566 112005 DiffBind 0 . 439|5.71|6.53|3.63|2.89|1.27e-08|8.88e-06 Promoter (<=1kb) 1 111568 112005 438 1 ENST00000608049 0 ROCK1P1 ENSG00000263006 ===== ====== ====== ======== ====== ====== =========================================== ======================================================= ======= ========= ======= ========== ========== =============== ============= ======== =============== .. _ChIPseeker: https://bioconductor.org/packages/release/bioc/html/ChIPseeker.html .. 
ChIPseeker vignette
: http://bioconductor.org/packages/release/bioc/vignettes/ChIPseeker/inst/doc/ChIPseeker.html"
toolshed.g2.bx.psu.edu/repos/bgruening/diffbind/diffbind/3.12.0+galaxy0	".. class:: infomark 
What it does
 DiffBind_ is a 
Bioconductor package
 that provides functions for processing ChIP-Seq data enriched for genomic loci where specific protein/DNA binding occurs, including peak sets identified by ChIP-Seq peak callers and aligned sequence read datasets. It is designed to work with multiple peak sets simultaneously, representing different ChIP experiments (antibodies, transcription factor and/or histone marks, experimental conditions, replicates) as well as managing the results of multiple peak callers. The primary emphasis of DiffBind is on identifying sites that are differentially bound between two sample groups. It includes functions to support the processing of peak sets, including overlapping and merging peak sets, counting sequencing reads overlapping intervals in peak sets, and identifying statistically significantly differentially bound sites based on evidence of binding affinity (measured by differences in read densities). To this end it uses statistical routines developed in an RNA-Seq context (primarily the Bioconductor packages edgeR and DESeq2). Additionally, the package builds on Rgraphics routines to provide a set of standardized plots to aid in binding analysis. The 
DiffBind User Guide
 includes a brief overview of the processing flow, followed by four sections of examples: the first focusing on the core task of obtaining differentially bound sites based on affinity data, the second working through the main plotting routines, the third discussing the use of a blocking factor, and the fourth revisiting occupancy data (peak calls) in more detail, as well as comparing the results of an occupancy-based analysis with an affinity-based one. Finally, certain technical aspects of the how these analyses are accomplished are detailed. 
Note this DiffBind tool requires a minimum of four samples (two groups with two replicates each).
 ----- 
Inputs
 DiffBind works primarily with peaksets, which are sets of genomic intervals representing candidate protein binding sites. Each interval consists of a chromosome, a start and end position, and usually a score of some type indicating confidence in, or strength of, the peak. Associated with each peakset are metadata relating to the experiment from which the peakset was derived. Additionally, files containing mapped sequencing reads (BAM files) need to be associated with each peakset (one for the ChIP data, and optionally another representing a control sample) Inputs for a group will be sorted by identifier before processing. For each group the corresponding sets of peak and BAM files need to be provided. Ideally this is accomplished by providing the data in collections. 
Groups
 You have to specify the name of the Group and the peak and BAM files for the two Groups you want to compare (e.g Resistant and Responsive) in the tool form above. Example: ============= ============= 
Sample
 
Group
 ------------- ------------- BT4741 Resistant BT4742 Resistant MCF71 Responsive MCF72 Responsive ============= ============= 
Peak files
 Result of your Peak calling experiment in bed format, one file for each sample is required. The peak caller, format and score column can be specified in the tool form above. The default settings expect narrowPeak bed format, which has the score in the 8th column (-log10pvalue), and can be output from MACS2. Example: ======= ======= ======= =============== ============== 1 2 3 4 
5 (Score)
 ======= ======= ======= =============== ============== chr18 215562 216063 peak_16037 56.11 chr18 311530 312105 peak_16038 222.49 chr18 356656 357315 peak_16039 92.06 chr18 371110 372092 peak_16040 123.86 chr18 395116 396464 peak_16041 1545.39 chr18 399014 400382 peak_16042 1835.19 chr18 499134 500200 peak_16043 748.32 chr18 503518 504552 peak_16044 818.30 chr18 531672 532274 peak_16045 159.30 chr18 568326 569282 peak_16046 601.11 ======= ======= ======= =============== ============== * BAM file which contains the mapped sequencing reads associated with each peakset, one file for each sample is required. * Optional: Control BAM file representing a control dataset. If used, has to be specified for all samples. Note that the DiffBind authors say control reads are best utilized prior to running DiffBind, at the peak calling stage (e.g. with MACS2) and in blacklists, see this 
Bioconductor post
. ----- 
Outputs
 This tool outputs * a table of differentially bound sites in Interval, BED or Tabular 0-based format Optionally, under 
Output Options
 you can choose to output * a PDF of plots (Heatmap, PCA, MA, Volcano, Boxplots) * a binding affinity matrix * the R script used by this tool * an RData file of the R objects generated * a text file with information on the analysis (number of Intervals, FriP scores, method used) 
Differentially Bound Sites
 The default output is Interval format, for information on Interval format see here
. Alternatively, you can choose to output BED or Tabular 0-based format as below. For an explanation of the 0-based and 1-based coordinate systems see this 
Biostars post
. Example - 
Interval format
: ====== ====== ====== ======== ===== ====== =========================================== Chrom Start End Name Score Strand 
Comment
 ====== ====== ====== ======== ===== ====== =========================================== chr18 394599 396513 DiffBind 0 . 1914|7.15|5.55|7.89|-2.35|7.06e-24|9.84e-21 chr18 111566 112005 DiffBind 0 . 439|5.71|6.53|3.63|2.89|1.27e-08|8.88e-06 chr18 346463 347342 DiffBind 0 . 879|5|5.77|3.24|2.52|6.51e-06|0.00303 chr18 399013 400382 DiffBind 0 . 1369|7.62|7|8.05|-1.04|1.04e-05|0.00364 chr18 371109 372102 DiffBind 0 . 993|4.63|3.07|5.36|-2.3|8.1e-05|0.0226 ====== ====== ====== ======== ===== ====== =========================================== Columns contain the following data: * 
Chrom
: Chromosome name * 
Start
: Start position of site * 
End
: End position of site * 
Score
: 0 * 
Name
: DiffBind * 
Strand
: Strand * 
Comment
: The pipe (""|"") separated values in this column correspond to: * 
width
: Length of site * 
Conc
: Mean read concentration over all the samples (the default calculation uses log2 normalized ChIP read counts with control read counts subtracted) * 
Conc_Group1
: Mean concentration over the first group (e.g. Responsive) * 
Conc_Group2
: Mean concentration over second group (e.g. Resistant) * 
Fold
: Fold shows the difference in mean concentrations between the two groups (e.g. Responsive - Resistant), with a positive value indicating increased binding affinity in the first group and a negative value indicating increased binding affinity in the second group. * 
p.value
: P-value confidence measure for identifying these sites as differentially bound * 
FDR
: a multiple testing corrected FDR p-value Example - 
BED format
: ===== ====== ====== ======== ===== ====== Chrom Start End Name Score Strand ===== ====== ====== ======== ===== ====== chr18 394599 396513 DiffBind 0 . chr18 111566 112005 DiffBind 0 . chr18 346463 347342 DiffBind 0 . chr18 399013 400382 DiffBind 0 . chr18 371109 372102 DiffBind 0 . ===== ====== ====== ======== ===== ====== Example - 
Tabular format
: ===== ====== ====== ======== ===== ====== ==== =============== ============== ===== ======== ======== Chrom Start End Name Score Strand Conc Conc_Responsive Conc_Resistant Fold p.value FDR ===== ====== ====== ======== ===== ====== ==== =============== ============== ===== ======== ======== chr18 394599 396513 DiffBind 0 . 7.15 5.55 7.89 -2.35 7.06E-24 9.84E-21 chr18 111566 112005 DiffBind 0 . 5.71 6.53 3.63 2.89 1.27E-08 8.88E-06 chr18 346463 347342 DiffBind 0 . 5 5.77 3.24 2.52 6.51E-06 0.00303 chr18 399013 400382 DiffBind 0 . 7.62 7 8.05 -1.04 1.04E-05 0.00364 chr18 371109 372102 DiffBind 0 . 4.63 3.07 5.36 -2.3 8.10E-05 0.0226 ===== ====== ====== ======== ===== ====== ==== =============== ============== ===== ======== ======== 
Binding Affinity Matrix
 The final result of counting is a binding affinity matrix containing a (normalized) read count for each sample at every potential binding site. With this matrix, the samples can be re-clustered using affinity, rather than occupancy, data. The binding affinity matrix can be used for QC plotting as well as for subsequent differential analysis. Note that this output is a tabular 0-based format. Example: ===== ====== ====== ========= ========= ========== ========== Chrom Start End MCF7_ER_1 MCF7_ER_2 BT474_ER_1 BT474_ER_2 ===== ====== ====== ========= ========= ========== ========== chr18 111567 112005 137.6152 59.87837 29.41393 19.95945 chr18 189223 189652 19.95945 12.60597 11.55547 23.11095 chr18 215232 216063 11.55547 15.75746 31.51493 72.48434 chr18 311530 312172 17.85846 11.55547 54.62588 43.07040 chr18 346464 347342 75.63583 40.96941 21.00995 16.80796 chr18 356560 357362 11.55547 14.70696 57.77737 53.57538 chr18 371110 372102 8.403982 9.454479 81.93882 82.98932 chr18 394600 396513 56.72687 43.07040 510.5419 438.0575 chr18 399014 400382 156.5241 117.6557 558.8648 496.8854 chr18 498906 500200 767.9138 278.3819 196.4430 181.7361 ===== ====== ====== ========= ========= ========== ========== ----- 
More Information
 Generally, processing data with DiffBind involves five phases: #. Reading in peaksets #. Occupancy analysis #. Counting reads #. Differential binding affinity analysis #. Plotting and reporting 
Reading in peaksets
: The first step is to read in a set of peaksets and associated metadata. Peaksets are derived either from ChIP-Seq peak callers, such as 
MACS2
, or using some other criterion (e.g. genomic windows, or all the promoter regions in a genome). A single experiment can have more than one associated peakset; e.g. if multiple peak callers are used for comparison purposes each sample would have more than one line in the sample sheet. Once the peaksets are read in, a merging function finds all overlapping peaks and derives a single set of unique genomic intervals covering all the supplied peaks (a consensus peakset for the experiment). 
Occupancy analysis
: Peaksets, especially those generated by peak callers, provide an insight into the potential occupancy of the protein being ChIPed for at specific genomic loci. After the peaksets have been loaded, it can be useful to perform some exploratory plotting to determine how these occupancy maps agree with each other, e.g. between experimental replicates (re-doing the ChIP under the same conditions), between different peak callers on the same experiment, and within groups of samples representing a common experimental condition. DiffBind provides functions to enable overlaps to be examined, as well as functions to determine how well similar samples cluster together. Beyond quality control, the product of an occupancy analysis may be a consensus peakset, representing an overall set of candidate binding sites to be used in further analysis. 
Counting reads
: Once a consensus peakset has been derived, DiffBind can use the supplied sequence read files to count how many reads overlap each interval for each unique sample. The peaks in the consensus peakset may be re-centered and trimmed based on calculating their summits (point of greatest read overlap) in order to provide more standardized peak intervals. The final result of counting is a binding affinity matrix containing a (normalized) read count for each sample at every potential binding site. With this matrix, the samples can be re-clustered using affinity, rather than occupancy, data. The binding affinity matrix is used for QC plotting as well as for subsequent differential analysis. 
Differential binding affinity analysis
: The core functionality of DiffBind is the differential binding affinity analysis, which enables binding sites to be identified that are statistically significantly differentially bound between sample groups. To accomplish this, first a contrast (or contrasts) is established, dividing the samples into groups to be compared. Next the core analysis routines are executed, by default using DESeq2. This will assign a p-value and FDR to each candidate binding site indicating confidence that they are differentially bound. 
Plotting and reporting
: Once one or more contrasts have been run, DiffBind provides a number of functions for reporting and plotting the results. MA plots give an overview of the results of the analysis, while correlation heatmaps and PCA plots show how the groups cluster based on differentially bound sites. Boxplots show the distribution of reads within differentially bound sites corresponding to whether they gain or lose affinity between the two sample groups. A reporting mechanism enables differentially bound sites to be extracted for further processing, such as annotation, motif, and pathway analyses. ----- 
References
 DiffBind Authors: Rory Stark, Gordon Brown (2011) Wrapper authors: Bjoern Gruening, Pavankumar Videm .. _DiffBind: https://bioconductor.org/packages/release/bioc/html/DiffBind.html .. 
Bioconductor package
: https://bioconductor.org/packages/release/bioc/html/DiffBind.html .. 
DiffBind User Guide
: https://bioconductor.org/packages/release/bioc/vignettes/DiffBind/inst/doc/DiffBind.pdf .. 
Bioconductor post
: https://support.bioconductor.org/p/69924/ .. 
here: https://galaxyproject.org/learn/datatypes/#interval .. 
Biostars post
: https://www.biostars.org/p/84686/"
toolshed.g2.bx.psu.edu/repos/iuc/fasta_nucleotide_color_plot/fasta_nucleotide_color_plot/1.0.1	"What it does
 Produces a graphical representation of FASTA data with each nucleotide represented by a selected color. ----- 
Options
 * 
Pixel Height per Nucleotide
 - height of each nucleotide in pixels * 
Pixel Width per Nucleotide
 - width of each nucleotide in pixels * 
A Color
 - color of A nucleotide * 
T Color
 - color of T nucleotide * 
G Color
 - color of G nucleotide * 
C Color
 - color of C nucleotide * 
N Color
 - color of N nucleotide"
toolshed.g2.bx.psu.edu/repos/iuc/genetrack/genetrack/1.0.1	"What it does
 GeneTrack separately identifies peaks on the forward ""+” (W) and reverse “-” (C) strand. The way that GeneTrack works is to replace each tag with a probabilistic distribution of occurrences for that tag at and around its mapped genomic coordinate. The distance decay of the probabilistic distribution is set by adjusting the value of the tool's 
Sigma to use when smoothing reads
 parameter. GeneTrack then sums the distribution over all mapped tags. This results in a smooth continuous trace that can be globally broadened or tightened by adjusting the sigma value. GeneTrack starts with the highest smoothed peak first, treating each strand separately if indicated by the data, then sets up an exclusion zone (centered over the peak) defined by the value of the 
Peak exclusion zone
 parameter (see figure). The exclusion zone prevents any secondary peaks from being called on the same strand within that exclusion zone. In rare cases, it may be desirable to set different exclusion zones upstream (more 5’) versus downstream (more 3’) of the peak. .. image:: $PATH_TO_IMAGES/genetrack.png GeneTrack continues through the data in order of peak height, until no other peaks are found, and in principle will call a peak at a single isolated tag, if no filter is set using the tool's 
Absolute read filter
 parameter. A filter value of 1 means that it will stop calling peaks when the tag count in the peak hits 1 (so single tag peaks will be excluded in this case). GeneTrack outputs 
chrom
 (chromosome number), 
strand
 (+/W or -/C strand), 
start
 (lower coordinate of exclusion zone), 
end
 (higher coordinate of exclusion zone), and 
value
 (peak height). Genetrack's GFF output reports the start (lower coordinate) and end (higher coordinate) of the exclusion zone. In principle, the width of the exclusion zone may be as large as the DNA region occupied by the native protein plus a steric exclusion zone between the protein and the exonuclease. On the other hand the site might be considerably smaller if the protein is in a denatured state during exonuclease digestion (since it is pre-treated with SDS). In general, higher resolution data or smaller binding site size data should use smaller sigma values. Large binding site size data such as 147 bp nucleosomal DNA use a larger sigma value like 20 (-s 20). For transcription factors mapped by ChIP-exo, sigma may initially be set at 5, and the exclusion zone set at 20 (-s 5 –e 20). Sigma is typically varied between ~3 and ~20. Too high of a sigma value may merge two independent nearby binding events. This may be desirable if closely bound factors are not distinguishable. Too low of a sigma value will cause some tags that contribute to a binding event to be excluded, because they may not be located sufficiently close to the main peak. If alternative (mutually exclusive) binding is expected for two overlapping sites, and these sites are to be independently recorded, then an empirically determined smaller exclusion zone width is set. Thus the value of sigma is set empirically for each mapped factor, depending upon the resolution and binding site size of the binding event. It might make sense to exclude peaks that have only a single tag, where -F 1 is used, or have their tags located on only a single coordinate (called Singletons, where stddev=0 in the output file). However, low coverage datasets might be improved by including them, if additional analysis (e.g., motif discovery) validates them. In addition, idealized action of the exonuclease in ChIP-exo might place all tags for a peak on a single coordinate. ----- 
Options
 * 
Sigma to use when smoothing reads
 - Smooths clusters of tags via a Gaussian distribution. * 
Peak exclusion zone
 - Exclusion zone around each peak, eliminating all other peaks on the same strand that are within a ± bp distance of the peak. * 
Exclusion zone of upstream called peaks
 - Defines the exclusion zone centered over peaks upstream of a peak. * 
Exclusion zone of downstream called peaks
 - Defines the exclusion zone centered over peaks downstream of a peak. * 
Filter
 - Absolute read filter, restricts output to only peaks with larger peak height. ----- 
Output gff Columns
 1. Chromosome 2. Script 3. Placeholder (no meaning) 4. Start of peak exclusion zone (-e 20) 5. End of peak exclusion zone 6. Tag sum (not peak height or area under curve, which LionDB provides) 7. Strand 8. Placeholder (no meaning) 9. Attributes (standard deviation of reads located within exclusion zone) = fuzziness of peak ----- 
Considerations
 In principle, the width of the exclusion zone may be as large as the DNA region occupied by the native protein plus a steric exclusion zone between the protein and the exonuclease. On the other hand the site might be considerably smaller if the protein is in a denatured state during exonuclease digestion (since it is pre-treated with SDS). In general, higher resolution data or smaller binding site size data should use smaller sigma values. Large binding site size data such as 147 bp nucleosomal DNA use a larger sigma value like 20 (-s 20). For transcription factors mapped by ChIP-exo, sigma may initially be set at 5, and the exclusion zone set at 20 (-s 5 –e 20). Sigma is typically varied between ~3 and ~20. Too high of a sigma value may merge two independent nearby binding events. This may be desirable if closely bound factors are not distinguishable. Too low of a sigma value will cause some tags that contribute to a binding event to be excluded, because they may not be located sufficiently close to the main peak. If alternative (mutually exclusive) binding is expected for two overlapping sites, and these sites are to be independently recorded, then an empirically determined smaller exclusion zone width is set. Thus, the value of sigma is set empirically for each mappedfactor depending upon the resolution and binding site size of the binding event. It might make sense to exclude peaks that have only a single tag, where -F 1 is used, or have their tags located on only a single coordinate (called Singletons, where stddev=0 in the output file). However, low coverage datasets might be improved by including them, if additional analysis (e.g., motif discovery) validates them. In addition, idealized action of the exonuclease in ChIP-exo might place all tags for a peak on a single coordinate."
toolshed.g2.bx.psu.edu/repos/iuc/genrich/genrich/0.5+galaxy2	".. class:: infomark 
What it does
 ------------------- 
Genrich
 Genrich is a peak-caller for genomic enrichment assays (e.g. ChIP-seq, ATAC-seq). It analyzes alignment files generated following the assay and produces a file detailing peaks of significant enrichment. ATAC-seq is a method for assessing genomic regions of open chromatin. Since only the ends of the DNA fragments indicate where the transposase enzyme was able to insert into the chromatin, it may not be optimal to interpret alignments. Genrich has an alternative analysis mode for ATAC-seq in which it creates intervals centered on transposase cut sites. The remainder of the peak-calling process (calculating pileups and significance values) is identical to the default analysis mode. Note that the interval lengths (not the fragment lengths) are used to sum the total sequence information for the calculation of control/background pileup values. ------------------- 
Inputs
 ------------------- Genrich analyzes alignment files in SAM/BAM format. SAM files must have a header. SAM/BAM files for multiple replicates can be specified, comma-separated (or space-separated, in quotes). Multiple SAM/BAM files for a single replicate should be combined in advance via samtools merge. The SAM/BAM files must be sorted by queryname (via samtools sort -n). ----------- 
Outputs
 ----------- As indicated, the output file is in ENCODE narrowPeak format. Here are details of the fields: * 1. chrom Name of the chromosome * 2. chromStart Starting position of the peak (0-based) * 3. chromEnd Ending position of the peak (not inclusive) * 4. name peak_N, where N is the 0-based count * 5. score Average AUC (total AUC / bp) × 1000, rounded to the nearest int (max. 1000) * 6. strand . (no orientation) * 7. signalValue Total area under the curve (AUC) * 8. pValue Summit -log10(p-value) * 9. qValue Summit -log10(q-value), or -1 if not available (e.g. with -p) * 10. peak Summit position (0-based offset from chromStart): the midpoint of the peak interval with the highest significance (the longest interval in case of ties) Example: chr1 894446 894988 peak_10 402 . 217.824936 4.344683 1.946031 317 chr1 895834 896167 peak_11 343 . 114.331093 4.344683 1.946031 90 Optional files -c Input SAM/BAM file(s) for control sample(s) Alignment files for control samples (e.g. input DNA) can be specified, although this is not strictly required. SAM/BAM files for multiple replicates can be listed, comma-separated (or space-separated, in quotes) and in the same order as the experimental files. Missing control files should be indicated with null. -f Output bedgraph-ish file for p/q values With a single replicate, this log file lists experimental/control pileup values, p- and q-values, and significance (
) for each interval. Example: chr1 894435 894436 33.000000 2.477916 3.183460 1.208321 chr1 894436 894442 34.000000 2.477916 3.231466 1.241843 chr1 894442 894446 35.000000 2.477916 3.278469 1.274561 chr1 894446 894447 36.000000 2.477916 3.324516 1.306471 * chr1 894447 894450 39.000000 2.477916 3.457329 1.398035 * chr1 894450 894451 40.000000 2.477916 3.499948 1.427253 * chr1 894451 894460 41.000000 2.477916 3.541798 1.455938 * With multiple replicates, this log file lists p-values of each replicate, combined p-value, q-value, and significance for each interval. Note that this file (as well as the -k file, below) is called ""bedgraph-ish"" because it contains multiple dataValue fields, which isn't strictly allowed in the bedGraph format. However, a simple application of awk can produce the desired bedgraph files for visualization purposes (see this awk reference for a guide to printing specific fields of input records). When peak-calling is skipped (-X), the significance column is not produced. -k Output bedgraph-ish file for pileups and p-values For each replicate, sequentially, this file lists a header line (# experimental file: <name>; control file: <name>), followed by experimental/control pileups and a p-value for each interval. This is the way to examine pileup values with multiple replicates, since the -f log file does not supply them in that case. -b Output BED file for reads/fragments/intervals This is an unsorted BED file of the reads/fragments/intervals analyzed. The 4th column gives the read name, number of valid alignments, 'E'xperimental or 'C'ontrol, and sample number (0-based), e.g. SRR5427886.59_2_E_0. -R Output file for PCR duplicates (only with -r) This log file lists the header of each read/fragment classified as a PCR duplicate, followed by the alignment, the header of the read/fragment it matched, and the alignment type. Example: SRR5427886.5958 chr4:185201876-185201975 SRR5427886.4688 paired SRR5427886.1826 chr12:34372610,+;chr1:91852878,- SRR5427886.2040 discordant SRR5427886.10866 chr14:53438632,+ SRR5427886.4746 single The duplicates from multiple input files are separated by a comment line listing the next filename, such as # experimental file #0: SRR5427886.bam. This file can be used to filter the original SAM/BAM file, using a simple script such as getReads.py, for example. -------------------- 
More Information
 -------------------- See the excellent 
Genrich documentation
 .. 
Genrich documentation
: https://github.com/jsh58/Genrich -------------------- 
Galaxy Wrapper Development
* -------------------- Author: Florian Heyl <heylf@informatik.uni-freiburg.de>"
toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_bdgbroadcall/2.2.9.1+galaxy0	"What it does
 This is 
bdgbroadcall
 utility from the MACS2_ Package. It is designed to call broad peaks (e.g., histone) from bedGraph datasets generated with MACS2. .. _MACS2: https://github.com/taoliu/MACS ------ Integration of MACS2 with Galaxy performed by Ziru Zhou and Bjoern Gruening."
toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_bdgcmp/2.2.9.1+galaxy0	"What it does
 This is 
bdgcmp
 utility from the MACS2_ Package. It is designed to deduct noise by comparing two signal tracks in bedGraph. .. _MACS2: https://github.com/taoliu/MACS ------ Integration of MACS2 with Galaxy performed by Ziru Zhou and Bjoern Gruening."
toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_bdgdiff/2.2.9.1+galaxy0	"What it does
 This is 
bdgdiff
 utility from the MACS2_ Package. It performs differential peak detection based on paired four bedgraph files. .. _MACS2: https://github.com/taoliu/MACS ------ Integration of MACS2 with Galaxy performed by Ziru Zhou and Bjoern Gruening."
toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_bdgpeakcall/2.2.9.1+galaxy0	"What it does
 This is 
bdgpeakcall
 utility from the MACS2_ Package. It calls peaks from bedGraph output. .. _MACS2: https://github.com/taoliu/MACS ------ Integration of MACS2 with Galaxy performed by Ziru Zhou and Bjoern Gruening."
toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_callpeak/2.2.9.1+galaxy0	".. class:: infomark 
What it does
 
callpeak
 is the main function of the MACS2_ package. MACS identifies enriched binding sites in ChIP-seq experiments. It captures the influence of genome complexity to evaluate the significance of enriched ChIP regions, and improves the spatial resolution of binding sites through combining the information of both sequencing tag position and orientation. ----- 
Inputs
 MACS can be used for ChIP-Seq data (Treatment) alone, or with a Control sample with the increase of specificity (recommended). A Treatment File is the only REQUIRED parameter for MACS. The file can be in BAM or BED format and this tool will autodetect the format using the first treatment file provided as input. If you have more than one alignment file per sample, you can select to pool them above. MACS can pool files together e.g. as 
-t A B C
 for treatment or 
-c A B C
 for control. Both single-end and paired-end mapping results can be input and you can specify if the data is from paired-end reads above. Paired-end mapping results can be input to MACS as a single BAM file, and just the left mate (5' end) tag will be automatically kept. However, when paired-end format (BAMPE) is specified, MACS will use the real fragments inferred from alignment results for reads pileup. 
Effective Genome Size
 PLEASE assign this parameter to fit your needs! It's the mappable genome size or effective genome size which is defined as the genome size which can be sequenced. Because of the repetitive features on the chromsomes, the actual mappable genome size will be smaller than the original size, about 90% or 70% of the genome size. The default hs -- 2.7e9 is recommended for UCSC human hg18 assembly. Here are all precompiled parameters for effective genome size from the MACS2_ website: hs: 2.7e9 mm: 1.87e9 ce: 9e7 dm: 1.2e8 Or see the 
deepTools
 website for updated information on calculating 
Effective Genome Size
. ----- 
Outputs
 This tool produces a BED file of narrowPeaks as default output. It can also produce additional outputs, which can be selected under the 
Additional Outputs
 option above. * 
a BED file of peaks
 (default) * a tabular file of peaks (compatible wih MultiQC) * a BED file of peak summits * two bedGraph files of scores, for treatment pileup and control lambda * a HTML summary page * a PDF plot (if model is built) * a BED file of broad peaks (if 
Composite broad regions
 is selected under Advanced Options) * a BED file of gapped peaks (if 
Composite broad regions
 is selected under Advanced Options) 
Peaks BED File
 The default output is the narrowPeak BED file (BED6+4 format). This contains the peak locations, together with peak summit, pvalue and qvalue. You can load it to UCSC genome browser. Example: ======= ========= ======= ============= ==== === ======= ======== ======= ======= 1 2 3 4 5 6 7 8 9 
10
 ======= ========= ======= ============= ==== === ======= ======== ======= ======= chr1 840081 840400 treat1_peak_1 69 . 4.89872 10.50944 6.91052 158 chr1 919419 919785 treat1_peak_2 87 . 5.85158 12.44148 8.70936 130 chr1 937220 937483 treat1_peak_3 66 . 4.87632 10.06728 6.61759 154 ======= ========= ======= ============= ==== === ======= ======== ======= ======= Columns contain the following data: * 
1st
: chromosome name * 
2nd
: start position of peak * 
3rd
: end position of peak * 
4th
: name of peak * 
5th
: integer score for display in genome browser (e.g. UCSC) * 
6th
: strand, either ""."" (=no strand) or ""+"" or ""-"" * 
7th
: fold-change * 
8th
: -log10pvalue * 
9th
: -log10qvalue * 
10th
: relative summit position to peak start 
Peaks tabular File
 A tabular file which contains information about called peaks. You can open it in Excel and sort/filter using Excel functions. This file is compatible with 
MultiQC
. Example: ======= ========= ======= ========== ============== ========== ================== =================== ================== ============= 
chr
 
start
 
end
 
length
 
abs_summit
 
pileup
 
-log10(pvalue)
 
fold_enrichment
 
-log10(qvalue)
 
name
 ======= ========= ======= ========== ============== ========== ================== =================== ================== ============= chr1 840082 840400 319 840240 4.00 10.50944 4.89872 6.91052 treat1_peak_1 chr1 919420 919785 366 919550 5.00 12.44148 5.85158 8.70936 treat1_peak_2 chr1 937221 937483 263 937375 4.00 10.06728 4.87632 6.61759 treat1_peak_3 ======= ========= ======= ========== ============== ========== ================== =================== ================== ============= Columns contain the following data: * 
chr
: chromosome name * 
start
: start position of peak * 
end
: end position of peak * 
length
: length of peak region * 
abs_summit
: absolute peak summit position * 
pileup
: pileup height at peak summit * 
-log10(pvalue)
: -log10(pvalue) for the peak summit (e.g. pvalue =1e-10, then this value should be 10) * 
fold_enrichment
: fold enrichment for this peak summit against random Poisson distribution with local lambda * 
-log10(qvalue)
: -log10(qvalue) at peak summit * 
name
: name of peak 
Note that these tabular file coordinates are 1-based which is different than the 0-based BED format (compare the start values in the BED and tabular Example above)
 
Summits BED File
 A BED file which contains the peak summits locations for every peaks. The 5th column in this file is -log10qvalue, the same as in the Peaks BED file. If you want to find the motifs at the binding sites, this file is recommended. The file can be loaded directly to UCSC genome browser. Remove the beginning track line if you want to analyze it by other tools. Example: ======= ========= ======= ============= ======= 1 2 3 4 
5
 ======= ========= ======= ============= ======= chr1 840239 840240 treat1_peak_1 6.91052 chr1 919549 919550 treat1_peak_2 8.70936 chr1 937374 937375 treat1_peak_3 6.61759 ======= ========= ======= ============= ======= Columns contain the following data: * 
1st
: chromosome name * 
2nd
: start position of peak * 
3rd
: end position of peak * 
4th
: name of peak * 
5th
: -log10qvalue 
BedGraph Files
 MACS2 will output two kinds of bedGraph files if the --bdg option is selected under the Additional Outputs option above, which contain the scores for the treatment fragment pileup and control local lambda, respectively. BedGraph files can be imported into genome browsers, such as UCSC genome browser, or be converted into even smaller bigWig files. For more information on bedGraphs, see the 
UCSC website here
. Example: 
Treatment pileup file
 ======= ========= ======= ======= 1 2 3 
4
 ======= ========= ======= ======= chr1 840146 840147 3.00000 chr1 840147 840332 4.00000 chr1 840332 840335 3.00000 ======= ========= ======= ======= 
Control lambda file
 ======= ========= ======= ======= 1 2 3 
4
 ======= ========= ======= ======= chr1 800953 801258 0.02536 chr1 801258 801631 0.25364 chr1 801631 801885 0.99858 ======= ========= ======= ======= Columns contain the following data: * 
1st
: chromosome name * 
2nd
: start position of peak * 
3rd
: end position of peak * 
4th
: treatment pileup score or control local lambda score 
Broad peaks File
 If the broad option (--broad) is selected unded Advanced Options above, MACS2 will output a broadPeaks file. When this flag is on, MACS will try to composite broad regions in BED12 ( a gene-model-like format ) by putting nearby highly enriched regions into a broad region with loose cutoff. The broad region is controlled by another cutoff through --broad-cutoff. The maximum length of broad region length is 4 times of d from MACS. The broad peaks file is in BED6+3 format which is similar to the narrowPeak file, except for missing the 10th column for annotating peak summits. Example: ======= ====== ====== ============= ==== === ======= ======= ======= 1 2 3 4 5 6 7 8 9 ======= ====== ====== ============= ==== === ======= ======= ======= chr1 840081 840400 treat1_peak_1 52 . 4.08790 8.57605 5.21506 chr1 919419 919785 treat1_peak_2 56 . 4.37270 8.90436 5.60462 chr1 937220 937483 treat1_peak_3 48 . 4.02343 8.06676 4.86861 ======= ====== ====== ============= ==== === ======= ======= ======= Columns contain the following data: * 
1st
: chromosome name * 
2nd
: start position of peak * 
3rd
: end position of peak * 
4th
: name of peak * 
5th
: integer score for display in genome browser (e.g. UCSC) * 
6th
: strand, either ""."" (=no strand) or ""+"" or ""-"" * 
7th
: fold-change * 
8th
: -log10pvalue * 
9th
: -log10qvalue 
Gapped peaks File
 If the broad option (--broad) is selected unded Advanced Options above, MACS2 will also output a gappedPeaks file. The gappedPeak file is in BED12+3 format and contains both the broad region and narrow peaks. The file can be loaded directly to UCSC genome browser. Example: ======= ========= ======= ============= === === ======= ======= === === === === ======= ======= ======= 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ======= ========= ======= ============= === === ======= ======= === === === === ======= ======= ======= chr1 840081 840400 treat1_peak_1 52 . 840081 840400 0 1 319 0 4.08790 8.57605 5.21506 chr1 919419 919785 treat1_peak_2 56 . 919419 919785 0 1 366 0 4.37270 8.90436 5.60462 chr1 937220 937483 treat1_peak_3 48 . 937220 937483 0 1 263 0 4.02343 8.06676 4.86861 ======= ========= ======= ============= === === ======= ======= === === === === ======= ======= ======= Columns contain the following data: * 
1st
: chromosome name * 
2nd
: start position of peak * 
3rd
: end position of peak * 
4th
: name of peak * 
5th
: 10
-log10qvalue, to be more compatible to show grey levels on UCSC browser * 
6th
: strand, either ""."" (=no strand) or ""+"" or ""-"" * 
7th
: start of the first narrow peak in the region * 
8th
: end of the peak * 
9th
: RGB color key, default colour is 0 * 
10th
: number of blocks, including the starting 1bp and ending 1bp of broad regions * 
11th
: length of each block, comma-separated values if multiple * 
12th
: start of each block, comma-separated values if multiple * 
13th
: fold-change * 
14th
: -log10pvalue * 
15th
: -log10qvalue ----- 
More Information
 MACS2 performs the following analysis steps: * Artificially extends reads to expected fragment length, and generates coverage map along genome. * Assumes background reads are Poisson distributed. Mean of the Poisson is locally variable and is estimated from control experiment (if available) in 5Kbp or 10Kbp around examined location. * For a given location, asks do we see more reads than we would have expected from the Poisson (p < 0.00005)? If Yes, MACS2 calls a peak. 
Tips of fine-tuning peak calling
 Check out these other MACS2 tools: * 
MACS2 bdgcmp
 can be used on the callpeak bedGraph files or bedGraph files from other resources to calculate score track. * 
MACS2 bdgpeakcall
 can be used on the file generated from bdgcmp or bedGraph file from other resources to call peaks with given cutoff, maximum-gap between nearby mergable peaks and minimum length of peak. bdgbroadcall works similarly to bdgpeakcall, however it will output a broad peaks file in BED12 format. * Differential calling tool 
MACS2 bdgdiff
, can be used on 4 bedGraph files which are scores between treatment 1 and control 1, treatment 2 and control 2, treatment 1 and treatment 2, treatment 2 and treatment 1. It will output the consistent and unique sites according to parameter settings for minimum length, maximum gap and cutoff. .. class:: warningmark If MACS2 fails, it is usually because it cannot build the model for peaks. You may want to extend 
mfold
 range by increasing the upper bound or play with 
Build model
* options. For more information, see the MACS2_ website. .. 
MACS2: https://github.com/taoliu/MACS .. 
Effective Genome Size
: http://deeptools.readthedocs.io/en/latest/content/feature/effectiveGenomeSize.html .. _
UCSC website here
: https://genome.ucsc.edu/goldenPath/help/bedgraph.html ------ Integration of MACS2 with Galaxy performed by Ziru Zhou and Bjoern Gruening."
toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_filterdup/2.2.9.1+galaxy0	"What it does
 This is 
filterdup
 utility from the MACS2_ Package. It removes duplicate reads and converts results to BED format. .. _MACS2: https://github.com/taoliu/MACS ------ Integration of MACS2 with Galaxy performed by Ziru Zhou and Bjoern Gruening."
toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_predictd/2.2.9.1+galaxy0	"What it does
 This is 
predictd
 utility from the MACS2_ Package. It predicts the 
d
 value or fragment size from alignment results. .. _MACS2: https://github.com/taoliu/MACS ------ Integration of MACS2 with Galaxy performed by Ziru Zhou and Bjoern Gruening."
toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_randsample/2.2.9.1+galaxy0	"What it does
 This is 
randsample
 utility from the MACS2_ Package. It randomly samples reads by number or percentage from an input file. .. _MACS2: https://github.com/taoliu/MACS ------ Integration of MACS2 with Galaxy performed by Ziru Zhou and Bjoern Gruening."
toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_refinepeak/2.2.9.1+galaxy0	"What it does
 This is 
refinepeak
 utility from the MACS2_ Package. It is an experimental utility that takes raw read alignments, refines peak summits and gives scores measuring balance of forward- backward tags. Inspired by the SPP_ pipeline. .. _MACS2: https://github.com/taoliu/MACS .. _SPP: http://compbio.med.harvard.edu/Supplements/ChIP-seq/ ------ Integration of MACS2 with Galaxy performed by Ziru Zhou and Bjoern Gruening."
toolshed.g2.bx.psu.edu/repos/greg/multigps/multigps/0.74.0	"What it does
 MultiGPS is a framework for analyzing collections of multi-condition ChIP-seq datasets and characterizing differential binding events between conditions. MultiGPS encourages consistency in the reported binding event locations across conditions and provides accurate estimation of ChIP enrichment levels at each event. MultiGPS performs significant EM optimization of binding events along the genome and across experimental conditions, and it integrates motif-finding via MEME. The tool loads all data into memory, so the potential exists for time and memory intensive analyses if running over many conditions or large datasets. ----- 
Options
 * 
Loading data:
 - 
Optional file containing reads from a control experiment
 - file containing reads from a control experiment - 
Fixed per-base limit
 - Fixed per-base limit (default: estimated from background model). - 
Poisson threshold for filtering per base
 - Look at neighboring positions to decide what the per-base limit should be. - 
Use non-unique reads
 - Use non-unique reads. - 
Fraction of the genome that is mappable for these experiments
 - Fraction of the genome that is mappable for these experiments - 
Turn off caching of the entire set of experiments?
 - Flag to turn off caching of the entire set of experiments (i.e. run slower with less memory). * 
Scaling control vs signal counts:
 - 
Use signal vs control scaling?
 - Flag to turn off auto estimation of signal vs control scaling factor - 
Use the median signal/control ratio as the scaling factor?
 - Flag to use scaling by median ratio (default = scaling by NCIS). - 
Use scaling by regression on binned tag counts?
 - Flag to use scaling by regression (default = scaling by NCIS). - 
Estimate scaling factor by SES?
 - Specify whether to estimate scaling factor by SES. - 
Multiply control counts by total tag count ratio and then by this factor
 - Multiply control counts by total tag count ratio and then by this factor (default: NCIS). - 
Window size for estimating scaling ratios
 - Window size in base pairs for estimating scaling ratios - 
Plot diagnostic information for the chosen scaling method?
 - Flag to plot diagnostic information for the chosen scaling method. * 
Running MultiGPS:
 - 
Optional binding event read distribution file
 - Binding event read distribution file for initializing models. The true distribution of reads around binding events is estimated during MultiGPS training. A default initial distribution appropriate for ChIP-seq data is used if this option is not specified. - 
Maximum number of training rounds for updating binding event read distributions
 - Maximum number of training rounds for updating binding event read distributions. - 
Perform binding model updates?
 - Perform binding model updates? - 
Minimum number of events to support an update of the read distribution
 - Minimum number of events to support an update of the read distribution - 
Perform binding model smoothing?
 - Smooth with a cubic spline using a specified smoothing factor. - 
Spline smoothing parameter
 - Smoothing parameter for smoothing cubic spline. - 
Perform Gaussian model smoothing?
 - Select ""Yes"" to use Gaussian model smoothing using a specified smoothing factor if binding model smoothing is not performed. - 
Allow joint events in model updates?
 - Specify whether to allow joint events in model updates. - 
Keep binding model range fixed to inital size?
 - Flag to keep binding model range fixed to inital size (default: vary automatically) - 
Poisson log threshold for potential region scanning
 - Poisson log threshold for potential region scanning. - 
Alpha scaling factor
 - Alpha scaling factor. Increasing this parameter results in stricter binding event calls. - 
Impose this alpha
 - The alpha parameter is a sparse prior on binding events in the MultiGPS model. It can be interpreted as a minimum number of reads that each binding event must be responsible for in the model. Default: estimate alpha automatically. - 
Share component configs in the ML step?
 - Flag to not share component configs in the ML step - 
Optional file containing a set of regions to ignore during MultiGPS training
 - File containing a set of regions to ignore during MultiGPS training. It’s a good idea to exclude the mitochondrial genome and other ‘blacklisted’ regions that contain artifactual accumulations of reads in both ChIP-seq and control experiments. MultiGPS will waste time trying to model binding events in these regions, even though they will not typically appear significantly enriched over the control (and thus will not be reported to the user). * 
MultiGPS priors:
 - 
Perform inter-experiment positional prior?
 - Flag to turn off inter-experiment positional prior (default=on). - 
Probability that events are shared across conditions
 - Probability that events are shared across conditions. - 
Perform both motif-finding and motif priors?
 - Flag to turn off motif-finding and motif priors. - 
Perform motif-finding only?
 - Flag to turn off motif priors only. - 
Number of motifs MEME should find for each condition
 - Number of motifs MEME should find for each condition. - 
Minimum motif width for MEME
 - minw arg for MEME. - 
Maximum motif width for MEME
 - maxw arg for MEME. * 
Reporting binding events:
 - 
Minimum Q-value (corrected p-value) of reported binding events
 - Minimum Q-value (corrected p-value) of reported binding events. - 
Minimum event fold-change vs scaled control
 - Minimum event fold-change vs scaled control. - 
Run differential enrichment tests?
 - Choose whether to run differential enrichment tests. - 
EdgeR over-dispersion parameter value
 - EdgeR over-dispersion parameter value. - 
Minimum p-value for reporting differential enrichment
 - Minimum p-value for reporting differential enrichment."
toolshed.g2.bx.psu.edu/repos/iuc/pe_histogram/pe_histogram/1.0.2	"What it does
 Produces an insert size histogram and basic statistics for a paired-end BAM file. Two outputs are produced: - a png image consisting of the histogram of the insert size frequency - a tabular file containing the alignment statistics ----- 
Options
 * 
Lower bp limit
 - the lower bp limit on insert size for calculating the histogram. * 
Upper bp limit
 - the upper bp limit on insert size for calculating the histogram."
toolshed.g2.bx.psu.edu/repos/iuc/repmatch_gff3/repmatch_gff3/1.0.3	"What it does
 RepMatch accepts two or more input datasets (in gff format https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md), and starts by defining peak-pair midpoints in the first dataset. It then discovers all peak-pair midpoints in the second dataset that are within the distance, defined by the tool's 
Maximum distance between peaks in different replicates to allow merging
 parameter, from the peak-pair midpoint coordinate in the first dataset. When encountering multiple candidates to match (one-to-many), RepMatch uses the method defined by the tool's 
Method of finding match
 parameter so that there is at most only a one-to-one match across the two datasets. This method provides the following options: * 
closest
 - matches only the closest one in bp distance. * 
largest
 - matches the one that contain the most number of reads. * 
all
 - both methods are run separately. RepMatch matching is an iterative process, as it attempts to find the centroid coordinate amongst all replicates. As such, the centroid is the point of reference for ""distqnce"" and ""closest"". This process can be sped up by increasing the tool's 
Step size
 parameter. The minimum number of replicates that can be matched for a match to occur is defined by the tool's 
Minimum number of replicates that must be matched for merging to occur
 parameter. Additional filters can be applied using the tool's 
Advanced options
, including a lower and upper limit for the C-W distance. .. image:: $PATH_TO_IMAGES/repmatch.png ----- 
Options
 * 
Distance
 - Maximum distance for discovering all peak-pair midpoints in a second dataset relative to the peak-pair midpoints in the first dataset * 
Method
 - Method to use when encountering multiple candidates to match so that there is at most only a one-to-one match across the two datasets. * 
Step Size
 - Distance for each iteration. * 
Replicates
 - Minimum number of replicates that can be matched for a match to occur. This value must be at least 2. * 
Lower Limit
 - Lower limit for the Crick-Watson distance filter. * 
Upper Limit
 - Upper limit for the Crick-Watson distance filter. ----- 
Output Data Files
 * 
Data MP
 - gff file consisting of only peak pairs - Columns are 
chr
, 
script
, 
blank
, 
peak start
, 
peak end
, 
blank
, 
normalized tag counts
, 
blank
 and 
info
. - Peak start and end are separated by one coordinate. - Normalized tag is the occupancy averaged across replicates. - Attributes include C-W distance, sum total of tag counts, number of replicates merged. * 
Data D
 - tabular file consisting of the list of all matched replicates. * 
Data UP
 - tabular file consisting of all unmatched peak-pairs. 
Output Statistics Files
 * 
Statistics Table
 - tabular file providing the description key of 
Data D
. * 
Statistics Histogram
 - graph of the number of matched locations having the indicated replicate counts. 
Comments on Replicates
 Three types of replicates may be considered. Biological replicates represent independently collected biological samples. At least two biological replicate must be performed for each experiment from which a conclusion is being drawn, and the conclusion must be evident in both biological replicates when analyzed separately. Technical replicates represent a re-run of the assay on the same biological material. This is usually done when one replicate fails to produce quality data, and is used to replace that earlier replicate. Sequencing replicates represent additional sequencing of the same successful library in order to obtain more reads should the analysis require it. The reads from individual sequencing replicates are usually merged without need for separate analysis."
toolshed.g2.bx.psu.edu/repos/iuc/resize_coordinate_window/resize_coordinate_window/1.0.2	"What it does
 Modifies the start and end coordinates of GFF data such that the new start and end position is based on a specified region size that is computed either from the existing start and end coordinates or centered on the midpoint between them. Region expansion may result in the new start or end coordinates crossing the chromosome boundary. The chromosome start is set to 0 or 1 using the 
Start coordinate
 parameter. The end is retrieved from a file within the Galaxy environment that includes the length of chromosomes for all genome builds. If these files are missing, the end coordinate is set to 2147483647, which is the maximum value of a signed 32 bit integer. The 
Handle chromosome boundaries by
 parameter handles chromosome boundaries that are crossed by expanding the region using one of the following options. * 
discarding the region
 - the region will be discarded and processing will continue with the next line in the dataset. * 
keeping the region by limiting the expansion to not cross the start or end coordinate boundary
 - expansion will be restricted to not cross the chromosome's start or end coordinates for the current region. * 
keeping the region by allowing the expansion to cross the start or end coordinate boundary
 - allow defined expansion, crossing the start boundary results in a negative start value. * 
outputting an error
 - Stop processing and display an error. ----- 
Example
 If the input dataset is:: chr1 genetrack . 17 37 918 + . stddev=5.96715849116 chr1 genetrack . 31 51 245 - . stddev=2.66582799529 chr1 genetrack . 40 60 2060 + . stddev=2.7859667372 Setting start coordinate to 1 and resizing the coordinate window by 13 from the computed midpoint of the start and end coordinates produces:: chr1 genetrack . 14 40 918 + . stddev=5.96715849116 chr1 genetrack . 28 54 245 - . stddev=2.66582799529` chr1 genetrack . 37 63 2060 + . stddev=2.7859667372"
toolshed.g2.bx.psu.edu/repos/iuc/seacr/seacr/1.3+galaxy1	".. class:: infomark 
What it does
 SEACR (Sparse Enrichment Analysis for CUT&RUN) is intended to call peaks and enriched regions from sparse CUT&RUN or chromatin profiling data in which background is dominated by ""zeroes"" (i.e. regions with no read coverage). CUT&RUN is an efficient epigenome profiling method that identifies sites of DNA binding protein enrichment genome-wide with high signal to noise and low sequencing requirements. Currently, the analysis of CUT&RUN data is complicated by its exceptionally low background, which renders programs designed for analysis of ChIP-seq data vulnerable to oversensitivity in identifying sites of protein binding. SEACR is a highly selective peak caller that definitively validates the accuracy of CUT&RUN for datasets with known true negatives. SEACR uses the global distribution of background signal to calibrate a simple threshold for peak calling. SEACR discriminates between true and false-positive peaks with near-perfect specificity from gold standard CUT&RUN datasets and efficiently identifies enriched regions for several different protein targets. 
Input
 SEACR requires files in UCSC bedgraph format from paired-end sequencing as input, which can be generated from read pair BED files. 
Output
 Results are stored in BED files with the folowing format. :: <chr> <start> <end> <total signal> <max signal> <max signal region> with - <chr> Chromosome - <start> Start coordinate - <end> End coordinate - <total signal> Total signal contained within denoted coordinates - <max signal> Maximum bedgraph signal attained at any base pair within denoted coordinates - <max signal region> Region representing the farthest upstream and farthest downstream bases within the denoted coordinates that are represented by the maximum bedgraph signal .. class:: infomark 
References
 More information are available on 
github &lt;https://github.com/FredHutch/SEACR&gt;
. A web interface can be found 
here &lt;https://seacr.fredhutch.org&gt;
."
toolshed.g2.bx.psu.edu/repos/devteam/sicer/peakcalling_sicer/1.1	"What it does
 SICER first and foremost is a filtering tool. Its main functions are:: 1. Delineation of the significantly ChIP-enriched regions, which can be used to associate with other genomic landmarks. 2. Identification of reads on the ChIP-enriched regions, which can be used for profiling and other quantitative analysis. View the original SICER documentation: http://home.gwu.edu/~wpeng/Software.htm. ------ .. class:: warningmark By default, SICER creates files that do not conform to standards (e.g. BED files are closed, not half-open). This could have implications for downstream analysis. To force the output of SICER to be formatted properly to standard file formats, check the 
""Fix off-by-one errors in output files""
 option. ------ 
Citation
 If you use this tool in Galaxy, please cite Blankenberg D, et al. 
In preparation."
toolshed.g2.bx.psu.edu/repos/iuc/tag_pileup_frequency/tag_pileup_frequency/1.0.2	"What it does
 Generates a frequency pileup of the 5' ends of aligned reads in a BAM file relative to reference points in a BED file. ----- 
Options
 
Global transformations
 - 
5' to 3' tag shift (bp)
 - moves all tags from the 5' to the 3' direction given a specified distance in base pairs. - 
Bin Size (bp)
 - bins tags together into specified size, useful for lower resolution data or for especially wide reference windows. - 
Set Tags to be equal
 - standardizes tag count to be equal to genome size, useful for replicate comparisons. 
Read parameters
 - 
Reads to Examine
 - output data for Read1, Read2, or combined (only for paired end BAM files). - 
Require Proper PE
 - require output reads to be properly paired (only for paired end BAM files). - 
Combine Strands
 - examine reads on a forward/reverse basis (strand separate) or ignoring read orientation (strand combined). 
Run parameters
 - 
Composite smoothing window (bp)
 - sliding window to smooth composite frequencies. - 
Output frequencies for heatmap
 - generate an additional dataset collection containing frequencies that can be used to generate a heatmap plot."
toolshed.g2.bx.psu.edu/repos/bgruening/featurestein/featurestein/1.1.4+galaxy0	".. class:: infomark 
What this tool does
 FeatureStein involves use of merged RDKit feature maps to score ligand poses in terms of how well the features overlap with features from a set of ligands, typically fragment screening hits. The features are molecular features like hydrogen bond donors and acceptors or hydrophobic groups. See this Jupyter notebook for more information: https://github.com/tdudgeon/jupyter_mpro/blob/master/featurestein/FeatureStein.ipynb and this blog for more general info on RDKit feature maps: http://rdkit.blogspot.com/2017/11/using-feature-maps.html This module generates the merged feature maps from a set of molecules (typically fragment screening hits) and then. scores a set of ligands using those merged feature maps. The fields that are created are: - FeatureStein_Quant: the sum of the scores for each feature in the ligand - FeatureStein_Qual: the FeatureStein_Quant score divided by the number of features (number between 0 and 1) ----- .. class:: infomark 
Input
 - fragments: SD-file of fragments to generate feature maps and merge - ligands: SD-file of ligands to score ----- .. class:: infomark 
Output
 SD-file of the ligands with the FeatureStein scores added."
toolshed.g2.bx.psu.edu/repos/earlhaminst/apoc/apoc/1.0+galaxy1	"*
* Description 
*
* APoc may be used to compare two pockets, a pocket against a set of pockets, or all-against-all between two sets of pockets. If you supply two structures to compare, the first structure is the template and the second structure is the query (or target). For each pair of structures, the program first performs a global structural comparison in sequential order using a standard TM-align algoritm. One may elect to bypass the global alignment to accelerate comparison. If no pocket found in the pdb structures, the program becomes a normal TM-align or stop if one chooses to bypass the global alignment. If there are pockets detected in the input files, it will compare pockets in sequential-order-independent manner by default. The ouput is arranged in pairs of structures compared. For each pair, the first alignment is the global alignment, followed by all-againat-all alignment of selected pockets. If you want a concise output without detailed alignment, add the ""-v 0"" option."
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_ob_addh/1.0	".. class:: infomark 
What this tool does
 Parses a molecular file and adds hydrogen atoms at a user-defined pH value. * Protocol:: 1. The hydrogen atoms included in the input molecule are deleted. 2. Protonation state is predicted at the target pH and the corresponding hydrogen atoms added accordingly. ----- .. class:: infomark 
Input
 3D format files are required, e.g. SDF_ .. 
SDF: http://en.wikipedia.org/wiki/Chemical_table_file ----- .. class:: warningmark 
Hint
 To avoid possible crashes, only molecules with more than five heavy atoms are parsed. ----- .. class:: infomark 
Output
 Same output format as the input format. ----- .. class:: infomark 
Cite
 
Open Babel
 .. 
Open Babel: http://openbabel.org/wiki/Main_Page N M O'Boyle, C Morley and G R Hutchison - 
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
 .. _
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
: http://www.biomedcentral.com/content/pdf/1752-153X-2-5.pdf"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_addh/openbabel_addh/3.1.1+galaxy1	".. class:: infomark 
What this tool does
 Parses a molecular file and adds hydrogen atoms at a user-defined pH value. * Protocol:: 1. The hydrogen atoms included in the input molecule are deleted. 2. Protonation state is predicted at the target pH and the corresponding hydrogen atoms added accordingly. ----- .. class:: infomark 
Input
 3D format files are required, e.g. SDF_ .. _SDF: http://en.wikipedia.org/wiki/Chemical_table_file ----- .. class:: warningmark 
Hint
 To avoid possible crashes, only molecules with more than five heavy atoms are parsed. ----- .. class:: infomark 
Output
 Same output format as the input format."
toolshed.g2.bx.psu.edu/repos/chemteam/gromacs_modify_topology/gromacs_modify_topology/0+galaxy0	"Tool to modify and add new information to GROMACS topology files. This is particularly useful when working with systems that were created outside of GROMACS (for example, files created in AMBER or CHARMM and then converted over via acpype). This tool can also be used to complement the ""gmx insert-molecules"" tool, which currently only modifies the GROMACS structure files (gro) and requires further modification of the topology file for the newly populated system to be simulation ready. .. class:: infomark 
Input
 1) The system topology file you are modifying, 2) a position restraint file (posres.itp) and specifying the name of the target molecule type you are restraining, 3) a molecule's atom types/nonbonded parameters to be inserted under the system's global [ atomtypes ], as well as 4) the corresponding bonded parameters of that particular molecule found under [ moleculetype ]. .. class:: infomark 
Outputs
 The new modified GROMACS topology file."
toolshed.g2.bx.psu.edu/repos/chemteam/alchemical_analysis/alchemical_analysis/1.0.2	".. class:: infomark 
What it does
 This tool can run analysis of alchemical free energy simulations. 
_
 .. class:: infomark 
Input
 - TAR archive file of the GROMACS XVG files from free energy simulations. Note that simulations should be at least 100 ps in length. 
___ .. class:: infomark 
Output
 - Report and free energy outputs. - Overlap matrix of free energy windows (as PNG). - Convergence plot (as PNG). - Curve Fitting Method (CFM) based consistency inspector (as PNG). - Free energy change breakdown (as PNG). - Thermodynamic Integration plot (as PNG)."
toolshed.g2.bx.psu.edu/repos/chemteam/biopdb_align_and_rmsd/biopdb_align_and_rmsd/1.79+galaxy1	"Tool to align protein structures and compute relative RMSDs, using the alpha carbon coordinates. .. class:: infomark 
Inputs
 PDB files for the reference structure, a model, as well as the starting and ending residues for the alignment. .. class:: infomark 
Outputs
 1) Tabular file containing the RMSD. 2) PDB file of the model with the aligned coordinates."
toolshed.g2.bx.psu.edu/repos/galaxy-australia/alphafold2/alphafold/2.3.2+galaxy0	".. class:: infomark | AlphaFold v2: AI-guided 3D structural prediction of proteins | | 
NOTE: this tool packages
 
a modified branch of AlphaFold v2.3.2. &lt;https://github.com/neoformit/alphafold/tree/release_2.3.2_galaxy&gt;
 | | This means that the neural network has been trained on PDBs with a release | date before 2021-09-30 (the training cutoff was 2018-04-30 until 
v2.3.0
). | | Find out more in the technical and release notes: | - 
Release notes for v2.3.2 &lt;https://github.com/deepmind/alphafold/releases/tag/v2.3.2&gt;
 - 
Technical notes for v2.3 &lt;https://github.com/deepmind/alphafold/blob/main/docs/technical_note_v2.3.0.md&gt;
 
What it does
 
What is AlphaFold?
 | AlphaFold is a program which uses neural networks to predict the tertiary (3D) structure of proteins. AlphaFold accepts an amino acid sequence in Fasta format, which will be ""folded"" into a 3D model. | 
What makes AlphaFold different?
 | The ability to use computers to predict 3D protein structures with high accuracy is desirable because it removes the time-consuming and costly process of determining structures experimentally. | In-silico protein folding has been an active field of research for decades, but existing tools were slower and far less reliable than AlphaFold. | AlphaFold represents a leap forward by regularly predicting structures to atomic-level accuracy, even when no similar structures are known. | 
Input
 
Amino acid sequence
 | AlphaFold monomer (default) accepts a 
single amino acid sequence
 in FASTA format. | You can choose to input either a file from your Galaxy history or paste a sequence into a text box. | If you choose the 
multimer
 option, you can supply a FASTA file containing 
multiple sequences
 to be folded concurrently into a multimer. | | For pairwise screening of target-candidate with multimer, you can submit a list of paired protein sequences in batch mode (i.e. two protein sequences in each FASTA file). | 
Outputs
 
Visualization
 An interactive 3D graphic of the best predicted molecular structures. This output can be opened in Galaxy to give a visual impression of the results, with different structural representations to choose from. Open the ""Visualization"" history output by clicking on the ""view data"" icon: .. image:: https://github.com/usegalaxy-au/galaxy-local-tools/blob/1a8d3e8daa7ccc5a345ca377697735ab95ed0666/tools/alphafold/static/img/alphafold-visualization.png?raw=true :height: 520 :alt: Result visualization | 
PDB files
 | PDB (Protein Data Bank) files (5 by default) are be created, ordered by rank, as predicted by AlphaFold. The tool produces 5 models by default, but this can be reduced with the ""Limit model outputs"" for a reduced run time. | These files describe the molecular structures and can be used for downstream analysis. e.g. 
in silico
 molecular docking. | 
PLEASE NOTE
 that all outputs have been renamed to their respective rank order, including model and model.pkl files. | 
Model confidence scores (optional)
 | This optional output produces a file which describes the confidence scores for each model (based on 
pLDDTs &lt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3799472/&gt;
, or the 
iptm+ptm
 score if run in multimer mode) which may be useful for downstream analysis. | Model confidence scores are also included as a column (replacing 
bFactor
) in the default PDB output. | | 
Model data files (ranked_n.pkl)
 | Per-model data stored in pickle files (a Python binary data format). These files can be used as inputs to downstream analysis software (such as Chimera X) for visualizing structures and computing kinetics between protein multimers and domains. | The tool will produce one 
.pkl
 output for each PDB model. | | 
pLDDT + PAE plots (optional)
 | A two-panel figure in PNG format showing: | a) pLDDT score plotted against residue position | b) a heatmap of predicted-alignment error (PAE) with residue position running along vertical and horizontal axes and color at each pixel indicating PAE value for the corresponding pair of residues. | Panel b) is only produced for 
monomer_ptm
 and 
multimer
 model presets. | | 
Model predicted-alignment error matrix (pae_ranked_n.csv)
 | Per-model predicted-alignment error (PAE) matrix - only available with the 
monomer_ptm
 and 
multimer
 model presets. | The tool will produce one 
.csv
 output for each PDB model. | | 
relax_metrics.json (optional)
 | A JSON-formatted text file containing relax metrics (primarily remaining violations). | | 
timings.json (optional)
 | A JSON-formatted text file containing the timings for each phase of the prediction. | | 
AlphaFold configuration
 | We have configured AlphaFold to run with the parameters suggested by default on 
AlphaFold's GitHub &lt;https://github.com/deepmind/alphafold&gt;
. | This means that it runs with Amber relaxation enabled, with relaxed PDB models collected as output datasets (ranked_
.pdb files). If there are additonal parameters that you would like to interact with, please 
send a support request to Galaxy AU &lt;https://site.usegalaxy.org.au/request/support&gt;
, or open an issue on 
our GitHub &lt;https://github.com/usegalaxy-au/tools-au&gt;
. | | 
External Resources
 We highly recommend checking out the 
Alphafold Protein Structure Database &lt;https://alphafold.ebi.ac.uk/&gt;
, which contains pre-computed structures for over 200 million known proteins. See also: - 
Google Deepmind's article on AlphaFold &lt;https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology&gt;
 - 
AlphaFold source code on GitHub &lt;https://github.com/deepmind/alphafold&gt;
_ 
Downstream analysis* | Obtaining a protein structure prediction is the first step in many analyses. | The 3D models created by AlphaFold can be used in downstream analysis, including the following: | - Inspecting protein features 3D viewers (pymol, chimera, ngl, blender) can be used to inspect active sites, regulatory domains, binding sites. - Molecular docking 3D structures can be used to predict the binding affinity of different compounds. This is especially useful in screening drug candidates. - Protein-protein interactions Proteins associate in many biological processes, including intracellular signalling pathways and protein complex formation. To predict these interactions, other programs may ingest 3D models predicted by AlphaFold. Proprietary softwares include 
GOLD &lt;https://www.ccdc.cam.ac.uk/solutions/csd-discovery/components/gold/&gt;
 and 
SeeSAR &lt;https://www.biosolveit.de/SeeSAR&gt;
, but many 
free and open-source options &lt;https://en.wikipedia.org/wiki/List_of_protein-ligand_docking_software&gt;
 are available such as 
AutoDock &lt;https://autodock.scripps.edu/&gt;
, 
SwissDock &lt;http://www.swissdock.ch/&gt;
, 
DockQ &lt;https://github.com/bjornwallner/DockQ&gt;
, 
MM-Align &lt;https://zhanggroup.org/MM-align/&gt;
 and 
TM-Align &lt;https://zhanggroup.org/TM-align/&gt;
_. Protein-protein interactions are often inferred from AlphaFold-Multimer predictions, which provide a level of confidence in binding affinity between homomer/heteromer subunits."
toolshed.g2.bx.psu.edu/repos/chemteam/mdanalysis_angle/mdanalysis_angle/1.0.0+galaxy0	".. class:: infomark 
What it does
 This tool calculates and plots the angle between three atoms. 
_
 .. class:: infomark 
Input
 - Trajectory file (DCD). - PDB file. - Segment IDs, residue IDs and names of the three atoms for calculating angles. Note that a MDAnalysis 'segment' is a larger organizational unit, for example one protein or all the solvent molecules or simply the whole system. 
___ .. class:: infomark 
Output
 - Tab-separated file of raw data of the angle between three atoms calculated for each frame. - Image (as png) of the time series graph."
toolshed.g2.bx.psu.edu/repos/chemteam/ambertools_antechamber/ambertools_antechamber/21.10+galaxy0	".. class:: infomark 
What it does
 Antechamber sets up Amber parameters for the input molecules provided. .. class:: infomark 
How it works
 - Select an input file (mol2, PDB) - Specify the correct charge, or keep the default charge of 0 - Adjust other parameters as needed. - Click on Execute .. class:: infomark 
Outputs created
 - A mol2 or PDB output is created with the forcefield compatible atom names. .. class:: infomark 
User guide and documentation
 - AmberTools 
userguide
 .. 
userguide
: http://ambermd.org/doc12/Amber19.pdf .. class:: infomark 
Feature requests
 Go to Galaxy Computational Chemistry and make a 
feature request
 .. 
feature request
: https://github.com/galaxycomputationalchemistry/galaxy-tools-compchem/issues/new"
toolshed.g2.bx.psu.edu/repos/chemteam/tleap/tleap/21.10+galaxy0	":: add a b UNIT/RESIDUE/ATOM 
a
 UNIT/RESIDUE/ATOM 
b
 Add the object 
b
 to the object 
a
. This command is used to place ATOMs within RESIDUEs, and RESIDUEs within UNITs. addAtomTypes { { ""H"" ""H"" ""sp3"" } { ""HO"" ""H"" ""sp3"" } ... } Add mapping of AMBER atom type to element and hybridization. Typically in leaprc. addH obj UNIT 
obj
 Add missing hydrogens and build external coordinates for 
obj
. addIons unit ion1 #ion1 [ion2 #ion2] UNIT 
unit
 UNIT 
ion1
 NUMBER 
#ion1
 UNIT 
ion2
 NUMBER 
#ion2
 Adds counterions in a shell around 
unit
 using a Coulombic potential on a grid. If 
#ion1
 is 0, the 
unit
 is neutralized (
ion1
 must be opposite in charge to 
unit
, and 
ion2
 cannot be specified). Otherwise, the specified numbers of 
ion1
 [
ion2
] are added [in alternating order]. If solvent is present, it is ignored in the charge and steric calculations, and if an ion has a steric conflict with a solvent molecule, the ion is moved to the center of said molecule, and the latter is deleted. (To avoid this behavior, either solvate 
after
 addIons, or use addIons2.) Ions must be monoatomic. Note that the one-at-a-time procedure is not guaranteed to globally minimize the electrostatic energy. When neutralizing regular-backbone nucleic acids, the first cations will generally be added between phosphates, leaving the final two ions to be placed somewhere around the middle of the molecule. The default grid resolution is 1 Angstrom, extending from an inner radius of (max ion size + max solute atom size) to an outer radius 4 Angstroms beyond. A distance-dependent dielectric is used for speed. addIons2 unit ion1 #ion1 [ion2 #ion2] UNIT 
unit
 UNIT 
ion1
 NUMBER 
#ion1
 UNIT 
ion2
 NUMBER 
#ion2
 Same as addIons, except solvent and solute are treated the same. addIonsRand unit ion1 #ion1 [ion2 #ion2] [separation] UNIT 
unit
 UNIT 
ion1
 NUMBER 
#ion1
 UNIT 
ion2
 NUMBER 
#ion2
 NUMBER 
separation
 Adds counterions in a shell around 
unit
 by replacing random solvent molecules. If 
#ion1
 is 0, the 
unit
 is neutralized (
ion1
 must be opposite in charge to 
unit
, and 
ion2
 cannot be specified). Otherwise, the specified numbers of 
ion1
 [
ion2
] are added [in alternating order]. If 
separation
 is specified, ions will be guaranteed to be more than that distance apart in Angstroms. Ions must be monoatomic. This procedure is much faster than addIons, as it does not calculate charges. Solvent must be present. It must be possible to position the requested number of ions with the given separation in the solvent. addPath path STRING 
path
 Add the directory in 
path
 to the list of directories that are searched for files specified by other commands. addPdbAtomMap list LIST 
list
 The atom Name Map is used to try to map atom names read from PDB files to atoms within residue UNITs when the atom name in the PDB file does not match an atom in the residue. This enables PDB files to be read in without extensive editing of atom names. The LIST is a LIST of LISTs: { {sublist} {sublist} ... } where each sublist is of the form { ""OddAtomName"" ""LibAtomName"" } Many 
odd' atom names can map to one
standard' atom name, but any single odd atom name maps only to the last standard atom name it was mapped to. addPdbResMap list LIST 
list
 The Name Map is used to map residue names read from PDB files to variable names within LEaP. The LIST is a LIST of LISTs: { {sublist} {sublist} ... } Each sublist contains two or three entries to add to the Name Map: { [terminalflag] PDBName LEaPVar } where the PDBName will be mapped to the LEaPVar. The terminalflag indicates the special cases of terminal residues: allowable values are 0 for beginning residues (N-terminal for proteins, 5' for nucleic acids) and 1 for ending residues (C-terminal for proteins, 3' for nucleic acids). If the terminalflag is given, the PDBName->LEaPVar name map will only be applied for the appropriate terminal residue. The 
leaprc' file included with the distribution contains default mappings. alias [alias[ string]] STRING _alias_ STRING _string_ alias string1 command - equivalence string1 to command. alias string1 - delete the alias for string1. alias - report all current aliases. alignAxes unit UNIT _unit_ Translate the geometric center of _unit_ to the origin and align the principle axes of the ATOMs within _unit_ along the coordinate axes. This is done by calculating the moment of inertia of the UNIT using unit mass for each ATOM, and then diagonalizing the resulting matrix and aligning the eigenvectors along the coordinate axes. This command modifies the coordinates of the UNIT. It may be especially useful for preparing long solutes such as nucleic acids for solvation. bond atom1 atom2 [ order ] ATOM _atom1_ ATOM _atom2_ STRING _order_ Create a bond between _atom1_ and _atom2_. Both of these ATOMs must be contained by the same UNIT. By default, the bond will be a single bond. By specifying ""S"", ""D"", ""T"", or ""A"" as the optional argument _order_ the user can specify a single, double, triple, or aromatic bond. bondByDistance container [ maxBond ] UNIT/RESIDUE/ATOM _container_ NUMBER _maxBond_ Create single bonds between all ATOMs in _container_ that are within _maxBond_ angstroms of each other. If _maxBond_ is not specified, a default distance of 2 angstroms used. center container UNIT/RESIDUE/ATOM _container_ Display the coordinates of the geometric center of the ATOMs within _container_. charge container UNIT/RESIDUE/ATOM _container_ This command calculates the total charge of the ATOMs within _container_. The unperturbed and perturbed total charge are displayed. check unit [ parmset ] UNIT _unit_ PARMSET/STRING _parmset_ This command can be used to check the UNIT for internal inconsistencies that could cause problems when performing calculations. This is a very useful command that should be used before a UNIT is saved with saveAmberParm or its variations. With the optional parmset, all missing parameters are placed in the PARMSET to allow for easy editing of those parameters. If a string is passed, a PARMSET will be created with that name. Currently it checks for the following possible problems: - Long bonds. A long bond is greater than 3.0 angstroms. - Short bonds. A short bond is less than 0.5 angstroms. - Non-integral total charge of the UNIT. - Missing types. - Close contacts between non-bonded ATOMs. A close contact is less than 1.5 angstroms. clearPdbAtomMap Clear the Name Map used for ``second-chance'' mapping of atom names in PDB files to atoms within residue UNITs. See addPdbAtomMap. clearPdbResMap Clear the Name Map used to map residue names in PDB files to variable names within LEaP. See addPdbResMap. clearVariables [ list ] LIST _list_ This command removes variables from LEaP. If the _list_ argument is provided, then only the variables in the LIST will be removed. If no argument is provided then ALL variables will be removed. variable = combine list object _variable_ LIST _list_ Combine the contents of the UNITs within _list_ into a single UNIT. The new UNIT is placed in _variable_. This command is similar to the sequence command except it does not link the ATOMs of the UNITs together. newvariable = copy variable object _newvariable_ object _variable_ Create an exact duplicate of the object _variable_. Changing the object _variable_ will not affect the object _newvariable_. This is in contrast to the situation created by ""newvariable = variable"" in which both names reference the same object. variable = createAtom name type charge ATOM _variable_ STRING _name_ STRING _type_ NUMBER _charge_ Return a new ATOM with _name_, _type_, and _charge_. variable = createParmset name PARMSET _variable_ STRING _name_ Return a new and empty PARMSET with the name _name_. variable = createResidue name RESIDUE _variable_ STRING _name_ Return a new and empty RESIDUE with the name _name_. variable = createUnit name UNIT _variable_ STRING _name_ Return a new and empty UNIT with the name _name_. crossLink res1 conn1 res2 conn2 RESIDUE _res1_ STRING _connect1_ RESIDUE _res2_ STRING _connect2_ Create a bond between ATOMs at the connection point specified by _conn1_ and _conn2_. The argument _conn1_ and _conn2_ can have the following values: Name_ Alternatives__ $connect0 $nend, $firstend $connect1 $cend, $lastend $connect2 $send, $disulphide $connect3 $connect4 $connect5 debugOff filename STRING _filename_ This command is a system debugging function. It turns off debugging messages from the source (.c) file _filename_. The symbol * matches all files. The default for all filenames is
off'. Note that system debugging is in effect only if LEaP was compiled with the preprocessor macro DEBUG defined. debugOn filename STRING 
filename
 This command is a system debugging function. It turns on debugging messages from the source (.c) file 
filename
. The symbol * matches all files. The default for all filenames is 
off'. Note that system debugging is in effect only if LEaP was compiled with the preprocessor macro DEBUG defined. debugStatus This command is a memory debugging function. It displays various messages that describe LEaP's usage of system resources. Note that memory debugging is in effect only if LEaP was compiled with the preprocessor macro MEMORY_DEBUG defined; MEMORY_DEBUG values range from 1 through 4 with the greatest being the most aggressive. deleteBond atom1 atom2 ATOM _atom1_ ATOM _atom2_ Remove the bond between the ATOMs _atom1_ and _atom2_. If no bond exists, an error will be displayed. deleteOffLibEntry library entry STRING _library_ STRING _entry_ Delete _entry_ from the Object File Format file named _library_. deleteRestraint unit a b [c [d]] UNIT _unit_ ATOM _a_ ATOM _b_ ATOM _c_ ATOM _d_ Remove a bond, angle, or torsion restraint from _unit_, depending on the number of ATOMs specified. desc variable object _variable_ Print a description of the object. deSelect obj UNIT/RESIDUE/ATOM _obj_ Clears the SELECT flag on all ATOMs within _obj_. See the select command. displayPdbAtomMap Display the Name Map used for ``second chance'' mapping of atom names in PDB files to atoms within residue UNITs. See addPdbAtomMap. displayPdbResMap Display the Name Map used to map residue names in PDB files to variable names within LEaP. See addPdbResMap. edit unit-parmset UNIT/PARMSET _unit-parmset_ In xLEaP this command creates a unit editor or parameter set editor that contains the UNIT or PARMSET _unit-parmset_. The user can view and edit the contents of the UNIT or PARMSET by using the mouse. If _unit-parmset_ is a PARMSET, then the user may select the Atom, Bond, Angle, Torsion, Improper Torsion, or Hydrogen Bond Parameters to edit by selecting the appropriate button. In tLEaP this command prints an error message. flip obj UNIT _obj_ Flips the chirality of the selected atoms within _obj_. groupSelectedAtoms unit name UNIT _unit_ STRING _name_ Create a group within _unit_ with the name _name_ using all of the ATOMs within the UNIT that are selected. If the group has already been defined then overwrite the old group. help [string] STRING _string_ This command prints a description of the command in _string_. If the STRING is not given then a list of legal STRINGs is provided. impose unit seqlist internals UNIT _unit_ LIST _seqlist_ LIST _internals_ The impose command allows the user to impose internal coordinates on the UNIT. The list of RESIDUEs to impose the internal coordinates upon is in _seqlist_. The internal coordinates to impose are in the LIST _internals_. The command works by looking into each RESIDUE within the UNIT that is listed in the _seqlist_ argument and attempts to apply each of the internal coordinates within _internals_. The _seqlist_ argument is a LIST of NUMBERs that represent sequence numbers or ranges of sequence numbers. Ranges of sequence numbers are represented by two-element LISTs that contain the first and last sequence number in the range. The user can specify sequence number ranges that are larger than what is found in the UNIT. For example the range { 1 999 } represents all RESIDUEs in a 200 RESIDUE UNIT. The _internals_ argument is a LIST of LISTs. Each sublist contains a sequence of ATOM names which are of type STRING followed by the value of the internal coordinate. See the output of help _types_ for details on specifying STRINGs. Examples of the impose command are: impose peptide { 1 2 3 } { { $N $CA $C $N -40.0 } { $C $N $CA $C -60.0 } } The RESIDUEs with sequence numbers 1, 2, and 5 within the UNIT peptide will assume an alpha helix conformation. impose peptide { 1 2 { 5 10 } 12 } { { ""CA"" ""CB"" 5.0 } } This will impose on the residues with sequence numbers 1, 2, 5, 6, 7, 8, 9, 10, and 12 within the UNIT peptide a bond length of 5.0 angstroms between the alpha and beta carbons. RESIDUEs without an ATOM named $CB (like glycine) will be unaffected. Three types of conformational change are supported; bond length changes, bond angle changes, and torsion angle changes. If the conformational change involves a torsion angle, then all dihedrals around the central pair of atoms are rotated. The entire list of internals are applied to each RESIDUE. list List all of the variables currently defined. listOff library STRING _library_ List the UNITs/PARMSETs stored within the Object File Format file named _library_. variable = loadAmberParams filename PARMSET _variable_ STRING _filename_ Load an AMBER format parameter set file and place it in _variable_. All interactions defined in the parameter set will be contained within _variable_. This command causes the loaded parameter set to be included in LEaP's list of parameter sets that are searched when parameters are required. General proper and improper torsion parameters are modified, the AMBER general type ""X"" is replaced with the LEaP general type ""?"". loadAmberPrep filename [ prefix ] STRING _filename_ STRING _prefix_ This command loads an AMBER PREP input file. For each residue that is loaded, a new UNIT is constructed that contains a single RESIDUE and a variable is created with the same name as the name of the residue within the PREP file. If the optional argument _prefix_ is provided it will be prefixed to each variable name; this feature is used to prefix united atom residues, which have the same names as all-atom residues, with the string $U to distinguish them. variable = loadMol2 filename STRING _filename_ Load a Sybyl Mol2-format file with the file name _filename_. The UNIT loaded will have the name specified for the MOLECULE in the input file. variable = loadMol3 filename STRING _filename_ Load a Sybyl-derived Mol3-format file with the file name _filename_. The UNIT loaded will have the name specified for the MOLECULE in the input file. More information: http://q4md-forcefieldtools.org/Tutorial/leap-mol3.php loadOff filename STRING _filename_ This command loads the Object File Format library within the file named _filename_. All UNITs and PARMSETs within the library will be loaded. The objects are loaded into LEaP under the variable names the objects had when they were saved. Variables already in existence that have the same names as the objects being loaded will be overwritten. PARMSETs loaded using this command are included in LEaP's library of PARMSETs that is searched whenever parameters are required. variable = loadPdb filename STRING _filename_ Load a Protein Data Bank format file with the file name _filename_. The sequence numbers of the RESIDUEs will be determined from the order of residues within the PDB file ATOM records. For each residue in the PDB file, LEaP searches the variables currently defined for variable names that match the residue name. If a match is found then the contents of the variable are copied into the UNIT created for the PDB structure. If no PDB
TER' card separates the current residue from the previous one, a bond is created between the connect1 ATOM of the previous residue and the connect0 atom of the new one. (A PDB TER record is also used to detect a new residue in the case of contiguous residues with identical residue sequence numbers.) As atoms are read from the ATOM records, their coordinates are written into the correspondingly named ATOMs within the residue being built. If the entire residue is read and it is found that ATOM coordinates are missing then external coordinates are built from the internal coordinates that were defined in the matching UNIT (residue) variable. This allows LEaP to build coordinates for hydrogens and lone pairs which are not specified in PDB files. loadPdbUsingSeq filename unitlist STRING 
filename
 LIST 
unitlist
 This command reads a Protein Data Bank format file from the file named 
filename
. This command is identical to loadPdb except it does not use the residue names within the PDB file. Instead, the sequence is defined by the user in 
unitlist
. For more details see loadPdb. logFile filename STRING 
filename
 This command opens the file with the file name 
filename
 as a log file. User input and ALL output is written to the log file. Output is written to the log file as if the verbosity level were set to 2. variable = matchVariables string LIST 
variable
 STRING 
string
 Create a LIST of variables with names that match 
string
. The 
string
 argument can contain the wildcard characters ""?"" and ""
"" to match any single character or substring of characters, respectively. measureGeom atom1 atom2 [ atom3 [ atom4 ] ] ATOM 
atom1
 
atom2
 
atom3
 
atom4
 Measure the distance, angle, or torsion between two, three, or four ATOMs, respectively. quit relax obj UNIT 
obj
 Relaxes the selected atoms within 
obj
. remove a b UNIT/RESIDUE/ATOM 
a
 UNIT/RESIDUE/ATOM 
b
 Remove the object 
b
 from the object 
a
. If 
a
 is not contained by 
b
 then an error message will be displayed. This command is used to remove ATOMs from RESIDUEs, and RESIDUEs from UNITs. If the object represented by 
b
 is not referenced by some variable name then it will be destroyed. restrainAngle unit a b c force angle UNIT 
unit
 ATOM 
a
 ATOM 
b
 ATOM 
c
 NUMBER 
force
 NUMBER 
angle
 Add an angle restraint to 
unit
 between atoms 
a
, 
b
, and 
c
, having force constant of 
force
, and equilibrium angle 
angle
. restrainBond unit a b force length UNIT 
unit
 ATOM 
a
 ATOM 
b
 NUMBER 
force
 NUMBER 
length
 Add a bond (distance) restraint to 
unit
 between atoms 
a
 and 
b
 with a force constant of 
force
 and an equilibrium distance of 
length
. restrainTorsion unit a b c d force phi multiplicity UNIT 
unit
 ATOM 
a
 ATOM 
b
 ATOM 
c
 ATOM 
d
 NUMBER 
force
 NUMBER 
phi
 NUMBER 
multiplicity
 Add a torsion restraint to 
unit
 between atoms 
a
, 
b
, 
c
, and 
d
, with a force constant of 
force
, an equilibrium torsion angle of 
phi
, and a multiplicity of 
multiplicity
. saveAmberParm unit topologyfilename coordinatefilename UNIT 
unit
 STRING 
topologyfilename
 STRING 
coordinatefilename
 Save the AMBER topology and coordinate files for the UNIT into the files named 
topologyfilename
 and 
coordinatefilename
 respectively. This command will cause LEaP to search its list of PARMSETs for parameters defining all of the interactions between the ATOMs within the UNIT. This command produces a topology file and a coordinate file which are identical in format to those produced by the AMBER program PARM, and which can be read into AMBER and SPASMS for energy minimization, dynamics, or nmode calculations. See also: saveAmberParmPol, saveAmberParmPert, and saveAmberParmPolPert for including atomic polarizabilities and preparing free energy perturbation calculations and saveAmberParmNetcdf for saving in a binary format. saveAmberParmNetcdf unit topologyfilename coordinatefilename UNIT 
unit
 STRING 
topologyfilename
 STRING 
coordinatefilename
 Save the AMBER topology and coordinate files for the UNIT into the files named 
topologyfilename
 and 
coordinatefilename
 respectively. This command will cause LEaP to search its list of PARMSETs for parameters defining all of the interactions between the ATOMs within the UNIT. This command produces a topology file and a coordinate file which can be read into AMBER and SPASMS for energy minimization, dynamics, or nmode calculations. The coordinate file written will be in the binary NetCDF AMBER restart format, which enables the writing of larger input files and quicker I/O. Use saveAmberParm for the regular ASCII coordinate format. saveAmberParmPert unit topologyfilename coordinatefilename UNIT 
unit
 STRING 
topologyfilename
 STRING 
coordinatefilename
 Save the AMBER topology and coordinate files for the UNIT into the files named 
topologyfilename
 and 
coordinatefilename
 respectively. This command will cause LEaP to search its list of PARMSETs for parameters defining all of the interactions between the ATOMs within the UNIT - including the perturbed ATOMs (which are ignored by the vanilla saveAmberParm command). This command produces a topology file and a coordinate file that are identical in format to those produced by the AMBER PARM program using the PERT option, and which can be read into AMBER and SPASMS for free energy calculations. saveAmberParmPol unit topologyfilename coordinatefilename Like saveAmberParm, but includes atomic polarizabilities in the topology file for use with IPOL=1 in Sander. The polarizabilities are according to atom type, and are defined in the 'mass' section of the parm.dat or frcmod file. Note: charges are normally scaled when polarizabilities are used - see scaleCharges for an easy way of doing this. saveAmberParmPolPert unit topologyfilename coordinatefilename Like saveAmberParmPert, but includes atomic polarizabilities in the topology file for use with IPOL=1 in Gibbs. The polarizabilities are according to atom type, and are defined in the 'mass' section of the parm.dat or frcmod file. Note: charges are normally scaled when polarizabilities are used - see scaleCharges for an easy way of doing this. saveAmberPrep unit filename UNIT 
unit
 STRING 
filename
 Save all residues in the UNIT to a prep.in file. All possible improper dihedrals are given for each residue, so unwanted ones need to be deleted from the file. 'Connect0' and 'connect1' atoms must be defined for each residue. saveMol2 unit filename option UNIT 
unit
 STRING 
filename
 NUMBER 
option
 Write UNIT to the file 
filename
 as a Mol2 format file. option = 0 for Default atom types option = 1 for AMBER atom types More information: https://upjv.q4md-forcefieldtools.org/Tutorial/leap-mol2.php saveMol3 unit filename option UNIT 
unit
 STRING 
filename
 NUMBER 
option
 Write UNIT to the file 
filename
 as a Mol3 format file. option = 0 for Default atom types option = 1 for AMBER atom types More information: http://q4md-forcefieldtools.org/Tutorial/leap-mol3.php saveOff object filename object 
object
 STRING 
filename
 The saveOff command allows the user to save UNITs, and PARMSETs to a file named 
filename
. The file is written using the Object File Format (OFF) and can accommodate an unlimited number of uniquely named objects. The names by which the objects are stored are the variable names specified in the argument of this command. If the file 
filename
 already exists then the new objects will be added to the file. If there are objects within the file with the same names as objects being saved then the old objects will be overwritten. The argument 
object
 can be a single UNIT, a single PARMSET, or a LIST of mixed UNITs and PARMSETs. savePdb unit filename UNIT 
unit
 STRING 
filename
 Write UNIT to the file 
filename
 as a PDB format file. scaleCharges container scale_factor UNIT/RESIDUE/ATOM 
container
 NUMBER 
scale_factor
 This command scales the charges in the object by 
scale_factor
, which must be > 0. It is useful for building systems for use with polarizable atoms, e.g. > x = copy solute > scaleCharges x 0.8 > y = copy WATBOX216 > scalecharges y 0.875 > solvatebox x y 10 > saveamberparmpol x x.top x.crd select obj UNIT/RESIDUE/ATOM 
obj
 Sets the SELECT flag on all ATOMs within 
obj
. See the deSelect command. variable = sequence list LIST 
list
 The sequence command is used to create a new UNIT by copying the contents of a LIST of UNITs. As each UNIT in the list is copied, a bond is created between its head atom and the tail ATOM of the previous UNIT, if both connect ATOMs are defined. If only one of the connect pair is defined, a warning is generated and no bond is created. If neither connection ATOM is defined then no bond is created. As each RESIDUE within a UNIT is copied, it is assigned a sequence number reflecting the order added. The order of RESIDUEs in multi-RESIDUE UNITs is maintained. This command builds reasonable starting coordinates for the new UNIT by assigning internal coordinates to the linkages between the component UNITs and building the Cartesian coordinates from these and the internal coordinates of the component UNITs. set default variable value STRING 
variable
 STRING 
value
 OR set container parameter object UNIT/RESIDUE/ATOM/STRING 
container
 STRING 
parameter
 object 
object/value
 This command sets the values of some global parameters (when the first argument is ""default"") or sets various parameters associated with 
container
. To see the possible variables for ""set default"", type ""help set_default"". The box parameter of a UNIT defines the bounding box of the UNIT; this is not a UNIT's periodic box. The setBox and solvate family of commands add a periodic box to a UNIT; for a description, type, e.g., ""help setBox"". The more useful parameters for each type of 
container
 are the following: container parameters values UNIT name STRING head, tail ATOM [e.g. unit.1.1] restype ""protein"" ""nucleic"" ""saccharide"" ""solvent"" ""undefined"" [sets all residues in UNIT] box LIST [side lengths: {A B C}] or NUMBER [cube side length] or ""null"" cap LIST [center, radius: {X Y Z R}] or ""null"" RESIDUE name STRING [e.g. restype [see UNIT] unit.1] connect0, connect1 ATOM [e.g. unit.1.1] imagingAtom ATOM [e.g. unit.1.1] ATOM name, pertName STRING [<= 4 chars] [e.g. type, pertType STRING [<= 2 chars] unit.1.1] element STRING pert ""true"" [or pert flag unset] charge, pertCharge DOUBLE position LIST [{X Y Z}] Allowed arguments to ""set default variable value"" are these: variables values descriptions PdbWriteCharges ""on"" add charges to each ATOM record ""off"" don't do this (default) OldPrmtopFormat ""on"" use prmtop format from Amber6 and earlier ""off"" use the new prmtop format (default) Gibbs ""on"" require perturbed atoms to be set explicitly (needed for gibbs) ""off"" set perturbed if Type != PertType (default) (OK for sander) UseResIds ""on"" put cols 22-27 of the input pdb file into ""off"" a RESIDUE_ID table in prmtop files; default is ""off""; only works with new prmtop formats, and when a single loadPdb command is used to create a unit. Charmm ""on"" include terms for CHARMM22 force fields ""off"" don't include these (default) DeleteExtraPointAngles ""on"" delete angles and torsions relating to extra points (default) ""off"" don't delete these (for older codes only) FlexibleWater ""on"" allow for flexible 3-point water models ""off"" assume 3-point water models are rigid (default) PBRadii ""bondi"" use Bondi radii for generalized Born ""mbondi"" use H-modified Bondi radii (default) ""mbondi2"" use H(N)-modified Bondi radii ""mbondi3"" ArgH and AspGluO modified Bondi2 radii ""parse"" Radii from the Sitkoff et al. parse parameters ""pbamber"" Huo and Kollman optimized radii (old!) ""amber6"" use radii that were the default in amber6 (only recommended for backwards compat.) Dielectric ""distance"" use distance-dependent dielectric (default) ""constant"" use constant dielectric dipole_damp_factor real sets the default value for ""DIPOLE_DAMP_FACTOR"" for dipole screening factors in Thole models. Valid value > 0.0 sceescalefactor real sets the default value for ""SCEE_SCALE_FACTOR"" for 1-4 EEL scaling factors. Valid value > 0.0. Default=1.2. scnbscalefactor real sets the default value for ""SCNB_SCALE_FACTOR"" for 1-4 NB scaling factors. Valid value > 0.0. Default=2.0. CMAP ""on"" include CMAP corrections for dihedrals ""off"" don't include these (default) PHIPSIMAP ""on"" include residue-based PHIPSI parameters ""off"" don't include these (default) ipol integer Sets the default value for IPOL. Valid values are 0 - 4. Default value is 0, meaning disabled. nocenter ""on"" coordinates will not be centered in the periodic simulation box ""off"" coordinates will be centered (default) reorder_residues ""on"" solvent will be moved to the end (default) ""off"" residue order will be maintained as input. Beta feature: use at your own risk! setBox solute enclosure [ buffer ] UNIT 
solute
 ""vdw"" OR ""centers"" 
enclosure
 object 
buffer
 The setBox command creates a periodic box around the 
solute
 UNIT, turning it into a periodic system for the simulation programs. It does not add any solvent to the system. The choice of ""vdw"" or ""centers"" determines whether the box encloses all entire atoms or just all atom centers - use ""centers"" if the system has been previously equilibrated as a periodic box. See the solvateBox command for a description of the buffer object, which extends either type of box by an arbitrary amount. showDefault [ variable OR all OR * ] STRING 
variable
 The showdefault command shows the values assigned to the variables by the ""set default"" command. Without variable, with ""all"", or with ""
"", all default variables are shown. solvateBox solute solvent buffer [ ""iso"" ] [ closeness ] UNIT 
solute
 UNIT 
solvent
 object 
buffer
 NUMBER 
closeness
 The solvateBox command creates a solvent box around the 
solute
 UNIT. The 
solute
 UNIT is modified by the addition of 
solvent
 RESIDUEs. The user may want to first align long solutes that are not expected to tumble using alignAxes, in order to minimize box volume. The normal choice for a TIP3 
solvent
 UNIT is WATBOX216. Note that constant pressure equilibration is required to bring the artificial box to reasonable density, since Van der Waals voids remain due to the impossibility of natural packing of solvent around the solute and at the edges of the box. The solvent box UNIT is copied and repeated in all three spatial directions to create a box containing the entire solute and a buffer zone defined by the 
buffer
 argument. The 
buffer
 argument defines the distance, in angstroms, between the wall of the box and the closest ATOM in the solute. If the buffer argument is a single NUMBER, then the buffer distance is the same for the x, y, and z directions, unless the ""iso"" option is used to make the box isometric, with the shortest box clearance = buffer. If ""iso"" is used, the solute is rotated to orient the principal axes, otherwise it is just centered on the origin. If the buffer argument is a LIST of three NUMBERS, then the NUMBERs are applied to the x, y, and z axes respectively. As the larger box is created and superimposed on the solute, solvent molecules overlapping the solute are removed. The optional 
closeness
 parameter can be used to control the extent to which 
solvent
 ATOMs overlap 
solute
 ATOMs. The default value of the 
closeness
 argument is 1.0, which allows no overlap. Smaller values allow solvent ATOMs to overlap 
solute
 ATOMs by (1 - closeness) * R
ij, where R
ij is the sum of the Van der Waals radii of solute and solvent atoms. Values greater than 1 enforce a minimum gap between solvent and solute of (closeness - 1) * R
ij. This command modifies the 
solute
 UNIT in several ways. First, the coordinates of the ATOMs are modified to move the center of a box enclosing the Van der Waals radii of the atoms to the origin. Secondly, the UNIT is modified by the addition of 
solvent
 RESIDUEs copied from the 
solvent
 UNIT. Finally, the box parameter of the new system (still named for the 
solute
) is modified to reflect the fact that a periodic, rectilinear solvent box has been created around it. solvateCap solute solvent position radius [ closeness ] UNIT 
solute
 UNIT 
solvent
 object 
position
 NUMBER 
radius
 NUMBER 
closeness
 The solvateCap command creates a solvent cap around the 
solute
 UNIT or a part thereof. The 
solute
 UNIT is modified by the addition of 
solvent
 RESIDUEs. The normal choice for a TIP3 
solvent
 UNIT is WATBOX216. The 
solvent
 box is repeated in all three spatial directions and 
solvent
 RESIDUEs selected to create a solvent sphere with a radius of 
radius
 Angstroms. The 
position
 argument defines where the center of the solvent cap is to be placed. If 
position
 is a UNIT, RESIDUE, ATOM, or a LIST of UNITs, RESIDUEs, or ATOMs, then the geometric center of the ATOMs within the object will be used as the center of the solvent cap sphere. If 
position
 is a LIST containing three NUMBERs then the 
position
 argument will be treated as a vector that defines the position of the solvent cap sphere center. The optional 
closeness
 parameter can be used to control the extent to which 
solvent
 ATOMs overlap 
solute
 ATOMs. The default value of the 
closeness
 argument is 1.0, which allows no overlap. Smaller values allow solvent ATOMs to overlap 
solute
 ATOMs by (1 - closeness) * R
ij, where R
ij is the sum of the Van der Waals radii of solute and solvent atoms. Values greater than 1 enforce a minimum gap between solvent and solute of (closeness - 1) * R
ij. This command modifies the 
solute
 UNIT in several ways. First, the UNIT is modified by the addition of 
solvent
 RESIDUEs copied from the 
solvent
 UNIT. Secondly, the cap parameter of the UNIT 
solute
 is modified to reflect the fact that a solvent cap has been created around the solute. solvateDontClip solute solvent buffer [ closeness ] UNIT 
solute
 UNIT 
solvent
 object 
buffer
 NUMBER 
closeness
 This command is identical to the solvateBox command except that the solvent box that is created is not clipped to the boundary of the 
buffer
 region. This command forms larger solvent boxes than does solvateBox because it does not cause solvent that is outside the buffer region to be discarded. This helps to preserve the periodic structure of properly constructed solvent boxes, preventing hot-spots from forming. solvateOct solute solvent buffer [ ""iso"" ] [ closeness ] UNIT 
solute
 UNIT 
solvent
 object 
buffer
 NUMBER 
closeness
 The solvateOct command is the same as solvateBox, except the corners of the box are sliced off, resulting in a truncated octahedron, which typically gives a more uniform distribution of solvent around the solute. In solvateOct, when a LIST is given for the buffer argument, four numbers are given instead of three, where the fourth is the diagonal clearance. If 0.0 is given as the fourth number, the diagonal clearance resulting from the application of the x,y,z clearances is reported. If a non-0 value is given, this may require scaling up the other clearances, which is also reported. Similarly, if a single number is given, any scaleup of the x,y,z buffer to accommodate the diagonal clip is reported. If the ""iso"" option is used, the isometric truncated octahedron is rotated to an orientation used by the PME code, and the box and angle dimensions output by the saveAmberParm
 commands are adjusted for PME code imaging. solvateShell solute solvent thickness [ closeness ] UNIT 
solute
 UNIT 
solvent
 NUMBER 
thickness
 NUMBER 
closeness
 The solvateShell command creates a solvent shell around the 
solute
 UNIT. The 
solute
 UNIT is modified by the addition of 
solvent
 RESIDUEs. The normal choice for a TIP3 
solvent
 UNIT is WATBOX216. The 
solvent
 box is repeated in all three spatial directions and 
solvent
 RESIDUEs selected to create a solvent shell with a radius of 
thickness
 Angstroms around the 
solute
. The 
thickness
 argument defines the maximum distance a 
solvent
 ATOM may be from the closest 
solute
 ATOM. The optional 
closeness
 parameter can be used to control overlap of 
solvent
 with 
solute
 ATOMs. The default value of the 
closeness
 argument is 1.0, which allows contact but no overlap. Please see the solvateBox command for more details on the 
closeness
 parameter. source filename STRING 
filename
 This command executes LEaP commands within a text file. To display the commands as they are read, see the verbosity command. The text within the source file must be formatted exactly like the text the user types into LEaP. transform atoms matrix CONTAINER/LIST 
atoms
 LIST 
matrix
 Translate all of the ATOMs within 
atoms
 by the (3X3) or (4X4) matrix defined by the 9 or 16 NUMBERs in the LIST of LISTs 
matrix
. { { r11 r12 r13 -tx } { r21 r22 r23 -ty } { r31 r32 r33 -tz } { 0 0 0 1 } } The diagonal upper left elements, rII can be used for symmetry operations, e.g. a reflection in the XY plane can be produced with r11=1, r22=1, r33=-1 where the other rIJ elements are 0. The -t column is used to specify translations along the appropriate axes (0 for no translation). translate atoms direction UNIT/RESIDUE/ATOM 
atoms
 LIST 
direction
 Translate all of the ATOMs within 
atoms
 by the vector defined by the three NUMBERs in the LIST _ direction_. verbosity level NUMBER 
level
 This command sets the level of output that LEaP provides the user. A value of 0 is the default, providing the minimum of messages. A value of 1 will produce more output, and a value of 2 will produce all of the output of level 1 and display the text of the script lines executed with the source command. zMatrix obj zmatrix UNIT/RESIDUE/ATOM 
obj
 LIST 
zmatrix
 The zMatrix command is quite complicated. It is used to define the external coordinates of ATOMs within 
obj
 using internal coordinates. The second parameter of the zMatrix command is a LIST of LISTs; each sub-list has several arguments: { a1 a2 bond12 } This entry defines the coordinate of 
a1
 by placing it 
bond12
 angstroms along the x-axis from ATOM 
a2
. If ATOM 
a2
 does not have coordinates defined then ATOM 
a2
 is placed at the origin. { a1 a2 a3 bond12 angle123 } This entry defines the coordinate of 
a1
 by placing it 
bond12
 angstroms away from ATOM 
a2
 making an angle of 
angle123
 degrees between 
a1
, 
a2
 and 
a3
. The angle is measured in a right hand sense and in the x-y plane. ATOMs 
a2
 and 
a3
 must have coordinates defined. { a1 a2 a3 a4 bond12 angle123 torsion1234 } This entry defines the coordinate of 
a1
 by placing it 
bond12
 angstroms away from ATOM 
a2
, creating an angle of 
angle123
 degrees between 
a1
, 
a2
, and 
a3
, and making a torsion angle of 
torsion1234
 between 
a1
, 
a2
, 
a3
, and 
a4
. { a1 a2 a3 a4 bond12 angle123 angle124 orientation } This entry defines the coordinate of 
a1
 by placing it 
bond12
 angstroms away from ATOM 
a2
, making angles 
angle123
 between ATOMs 
a1
, 
a2
, and 
a3
, and 
angle124
 between ATOMs 
a1
, 
a2
, and 
a4
. The argument 
orientation
 defines whether the ATOM 
a1
 is above or below a plane defined by the ATOMs 
a2
, 
a3
, and 
a4
. If 
orientation
 is positive then 
a1
 will be placed in such a way so that the inner product of (
a3
-
a2
) cross (
a4
-
a2
) with (
a1
-
a2
) is positive. Otherwise 
a1
 will be placed on the other side of the plane. This allows the coordinates of a molecule like fluoro-chloro-bromo-methane to be defined without having to resort to dummy atoms. The first arguments within the zMatrix entries ( 
a1
, 
a2
, 
a3
, 
a4
 ) are either ATOMs or STRINGs containing names of ATOMs within 
obj
. The subsequent arguments are all NUMBERs. Any ATOM can be placed at the 
a1
 position, even those that have coordinates defined. This feature can be used to provide an endless supply of dummy atoms, if they are required. A predefined dummy atom with the name ""
"" (a single asterisk, no quotes) can also be used. No order is imposed in the sub-lists. The user can place sub-lists in arbitrary order, as long as they maintain the requirement that all atoms 
a2
, 
a3
, and 
a4
 must have external coordinates defined, except for entries that define the coordinate of an ATOM using only a bond length."
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_im_cluster_butina/ctb_im_cluster_butina/1.1.4+galaxy0	".. class:: infomark 
What this tool does
 This tool performs Butina clustering for a set of input molecules, using the chemistry toolkit RDKit, and returns results in SDF format. ----- .. class:: infomark 
Input
 | - Molecules in 
SDF format
_ | - A number of other parameters can be set, including the fingerprint type and the similarity metric to use. .. _SDF format: http://en.wikipedia.org/wiki/Chemical_table_file ----- .. class:: infomark 
Output
 SD file containing clusters."
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_im_cluster_butina_matrix/ctb_im_cluster_butina_matrix/1.1.4+galaxy0	".. class:: infomark 
What this tool does
 This tool performs Butina clustering for a set of input molecules, using the chemistry toolkit RDKit, and returns results in tabular format. ----- .. class:: infomark 
Input
 | - Molecules in 
SDF format
_ | - A number of other parameters can be set, including the fingerprint type and the similarity metric to use. .. _SDF format: http://en.wikipedia.org/wiki/Chemical_table_file ----- .. class:: infomark 
Output
 Tabular file containing clusters."
toolshed.g2.bx.psu.edu/repos/bgruening/mordred/ctb_mordred_descriptors/1.2.0+galaxy0	"Calculates up to 1825 molecular descriptors using the Mordred package. A list of all descriptors is located here_. .. _here: https://github.com/simonbray/mordred-descriptors .. class:: infomark 
Input
 A file containing multiples chemical structures, either in SMILES, InChi or SDF format. ----- .. class:: infomark 
Output
 A tabular file, in which each column represents a molecular descriptor (1613 in total, or 1825 if 3D descriptors are included). Each row describes a single molecule. Empty cells indicate that a descriptor could not be calculated for that molecule. Rows which are entirely empty most likely indicate a wrongly encoded molecule."
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_im_pbf_ev/ctb_im_pbf_ev/1.1.4+galaxy0	".. class:: infomark 
What this tool does
 Calculate plane of best fit for molecules. .. class:: infomark 
Input
 | - Input file in 
SDF Format
_ .. _SDF Format: http://en.wikipedia.org/wiki/Chemical_table_file .. _SMILES: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 SD-file with plane of best fit."
toolshed.g2.bx.psu.edu/repos/bgruening/autodock_vina_prepare_box/prepare_box/2021.03.5+galaxy0	".. class:: infomark 
Description
 This tool calculates a confounding box around an input ligand or pocket and uses it to generate the input parameters for an AutoDock Vina job. If you already know the coordinates of a ligand which binds to the protein, you can simply use this file as the tool input. Alternatively, you can search for pockets using the fpocket tool and use one of the output PDB files as an input. The output file can be fed into the AutoDock Vina tool as an alternative to creating the parameter file manually. ----- .. class:: infomark 
Inputs:
 This tool requires: * An input file (MOL, SDF, PDB or MOL2 format) representing either a ligand, or a pocket found with the fpocket tool. * (OPTIONAL) Buffers for each direction (x,y,z), which defaults to 0 Å. This value will be added to the confounding box that the tool generates in each respective direction. We recommend that you visualise the calculated box from an initial run of this tool, and calculate the expansion needed in each direction to cover the area of the binding site you wish to explore. ----- .. class:: infomark 
Output:
 The output for this tool is a txt file containing the parameters needed to run an autodock vina docking calculation with the docking tool. For example: :: size_x = 12.443000000000001 size_y = 11.888 size_z = 9.290999999999997 center_x = -29.8395 center_y = 4.364 center_z = -64.5925 num_modes = 9999 energy_range = 9999 exhaustiveness = 10 cpu = 4 seed = 1 The values for num_modes, energy range, cpu and seed are set to default values here. This file can be used as the box parameter for the docking tool."
toolshed.g2.bx.psu.edu/repos/bgruening/chembl/chembl_structure_pipeline/1.0.0+galaxy0	"Apply the ChEMBL chemical curation pipeline to a set of chemical structures in SDF format. The pipeline is described in detail in the citation provided (Bento et al., 2020). The pipeline consists of three components: - a Standardizer which formats compounds according to defined rules and conventions, based mostly on FDA/IUPAC guidelines. - a GetParent component that removes any salts and solvents from the compound to create its parent. - a Checker to test the validity of chemical structures and flag any serious errors. Errors are given a code from 0 (least serious) to 10 (most serious), the highest of which is stored in the SDF field 
&lt;MaxPenaltyScore&gt;
. A list of all errors encountered is recorded under 
&lt;IssueMessages&gt;
. Either one or more of these protocols can be applied in a single Galaxy job. ----- .. class:: infomark 
Input
 One or more molecules in MOL/SDF format. ----- .. class:: infomark 
Output
 A MOL/SD-file containing the processed molecules."
toolshed.g2.bx.psu.edu/repos/bgruening/chembl_structure_pipeline/chembl_structure_pipeline/1.0.0+galaxy0	"Apply the ChEMBL chemical curation pipeline to a set of chemical structures in SDF format. The pipeline is described in detail in the citation provided (Bento et al., 2020). The pipeline consists of three components: - a Standardizer which formats compounds according to defined rules and conventions, based mostly on FDA/IUPAC guidelines. - a GetParent component that removes any salts and solvents from the compound to create its parent. - a Checker to test the validity of chemical structures and flag any serious errors. Errors are given a code from 0 (least serious) to 10 (most serious), the highest of which is stored in the SDF field 
&lt;MaxPenaltyScore&gt;
. A list of all errors encountered is recorded under 
&lt;IssueMessages&gt;
. Either one or more of these protocols can be applied in a single Galaxy job. ----- .. class:: infomark 
Input
 One or more molecules in MOL/SDF format. ----- .. class:: infomark 
Output
 A MOL/SD-file containing the processed molecules."
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_change_title/0.0.1	".. class:: infomark 
What this tool does
 Changes the title of a molecule file to a metadata value of a given ID in the same molecule file. ----- .. class:: infomark 
Input
 
SD-file
 with metadata including the given ID. .. _SD-file: http://en.wikipedia.org/wiki/Chemical_table_file ----- .. class:: infomark 
Output
 Same as input with changed title tag. ----- .. class:: infomark 
Cite
 
Open Babel
 .. _Open Babel: http://openbabel.org/wiki/Main_Page"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_change_title/openbabel_change_title/3.1.1+galaxy0	".. class:: infomark 
What this tool does
 Reassigns the title of a molecule file (SDF or MOL2) to a metadata value of a given ID in the same molecule file. For example, if the metadata identifier chosen is PUBCHEM_SHAPE_VOLUME, then the title of the file (in the first line) will be replaced by the variable listed under PUBCHEM_SHAPE_VOLUME. ----- .. class:: infomark 
Input
 
SD-file
_ with metadata including the given ID. .. _SD-file: http://en.wikipedia.org/wiki/Chemical_table_file ----- .. class:: infomark 
Output
 Same as input, with changed title tag."
toolshed.g2.bx.psu.edu/repos/bgruening/sucos_clustering/sucos_clustering/2020.03.4+galaxy0	".. class:: infomark 
What it does?
 This tool clusters molecules based on the overlap of 3D features as determined by SuCOS. Clustering uses a clustering threshold that can be set by the user. The default threshold is 0.8. The original SuCOS code is on GitHub_ under a MIT license. The SuCOS work is described here_. .. _GitHub: https://github.com/susanhleung/SuCOS .. _here: https://chemrxiv.org/articles/SuCOS_is_Better_than_RMSD_for_Evaluating_Fragment_Elaboration_and_Docking_Poses/8100203 .. class:: infomark 
Input
 Molecules such as an SD file dataset in the history. .. class:: infomark 
Output
 A series of SD files, one for each cluster containing the molecules in that cluster."
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_compound_convert/0.1	".. class:: infomark 
What this tool does
 The compound converter joins several 
Open Babel
 command prompt converters in an easy to use tool. It converts various chemistry and moleculare modeling data files. The output format can be specified as well as several parameters. Some parameters are available for all tools (e.g. protonation state and pH) others are specific for a given output format (e.g. exclude isotopes for conversion to canSMI). ----- .. class:: infomark 
Output
 Can be specified manually. ----- .. class:: infomark 
Cite
 N M O'Boyle, M Banck, C A James, C Morley, T Vandermeersch, and G R Hutchison - 
Open Babel: An open chemical toolbox.
 .. 
Open Babel: An open chemical toolbox.
: http://www.jcheminf.com/content/3/1/33 
Open Babel
 .. _
Open Babel
: http://openbabel.org/wiki/Main_Page"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_obgrep/0.1	".. class:: infomark 
What this tool does
 Uses the Open Babel Obgrep_ to search for molecules inside multi-molecule files (e.g. SMI, SDF, etc.) or across multiple files. It is known that not all SMARTS features from the Daylight Toolkit are supported, please have a look here_. .. 
Obgrep: http://openbabel.org/wiki/Obgrep .. _here: http://openbabel.org/wiki/SMARTS ----- .. class:: infomark 
Input
 | - 
SD-Format
 | - 
SMILES Format
 .. _SD-Format: http://en.wikipedia.org/wiki/Chemical_table_file .. _SMILES Format: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 Same as input format. ----- .. class:: infomark 
Cite
 N M O'Boyle, M Banck, C A James, C Morley, T Vandermeersch, and G R Hutchison - 
Open Babel: An open chemical toolbox.
 .. 
Open Babel: An open chemical toolbox.
: http://www.jcheminf.com/content/3/1/33 
Open Babel
 .. _
Open Babel
: http://openbabel.org/wiki/Main_Page"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_compound_convert/openbabel_compound_convert/3.1.1+galaxy1	".. class:: infomark 
What this tool does
 The compound converter combines several Open Babel command prompt converters into a single easy-to-use tool. It interconverts various chemistry and molecular modeling data files. The output format can be specified as well as several parameters. Some parameters are available for all tools (e.g. protonation state and pH) while others are specific for a given output format (e.g. exclude isotopes for conversion to canSMI). ----- .. class:: infomark 
Output
 Can be specified manually."
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_obgrep/openbabel_obgrep/3.1.1+galaxy1	".. class:: infomark 
What this tool does
 Uses the Open Babel Obgrep_ to search for molecules inside multi-molecule files (e.g. SMI, SDF, etc.) or across multiple files. Not all SMARTS features from the original implementation in the Daylight Toolkit are supported by OpenBabel; please have a look here_. The search query can be submitted either as a single SMARTS pattern or as a file containing multiple SMARTS patterns. .. 
Obgrep: https://open-babel.readthedocs.io/en/latest/Command-line_tools/babel.html#filtering-molecules-from-a-multimolecule-file .. _here: https://open-babel.readthedocs.io/en/latest/Command-line_tools/babel.html#smarts-descriptor ----- .. class:: infomark 
Input
 | - 
SD-Format
 | - 
SMILES Format
_ .. _SD-Format: http://en.wikipedia.org/wiki/Chemical_table_file .. _SMILES Format: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 Same as input format."
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_ob_genProp/1.0	".. class:: infomark 
What this tool does
 Computes several physico-chemical properties for a set of molecules. The following physico-chemical properties and descriptors are computed for each molecule: - number of hydrogen-bond donor and acceptor groups - number of rotatable bonds - logP - number of rings - number of heavy atoms - molecular weight - total Polar Surface Area - molecular refractivity - Canonical SMILES - InChI string - InChI-Key - Spectrophores(TM) ----- .. class:: infomark 
Input
 - SDF_ - MOL2_ .. 
SDF: http://en.wikipedia.org/wiki/Chemical_table_file .. _MOL2: http://www.tripos.com/index.php?family=modules,SimplePage,Mol2_File_Format2009 3D coordinates of the molecules have to be provided. ----- .. class:: warningmark 
Hint
 the generation of Spectrophores(TM) requires the previous addition of explicit hydrogen atoms and the proper definition of 3D coordinates. The user is directed towards the corresponding tools if accurate Spectrophores(TM) descriptors are required. ----- .. class:: infomark 
Output
 Either a SD-file containing several computed physico-chemical properties stored as metadata or a tabular file with the metadata stored in columns. ----- .. class:: infomark 
Cite
 N M O'Boyle, C Morley and G R Hutchison - 
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
 Silicos_ - |Spectrophores (TM)| is a registered tool implemented in the open-source OpenBabel. .. |Spectrophores (TM)| unicode:: Spectrophores U+2122 
Open Babel
 .. _Open Babel: http://openbabel.org/wiki/Main_Page .. 
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
: http://www.biomedcentral.com/content/pdf/1752-153X-2-5.pdf .. _Silicos: http://openbabel.org/docs/dev/Fingerprints/spectrophore.html"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_genprop/openbabel_genProp/3.1.1+galaxy1	".. class:: infomark 
What this tool does
 Computes several physico-chemical properties for a set of molecules. The following physico-chemical properties and descriptors are computed for each molecule: - number of hydrogen-bond donor and acceptor groups - number of rotatable bonds - logP - number of rings - number of heavy atoms - molecular weight - total Polar Surface Area - molecular refractivity - Canonical SMILES - InChI string - InChI-Key - Spectrophores(TM) ----- .. class:: infomark 
Input
 - SDF_ - MOL2_ .. 
SDF: http://en.wikipedia.org/wiki/Chemical_table_file .. _MOL2: http://www.tripos.com/index.php?family=modules,SimplePage,Mol2_File_Format2009 3D coordinates of the molecules have to be provided. ----- .. class:: warningmark 
Hint
 the generation of Spectrophores(TM) requires the previous addition of explicit hydrogen atoms and the proper definition of 3D coordinates. The user is directed towards the corresponding tools if accurate Spectrophores(TM) descriptors are required. ----- .. class:: infomark 
Output
 Either a SD-file containing several computed physico-chemical properties stored as metadata or a tabular file with the metadata stored in columns. ----- .. class:: infomark 
Cite
 N M O'Boyle, C Morley and G R Hutchison - 
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
 Silicos_ - |Spectrophores (TM)| is a registered tool implemented in the open-source OpenBabel. .. |Spectrophores (TM)| unicode:: Spectrophores U+2122 
Open Babel
 .. _Open Babel: https://open-babel.readthedocs.io/en/latest/index.html .. 
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
: http://www.biomedcentral.com/content/pdf/1752-153X-2-5.pdf .. _Silicos: https://open-babel.readthedocs.io/en/latest/Fingerprints/spectrophore.html"
toolshed.g2.bx.psu.edu/repos/bgruening/confab/ctb_confab/3.1.1+galaxy0	".. class:: infomark 
What this tool does
 Confab_ is a conformation generator. The algorithm starts with an input 3D structure which, after some initialization steps, is used to generate multiple conformers, which are filtered on-the-fly to identify diverse low-energy conformers. .. _Confab: https://open-babel.readthedocs.io/en/latest/3DStructureGen/multipleconformers.html#confab ----- .. class:: infomark 
Input
 * Example:: 21.2060 9.9350 63.0810 C 0 0 0 0 0 0 0 0 0 0 0 0 21.2410 9.4460 64.5510 C 0 0 0 0 0 0 0 0 0 0 0 0 22.0000 8.1250 64.6300 C 0 0 0 0 0 0 0 0 0 0 0 0 21.7010 7.3010 65.5120 O 0 0 0 0 0 0 0 0 0 0 0 0 23.1180 7.8720 63.7340 C 0 0 0 0 0 0 0 0 0 0 0 0 23.4530 8.7270 62.7850 C 0 0 0 0 0 0 0 0 0 0 0 0 24.6970 8.4430 61.9510 C 0 0 0 0 0 0 0 0 0 0 0 0 ....... 1 2 1 0 0 0 0 1 11 1 0 0 0 0 2 3 1 0 0 0 0 3 4 2 0 0 0 0 3 5 1 0 0 0 0 5 6 2 0 0 0 0 6 7 1 0 0 0 0 RMSD cutoff (in Angstrom) 0.5 Energy cutoff (in kcal/mol) 50.0 Max number of conformers to test 100000 Include the input conformation as the first conformer False ----- .. class:: infomark 
Output
 * Example:: 23 26 0 0 0 0 0 0 0 0999 V2000 21.2060 9.9350 63.0810 C 0 0 0 0 0 0 0 0 0 0 0 0 21.2410 9.4460 64.5510 C 0 0 0 0 0 0 0 0 0 0 0 0 22.0000 8.1250 64.6300 C 0 0 0 0 0 0 0 0 0 0 0 0 21.7010 7.3010 65.5120 O 0 0 0 0 0 0 0 0 0 0 0 0 23.1180 7.8720 63.7340 C 0 0 0 0 0 0 0 0 0 0 0 0 23.4530 8.7270 62.7850 C 0 0 0 0 0 0 0 0 0 0 0 0 24.6970 8.4430 61.9510 C 0 0 0 0 0 0 0 0 0 0 0 0 24.4490 8.6370 60.4430 C 0 0 0 0 0 0 0 0 0 0 0 0 23.7890 9.9970 60.0980 C 0 0 2 0 0 0 0 0 0 0 0 0 22.4340 10.0950 60.8720 C 0 0 1 0 0 0 0 0 0 0 0 0 22.6140 10.0230 62.4340 C 0 0 1 0 0 0 0 0 0 0 0 0 21.6330 11.3540 60.4500 C 0 0 0 0 0 0 0 0 0 0 0 0 21.4320 11.4340 58.9110 C 0 0 0 0 0 0 0 0 0 0 0 0 22.7860 11.4040 58.1690 C 0 0 1 0 0 0 0 0 0 0 0 0 23.4830 10.0600 58.5980 C 0 0 1 0 0 0 0 0 0 0 0 0 24.6740 9.9180 57.6180 C 0 0 0 0 0 0 0 0 0 0 0 0 24.0720 10.4500 56.2670 C 0 0 0 0 0 0 0 0 0 0 0 0 22.7140 11.1490 56.6270 C 0 0 2 0 0 0 0 0 0 0 0 0 23.6590 12.6770 58.4540 C 0 0 0 0 0 0 0 0 0 0 0 0 23.4270 11.2460 63.0070 C 0 0 0 0 0 0 0 0 0 0 0 0 22.3750 12.3880 55.7810 C 0 0 0 0 0 0 0 0 0 0 0 0 23.2120 12.8760 55.0520 O 0 0 0 0 0 0 0 0 0 0 0 0 21.0090 12.9760 55.8570 C 0 0 0 0 0 0 0 0 0 0 0 0 :: 1 2 1 0 0 0 0 1 11 1 0 0 0 0 2 3 1 0 0 0 0 3 4 2 0 0 0 0 3 5 1 0 0 0 0 5 6 2 0 0 0 0 6 7 1 0 0 0 0 6 11 1 0 0 0 0 7 8 1 0 0 0 0 9 8 1 6 0 0 0 10 9 1 1 0 0 0"
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_im_constrained_conf_gen/ctb_im_constrained_conf_gen/1.1.4+galaxy0	".. class:: infomark 
What this tool does
 This tool generates constrained conformers for a set of input molecules, using the chemistry toolkit RDKit. ----- .. class:: infomark 
Input
 | - Molecules in 
SDF format
_ | - Reference molecule in SDF format. .. _SDF format: http://en.wikipedia.org/wiki/Chemical_table_file ----- .. class:: infomark 
Output
 SD-file containing generated conformers."
toolshed.g2.bx.psu.edu/repos/chemteam/acpype_amber2gromacs/acpype_Amber2Gromacs/21.10+galaxy0	"Tool to produce GROMACS topology and coordinate files from systems generated with AmberTools' Tleap. .. class:: infomark 
Input
 The input files are the standard topology (prmtop) and coordinate (inpcrd) files that are generally produced as outputs when processing a structure through Tleap. .. class:: infomark 
Outputs
 GROMACS topology (TOP) and coordinate (GRO) files."
toolshed.g2.bx.psu.edu/repos/chemteam/parmconv/parmconv/21.10+galaxy0	".. class:: infomark 
What it does
 This tool converts parameter and topology files that represent a solvated complex into parameter files for the ligand, receptor, complex and solvated complex in AMBER prmtop format. These files are needed for MMGBSA/MMPBSA calculations. .. class:: infomark 
How it works
 AmberTools' ParmEd is used to strip unneeded atoms and save the parameter files. The stripmasks are defined by the user. .. class:: infomark 
Outputs created
 prmtop files for the ligand, receptor, complex and solvated complex. .. class:: infomark 
User guide and documentation
 - The 
AmberTools Manual
 - The 
Parmed Documentation
 .. 
AmberTools Manual
: https://ambermd.org/doc12/Amber18.pdf .. 
Parmed Documentation
: https://parmed.github.io/ParmEd/html/index.html"
toolshed.g2.bx.psu.edu/repos/chemteam/qiskit_xyz2pdb/qiskit_xyz2pdb/0.1.2+galaxy0	".. class:: infomark 
What it does
 This tool takes the XYZ files from Qiskit's protein folding algorithm and converts them to a proper alpha carbon PDB file. 
_
 .. class:: infomark 
Input
 - XYZ file of coarse grain model coordinates 
___ .. class:: infomark 
Output
 - Alpha carbon PDB file in either trace or HETATM format"
toolshed.g2.bx.psu.edu/repos/chemteam/mdanalysis_cosine_analysis/mdanalysis_cosine_analysis/1.0.0+galaxy0	".. class:: infomark 
What it does
 The cosine content of the principal components is a good indicator for determining the quality of the sampling and can be used to determine the convergence of the MD simulation. This tool can calculate the cosine content of a user-defined PCA projection. Note: it uses zero-based indexes (i.e. 0 is the first principal component). 
_
 .. class:: infomark 
Input
 - Trajectory file (DCD). - PDB file. 
___ .. class:: infomark 
Output
 - The cosine content value. - Tab-separated file of raw data of the first three principal components."
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_frankenstein_ligand/ctb_frankenstein_ligand/2013.1-0+galaxy0	".. class:: infomark 
What it does
 Converts an SD file containing multiple molecules into a 'Frankenstein ligand' with a 3D shape combining all ligands, which can be used for definition of an active site with rDock. This tool is based on a Perl script originally written by Peter Schmidtke. 
_
 .. class:: infomark 
Input
 - SD file containing multiple molecules. 
___ .. class:: infomark 
Output
 - SD file containing the 'Frankenstein ligand'."
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_makendx/gmx_makendx/2022+galaxy0	".. class:: infomark 
What it does
 Generates GROMACS custom index files. 
_
 .. class:: infomark 
Input
 - A PDB or GRO file. - A selection command for the new index group to be included in the output file. Combine previously defined groups using identifying values with the operators '|' (or) and & (and). For example, '1 | 4' will create a new index group containing all atoms contained within the first and fourth index groups already specified (according to their position in the input file). 
___ .. class:: infomark 
Output
 - A GROMACS index (ndx) file, containing the 'default' index groups and the new group at the end of the file."
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_restraints/gmx_restraints/2022+galaxy0	".. class:: infomark 
What it does
 This tool calculates a position restraint (ITP) file for part of a system, specified in provided GRO and NDX files. 
_
 .. class:: infomark 
Input
 - GRO structure file. - Index (NDX) file. - Force constant vector (in Cartesian space) for applying restraints. E.g. '1000 1000 1000' will apply a force constraint of 1000 kJ/mol nm^2 along each (XYZ) axis. - Index number of the group for which restraints should be calculated, i.e. the position of the group in the index file. 
___ .. class:: infomark 
Output
 - Position restraint (itp) file, which may be useful for system equilibration."
toolshed.g2.bx.psu.edu/repos/chemteam/bio3d_dccm/bio3d_dccm/2.3.4	".. class:: infomark 
What it does
 The extent to which the atomic fluctuations/displacements of a system are correlated with one another can be assessed by examining the magnitude of all pairwise cross-correlation coefficients. The DCCM Analysis tool can plot the atom-wise cross-correlations using the Bio3D package. Negative values (negative correlation) indicate the atoms move in opposite directions, while positive values (positive correlation) indicate the atoms move in the same direction. 
_
 .. class:: infomark 
Input
 - Input file in PDB format - Input file in DCD format 
___ .. class:: infomark 
Output
 - Image (as PNG) of the DCCM plot - Tab-separated file of raw data"
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_rdkit_descriptors/ctb_rdkit_descriptors/2020.03.4+galaxy1	".. class:: infomark 
What this tool does
 | RDKit is an open source toolkit for cheminformatics and machine learning. | This tool calculates various molecular descriptors for the input data. The table below shows a brief overview of the descriptors (or families of related descriptors) which are calculated. | | - Gasteiger/Marsili Partial Charges | - BalabanJ | - BertzCT | - Ipc | - HallKierAlpha | - Kappa1 - Kappa3 | - Chi0, Chi1 | - Chi0n - Chi4n | - Chi0v - Chi4v | - FormalCharge | - MolLogP | - MolMR | - MolWt | - HeavyAtomCount | - HeavyAtomMolWt | - NHOHCount | - NOCount | - NumHAcceptors | - NumHDonors | - NumHeteroatoms | - NumRotatableBonds | - NumValenceElectrons | - RingCount | - SSSR | - TPSA | - LabuteASA | - PEOE_VSA1 - PEOE_VSA14 | - SMR_VSA1 - SMR_VSA10 | - SlogP_VSA1 - SlogP_VSA12 | - EState_VSA1 - EState_VSA11 | - VSA_EState1 - VSA_EState10 | - Topliss fragments | | A full list of the descriptors can be obtained here_. .. 
here: https://rdkit.readthedocs.org/en/latest/GettingStartedInPython.html#list-of-available-descriptors ----- .. class:: warningmark 
Hint
 Use the 
cut columns from a table
 tool to select just the desired descriptors. Alternatively, if you just want to calculate a subset of the descriptors, there is an option available to do so. ----- .. class:: infomark 
Input
 | - 
SDF Format
 | - 
SMILES Format
 | - 
Corina MOL2
 | - PDB | - InChi .. _SDF Format: http://en.wikipedia.org/wiki/Chemical_table_file .. _SMILES Format: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification .. _Corina MOL2: http://www.molecular-networks.com/products/corina ----- .. class:: infomark 
Output
 Tabular file, where each descriptor (value) is shown in a separate column."
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_structure_distance_finder/openbabel_structure_distance_finder/3.1.1+galaxy0	".. class:: infomark 
What it does
 This tool reports distances of ligands to reference points provided as atoms in a PDB file. It is useful for finding out to what extent a ligand occupies an active site. .. class:: infomark 
Input
 An SD-file containing 3D molecules. The points parameter contains the points to compare to as X, Y and Z coordinates, one point per line. An example is: 5.655 1.497 18.223 1.494 -8.367 18.574 13.034 6.306 25.232 This would encode 3 points. Each record in the SDF input is read and the closest heavy atom to each of the points is recorded as properties in the output SDF. .. class:: infomark 
Output
 The same SD file as the input but with a series of ""distance?"" properties added where ? is the index of the points being compared to. In the example there would be properties for distance1, distance2 and distance3."
toolshed.g2.bx.psu.edu/repos/chemteam/mdanalysis_dihedral/mdanalysis_dihedral/1.0.0+galaxy0	".. class:: infomark 
What it does
 This tool calculates and plots dihedral angles (angle between two intersecting planes) between four atoms. 
_
 .. class:: infomark 
Input
 - Trajectory file (DCD). - PDB file. - Segment IDs, residue IDs and names of the four atoms to calculate the dihedral angle. Note that a MDAnalysis 'segment' is a larger organizational unit, for example one protein or all the solvent molecules or simply the whole system. 
___ .. class:: infomark 
Output
 - Tab-separated file of raw data of the dihedral angle calculated for each frame. - Image (as png) of the time series graph."
toolshed.g2.bx.psu.edu/repos/chemteam/mdanalysis_distance/mdanalysis_distance/1.0.0+galaxy0	".. class:: infomark 
What it does
 This tool calculates and plots the distance between pairs of atoms. Two modes are available: single pair mode, where distances are calculated between two specified atoms, and multiple pair mode, where two lists of atoms need to be provided. 
_
 .. class:: infomark 
Input
 - Trajectory file (DCD). - PDB file. In single pair mode, segment IDs, residue IDs and names of two atoms are selected. Note that a MDAnalysis 'segment' is a larger organizational unit, for example one protein or all the solvent molecules or simply the whole system. In multiple pair mode, two files need to be uploaded, each with one or more atom groups defined using the MDAnalysis atom selection, each on a new line. For example: :: resid 163 resid 56 resid 12 and type N All possible distances between the two sets of atom groups will be calculated. For example, if List 1 contains 5 atoms and List 2 contains 8 atoms, the output file will contain 40 columns, each with the distance between one group in List 1 and one group in List 2. Note that in multiple pair mode, if the group has multiple atoms, the center of mass will be used for the calculation. 
___ .. class:: infomark 
Output
 - Tab-separated file of raw data of distance between two atoms calculated for each frame. - Image (as png) of the time series graph."
toolshed.g2.bx.psu.edu/repos/bgruening/qed/ctb_silicos_qed/2021.03.4+galaxy0	".. class:: infomark 
What this tool does
 Estimates the drug-likeness of molecules, based on eight commonly used molecular properties, and reports a score between 0 (all properties unfavourable) to 1 (all properties favourable). Two possible methods to weight the features are available (QED\ :sub:
w,mo
\ , QED\ :sub:
w,max
), as well as an option to leave features unweighted (QED\ :sub:
w,u
). The eight properties used are: molecular weight (MW), octanol–water partition coefficient (ALOGP), number of hydrogen bond donors (HBDs), number of hydrogen bond acceptors (HBAs), molecular polar surface area (PSA), number of rotatable bonds (ROTBs), number of aromatic rings (AROMs) and number of structural alerts (ALERTS). ----- .. class:: warningmark 
Hint
 All invalid, blank and comment lines are skipped when performing computations. The number of skipped lines is displayed in the resulting history item. The method refers to a set of weights that can be applied to the features. These are derived in the 
original paper
 describing QED. - QED\ :sub:
w,max
 using the set of weights that give maximal information content - QED\ :sub:
w,mo
 using the mean weights of the optimal 1,000 weight combinations that give the highest information content - QED\ :sub:
w,u
 with all weights as unity i.e. unweighted. .. _original paper: https://www.nature.com/articles/nchem.1243 ----- .. class:: infomark 
Input
 | - 
SDF format
 | - 
SMILES format
_ .. _SDF format: http://en.wikipedia.org/wiki/Chemical_table_file .. _SMILES format: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 A table listing the values of the eight features, the QED score, the name of the molecule, and the number of Lipinski rules which the molecule obeys. +--------+-------+-----+-----+--------+------+------+--------+-------+----------------+-----+ | MW | ALOGP | HBA | HBD | PSA | ROTB | AROM | ALERTS | QED | NAME | Ro5 | +========+=======+=====+=====+========+======+======+========+=======+================+=====+ | 286.34 | 1.092 | 6 | 3 | 101.88 | 4 | 2 | 1 | 0.737 | Abacavir | 0 | +--------+-------+-----+-----+--------+------+------+--------+-------+----------------+-----+ | 181.21 | 0.481 | 4 | 2 | 83.47 | 5 | 0 | 2 | 0.487 | Acamprosate | 0 | +--------+-------+-----+-----+--------+------+------+--------+-------+----------------+-----+ | 336.43 | 2.365 | 5 | 3 | 87.66 | 11 | 1 | 1 | 0.540 | Acebutolol | 0 | +--------+-------+-----+-----+--------+------+------+--------+-------+----------------+-----+ | 151.16 | 1.351 | 2 | 2 | 49.33 | 2 | 1 | 1 | 0.633 | Acetaminophen | 0 | +--------+-------+-----+-----+--------+------+------+--------+-------+----------------+-----+ | 222.25 | 0.225 | 5 | 2 | 115.04 | 3 | 1 | 1 | 0.727 | Acetazolamide | 0 | +--------+-------+-----+-----+--------+------+------+--------+-------+----------------+-----+ | 324.40 | 3.291 | 4 | 2 | 92.34 | 6 | 1 | 1 | 0.772 | Acetohexamide | 0 | +--------+-------+-----+-----+--------+------+------+--------+-------+----------------+-----+ | 411.57 | 3.492 | 6 | 1 | 47.02 | 7 | 2 | 1 | 0.688 | Acetophenazine | 0 | +--------+-------+-----+-----+--------+------+------+--------+-------+----------------+-----+ | 329.37 | 3.327 | 4 | 1 | 39.72 | 4 | 2 | 0 | 0.917 | Paroxetine | 0 | +--------+-------+-----+-----+--------+------+------+--------+-------+----------------+-----+ | 270.21 | 3.146 | 3 | 1 | 55.13 | 4 | 2 | 0 | 0.915 | Leflunomide | 0 | +--------+-------+-----+-----+--------+------+------+--------+-------+----------------+-----+"
toolshed.g2.bx.psu.edu/repos/chemteam/mdanalysis_endtoend/mdanalysis_endtoend/1.0.0+galaxy0	".. class:: infomark 
What it does
 End-to-End distance timeseries and histogram for proteins. The termini are chosen as ends with the N atom of the N-terminus and the C atom of the C-terminus chosen. 
_
 .. class:: infomark 
Input
 - Trajectory file (DCD). - PDB file. - Segment ID for the protein - Graph series label and title Note that a MDAnalysis 'segment' is a larger organizational unit, for example one protein or all the solvent molecules or simply the whole system. 
___ .. class:: infomark 
Output
 - Tab-separated file of timeseries raw data - Image (as png) of the end-to-end timeseries and histogram"
toolshed.g2.bx.psu.edu/repos/bgruening/enumerate_charges/enumerate_charges/2020.03.4+galaxy0	".. class:: infomark 
What this tool does
 Dimorphite DL provides rules on how to enumerate the different charge forms of molecules using RDKit. Source code can be found here: https://git.durrantlab.pitt.edu/jdurrant/dimorphite_dl ----- .. class:: infomark 
Input
 - 
SMILES Format
_ .. _SMILES Format: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 SMILES format with the enumerated molecules. If an ID was provided in the input this is retained in the output."
toolshed.g2.bx.psu.edu/repos/chemteam/mdanalysis_extract_rmsd/mdanalysis_extract_rmsd/1.0.0+galaxy2	".. class:: infomark 
What it does
 This tool takes collections of MD structures and trajectories and inputs and performs the following steps: - aligns them to a reference structure - calculates RMSD differences for a selected group of atoms between all possible pairs of trajectories at all time points - returns RMSD data as a three-dimensional tensor. Note: in an older version of this tool trajectories were aligned to a reference group prior to RMSD calculation. This is no longer supported; you should perform alignment yourself using a more efficient tool such as 'Modify/convert GROMACS trajectories'. 
_
 .. class:: infomark 
Input
 - Collection of structure files (PDB, GRO). - Collection of trajectory files (DCD, XTC, TRR). - Single structure file for alignment. - User selection of fitting group, alignment group, start and end frames of the trajectory, and a frame step for the calculation. 
___ .. class:: infomark 
Output
 The output consists of a three-dimensional numpy array saved in JSON format, with dimensions N x N x t, where N is the number of trajectories and t is the number of time frames. Thus, the file effectively contains multiple distance matrices (one for each time step) representing the RMSD between all pairs of trajectories for a chosen group of atoms. It may be more useful to flatten the tensor to a two-dimensional matrix by averaging or slicing on the time axis; this can be achieved using the 'Hierarchical clustering' tool."
toolshed.g2.bx.psu.edu/repos/chemteam/biomd_extract_clusters/biomd_extract_clusters/0.1.5.2+galaxy1	".. class:: infomark 
What it does
 This tool takes the hierarchical clustering linkage array (in tabular format) produced by the 'Hierarchical clustering' tool and returns a list of clusters and their members in JSON format. 
_
 .. class:: infomark 
Input
 - Clustering linkage array. - User-selected distance threshold for clustering and minimum number of members a cluster must have to be returned. 
___ .. class:: infomark 
Output
 - JSON file containing a list of clusters and their members."
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_energy/gmx_energy/2022+galaxy1	".. class:: infomark 
What it does
 This tool extracts energy components from an energy (EDR) binary file. A list of terms to select is provided. 
_
 .. class:: infomark 
Input
 - EDR binary file. - Selection of terms to extract. 
___ .. class:: infomark 
Output
 - Tabular or XVG file containing selected terms and some header lines"
toolshed.g2.bx.psu.edu/repos/bgruening/sdf_to_tab/sdf_to_tab/2020.03.4+galaxy0	".. class:: infomark 
What this tool does
 Structure-data (SD-) files may contain values for various properties saved under each of the records. These are indicated using angled brackets, as in the following extract:: > <TORSDO> F 3 > <SCORE> -4.9 > <RMSD_LB> 0.000 > <RMSD_UB> 0.000 This tool extracts properties for all molecules saved within an input SD-file, using RDKit, and saves them to tabular format. ----- .. class:: infomark 
Input
 An SD-file. The properties to be extracted can also be specified, separated with a comma; for example, SCORE,RMSD_LB,RMSD_UB. If the field is left blank, all properties found in the SD-file will be saved, excluding properties with values that contain newline or tab characters, which would disrupt the tabular format. ----- .. class:: infomark 
Output
 Tabular file, where each property is shown in a separate column, with compounds found in the SD-file listed in each row:: Name RMSD_LB RMSD_UB SCORE pose1 0.0 0.0 -4.9 pose2 0.118 2.246 -4.9 pose3 2.96 5.795 -4.9 pose4 2.958 5.379 -4.8 pose5 2.763 5.379 -4.5 pose6 3.106 4.85 -4.4 pose7 2.847 5.816 -4.4 pose8 3.964 5.892 -4.3 pose9 3.971 6.363 -4.3"
toolshed.g2.bx.psu.edu/repos/chemteam/gromacs_extract_topology/gromacs_extract_topology/0+galaxy0	"Tool to extract the [ atomtypes ] and [ moleculetype ] sections from a topology file. Both of these outputs can then be used as inputs in the Add Topologies tool. This tool can be used for a number of purposes, including but not limited to extracting the topology information required to complement the output of ""gmx insert-molecules"". .. class:: infomark 
Input
 The system topology file you are extracting parameters from. .. class:: infomark 
Outputs
 1) Text file containing the atom types and nonbonded parameters. 2) Text file containing the molecule type information with bonded parameters."
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_filter/1.0	".. class:: infomark 
What this tool does
 Filters a library of compounds based on user-defined physico-chemical parameters or predefined options (e.g. Ro5, lead-like properties, etc.). Multiple parameters can be selected for more specific queries. ----- .. class:: warningmark 
Hint
 | If your input file is in SDF format you can use the 
Compute physico-chemical properties
 tool to precalulate the properties and use the filter on that precomputed dataset. It should be faster and can be reused but it's bigger than a SMILES file. | | For exact matches please use the target value for both minimum and maximum parameters (e.g. a selection of exactly 4 rotatable bonds can be performed by selecting 4 as minimum and maximum value). | | Selecting the same property multiple times with different parameters will result in querying the largest overlapping subset of values for the parameter (e.g. a selection of between 0 and 3 rotatable bonds plus a selection between 2 and 4 will result in a query for compounds between 2 and 3 rotatable bonds). ----- .. class:: infomark 
Definition of the pre-defined filtering rules
 
# Lipinski's Rule of Five:
 =< 5 Hydrogen-bond donor groups =< 10 Hydrogen-bond acceptor groups =< 500 Molecular weight =< 5 octanol/water partition coefficient (log P) 
# Lead Like properties
 (Teague, Davis, Leeson, Oprea, Angew Chem Int Ed Engl. 1999 Dec 16;38(24):3743-3748): =< 7 rotatable bonds =< 350 Molecular weight =< 3.5 octanol/water partition coefficient (log P) 
# Drug Like properties
 (Lipinski, J Pharmacol Toxicol Methods. 2000 Jul-Aug;44(1):235-49): =< 10 Hydrogen-bond acceptor groups =< 8 rotatable bonds 150 =< Molecular weight =< 500 =< 150 Polar Surface Area =< 5 octanol/water partition coefficient (log P) 
# Fragment Like properties
 (Carr RA, Congreve M, Murray CW, Rees DC, Drug Discov Today. 2005 Jul 15;10(14):987): =< 5 rotatable bonds =< 250 Molecular weight =< 2.5 octanol/water partition coefficient (log P) ----- .. class:: infomark 
Input
 | - 
SD-Format
 | - 
SMILES Format
 .. 
SD-Format: http://en.wikipedia.org/wiki/Chemical_table_file .. _SMILES Format: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 | SDF formatted coordinates of the molecules, with selected properties stored as meta-data for each compound. | | SMILES, InChI or mol2 formatted files containing the 1D strings or 3D coordinates of each compound. ----- .. class:: infomark 
Cite
 N M O'Boyle, C Morley and G R Hutchison - 
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
 .. 
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
: http://journal.chemistrycentral.com/content/2/1/5 N M O'Boyle, M Banck, C A James, C Morley, T Vandermeersch, and G R Hutchison - 
Open Babel: An open chemical toolbox.
 .. 
Open Babel: An open chemical toolbox.
: http://www.jcheminf.com/content/3/1/33 
Open Babel
 .. _
Open Babel
: http://openbabel.org/wiki/Main_Page"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_filter/openbabel_filter/3.1.1+galaxy0	".. class:: infomark 
What this tool does
 Filters a library of compounds based on user-defined physico-chemical parameters or predefined options (e.g. Ro5, lead-like properties, etc.). Multiple parameters can be selected for more specific queries. ----- .. class:: warningmark 
Hint
 | If your input file is in SDF format you can use the 
Compute physico-chemical properties
 tool to precalulate the properties and use the filter on that precomputed dataset. This should be faster and the file can be reused, although it is larger than a SMILES file. | | For exact matches use the target value for both minimum and maximum parameters (e.g. a selection of exactly 4 rotatable bonds can be performed by selecting 4 as minimum and maximum value). | | Selecting the same property multiple times with different parameters will result in querying the largest overlapping subset of values for the parameter (e.g. a selection of between 0 and 3 rotatable bonds plus a selection between 2 and 4 will result in a query for compounds between 2 and 3 rotatable bonds). ----- .. class:: infomark 
Definition of the pre-defined filtering rules
 
# Lipinski's Rule of Five:
 =< 5 Hydrogen-bond donor groups =< 10 Hydrogen-bond acceptor groups =< 500 Molecular weight =< 5 octanol/water partition coefficient (log P) 
# Lead Like properties
 (Teague, Davis, Leeson, Oprea, Angew Chem Int Ed Engl. 1999 Dec 16;38(24):3743-3748): =< 7 rotatable bonds =< 350 Molecular weight =< 3.5 octanol/water partition coefficient (log P) 
# Drug Like properties
 (Lipinski, J Pharmacol Toxicol Methods. 2000 Jul-Aug;44(1):235-49): =< 10 Hydrogen-bond acceptor groups =< 8 rotatable bonds 150 =< Molecular weight =< 500 =< 150 Polar Surface Area =< 5 octanol/water partition coefficient (log P) 
# Fragment Like properties
 (Carr RA, Congreve M, Murray CW, Rees DC, Drug Discov Today. 2005 Jul 15;10(14):987): =< 5 rotatable bonds =< 250 Molecular weight =< 2.5 octanol/water partition coefficient (log P) ----- .. class:: infomark 
Input
 | - 
SD-Format
 | - 
SMILES Format
 .. _SD-Format: http://en.wikipedia.org/wiki/Chemical_table_file .. _SMILES Format: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 | SDF formatted coordinates of the molecules, with selected properties stored as meta-data for each compound. | | SMILES, InChI or mol2 formatted files containing the 1D strings or 3D coordinates of each compound."
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_rmsd/gmx_rmsd/2022+galaxy0	".. class:: infomark 
What it does
 This tool calculates a structure's Root Mean Square Deviation (RMSD), using GROMACS. A second trajectory can also be used when trying to compare the RMSDs of two simulations of the same system. In both the single and dual trajectory functionality, the same reference groups are used and taken from the structure file (tpr, pdb, or gro). 
_
 .. class:: infomark 
Input
 - GRO, PDB, or TPR structure file. - TRR or XTC trajectory file. 
___ .. class:: infomark 
Output
 - XVG file containing RMSD results."
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_rmsf/gmx_rmsf/2022+galaxy0	".. class:: infomark 
What it does
 This tool calculates a structure's Root Mean Square Fluctuation (RMSF), using GROMACS. 
_
 .. class:: infomark 
Input
 - GRO, PDB, or TPR structure file. - TRR or XTC trajectory file. 
___ .. class:: infomark 
Output
 - XVG file containing RMSF results."
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_rg/gmx_rg/2022+galaxy0	".. class:: infomark 
What it does
 This tool computes the radius of gyration of a molecular structure, which provides an idea of how compact versus how open or extended the molecule's most commonly adopted conformation is, and how it changes throughout a trajectory. It can often be determined experimentally, so computing this from a simulation and comparing the results can also allow the researcher to verify the accuracy of the chosen force field for that particular system. 
_
 .. class:: infomark 
Input
 - TPR file of the simulated system. - XTC or TRR trajectory file from a prior simulation. - Index file generated using the 'Create GROMACS index files' tool (make_ndx). 
___ .. class:: infomark 
Output
 - XVG file containing the radius of gyration at every frame of the trajectory."
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_get_builtin_file/gmx_get_builtin_file/2022+galaxy0	".. class:: infomark 
What it does
 Loads a built-in GROMACS file into your history. 
_
 .. class:: infomark 
Input
 - Filename. 
___ .. class:: infomark 
Output
 - GROMACS file."
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_em/gmx_em/2022+galaxy0	".. class:: infomark 
What it does
 This tool performs energy minimization of a system prior to a GROMACS simulation. 
_
 .. class:: infomark 
Input
 - GRO structure file. - Topology (TOP) file. - MDP parameter file (optional) To take advantage of all GROMACS features, upload an MDP file with simulation parameters. Otherwise, choose parameters through the Galaxy interface. See the 
manual
 for more information on the options. .. 
manual
: http://manual.gromacs.org/documentation/2018/user-guide/mdp-options.html 
___ .. class:: infomark 
Output
 - GRO structure file."
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_setup/gmx_setup/2022+galaxy0	".. class:: infomark 
What it does
 This tool performs the initial setup prior to a GROMACS simulation. This entails producing a topology from an input structure using the pdb2gmx command. Please note that the tool will only successfully generate a topology for residues it recognizes (i.e. standard amino acids). If the structure contains other components such as ligands, these should be separately parameterized with the AmberTools or ACPYPE tools. In this case, make sure to also use an AMBER forcefield for this tool to ensure compatability with the ligand topology. 
_
 .. class:: infomark 
Input
 - PDB file. - Water model and forcefield must be specified. 
___ .. class:: infomark 
Output
 - GROMACS topology (TOP) file. - Position restraint (itp) file, which may be useful for system equilibration. - GRO structure file."
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_md/gmx_md/2019.1	Upload GRO and TOP files for the production simulation, as well as the checkpoint (CPT) file from the NPT equilibration. To take advantage of all GROMACS features, upload an MDP file with simulation parameters. Otherwise, choose parameters through the Galaxy interface. See http://manual.gromacs.org/documentation/2018/user-guide/mdp-options.html for more information on the options.
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_sim/gmx_sim/2022+galaxy0	".. class:: infomark 
What it does
 This tool performs a molecular dynamics simulation with GROMACS. 
_
 .. class:: infomark 
Input
 - GRO structure file. - Topology (TOP) file. A variety of other options can also be specified: - MDP parameter file to take advantage of all GROMACS features. Otherwise, choose parameters through the Galaxy interface. See the 
manual
 for more information on the options. - Accepting and producing checkpoint (CPT) input/output files, which allows sequential MD simulations, e.g. when performing NVT and NPT equilibration followed by a production simulation. - Position restraint (ITP) file, useful for equilibrating solvent around a protein. - Choice of ensemble: NVT or NPT. - Whether to return trajectory (XTC or TRR) and/or structure (GRO or PDB) files. .. 
manual
: http://manual.gromacs.org/documentation/2018/user-guide/mdp-options.html 
___ .. class:: infomark 
Output
 - Structure and/or trajectory files as specified in the input."
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_solvate/gmx_solvate/2022+galaxy0	".. class:: infomark 
What it does
 This tool solvates a system prior to a GROMACS simulation. In addition, sodium or chloride ions can be added if necessary to ensure the system is charge-neutral. 
_
 .. class:: infomark 
Input
 - GRO structure file. - Topology (TOP) file. In addition, a water model must be selected - this should be consistent with the one selected previously in the system setup. 
___ .. class:: infomark 
Output
 - GRO structure file."
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_editconf/gmx_editconf/2022+galaxy0	".. class:: infomark 
What it does
 This tool performs the initial setup prior to a GROMACS simulation. This entails producing a topology from an input structure using the pdb2gmx command and also defining a simulation box with editconf. Please note that the tool will only successfully generate a topology for residues it recognizes (i.e. standard amino acids). If the structure contains other components such as ligands, these should be separately parameterized with the AmberTools or ACPYPE tools. In this case, make sure to also use an AMBER forcefield for this tool to ensure compatability with the ligand topology. 
_
 .. class:: infomark 
Input
 - PDB file. - Water model and forcefield must be specified. - Parameters for the simulation box (dimensions and shape). 
___ .. class:: infomark 
Output
 - GROMACS topology (TOP) file. - Position restraint (itp) file, which may be useful for system equilibration. - GRO structure file."
toolshed.g2.bx.psu.edu/repos/chemteam/ambertools_acpype/ambertools_acpype/21.10+galaxy0	"Tool to produce GROMACS topologies for small molecules using the acpype interface to AmberTools. .. class:: infomark 
Input
 Either a mol2 file (more appropriate for small structures) or a pdb file. If you want to parameterize a large macromolecule (which is more likely to be stored in PDB format, e.g. a protein) consider using a tool such as 'GROMACS initial setup' instead. .. class:: infomark 
Outputs
 GROMACS topology for the ligand, in itp format. Optional: structure file, in gro format."
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_im_conformers/ctb_im_conformers/1.1.4+galaxy0	".. class:: infomark 
What this tool does
 This tool generates conformers for a set of input molecules, using the chemistry toolkit RDKit. ----- .. class:: infomark 
Input
 | - Molecules in 
SDF format
_ | - A number of other parameters can be set; the most important include the number of conformers to generate and the minimum RMSD difference between them. .. _SDF format: http://en.wikipedia.org/wiki/Chemical_table_file ----- .. class:: infomark 
Output
 SD-file containing generated conformers."
toolshed.g2.bx.psu.edu/repos/bgruening/get_online_data/get_pdb/0.1.0	Download a protein structure in PDB format from the Protein Data Bank using its four-letter accession code.
toolshed.g2.bx.psu.edu/repos/bgruening/get_pdb/get_pdb/0.1.0	Download a protein structure in PDB format from the Protein Data Bank using its four-letter accession code.
toolshed.g2.bx.psu.edu/repos/chemteam/biomd_rmsd_clustering/biomd_rmsd_clustering/0.1.5.2+galaxy1	".. class:: infomark 
What it does
 This tool takes the three-dimensional tensor file (in JSON format) produced by the 'Extract RMSD distance matrix data' tool and flattens it along the time axix to give a two-dimensional distance matrix. Optionally, it also plots the distance matrix as a heatmap with matplotlib, performs hierarchical clustering with scipy, and plots the corresponding dendrogram. 
_
 .. class:: infomark 
Input
 - Three-dimensional tensor (JSON). - User selection of desired outputs, clustering method and other parameters 
___ .. class:: infomark 
Output
 - Tabular file containing a two-dimensional N x N distance matrix, where N is the number of MD trajectories - Optional: a heatmap representing the distance matrix. - Optional: a tabular file containing the cluster linkage array produced by hierarchical clustering of the distance matrix - Optional: A dendrogram representing the hierarchical clustering."
toolshed.g2.bx.psu.edu/repos/chemteam/mdanalysis_hbonds/mdanalysis_hbonds/1.0.0+galaxy0	".. class:: infomark 
What it does
 This tool calculates hydrogen bonds between two segments of the system. Note that a MDAnalysis 'segment' is a larger organizational unit, for example one protein or all the solvent molecules or simply the whole system. 
_
 .. class:: infomark 
Input
 - Trajectory file (DCD). - PDB file. - Segment IDs of the two segments. - Cutoff distance and angle. 
___ .. class:: infomark 
Output
 - Tabular files containing H-bond frequency, number of H-bonds Per time step, and time steps for each observed H-bond."
toolshed.g2.bx.psu.edu/repos/chemteam/vmd_hbonds/vmd_hbonds/1.9.3	".. class:: infomark 
What it does
 This tool calculates hydrogen bonds between two segments of a system in a molecular dynamics trajectory. 
_
 .. class:: infomark 
Input
 - Trajectory file (DCD/XTC). - PDB or GRO file. - Atom selection commands for two regions to be compared - e.g. 'not water' or 'segid PRO'. See here_ for more information on the VMD atom selection language. .. _here: https://www.ks.uiuc.edu/Research/vmd/vmd-1.3/ug/node132.html 
___ .. class:: infomark 
Output
 - Number of H-bonds for each time step and and their occupancy."
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_machine_learning/ctb_mds_plot/0.1	"Note
. You need an NxN similarity matrix as input. Use the NxN Clustering tool to generate one. 
What it does
 Scatter plot of similarity matrix after embeding in 2D coordinates using Multidimensional Scaling (MDS). ----- 
Example
 * input:: Similarity Matrix * output:: Scatter plot .. image:: $PATH_TO_IMAGES/mds_plot.png"
toolshed.g2.bx.psu.edu/repos/chemteam/md_converter/md_converter/1.9.6+galaxy0	"What it does
 This tool interconverts between MD trajectory file formats: xtc, trr, dcd and netcdf. 
_
 .. class:: infomark 
Input
 - Trajectory file (trr, xtc, dcd, netcdf) 
___ .. class:: infomark 
Output
 - Trajectory file (trr, xtc, dcd, netcdf)"
toolshed.g2.bx.psu.edu/repos/chemteam/mmpbsa_mmgbsa/mmpbsa_mmgbsa/21.10+galaxy0	".. class:: infomark 
What it does
 This tool calculates the Molecular Mechanics Poisson-Boltzman Surface Area (MMPBSA), which is an estimate of the binding free energy between a ligand and a receptor. It also can calculate the MMGBSA, which is a common alternative, replacing the Poisson-Boltzmann term with the General Born approximation. .. class:: infomark 
How it works
 Prior to using this tool, simulations of the ligand complexed with the receptor must be run. This tool, which wraps AmberTools, requires a prmtop (Amber parameter topology) file for the receptor, ligand and the complex and the trajectory in netCDF format. - Single Trajectory Estimate: A single simulation of the complex in water is run. The trajectory of this complex is used to estimate the MMPBSA or MMGBSA, depending on the options chosen. - Multiple Trajectory Estimate: Separate simulations of the complex in water, the receptor in water and the ligand in water are run. This is useful if the ligand (or receptor) is expected to have a significantly different conformation in solution compared to in the complex, but is otherwise not recommended as it increases the uncertainty of the results. The trajectories are used to estimate the MMPBSA or MMGBSA depending on the options chosen. If simulations were performed using the Galaxy GROMACS tools, the topology (in top format) and trajectories (in xtc format) can be converted to Amber prmtop and netcdf formats using the Convert Parameters and MDTraj file converter tools respectively. .. class:: infomark 
Outputs created
 - The statistics file which includes all information about the frames analysed and average energies. The DELTA G binding is estimated. If negative this is a favourable binding. Note that by default the entropy contribution to binding (unfavourable) is not calculated. A normal mode analysis is needed. - The parameter file contains the input parameters passed from Galaxy to MMPBSA.py in the expected MMPBSA input format. - (Optional, if decomposition analysis is performed) The decomposition file contains a breakdown of each residue's contribution to the energy. For example, using the default energy decomposition scheme (1) the interaction of each residue with the rest of the system is calculated and listed. - (Optional, if the ""Keep additional files?"" option is chosen) A collection containing all temporary files generated in the course of the calculation. This may be useful for debugging. .. class:: infomark 
User guide and documentation
 - The 
AmberTools Manual
 - The 
Amber Tutorial
 on using MMPBSA.py - There are many more complex flags available. This Galaxy wrapper only supports GB and PB binding free energies and decomposition. Parallel calculations are not supported at present. .. 
Amber Tutorial
: http://ambermd.org/tutorials/advanced/tutorial3/py_script/index.htm .. 
AmberTools Manual
: https://ambermd.org/doc12/Amber18.pdf"
toolshed.g2.bx.psu.edu/repos/bgruening/sucos_max_score/sucos_max_score/2020.03.4+galaxy0	".. class:: infomark 
What it does
 This tool determines the maximum SuCOS score of ligands, presumed to be potential follow on compounds, compared to a set of clustered reference compounds, presumed to be fragment screening hits. Each ligand to be scored is compared to all of the reference compounds with the highest score being recorded, along with the cluster it came from and the index of the molecule within that cluster. The original SuCOS code is on GitHub_ under a MIT license. The SuCOS work is described here_. .. _GitHub: https://github.com/susanhleung/SuCOS .. _here: https://chemrxiv.org/articles/SuCOS_is_Better_than_RMSD_for_Evaluating_Fragment_Elaboration_and_Docking_Poses/8100203 .. class:: infomark 
Input
 The clustered reference compounds are likely to have been generated using the ""Cluster ligands using SuCOS"" tool and will comprise a SDF format file for each cluster. The ligands to be scored are supplied in a SDF file. Optional filtering of the output is possible, for instance to retain only records with SuCOS scores greater that a certain value. Use the optional 'Filter field' and 'Filter value' parameters. The 'Filter field' would typically be one of the properties listed below. .. class:: infomark 
Output
 The same SD file as the input ligands with the following properties added: * Max_SuCOS_Score - the best (maximum) SuCOS score * Max_SuCOS_FeatureMap_Score - the corresponding FeatureMap score * Max_SuCOS_Protrude_Score - the corresponding Protrude score * Max_SuCOS_Cluster - the file name of the cluster that contained the max score * Max_SuCOS_Index - the index of the cluster that contained the max score (first record is index 1) * Cum_SuCOS_Score - the cumulative SuCOS score for all overlays (the sum of the individual scores) * Cum_SuCOS_FeatureMap_Score - the corresponding FeatureMap score * Cum_SuCOS_Protrude_Score - the corresponding Protrude score"
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_merge_topology_files/gmx_merge_topology_files/3.4.3+galaxy0	".. class:: infomark 
What it does
 This tool merges GROMACS topologies and structure (GRO) files, calculated separately for a protein and ligand, into combined topology and GRO files for the resulting complex. The tool will work best if used with the outputs of the 'acpype' and 'GROMACS initial setup' tools. If the input files are formatted unusually or incorrectly, it will probably fail. 
_
 .. class:: infomark 
Input
 - TOP file for the protein topology - A TOP or ITP file for the ligand topology - GRO file for the protein structure - GRO file for the ligand structure 
___ .. class:: infomark 
Output
 - TOP file for the protein-ligand complex. - GRO file for the protein-ligand complex."
toolshed.g2.bx.psu.edu/repos/chemteam/gmx_trj/gmx_trj/2022+galaxy1	".. class:: infomark 
What it does
 This tool allows manipulation of GROMACS trajectories, drawing on the trjcat and trjconv commands. 
_
 .. class:: infomark 
Input
 - One or more trajectory files (XTC or TRR) - Structure file (optional) - Various options can be set 
___ .. class:: infomark 
Output
 - GROMACS trajectory or structure file (XTC, TRR, PDB, GRO)"
toolshed.g2.bx.psu.edu/repos/bgruening/osra/ctb_osra/2.1.0	".. class:: infomark 
What this tool does
 OSRA_ (Optical Structure Recognition Application) is a utility designed to convert graphical representations of chemical structures, contained within a PDF file, into a SMILES or SDF representation. .. 
OSRA: http://cactus.nci.nih.gov/osra/ .. 
Optical Structure Recognition Software To Recover Chemical Information: OSRA, An Open Source Solution
: http://pubs.acs.org/doi/abs/10.1021/ci800067r"
toolshed.g2.bx.psu.edu/repos/bgruening/chemfp/ctb_chemfp_mol2fps/1.6.1+galaxy0	".. class:: infomark 
What this tool does
 This tool uses chemfp to calculate molecular fingerprints, supporting a number of common file formats. Chemfp uses OpenBabel, OpenEye and RDKit. For more information check the websites listed below:: - http://www.rdkit.org/docs/GettingStartedInPython.html#fingerprinting-and-molecular-similarity - http://openbabel.org/wiki/Tutorial:Fingerprints ----- .. class:: infomark 
Input
 FPS fingerprint file format * Example:: - SD-File 28434379 -OEChem-02031205132D 37 39 0 0 0 0 0 0 0999 V2000 8.1648 -1.8842 0.0000 O 0 0 0 0 0 0 0 0 0 0 0 0 6.0812 -0.2134 0.0000 N 0 0 0 0 0 0 0 0 0 0 0 0 6.0812 -1.8229 0.0000 N 0 0 0 0 0 0 0 0 0 0 0 0 2.5369 -2.0182 0.0000 N 0 0 0 0 0 0 0 0 0 0 0 0 6.3919 0.7371 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 7.3704 0.9433 0.0000 C 0 0 0 0 ...... 1 15 1 0 0 0 0 1 35 1 0 0 0 0 2 5 1 0 0 0 0 2 11 1 0 0 0 0 2 12 1 0 0 0 0 3 12 2 0 0 0 0 3 13 1 0 0 0 0 4 18 1 0 0 0 0 ...... >PUBCHEM_COMPOUND_CID< 28434379 > <PUBCHEM_COMPOUND_CANONICALIZED> 1 > <PUBCHEM_CACTVS_COMPLEXITY> 280 > <PUBCHEM_CACTVS_HBOND_ACCEPTOR> 2 > <PUBCHEM_CACTVS_HBOND_DONOR> 2 > <PUBCHEM_CACTVS_ROTATABLE_BOND> 2 > <PUBCHEM_CACTVS_SUBSKEYS> AAADceBzIAAAAAAAAAAAAAAAAAAAAWAAAAAwYAAAAAAAAFgB8AAAHgAQCAAACCjhlwYx0LdMEgCgASZiZASCgC0hEqAJ2CA4dJiKeKLA2dGUJAhokALYyCcQAAAAAACAAAQAACAAAQAACAAAQAAAAAAAAA== > - type : FP2 ----- .. class:: infomark 
Output
 * Example:: #FPS1 #num_bits=1021 #type=OpenBabel-FP2/1 #software=OpenBabel/2.3.0 #source=/tmp/dataset_409.dat.sdf #date=2012-02-03T11:13:39 c0000000000008c0000846000400000000000010800000000000004000000000100010000700802170000018000000c 0010000000020600208008000008000000c000c02c00002000000c00000100000008001400c800001c0180000000300 10000000000080000000c0000060000c0000060810000010000000800102000000 28434379"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_multi_obgrep/0.1	".. class:: infomark 
What this tool does
 Uses the Open Babel Obgrep_ to search for molecules inside multi-molecule files (e.g. SMI, SDF, etc.) or across multiple files. It is known that not all SMARTS features from the Daylight Toolkit are supported, please have a look here_. .. 
Obgrep: http://openbabel.org/wiki/Obgrep .. _here: http://openbabel.org/wiki/SMARTS ----- .. class:: infomark 
Input
 | - 
SD-Format
 | - 
SMILES Format
 .. _SD-Format: http://en.wikipedia.org/wiki/Chemical_table_file .. _SMILES Format: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 Same as input format. ----- .. class:: infomark 
Cite
 N M O'Boyle, M Banck, C A James, C Morley, T Vandermeersch, and G R Hutchison - 
Open Babel: An open chemical toolbox.
 .. 
Open Babel: An open chemical toolbox.
: http://www.jcheminf.com/content/3/1/33 
Open Babel
 .. _
Open Babel
: http://openbabel.org/wiki/Main_Page"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_multi_obgrep/openbabel_multi_obgrep/2.4.1.0	".. class:: infomark 
What this tool does
 Uses the Open Babel Obgrep_ to search for molecules inside multi-molecule files (e.g. SMI, SDF, etc.) or across multiple files. It is known that not all SMARTS features from the Daylight Toolkit are supported, please have a look here_. .. 
Obgrep: http://openbabel.org/wiki/Obgrep .. _here: http://openbabel.org/wiki/SMARTS ----- .. class:: infomark 
Input
 | - 
SD-Format
 | - 
SMILES Format
_ .. _SD-Format: http://en.wikipedia.org/wiki/Chemical_table_file .. _SMILES Format: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 Same as input format."
toolshed.g2.bx.psu.edu/repos/bgruening/natural_product_likeness/ctb_np-likeness-calculator/2.1	"What this tool does
 The 
Natural-Product-Likeness Scorer
_ calculates the Natural Product(NP)-likeness of a molecule, i.e. the similarity of the molecule to the structure space covered by known natural products. The more positive the score, the higher the NP-likeness and vice versa. .. _Natural-Product-Likeness Scorer: http://sourceforge.net/projects/np-likeness/ .. image:: $PATH_TO_IMAGES/score_distribution.png"
toolshed.g2.bx.psu.edu/repos/bgruening/chemfp/ctb_chemfp_nxn_clustering/1.6.1+0	".. class:: infomark 
What this tool does
 Based on a set of fingerprints, generates a square self-similarity (NxN) matrix, as well as a dendrogram visualizing the clusters derived from it using hierarchical clustering. For the clustering and the fingerprint handling the chemfp_ project is used. .. _chemfp: http://chemfp.com/ ----- .. class:: warningmark 
Hint
 The plotting of the dendrogram is sensible only with a small dataset - if more than around 20 fingerprints are used the plot will be unreadable. ----- .. class:: infomark 
Input
 Molecular fingerprints in FPS format. Open Babel Fastsearch index is not supported. * Example:: - fingerprints in FPS format #FPS1 #num_bits=881 #type=CACTVS-E_SCREEN/1.0 extended=2 #software=CACTVS/unknown #source=/home/mohammed/galaxy-central/database/files/000/dataset_423.dat #date=2012-02-09T13:20:37 07ce04000000000000000000000000000080060000000c000000000000001a800f0000780008100000701487e960cc0bed3248000580644626004101b4844805901b041c2e 19511e45039b8b2926101609401b13e40800000000000100200000040080000010000002000000000000 55169009 07ce04000000000000000000000000000080060000000c000000000000001a800f0000780008100000701087e960cc0bed3248000580644626004101b4844805901b041c2e 19111e45039b8b2926105609401313e40800000000000100200000040080000010000002000000000000 55079807 ........ - Tanimoto threshold : 0.8 (between 0 and 1) ----- .. class:: informark 
Output
 * Example:: .. image:: $PATH_TO_IMAGES/NxN_clustering.png"
toolshed.g2.bx.psu.edu/repos/bgruening/chemical_data_sources/ctb_online_data_fetch/0.2	".. class:: infomark 
What this tool does
 Fetch data via FTP or HTTP and store them in your history. ----- .. class:: infomark 
Input
 Supported filetypes are: - gz/gzip - rar ZIP is supported with recursive extracting of specific filetypes."
toolshed.g2.bx.psu.edu/repos/bgruening/get_online_data/ctb_online_data_fetch/0.4	".. class:: infomark 
What this tool does
 Fetch data via FTP or HTTP and store them in your history. Supply one or more URLs; all files with the chosen file extensions will be extracted. Caution: all files are concatenated together. ----- .. class:: infomark 
Input
 Supported filetypes are: - gz/gzip - ZIP (with recursive extracting of specific filetypes)"
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_im_o3dalign/ctb_im_o3dalign/1.1.4+galaxy0	".. class:: infomark 
What this tool does
 Aligns molecules using RDKit's Open 3D Align. .. class:: infomark 
Input
 | - Input file in 
SDF Format
_ .. _SDF Format: http://en.wikipedia.org/wiki/Chemical_table_file .. _SMILES: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 SD-file of aligned compounds."
toolshed.g2.bx.psu.edu/repos/bgruening/openmg/ctb_openmg/0.1+galaxy0	".. class:: infomark 
What this tool does
 Takes an molecular formula (such as C6H6, C2H5NO2) and returns all possible structures with this elemental composition."
toolshed.g2.bx.psu.edu/repos/bgruening/openduck_chunk/openduck_chunk/0.1.1	".. class:: infomark 
What it does
 Produce chunk automatically for dynamic undocking (DUck). Chunking entails selecting a region of the protein which is used for a DUck simulation. This Galaxy tool uses OpenDUck, an open-source implementation of the original DUck method, using OpenMM and AmberTools. 
_
 .. class:: infomark 
Input
 - PDB file for protein - SDF/MOL file for ligand - Parameters defining the protein-ligand interaction 
___ .. class:: infomark 
Output
 - PDB file containing the chunked protein ready for system preparation. A tar file is also produced as a optional output, containing all files produced by the tool."
toolshed.g2.bx.psu.edu/repos/galaxyp/openms_openpepxl/OpenPepXL/3.1+galaxy0	Tool for protein-protein cross-linking identification using labeled linkers. For more information, visit https://openms.de/doxygen/release/3.1.0/html/TOPP_OpenPepXL.html
toolshed.g2.bx.psu.edu/repos/chemteam/packmol/packmol/18.169.1	".. class:: infomark 
What it does
 Packmol creates initial MD configurations from input molecules - for example, creating a water and urea mixture. .. class:: infomark 
How it works
 - Select a molecule file type e.g. pdb, xyz - Select a single molecule from your history (can select multiple) - For each molecule: - choose how many molecules to create (number variable) - optionally choose: - molecular radius - how to number the molecules (resnumber variable) - choose one or more placement constraint(s). Each constraint has different parameters: - for example, specify placement inside a cube of size 40 Angstroms placed at the origin - all units are in Angstroms for distances and degrees for angles .. class:: infomark 
Outputs created
 - A pdb file; view this in Galaxy by clicking on the 'visualize' icon and choosing a viewer. The NGLViewer works well for small molecules. - A packmol input script (for debugging and repeatability) .. class:: infomark 
User guide and documentation
 - Packmol 
userguide
 - Calculating the number of molecules using the 
volume guesser
 .. class:: infomark 
Known issues
 - fixed constraint only allows 1 atom but this is not set for the user. The user has to set this parameter. The job may fail. .. 
userguide
: http://m3g.iqm.unicamp.br/packmol/userguide.shtml#more .. 
volume guesser
: http://m3g.iqm.unicamp.br/packmol/utilities.shtml"
toolshed.g2.bx.psu.edu/repos/chemteam/bio3d_pca/bio3d_pca/2.3.4	".. class:: infomark 
What it does
 Principal component analysis (PCA) can be used to determine the relationship between statistically meaningful conformations (major global motions) sampled during the trajectory. 
_
 .. class:: infomark 
Input
 - Input file in PDB format - Input file in DCD format 
___ .. class:: infomark 
Output
 - Image (as PNG) of the PCA plot - Image (as PNG) of the PCA clustered plot - Image (as PNG) of the first principal component plotted on RMSF - Tab-separated file of raw data"
toolshed.g2.bx.psu.edu/repos/chemteam/bio3d_pca_visualize/bio3d_pca_visualize/2.3.4	".. class:: infomark 
What it does
 This tool can generate small trajectories of a given principal component. 
_
 .. class:: infomark 
Input
 - Input file in PDB format - Input file in DCD format 
___ .. class:: infomark 
Output
 - A short trajectory (as PDB) of the given principal component ID."
toolshed.g2.bx.psu.edu/repos/chemteam/pulchra/pulchra/3.06+galaxy0	".. class:: infomark 
What it does
 This tool rebuilds a protein structure from reduced representations (alpha carbon trace), producing an all-heavy atom PDB file. 
_
 .. class:: infomark 
Input
 - Alpha carbon trace PDB file 
___ .. class:: infomark 
Output
 - Rebuilt all-heavy atom PDB file"
toolshed.g2.bx.psu.edu/repos/bgruening/padel/padel/2.21	"The PaDEL descriptor tool calculates different kinds of molecular descriptors and fingerprints, using the Chemistry Development Kit (CDK). Descriptors include atom type, electrotopological state descriptors, Crippen's logP and MR, extended topochemical atom (ETA) descriptors, McGowan volume, molecular linear free energy relation descriptors, ring counts, count of chemical substructures identified by Laggner, binary fingerprints and count of chemical substructures. For more details, please consult the cited publication. ----- .. class:: infomark 
Input
 - SDF Format: http://en.wikipedia.org/wiki/Chemical_table_file - SMILES Format: http://www.molecular-networks.com/products/corina ----- .. class:: infomark 
Output
 - Tabular file, where each descriptor (value) is shown in a separate column."
toolshed.g2.bx.psu.edu/repos/chemteam/ambertools_parmchk2/ambertools_parmchk2/21.10+galaxy0	".. class:: infomark 
What it does
 Antechamber parameter check reads in a mol2 file and writes out a force field modification (frcmod) file containing any force field parameters that are needed for the molecule but not supplied by the force field file. Problematic parameters, if any, are indicated in the frcmod file with the note, “ATTN, need revision”, and are typically given values of zero. .. class:: infomark 
How it works
 - Select a mol input file (mol2) - Choose GAFF or GAFF2 parameter set .. class:: infomark 
Outputs created
 - Outputs a frcmod file as text .. class:: infomark 
User guide and documentation
 - AmberTools 
userguide
 .. 
userguide
: http://ambermd.org/doc12/Amber19.pdf .. class:: infomark 
Feature requests
 Go to Galaxy Computational Chemistry and make a 
feature request
 .. 
feature request
: https://github.com/galaxycomputationalchemistry/galaxy-tools-compchem/issues/new"
toolshed.g2.bx.psu.edu/repos/bgruening/align_it/ctb_alignit_create_db/0.1	".. class:: infomark 
What this tool does
 Align-it_ is a tool to align molecules according to their pharmacophores. A pharmacophore is an abstract concept based on the specific interactions observed in drug-receptor interactions: hydrogen bonding, charge transfer, electrostatic and hydrophobic interactions. Each pharmacophore contains multiple lines, representing so-called pharmacophore points, each of which corresponds to a functional group of the molecule. Molecular modeling and/or screening based on pharmacophore similarities has been proven to be an important and useful method in drug discovery. The functionality of Align-it_ consists mainly of two parts. The first functionality is the generation of pharmacophores from molecules (the function of this tool). Secondly, pairs of pharmacophores can be aligned (use the tool 
Pharmacophore Alignment
). The resulting score is calculated from the volume overlap resulting of the alignments. .. 
Align-it: http://silicos-it.be.s3-website-eu-west-1.amazonaws.com/software/align-it/1.0.4/align-it.html ----- .. class:: infomark 
Input
 Upload a file containing molecular representations in MOL, MOL2, SMILES or SDF format. * Example (SDF):: - database 30 31 0 0 0 0 0 0 0999 V2000 1.9541 1.1500 -2.5078 Cl 0 0 0 0 0 0 0 0 0 0 0 0 1.1377 -1.6392 2.1136 Cl 0 0 0 0 0 0 0 0 0 0 0 0 -3.2620 -2.9284 -1.0647 O 0 0 0 0 0 0 0 0 0 0 0 0 -2.7906 -1.9108 0.9092 O 0 0 0 0 0 0 0 0 0 0 0 0 0.2679 -0.2051 -0.3990 N 0 0 0 0 0 0 0 0 0 0 0 0 -2.0640 0.5139 -0.3769 C 0 0 0 0 0 0 0 0 0 0 0 0 -0.7313 0.7178 -0.0192 C 0 0 0 0 0 0 0 0 0 0 0 0 -2.4761 -0.6830 -1.1703 C 0 0 0 0 0 0 0 0 0 0 0 0 1.6571 -0.2482 -0.1795 C 0 0 0 0 0 0 0 0 0 0 0 0 -3.0382 1.4350 0.0081 C 0 0 0 0 0 0 0 0 0 0 0 0 -0.3728 1.8429 0.7234 C 0 0 0 0 0 0 0 0 0 0 0 0 -2.6797 2.5600 0.7506 C 0 0 0 0 0 0 0 0 0 0 0 0 -1.3470 2.7640 1.1083 C 0 0 0 0 0 0 0 0 0 0 0 0 2.5353 0.3477 -1.0918 C 0 0 0 0 0 0 0 0 0 0 0 0 2.1740 -0.8865 0.9534 C 0 0 0 0 0 0 0 0 0 0 0 0 -2.8480 -1.8749 -0.3123 C 0 0 0 0 0 0 0 0 0 0 0 0 3.9124 0.3058 -0.8739 C 0 0 0 0 0 0 0 0 0 0 0 0 3.5511 -0.9285 1.1713 C 0 0 0 0 0 0 0 0 0 0 0 0 4.4203 -0.3324 0.2576 C 0 0 0 0 0 0 0 0 0 0 0 0 -1.7086 -0.9792 -1.8930 H 0 0 0 0 0 0 0 0 0 0 0 0 -3.3614 -0.4266 -1.7676 H 0 0 0 0 0 0 0 0 0 0 0 0 -0.0861 -1.1146 -0.6780 H 0 0 0 0 0 0 0 0 0 0 0 0 -4.0812 1.2885 -0.2604 H 0 0 0 0 0 0 0 0 0 0 0 0 0.6569 2.0278 1.0167 H 0 0 0 0 0 0 0 0 0 0 0 0 -3.4382 3.2769 1.0511 H 0 0 0 0 0 0 0 0 0 0 0 0 -1.0683 3.6399 1.6868 H 0 0 0 0 0 0 0 0 0 0 0 0 4.6037 0.7654 -1.5758 H 0 0 0 0 0 0 0 0 0 0 0 0 3.9635 -1.4215 2.0480 H 0 0 0 0 0 0 0 0 0 0 0 0 5.4925 -0.3651 0.4274 H 0 0 0 0 0 0 0 0 0 0 0 0 -3.5025 -3.7011 -0.5102 H 0 0 0 0 0 0 0 0 0 0 0 0 - cutoff : 0.0 ----- .. class:: infomark 
Output
 A series of lines for each molecule, each describing a pharmocophore point (functional group) of the molecules. The first column specifies a functional type (e.g. HACC is a hydrogen bond acceptor; HYBL is a aromatic, lipophilic region). Subsequent columns specify the position of the center in three-dimensional space; see the documentation
 for more details. .. _documentation: http://silicos-it.be.s3-website-eu-west-1.amazonaws.com/software/align-it/1.0.4/align-it.html * Example:: - aligned Pharmacophores 3033 HYBL -1.98494 1.9958 0.532089 0.7 0 0 0 0 HYBL 3.52122 -0.309347 0.122783 0.7 0 0 0 0 HYBH -3.262 -2.9284 -1.0647 1 1 -3.5666 -3.7035 -1.61827 HDON 0.2679 -0.2051 -0.399 1 1 -0.076102 -0.981133 -0.927616 HACC -2.7906 -1.9108 0.9092 1 1 -2.74368 -1.94015 1.90767 $$$$"
toolshed.g2.bx.psu.edu/repos/bgruening/align_it/ctb_alignit/1.0.4+galaxy0	".. class:: infomark 
What this tool does
 Align-it_ is a tool to align molecules according to their pharmacophores. A pharmacophore is an abstract concept based on the specific interactions observed in drug-receptor interactions: hydrogen bonding, charge transfer, electrostatic and hydrophobic interactions. Molecular modeling and/or screening based on pharmacophore similarities has been proven to be an important and useful method in drug discovery. The functionality of Align-it_ consists mainly of two parts. The first functionality is the generation of pharmacophores from molecules (use the tool 
Pharmacophore generation
 if you want to store these for further use). Secondly, the similarity between pairs of pharmacophores can be quantified. For this purpose, the pharmacophores need to be aligned in three-dimensional space and the overlap volume of the alignments calculated. .. _Align-it: http://silicos-it.be.s3-website-eu-west-1.amazonaws.com/software/align-it/1.0.4/align-it.html ----- .. class:: infomark 
Input
 Upload a file containing molecular representations in MOL, MOL2, SMILES or SDF format; alternatively, if you have already calculated the pharmacophores, upload the .phar file containing them. * Example (SDF):: - database 30 31 0 0 0 0 0 0 0999 V2000 1.9541 1.1500 -2.5078 Cl 0 0 0 0 0 0 0 0 0 0 0 0 1.1377 -1.6392 2.1136 Cl 0 0 0 0 0 0 0 0 0 0 0 0 -3.2620 -2.9284 -1.0647 O 0 0 0 0 0 0 0 0 0 0 0 0 -2.7906 -1.9108 0.9092 O 0 0 0 0 0 0 0 0 0 0 0 0 0.2679 -0.2051 -0.3990 N 0 0 0 0 0 0 0 0 0 0 0 0 -2.0640 0.5139 -0.3769 C 0 0 0 0 0 0 0 0 0 0 0 0 -0.7313 0.7178 -0.0192 C 0 0 0 0 0 0 0 0 0 0 0 0 -2.4761 -0.6830 -1.1703 C 0 0 0 0 0 0 0 0 0 0 0 0 1.6571 -0.2482 -0.1795 C 0 0 0 0 0 0 0 0 0 0 0 0 -3.0382 1.4350 0.0081 C 0 0 0 0 0 0 0 0 0 0 0 0 -0.3728 1.8429 0.7234 C 0 0 0 0 0 0 0 0 0 0 0 0 -2.6797 2.5600 0.7506 C 0 0 0 0 0 0 0 0 0 0 0 0 -1.3470 2.7640 1.1083 C 0 0 0 0 0 0 0 0 0 0 0 0 2.5353 0.3477 -1.0918 C 0 0 0 0 0 0 0 0 0 0 0 0 2.1740 -0.8865 0.9534 C 0 0 0 0 0 0 0 0 0 0 0 0 -2.8480 -1.8749 -0.3123 C 0 0 0 0 0 0 0 0 0 0 0 0 3.9124 0.3058 -0.8739 C 0 0 0 0 0 0 0 0 0 0 0 0 3.5511 -0.9285 1.1713 C 0 0 0 0 0 0 0 0 0 0 0 0 4.4203 -0.3324 0.2576 C 0 0 0 0 0 0 0 0 0 0 0 0 -1.7086 -0.9792 -1.8930 H 0 0 0 0 0 0 0 0 0 0 0 0 -3.3614 -0.4266 -1.7676 H 0 0 0 0 0 0 0 0 0 0 0 0 -0.0861 -1.1146 -0.6780 H 0 0 0 0 0 0 0 0 0 0 0 0 -4.0812 1.2885 -0.2604 H 0 0 0 0 0 0 0 0 0 0 0 0 0.6569 2.0278 1.0167 H 0 0 0 0 0 0 0 0 0 0 0 0 -3.4382 3.2769 1.0511 H 0 0 0 0 0 0 0 0 0 0 0 0 -1.0683 3.6399 1.6868 H 0 0 0 0 0 0 0 0 0 0 0 0 4.6037 0.7654 -1.5758 H 0 0 0 0 0 0 0 0 0 0 0 0 3.9635 -1.4215 2.0480 H 0 0 0 0 0 0 0 0 0 0 0 0 5.4925 -0.3651 0.4274 H 0 0 0 0 0 0 0 0 0 0 0 0 -3.5025 -3.7011 -0.5102 H 0 0 0 0 0 0 0 0 0 0 0 0 - cutoff : 0.0 ----- .. class:: infomark 
Output
 The format of the output file is shown in the table below: +--------+-----------------------------------------------------------------------+ | Column | Content | +========+=======================================================================+ | 1 | Id of the reference structure | +--------+-----------------------------------------------------------------------+ | 2 | Maximum volume of the reference structure | +--------+-----------------------------------------------------------------------+ | 3 | Id of the database structure | +--------+-----------------------------------------------------------------------+ | 4 | Maximum volume of the database structure | +--------+-----------------------------------------------------------------------+ | 5 | Maximum volume overlap of the two structures | +--------+-----------------------------------------------------------------------+ | 6 | Overlap between pharmacophore and exclusion spheres in the reference | +--------+-----------------------------------------------------------------------+ | 7 | Corrected volume overlap between database pharmacophore and reference | +--------+-----------------------------------------------------------------------+ | 8 | Number of pharmacophore points in the processed pharmacophore | +--------+-----------------------------------------------------------------------+ | 9 | TANIMOTO score | +--------+-----------------------------------------------------------------------+ | 10 | TVERSKY_REF score | +--------+-----------------------------------------------------------------------+ | 11 | TVERSKY_DB score | +--------+-----------------------------------------------------------------------+"
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_im_max_min_picker/ctb_im_max_min_picker/1.1.4+galaxy0	".. class:: infomark 
What this tool does
 This molecule selects a subset from a library, ensuring the subset reflects the diversity of the library overall by applying Butina clustering. ----- .. class:: infomark 
Input
 | - Compounds in 
SDF Format
_ | - Various options: similarity threshold, type of fingerprint, seem molecule. .. _SDF Format: http://en.wikipedia.org/wiki/Chemical_table_file ----- .. class:: infomark 
Output
 SD-file containing picked compounds."
toolshed.g2.bx.psu.edu/repos/bgruening/autodock_vina_prepare_ligand/prepare_ligand/1.5.7+galaxy0	".. class:: infomark 
What this tool does
 This tool uses the MGLTools programming package to convert a mol2 molecule file to a pdbqt molecule file, which the Autodock Vina program uses to perform molecular docking. The output file has a similar format to the pdb input, with atom types modified to conform to AutoDock atom types and an extra column containing Gasteiger charges added. ----- .. class:: infomark 
Input
 A mol2 file is required to use this tool. * Example:: @<TRIPOS>MOLECULE NuBBE_1 21 21 0 0 0 SMALL GASTEIGER @<TRIPOS>ATOM 1 O -0.9469 -0.4359 -3.2099 O.2 1 LIG1 -0.2449 2 C -1.2069 0.2449 -2.2351 C.2 1 LIG1 0.3396 3 O -1.9353 -0.1138 -1.1515 O.3 1 LIG1 -0.4570 4 C -2.3385 -1.4828 -1.0650 C.3 1 LIG1 0.1113 5 C -1.7447 -2.0319 0.2001 C.2 1 LIG1 -0.0480 6 C -0.5501 -2.6439 0.3381 C.2 1 LIG1 -0.0731 7 C 0.4046 -2.8855 -0.8015 C.3 1 LIG1 -0.0437 8 C -0.0862 -3.1529 1.6937 C.3 1 LIG1 -0.0285 9 C 0.7304 -2.1137 2.4728 C.3 1 LIG1 -0.0308 10 C 1.1870 -2.6184 3.8183 C.2 1 LIG1 -0.0850 11 C 2.3521 -3.2333 4.1099 C.2 1 LIG1 -0.0796 12 C 3.4007 -3.6048 3.0986 C.3 1 LIG1 -0.0440 13 C 2.6989 -3.6136 5.5272 C.3 1 LIG1 -0.0440 14 C -0.7776 1.6572 -2.0661 C.ar 1 LIG1 0.0627 15 C -0.6402 2.2240 -0.7903 C.ar 1 LIG1 -0.0026 16 C -0.2486 3.5509 -0.6768 C.ar 1 LIG1 0.1590 17 O -0.1045 4.1587 0.5397 O.3 1 LIG1 -0.5033 18 C 0.0124 4.3080 -1.8161 C.ar 1 LIG1 0.1583 19 O 0.3891 5.6141 -1.6913 O.3 1 LIG1 -0.5033 20 C -0.1020 3.7535 -3.0825 C.ar 1 LIG1 -0.0158 21 C -0.4936 2.4191 -3.2122 C.ar 1 LIG1 -0.0441 @<TRIPOS>BOND 1 1 2 2 2 2 3 1 3 3 4 1 4 4 5 1 5 5 6 2 6 6 7 1 7 6 8 1 8 8 9 1 9 9 10 1 10 10 11 2 11 11 12 1 12 11 13 1 13 2 14 1 14 14 15 ar 15 15 16 ar 16 16 17 1 17 16 18 ar 18 18 19 1 19 18 20 ar 20 20 21 ar 21 14 21 ar ----- .. class:: infomark 
Output
 The output is a pdbqt molecule file. * Example:: REMARK 9 active torsions: REMARK status: ('A' for Active; 'I' for Inactive) REMARK 1 A between atoms: C_2 and O_3 REMARK 2 A between atoms: C_2 and C_14 REMARK 3 A between atoms: O_3 and C_4 REMARK 4 A between atoms: C_4 and C_5 REMARK 5 A between atoms: C_6 and C_8 REMARK 6 A between atoms: C_8 and C_9 REMARK 7 A between atoms: C_9 and C_10 REMARK 8 A between atoms: C_16 and O_17 REMARK 9 A between atoms: C_19 and O_20 ROOT ATOM 1 O LIG d 1 -0.947 -0.436 -3.210 0.00 0.00 -0.259 OA ATOM 2 C LIG d 1 -1.207 0.245 -2.235 0.00 0.00 0.293 C ENDROOT BRANCH 2 3 ATOM 3 O LIG d 1 -1.935 -0.114 -1.151 0.00 0.00 -0.314 OA BRANCH 3 4 ATOM 4 C LIG d 1 -2.338 -1.483 -1.065 0.00 0.00 0.206 C BRANCH 4 5 ATOM 5 C LIG d 1 -1.745 -2.032 0.200 0.00 0.00 0.002 C ATOM 6 C LIG d 1 -0.550 -2.644 0.338 0.00 0.00 -0.085 C ATOM 7 C LIG d 1 0.405 -2.885 -0.801 0.00 0.00 0.043 C BRANCH 6 8 ATOM 8 C LIG d 1 -0.086 -3.153 1.694 0.00 0.00 0.037 C BRANCH 8 9 ATOM 9 C LIG d 1 0.730 -2.114 2.473 0.00 0.00 0.031 C BRANCH 9 10 ATOM 10 C LIG d 1 1.187 -2.618 3.818 0.00 0.00 -0.024 C ATOM 11 C LIG d 1 2.352 -3.233 4.110 0.00 0.00 -0.091 C ATOM 12 C LIG d 1 3.401 -3.605 3.099 0.00 0.00 0.042 C ATOM 13 C LIG d 1 2.699 -3.614 5.527 0.00 0.00 0.042 C ENDBRANCH 9 10 ENDBRANCH 8 9 ENDBRANCH 6 8 ENDBRANCH 4 5 ENDBRANCH 3 4 ENDBRANCH 2 3 BRANCH 2 14 ATOM 14 C LIG d 1 -0.778 1.657 -2.066 0.00 0.00 0.042 A ATOM 15 C LIG d 1 -0.640 2.224 -0.790 0.00 0.00 0.057 A ATOM 16 C LIG d 1 -0.249 3.551 -0.677 0.00 0.00 0.099 A ATOM 17 C LIG d 1 0.012 4.308 -1.816 0.00 0.00 0.098 A ATOM 18 C LIG d 1 -0.102 3.753 -3.083 0.00 0.00 0.040 A ATOM 19 C LIG d 1 -0.494 2.419 -3.212 0.00 0.00 0.020 A BRANCH 16 20 ATOM 20 O LIG d 1 -0.104 4.159 0.540 0.00 0.00 -0.358 OA ATOM 21 HO LIG d 1 0.164 5.067 0.617 1.00 0.00 0.217 HD ENDBRANCH 16 20 BRANCH 17 22 ATOM 22 O LIG d 1 0.389 5.614 -1.691 0.00 0.00 -0.358 OA ATOM 23 HO LIG d 1 0.567 6.131 -2.469 1.00 0.00 0.217 HD ENDBRANCH 17 22 ENDBRANCH 2 14 TORSDOF 9"
toolshed.g2.bx.psu.edu/repos/bgruening/prepare_ligands_for_docking/prepare_ligands_for_docking/3.1.1+galaxy0	"What it does?
 This tool uses OpenBabel to convert an input molecule file, typically a SD file, to individual output molecule files in pdbqt, pdb, mol or mol2 formats. There is one output file for each record in the input. Protonation is performed at a specified pH and 3D coordinates can optionally be generated. 3D coordinate generation should be used when the docking program requires 3D structures and the input is not 3D. The most typical usage is to process a set of ligands in a SD file that will be docked by VINA. In this case the pdbqt output format should be used, and the resulting collection of molecules can be used as input by VINA. 
Input
 Molecules such as an SD file dataset in history. 
Output
 A collection of individual molecule files in the specified format."
toolshed.g2.bx.psu.edu/repos/bgruening/autodock_vina_prepare_receptor/prepare_receptor/1.5.7+galaxy0	".. class:: infomark 
What this tool does
 This tool uses the MGLTools programming package to convert a pdb file to pdbqt file, which the Autodock Vina program uses to perform molecular docking. The output file has a similar format to the pdb input, with atom type modified to conform to AutoDock atom types and an extra column containing Gasteiger charges added. ----- .. class:: infomark 
Input
 A pdb input file is required. * Example:: ATOM 1 C ACE A 49 7.007 -4.529 9.096 1.00 0.00 C ATOM 2 O ACE A 49 7.822 -3.710 8.650 1.00 0.00 O ATOM 3 CH3 ACE A 49 6.132 -5.342 8.166 1.00 0.00 C ATOM 4 1HH3 ACE A 49 6.747 -5.942 7.510 1.00 0.00 H ATOM 5 2HH3 ACE A 49 5.492 -5.996 8.739 1.00 0.00 H ATOM 6 3HH3 ACE A 49 5.518 -4.686 7.568 1.00 0.00 H ATOM 7 N ASP A 50 6.886 -4.713 10.549 1.00 65.94 N ATOM 8 CA ASP A 50 7.845 -3.806 11.248 1.00 64.98 C ATOM 9 C ASP A 50 8.508 -4.537 12.430 1.00 62.63 C ATOM 10 O ASP A 50 7.898 -5.423 13.032 1.00 63.05 O ATOM 11 CB ASP A 50 7.146 -2.511 11.685 1.00 66.87 C ATOM 12 CG ASP A 50 8.017 -1.267 11.465 1.00 68.22 C ATOM 13 OD1 ASP A 50 9.140 -1.204 12.030 1.00 67.77 O ATOM 14 OD2 ASP A 50 7.570 -0.351 10.729 1.00 69.97 O1- ATOM 15 N LEU A 51 9.760 -4.189 12.730 1.00 60.07 N ATOM 16 CA LEU A 51 10.551 -4.934 13.712 1.00 57.24 C ATOM 17 C LEU A 51 10.592 -4.245 15.069 1.00 56.84 C ATOM 18 O LEU A 51 10.689 -3.020 15.157 1.00 57.09 O ATOM 19 CB LEU A 51 11.979 -5.179 13.205 1.00 55.78 C ATOM 20 CG LEU A 51 12.250 -6.074 11.987 1.00 54.47 C ATOM 21 CD1 LEU A 51 13.717 -6.000 11.623 1.00 52.72 C ATOM 22 CD2 LEU A 51 11.857 -7.523 12.233 1.00 53.53 C ATOM 23 N THR A 52 10.527 -5.061 16.117 1.00 55.68 N ATOM 24 CA THR A 52 10.525 -4.607 17.507 1.00 55.35 C ATOM 25 C THR A 52 11.729 -5.165 18.262 1.00 53.35 C ATOM 26 O THR A 52 12.413 -6.076 17.786 1.00 52.40 O ATOM 27 CB THR A 52 9.185 -4.964 18.195 1.00 56.95 C ----- .. class:: infomark 
Output
 The output is a pdbqt file - effectively the same as a pdb file, but with an extra column containing Gasteiger charges. * Example:: ATOM 1 C ACE A 49 7.007 -4.529 9.096 1.00 0.00 0.214 C ATOM 2 O ACE A 49 7.822 -3.710 8.650 1.00 0.00 -0.274 OA ATOM 3 CH3 ACE A 49 6.132 -5.342 8.166 1.00 0.00 0.117 C ATOM 4 N ASP A 50 6.886 -4.713 10.549 1.00 65.94 -0.348 N ATOM 5 HN ASP A 50 6.246 -5.360 11.009 1.00 0.00 0.163 HD ATOM 6 CA ASP A 50 7.845 -3.806 11.248 1.00 64.98 0.186 C ATOM 7 C ASP A 50 8.508 -4.537 12.430 1.00 62.63 0.241 C ATOM 8 O ASP A 50 7.898 -5.423 13.032 1.00 63.05 -0.271 OA ATOM 9 CB ASP A 50 7.146 -2.511 11.685 1.00 66.87 0.147 C ATOM 10 CG ASP A 50 8.017 -1.267 11.465 1.00 68.22 0.175 C ATOM 11 OD1 ASP A 50 9.140 -1.204 12.030 1.00 67.77 -0.648 OA ATOM 12 OD2 ASP A 50 7.570 -0.351 10.729 1.00 69.97 -0.648 OA ATOM 13 N LEU A 51 9.760 -4.189 12.730 1.00 60.07 -0.346 N ATOM 14 HN LEU A 51 10.177 -3.384 12.264 1.00 0.00 0.163 HD ATOM 15 CA LEU A 51 10.551 -4.934 13.712 1.00 57.24 0.177 C ATOM 16 C LEU A 51 10.592 -4.245 15.069 1.00 56.84 0.241 C ATOM 17 O LEU A 51 10.689 -3.020 15.157 1.00 57.09 -0.271 OA ATOM 18 CB LEU A 51 11.979 -5.179 13.205 1.00 55.78 0.038 C ATOM 19 CG LEU A 51 12.250 -6.074 11.987 1.00 54.47 -0.020 C ATOM 20 CD1 LEU A 51 13.717 -6.000 11.623 1.00 52.72 0.009 C ATOM 21 CD2 LEU A 51 11.857 -7.523 12.233 1.00 53.53 0.009 C ATOM 22 N THR A 52 10.527 -5.061 16.117 1.00 55.68 -0.344 N ATOM 23 HN THR A 52 10.475 -6.064 15.941 1.00 0.00 0.163 HD"
toolshed.g2.bx.psu.edu/repos/bgruening/chemical_data_sources/ctb_pubchem_download_assays/1.0.0	".. class:: infomark 
What this tool does
 This tool will fetch one PubChem_ Assay file after another and concatenating them. It is possible to optionally filter by PUBCHEM_ACTIVITY_OUTCOME. Columns in the result file: - column 1: PubChem AID (assay id) - column 1: PubChem SID (substance id) - column 2: PubChem CID (compound id) - column 3: PubChem Activity Outcome 1-Inactive 2-Active 3-Inconclusive 4-Unspecified 5-Probe - column 4: PubChem activity score, the higher value, the more active - column 5: Test result specific comment - column 6 and beyond: All remaining columns starting from the 7th column are the TID ""names"" defined in the associated assay description given by the XML file under the corresponding Description/ directory. These ""names"" can also be found in the ""Result Definitions"" section of the assay summary page: e.g. http://pubchem.ncbi.nlm.nih.gov/assay/assay.cgi?aid=2244#aDefinitions .. _PubChem: http://pubchem.ncbi.nlm.nih.gov/ ----- .. class:: infomark 
Output
 The output will be one large SMILES file."
toolshed.g2.bx.psu.edu/repos/bgruening/get_pubchem/ctb_pubchem_download_assays/0.3	".. class:: infomark 
What this tool does
 This tool will fetch one PubChem_ Assay file after another and concatenating them. It is possible to optionally filter by PUBCHEM_ACTIVITY_OUTCOME. Columns in the result file: - column 1: PubChem AID (assay id) - column 1: PubChem SID (substance id) - column 2: PubChem CID (compound id) - column 3: PubChem Activity Outcome 1-Inactive 2-Active 3-Inconclusive 4-Unspecified 5-Probe - column 4: PubChem activity score, the higher value, the more active - column 5: Test result specific comment - column 6 and beyond: All remaining columns starting from the 7th column are the TID ""names"" defined in the associated assay description given by the XML file under the corresponding Description/ directory. These ""names"" can also be found in the ""Result Definitions"" section of the assay summary page: e.g. http://pubchem.ncbi.nlm.nih.gov/assay/assay.cgi?aid=2244#aDefinitions .. _PubChem: http://pubchem.ncbi.nlm.nih.gov/ ----- .. class:: infomark 
Output
 The output will be one large SMILES file."
toolshed.g2.bx.psu.edu/repos/bgruening/chemical_data_sources/ctb_pubchem_download_as_smiles/1.0.0	".. class:: infomark 
What this tool does
 This tool will fetch one PubChem_ file after another and convert them to canonical SMILES. .. _PubChem: http://pubchem.ncbi.nlm.nih.gov/ ----- .. class:: infomark 
Output
 The output will be one large SMILES file."
toolshed.g2.bx.psu.edu/repos/bgruening/get_pubchem/ctb_pubchem_download_as_smiles/0.3	".. class:: infomark 
What this tool does
 This tool will fetch one PubChem_ file after another and convert them to canonical SMILES. .. _PubChem: http://pubchem.ncbi.nlm.nih.gov/ ----- .. class:: infomark 
Output
 The output will be one large SMILES file."
toolshed.g2.bx.psu.edu/repos/recetox/qcxms_getres/qcxms_getres/5.2.1+galaxy4	The QCxMS getres tool is used to simulate mass spectra for a given molecule. This tool take res and molecules files and generates simulated mass spectra.
toolshed.g2.bx.psu.edu/repos/recetox/qcxms_neutral_run/qcxms_neutral_run/5.2.1+galaxy7	The QCxMS Neutral Run tool serves as the first step in preparing for production runs. The tool execute neutral runs for mass spectrometry simulations using the GFN2-xTB and GFN1-xTB quantum chemistry methods. For detail information visit the documentation at https://xtb-docs.readthedocs.io/en/latest/qcxms_doc/qcxms_run.html#excecuting-the-production-runs
toolshed.g2.bx.psu.edu/repos/recetox/qcxms_production_run/qcxms_production_run/5.2.1+galaxy5	The QCxMS production run tool is used to simulate mass spectra for a given molecule using the QCxMS (Quantum Chemistry by Mass Spectrometry) method. This tool allows you to perform QCxMS production runs and generates res files which are further used by QCxMS get results tool to produce the simulated mass spectra. For detail information visit the documentation at https://xtb-docs.readthedocs.io/en/latest/qcxms_doc/qcxms_run.html#excecuting-the-production-runs
toolshed.g2.bx.psu.edu/repos/bgruening/rdconf/rdconf/2020.03.4+galaxy0	".. class:: infomark 
What this tool does
 This tool generates low-energy conformers for a set of input molecules, using the chemistry toolkit RDKit. It is based on a script written by David Koes. ----- .. class:: infomark 
Input
 | - Molecules in 
SMI format
 | - A number of other parameters can be set; the most important include the number of conformers (default 20) to generate and the minimum RMSD difference (default 0.7) between them. .. _SMI format: https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system ----- .. class:: infomark 
Output
 
SD-file
 containing generated conformers. .. _SD-file: http://en.wikipedia.org/wiki/Chemical_table_file"
toolshed.g2.bx.psu.edu/repos/chemteam/mdanalysis_rdf/mdanalysis_rdf/0.19	".. class:: infomark 
What it does
 The Radial Distribution Function (RDF) , g(r), also called pair distribution function or pair correlation function function can be used to find how density varies as a function of distance from a reference atom. 
_
 .. class:: infomark 
Input
 - Trajectory file (DCD). - PDB file. - Segids, resids and names of two atoms to calculate distances. 
___ .. class:: infomark 
Output
 - Tab-separated file of raw data of the RDF. - Image (as png) of the RDF plot."
toolshed.g2.bx.psu.edu/repos/chemteam/bio3d_rmsd/bio3d_rmsd/2.3.4	".. class:: infomark 
What it does
 The Root Mean Square Deviation (RMSD) is a standard measure of structural distance between coordinates. This tool can calculate and plot the RMSD of the selected section of the protein (or other molecule). 
_
 .. class:: infomark 
Input
 - Input file in PDB format - Input file in DCD format 
___ .. class:: infomark 
Output
 - Image (as PNG) of the RMSD plot. - Image (as PNG) of the RMSD histogram plot. - Tab-separated file of raw data."
toolshed.g2.bx.psu.edu/repos/chemteam/bio3d_rmsf/bio3d_rmsf/2.3.4	".. class:: infomark 
What it does
 The Root Mean Square Fluctuation (RMSF) provides a measure of conformational variance i.e. which portions of structure are fluctuating from their mean structure the most (or least). This tool can calculate and plot the RMSF of the selected section of the protein (or other molecule). 
_
 .. class:: infomark 
Input
 - Input file in PDB format - Input file in DCD format 
___ .. class:: infomark 
Output
 - Image (as PNG) of the RMSF plot - Tab-separated file of raw data"
toolshed.g2.bx.psu.edu/repos/chemteam/mdanalysis_ramachandran_protein/mdanalysis_ramachandran_protein/1.0.0+galaxy0	".. class:: infomark 
What it does
 Creates a Ramachandran plot for proteins. All protein [φ,ψ] angles for a given segment ID are automatically selected for an entire trajectory and an averaged Ramachandran plot is returned. Optionally the selection can be refined by residue ID or residue name. Optionally the average Ramachandran plot can be deconvoluted and returned grouped by residue name or residue ID. 
_
 .. class:: infomark 
Input
 - Structure file (PDB) - Trajectory file (DCD). - Segment ID - Optionally: Group data by residue name or residue ID - Optionally: refine selection by residue ID range or residue name (e.g. ALA) Note that a MDAnalysis 'segment' is a larger organizational unit, for example one protein or all the solvent molecules or simply the whole system. 
___ .. class:: infomark 
Output
 - Image (as png) of the Ramachandran from MDanalyis with the allowed regions as a background. - Image (as png) of the Ramachandran plot as a distribution. - Data (H5 format). All phi, psi timeseries raw data in HDF5 format. - All plot images tabulated (in html). To view the HTML in Galaxy this tool must be added to the allowlist by a Galaxy Admin."
toolshed.g2.bx.psu.edu/repos/chemteam/mdanalysis_ramachandran_plot/mdanalysis_ramachandran_plot/1.0.0+galaxy0	".. class:: infomark 
What it does
 A Ramachandran plot ([φ,ψ] plot) was originally developed as a way to visualize the energetically allowed regions for backbone dihedral angles ψ and φ of an amino acid. It can be also used to calculate glycosidic φ and ψ angles formed between carbohydrates. This tool can calculate and plot the histogram (Ramachandran plot) of user-defined φ and ψ angles of a trajectory. 
_
 .. class:: infomark 
Input
 - Trajectory file (DCD). - PDB file. - Text file in YAML format with Segment IDs, residue IDs and names of the four atoms to calculate dihedrals. Note that a MDAnalysis 'segment' is a larger organizational unit, for example one protein or all the solvent molecules or simply the whole system. - For protein φ and ψ dihedral definitions see https://proteinstructures.com/Structure/Structure/Ramachandran-plot.html - For glycan φ and ψ dihedral definitions, see - 
Glycan Structure Website
 - The glycosidic torsion angle definition is adopted from the crystallographic definition; O5-C1-O1-C'x (Φ; phi), C1-O1-C'x-C'x-1 (Ψ; psi), and O1-C'6-C'5-O'5 (ω; omega). The torsion angle between the first residue of the N-glycan chain and the side chain of the asparagine residue is defined as O5-C1-N'D2-C'G (Φ; phi) and C1-N'D2-C'G-C'B (Ψ; psi). The torsion angle between the first residue of the O-glycan chain and the side chain of the serine residue is defined as O5-C1-O'G-C'B (Φ; phi) and C1-O'G-C'B-C'A (Ψ; psi). For threonine residue, OG1 is used instead of OG. The atom names are based on the CHARMM topology. - 
Glycosciences Website
 - NMR definition - Φ phi: H1-C1-O1-C′X Ψ psi: C1-O1-C′X-H′X ω omega: O1-C′6-C′5-H′5 Crystallographic definition - Φ phi: O5-C1-O1-C′X Ψ psi: C1-O1-C′X-C′X+1 ω omega: O1-C′6-C′5-O′5 - An example of a yaml formatted selection for φ-ψ of a small glycoprotein called PROF with a carbohydrate portion called CARA where φ=O5-C1-OG1-CB1 and ψ=C1-OG1-CB-CA for the selected segment and residue ids. .. code-block:: yaml ramachandran1: phi: atom1: segid: CARA resid: 1 name: O5 atom2: segid: CARA resid: 1 name: C1 atom3: segid: PROF resid: 4 name: OG1 atom4: segid: PROF resid: 4 name: CB psi: atom1: segid: CARA resid: 1 name: C1 atom2: segid: PROF resid: 4 name: OG1 atom3: segid: PROF resid: 4 name: CB atom4: segid: PROF resid: 4 name: CA comment: pick visually using VMD using labels. Go to labels, dihedral to see the information about resname resid and atomname and then lookup the segname for ach atom. 
___ .. class:: infomark 
Output
 - Tab-separated file of raw data of the φ,ψ angles over time. - Image (as png) of the Ramachandran plot. .. 
Glycan Structure Website
: http://www.glycanstructure.org/fragment-db/howto .. 
Glycosciences Website
: http://www.glycosciences.de/tools/glytorsion/"
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_im_rxn_smarts_filter/ctb_im_rxn_smarts_filter/1.1.4+galaxy0	ABCCDEFGHIJKLMNOPQRSTUVWXYZ
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_im_rxn_maker/ctb_im_rxn_maker/1.1.4+galaxy0	".. class:: infomark 
What this tool does
 Provides the output of reactions between input molecules and a list of reagents (both in SDF format). .. class:: infomark 
Input
 | - Input file in 
SDF Format
 | - Reagent file in 
SDF Format
 In addition, a list of reaction types needs to be specified. .. _SDF Format: http://en.wikipedia.org/wiki/Chemical_table_file .. _SMILES: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 SD-file with reaction products."
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_remIons/1.0	".. class:: infomark 
What this tool does
 Parses a multiple molecules file and deletes any present counterions or fragments. ----- .. class:: warningmark 
Hint
 | Only the 
largest fragment
 on every molecule is extracted. | | Only molecules with more than 5 heavy atoms are parsed. ----- .. class:: infomark 
Cite
 N M O'Boyle, C Morley and G R Hutchison - 
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
 .. 
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
: http://journal.chemistrycentral.com/content/2/1/5 N M O'Boyle, M Banck, C A James, C Morley, T Vandermeersch, and G R Hutchison - 
Open Babel: An open chemical toolbox.
 .. 
Open Babel: An open chemical toolbox.
: http://www.jcheminf.com/content/3/1/33 
Open Babel
 .. 
Open Babel
: http://openbabel.org/wiki/Main_Page"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_remions/openbabel_remIons/3.1.1+galaxy2	".. class:: infomark 
What this tool does
 Parses a multiple molecules file and deletes any counterions or fragments present. ----- .. class:: warningmark 
Hint
 | For each molecule, all fragments except for the largest are deleted. | | Only molecules with more than 5 heavy atoms are parsed."
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_remDuplicates/1.0	".. class:: infomark 
What this tool does
 Filters a library of compounds and removes duplicated molecules. ----- .. class:: warningmark 
Hint
 Comparison based on Canonical SMILES without stereochemistry may be useful in cases where this information is not crucial for library preparation. Several VS tools will automatically generate stereoisomeric forms. ----- .. class:: infomark 
Input
 | - 
InChI
 | - 
SMILES Format
 .. 
InChI: http://www.iupac.org/home/publications/e-resources/inchi.html .. _SMILES Format: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 Same as input format. ----- .. class:: infomark 
Cite
 N M O'Boyle, M Banck, C A James, C Morley, T Vandermeersch, and G R Hutchison - 
Open Babel: An open chemical toolbox.
 .. 
Open Babel: An open chemical toolbox.
: http://www.jcheminf.com/content/3/1/33 
Open Babel
 .. _
Open Babel
: http://openbabel.org/wiki/Main_Page"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_remduplicates/openbabel_remDuplicates/3.1.1+galaxy0	".. class:: infomark 
What this tool does
 Filters a library of compounds and removes duplicated molecules. ----- .. class:: warningmark 
Hint
 Comparison based on Canonical SMILES without stereochemistry may be useful in cases where this information is not crucial for library preparation. Several VS tools will automatically generate stereoisomeric forms. ----- .. class:: infomark 
Input
 | - 
InChI
 | - 
SMILES Format
 .. _InChI: https://en.wikipedia.org/wiki/International_Chemical_Identifier .. _SMILES Format: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 Same as input format."
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_remove_protonation_state/0.1	".. class:: infomark 
What this tool does
 Removes the protonation state of every atom. ----- .. class:: infomark 
Cite
 
Open Babel
_ .. _Open Babel: http://openbabel.org/wiki/Main_Page"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_remove_protonation_state/openbabel_remove_protonation_state/3.1.1+galaxy1	".. class:: infomark 
What this tool does
 Removes the protonation state of every atom."
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_remSmall/1.0	".. class:: infomark 
What this tool does
 Filters a library of compounds and removes small molecules below a predefined input number of atoms. ----- .. class:: warningmark 
Hint
 Some libraries may contain molecules without a 1D/3D descriptor. These molecules may provoke crashes of any other tool. It is strongly adviced to run this tool before proceeding to any further steps. ----- .. class:: infomark 
Output
 Same as input format. ----- .. class:: infomark 
Cite
 N M O'Boyle, M Banck, C A James, C Morley, T Vandermeersch, and G R Hutchison - 
Open Babel: An open chemical toolbox.
 .. 
Open Babel: An open chemical toolbox.
: http://www.jcheminf.com/content/3/1/33 
Open Babel
 .. 
Open Babel
: http://openbabel.org/wiki/Main_Page"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_remsmall/openbabel_remSmall/3.1.1+galaxy0	".. class:: infomark 
What this tool does
 Filters a library of compounds and removes small molecules below a predefined input number of atoms. ----- .. class:: warningmark 
Hint
 Some libraries may contain molecules without a 1D/3D descriptor, which may provoke crashes of any other tool. It is strongly advised to run this tool before proceeding to any further steps. ----- .. class:: infomark 
Output
 Same as input format."
toolshed.g2.bx.psu.edu/repos/bgruening/openduck_run_smd/openduck_run_smd/0.1.1	".. class:: infomark 
What it does
 Perform steered molecular dynamics runs for dynamic undocking (DUck), performing system preparation (topology calculation and minimization), an initial MD run, and SMD runs. This Galaxy tool uses OpenDUck, an open-source implementation of the original DUck method, using OpenMM and AmberTools. 
_
 .. class:: infomark 
Input
 - PDB file for apoprotein - PDB file for chunk - SDF/MOL file for ligand - Parameters defining the protein-ligand interaction - Parameters for MD and SMD runs 
___ .. class:: infomark 
Output
 - Ligand in SDF/MOL format, with added 
&lt;SCORE.DUCK_WQB&gt;
 parameter. A tar file is also produced as a optional output, containing all files produced by the tool."
toolshed.g2.bx.psu.edu/repos/bgruening/rxdock_sort_filter/rxdock_sort_filter/2013.1.1_148c5bd1+galaxy0	".. class:: infomark This tool sorts and filters SD files using the sdsort and sdfilter tools from the rxDock suite. See https://www.rxdock.org for more details about rxDock and associated programs. The expected use is for filtering and sorting virtual screening results such as docking. ----- .. class:: infomark 
Inputs
 An SD-file, together with names of fields to sort and group records by, and the number of records to appear in the output. The sorting is performed on groups of molecules, with the group being identified by a field in the SDF (the name_field parameter). Records from a group MUST be sequential. The records within each group are sorted by the value of a field in the SDF (the sort_field parameter) and you can specify ascending or descending order (the descending parameter). Finally a number of top scoring (the top parameter, typically having a value of 1) are written to the output. ----- .. class:: infomark 
Outputs
 An SD-file, containing molecules filtered by the field specified."
toolshed.g2.bx.psu.edu/repos/bgruening/chemfp/ctb_sdf2fps/1.6.1+0	".. class:: infomark 
What this tool does
 Read an input SD file, extract the fingerprints and store them in a FPS-file. ----- .. class:: infomark 
Input
 
SD-Format
, storing the atom types, together with the Cartesian coordinates. .. 
SD-Format
: http://en.wikipedia.org/wiki/Chemical_table_file * Example:: 28434379 -OEChem-02031205132D 37 39 0 0 0 0 0 0 0999 V2000 8.1648 -1.8842 0.0000 O 0 0 0 0 0 0 0 0 0 0 0 0 6.0812 -0.2134 0.0000 N 0 0 0 0 0 0 0 0 0 0 0 0 6.0812 -1.8229 0.0000 N 0 0 0 0 0 0 0 0 0 0 0 0 2.5369 -2.0182 0.0000 N 0 0 0 0 0 0 0 0 0 0 0 0 6.3919 0.7371 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 7.3704 0.9433 0.0000 C 0 0 0 0 ...... 1 15 1 0 0 0 0 1 35 1 0 0 0 0 2 5 1 0 0 0 0 2 11 1 0 0 0 0 2 12 1 0 0 0 0 3 12 2 0 0 0 0 3 13 1 0 0 0 0 4 18 1 0 0 0 0 ...... >PUBCHEM_COMPOUND_CID< 28434379 > <PUBCHEM_COMPOUND_CANONICALIZED> 1 > <PUBCHEM_CACTVS_COMPLEXITY> 280 > <PUBCHEM_CACTVS_HBOND_ACCEPTOR> 2 > <PUBCHEM_CACTVS_HBOND_DONOR> 2 > <PUBCHEM_CACTVS_ROTATABLE_BOND> 2 > <PUBCHEM_CACTVS_SUBSKEYS> AAADceBzIAAAAAAAAAAAAAAAAAAAAWAAAAAwYAAAAAAAAFgB8AAAHgAQCAAACCjhlwYx0LdMEgCgASZiZASCgC0hEqAJ2CA4dJiKeKLA2dGUJAhokALYyCcQAAAAAACAAAQAACAAAQAACAAAQAAAAAAAAA== > ----- .. class:: infomark 
Output
 After the first few lines, starting with a hash symbol, which contain generic information, the fingerprints are listed as hexadecimal strings. * Example:: #FPS1 #num_bits=881 #type=CACTVS-E_SCREEN/1.0 extended=2 #software=CACTVS/unknown #source=/home/mohammed/galaxy-central/database/files/000/dataset_409.dat #date=2012-02-03T10:44:12 07ce04000000000000000000000000000080060000000c0600 00000000001a800f0000780008100000101487e9608c0bed32 48000580644626204101b4844805901b041c2e19511e45039b 8b2924101609401b13e4080000000000010020000004008000 0010000002000000000000 28434379"
toolshed.g2.bx.psu.edu/repos/bgruening/sucos_docking_scoring/sucos_docking_scoring/2020.03.4+galaxy1	".. class:: infomark 
What it does
 This tool determines the shape and feature overlap of docked ligand poses compared to a reference molecule, usually a known ligand. The RDKit FeatureMap functionality is used to do the scoring. The original SuCOS code is on GitHub_ under a MIT license. The SuCOS work is described here_. .. _GitHub: https://github.com/susanhleung/SuCOS .. _here: https://chemrxiv.org/articles/SuCOS_is_Better_than_RMSD_for_Evaluating_Fragment_Elaboration_and_Docking_Poses/8100203 .. class:: infomark 
Input
 Molecules such as an SD file dataset from the history. .. class:: infomark 
Output
 The same SD file with a ""SuCOS_Score"" property added. A score of 1.0 infers a perfect overlap, a score of 0.0 no overlap. A rule of thumb is that poses with a score greater than 0.5 can be be considered ""useful""."
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_im_screen/ctb_im_screen/1.1.4+galaxy0	".. class:: infomark 
What this tool does
 Screen a library against a compound, filtering by fingerprint similarity and a number of other properties. .. class:: infomark 
Input
 | - Compound library in 
SDF Format
 | - Compound to screen against in 
SMILES
 or 
SDF Format
_ | - Type of fingerprint | - Required molecular weight and heavy atom count .. _SDF Format: http://en.wikipedia.org/wiki/Chemical_table_file .. _SMILES: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 SD-file of screened compounds, with new 'Similarity' property."
toolshed.g2.bx.psu.edu/repos/bgruening/chembl/chembl/0.10.1+galaxy4	"Search the ChEMBL database for compounds which resemble a SMILES string. Two search options are possible: similarity (searches for compounds which are similar to the input within a specified Tanimoto cutoff) and substructure (searches for compounds which contain the input substructure). Results can be filtered for compounds which are 1) approved drugs 2) biotherapeutic 3) natural products and 4) fulfil all of the Lipinski rule of five criteria. ----- .. class:: infomark 
Input
 A single molecule in SMILES format. This can be submitted either as text or as a file containing the SMILES string on the first line. Note that if the file contains multiple lines, only the SMILES string on the first line will be used for the search. ----- .. class:: infomark 
Output
 A SMILES file with search results, each on a new line."
toolshed.g2.bx.psu.edu/repos/bgruening/simsearch/ctb_simsearch/1.6.1+galaxy0	".. class:: infomark 
What this tool does
 Similarity searches using a variety of different fingerprints using either the chemfp_ FPS type or the Open Babel FastSearch_ index. .. _chemfp: http://chemfp.com/ .. _FastSearch: http://openbabel.org/wiki/FastSearch"
toolshed.g2.bx.psu.edu/repos/chemteam/md_converter/md_slicer/1.9.7+galaxy0	"What it does
 This tool extracts a segment from a molecular dynamcics trajectory (i.e. performs a slice). 
_
 .. class:: infomark 
Input
 - Trajectory file (xtc, dcd) In addition, a start frame and end frame have to be specified for the output trajectory, as well as a value for the 'stride' (to save only every nth frame in the output file). 
___ .. class:: infomark 
Output
 - Trajectory file (xtc, dcd) extracted from the input file."
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_spectrophore_search/openbabel_spectrophore_search/3.1.1+galaxy1	".. class:: infomark 
What does this tool do?
 This tool computes the Euclidean distance between the |Spectrophores (TM)| descriptors of the target to each molecule stored in the library. |Spectrophores (TM)| search |Spectrophores (TM)| is a screening technology by Silicos_ which converts three-dimensional molecular property data into one-dimensional spectra. Typical characteristics that can be converted include electrostatic potentials, molecular shape, lipophilicity, hardness and softness potentials. The computation is independent of the position and orientation of a molecule and allows easy comparison of |Spectrophores (TM)| of different molecules. Molecules with similar three-dimensional properties and shape, and therefore also similar biological activities, always have similar |Spectrophores (TM)|. As a result this technique is a very powerful tool to investigate the similarity of molecules and can be applied as a screening tool for molecular databases, virtual screening, and database characterisations. 
Advantages:
 - |Spectrophores (TM)| can realistically compute ligand-protein interactions based on aforementioned molecular descriptors - |Spectrophores (TM)| can be applied in both a ligand- or target-based setting - |Spectrophores (TM)| can distinguish, if needed, between the different enantiomers of stereo-selective compounds - |Spectrophores (TM)| can be computed rapidly ----- .. class:: warningmark 
Hint:
 this tool is useful to select compounds with similar chemical features to a target, but accounting for the discovery of diverse scaffolds. This is in contrast to the results expected in a similarity search based on atom connectivity. ----- .. class:: infomark 
Input
 The target molecule must be a SD formatted file with the |Spectrophores (TM)| descriptors stored as metadata. Such files can be generated using the 
Compute physico-chemical properties
 tool. ----- .. class:: infomark 
Output
 The library of compounds is a tabular file with one line per compound. One column contains the |Spectrophores (TM)| descriptors. ----- .. class:: infomark 
Cite
 N M O'Boyle, C Morley and G R Hutchison - 
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
 Silicos
 - |Spectrophores (TM)| is a registered tool implemented in the open-source OpenBabel. .. _
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
: http://www.biomedcentral.com/content/pdf/1752-153X-2-5.pdf .. _Silicos: https://open-babel.readthedocs.io/en/latest/Fingerprints/spectrophore.html .. |Spectrophores (TM)| unicode:: Spectrophores U+2122"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_spectrophore_search/1.0	".. class:: infomark 
What does this tool do?
 This tool computes the Euclidean distance between the Spectrophores(TM) descriptors of the target to each molecule stored in the library. |Spectrophores (TM)| search |Spectrophores (TM)| is a screening technology by Silicos_ which converts three-dimensional molecular property data into one-dimensional spectra. Typical characteristics that can be converted include electrostatic potentials, molecular shape, lipophilicity, hardness and softness potentials. The computation is independent of the position and orientation of a molecule and allows an easy comparison of |Spectrophores (TM)| of different molecules. Molecules with similar three-dimensional properties and shape, and therefore also similar biological activities, always have similar |Spectrophores (TM)|. As a result this technique is a very powerful tool to investigate the similarity of molecules and can be applied as a screening tool for molecular databases, virtual screening, and database characterisations. 
Advantages:
 - |Spectrophores (TM)| can realistically compute ligand-protein interactions based on aforementioned molecular descriptors - |Spectrophores (TM)| can be applied in both a ligand- or target-based setting - |Spectrophores (TM)| can distinguish, if needed, between the different enantiomers of stereo-selective compounds - |Spectrophores (TM)| can be computed fast .. |Spectrophores (TM)| unicode:: Spectrophores U+2122 ----- .. class:: warningmark 
Hint
 this tool is useful to select compounds with similar chemical features to a target, but accounting for the discovery of diverse scaffolds. This is in contrast to the results expected in a similarity search based on atom connectivity. ----- .. class:: infomark 
Input
 The target molecule must be a SD formatted file with the |Spectrophores (TM)| descriptors stored as metadata. Such files can be generated using the 
Compute physico-chemical properties
 tool. ----- .. class:: infomark 
Output
 The library of compounds is a tabular file with one line per compound. One column contains the |Spectrophores (TM)| descriptors. ----- .. class:: infomark 
Cite
 N M O'Boyle, C Morley and G R Hutchison - 
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
 Silicos
 - |Spectrophores (TM)| is a registered tool implemented in the open-source OpenBabel. .. _
Pybel: a Python wrapper for the OpenBabel cheminformatics toolkit
: http://www.biomedcentral.com/content/pdf/1752-153X-2-5.pdf .. _Silicos: http://openbabel.org/docs/dev/Fingerprints/spectrophore.html"
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_im_standardize/ctb_im_standardize/1.1.4+galaxy0	".. class:: infomark 
What this tool does
 Standardizes an SD-file using RDKit. .. class:: infomark 
Input
 | - Input file in 
SDF Format
_ .. _SDF Format: http://en.wikipedia.org/wiki/Chemical_table_file .. _SMILES: http://en.wikipedia.org/wiki/Simplified_molecular_input_line_entry_specification ----- .. class:: infomark 
Output
 SD-file of standardized compounds."
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_subsearch/0.1	".. class:: infomark 
What this tool does
 Substructure search in based on Open Babel FastSearch_ Index. It uses molecular fingerprints to prepare and search an index of a multi-molecule datafile. .. 
FastSearch: http://openbabel.org/wiki/FastSearch ----- .. class:: infomark 
Input
 SMILES or SMARTS patterns are possible. SD- and InChI files are converted to SMILES. ----- .. class:: infomark 
Cite
 
Open Babel
 .. _Open Babel: http://openbabel.org/wiki/Main_Page"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_subsearch/openbabel_subsearch/3.1.1+galaxy1	".. class:: infomark 
What this tool does
 Substructure search in based on Open Babel FastSearch_ Index. It uses molecular fingerprints to prepare and search an index of a multi-molecule datafile. .. _FastSearch: https://open-babel.readthedocs.io/en/latest/Command-line_tools/babel.html#substructure-and-similarity-searching ----- .. class:: infomark 
Input
 SMILES or SMARTS patterns are possible. SD- and InChI files are converted to SMILES."
toolshed.g2.bx.psu.edu/repos/bgruening/chemfp/ctb_chemfp_butina_clustering/1.6.1+0	".. class:: infomark 
What this tool does
 Unsupervised non-hierarchical clustering of molecular fingerprints, based on the Taylor-Butina algorithm, which guarantees that every cluster contains molecules which are within a distance cutoff of the central molecule. This tool is based on the chemfp_ project. .. _chemfp: http://chemfp.com/ ----- .. class:: infomark 
Input
 | Molecular fingerprints in FPS format. | Open Babel Fastsearch index is not supported. * Example:: - fingerprints in FPS format #FPS1 #num_bits=881 #type=CACTVS-E_SCREEN/1.0 extended=2 #software=CACTVS/unknown #source=/home/mohammed/galaxy-central/database/files/000/dataset_423.dat #date=2012-02-09T13:20:37 07ce04000000000000000000000000000080060000000c000000000000001a800f0000780008100000701487e960cc0bed3248000580644626004101b4844805901b041c2e 19511e45039b8b2926101609401b13e40800000000000100200000040080000010000002000000000000 55169009 07ce04000000000000000000000000000080060000000c000000000000001a800f0000780008100000701087e960cc0bed3248000580644626004101b4844805901b041c2e 19111e45039b8b2926105609401313e40800000000000100200000040080000010000002000000000000 55079807 ........ - Tanimoto threshold : 0.8 (between 0 and 1) ----- .. class:: infomark 
Output
 * Example:: 0 true singletons => 0 false singletons => 1 clusters 55091849 has 12 other members => 6499094 6485578 55079807 3153534 55102353 55091466 55091416 6485577 55169009 55091752 55091467 55168823"
toolshed.g2.bx.psu.edu/repos/chemteam/traj_selections_and_merge/traj_selections_and_merge/1.9.7+galaxy0	".. class:: infomark 
What it does
 This tool is a filter and multijoin for trajectory files. You can select specific molecules (or atoms) and save out a new structure and trajectory file. If there are multiple trajectories selected, then these will all be combined into a single trajectory output. Use this tool to: - reduce the size of trajectories for later analysis and help with selections where analysis tools may not have selection features that you need. - combine trajectories for analysis, this is imperative for proper molecular dynamics analysis. 
_
 .. class:: infomark 
Input
 - Structure file (pdb,gro) - Choose a format of trajectory file (trr,xtc, dcd, netcdf) - Trajectory file(s) (the files displayed are filtered by the trajectory type chosen). - A valid mdtraj selection for the system under consideration. For example: not water and segname CARA and not type H 
___ .. class:: infomark 
Output
 - Structure file (same as input format) - Trajectory file (same as input format)"
toolshed.g2.bx.psu.edu/repos/bgruening/autodock_vina/docking/1.2.3+galaxy0	"This tool performs protein-ligand docking using the Autodock Vina program. ----- .. class:: infomark 
Inputs
 The first two inputs required are files describing the receptor (in the pdbqt format) and ligands (in SDF of PDBQT format) respectively. These files are produced by the receptor and ligand preparation tools. If using PDBQT format for the ligands only a single ligand can be specified. If using SDF you can include multiple ligands and those ligands are converted to individual PDBQT format files using openbabel as the first step of tool execution. You can specify the pH for protonation by openbabel VINA will dock each of the ligands in the SDF file sequentially. If there are a large number of ligands then first split them into a collection of smaller files e.g. using the splitter tool. This allows each split chunk of molecules to be docked as a separate task. The optimal size of the chunk will depend on the number of ligands and the capacity of the execution environment. In addition, parameters for docking must be defined. The Cartesian coordinates of the center of the binding site should be provided, along with the size of the binding site along each dimension. Effectively, this defines a cuboidal volume in which docking is performed. Alternatively, a config file can be uploaded containing this information - such a file can be generated from the box parameter calculation file. ----- .. class:: infomark 
Outputs
 SDF files are produced as output. There is one file for each ligand in the input. Each entry in the file is a docked pose for that ligand. The binding affinity scores are contained within the SDF file.:: OpenBabel06171915303D 23 23 0 0 0 0 0 0 0 0999 V2000 66.9030 73.3450 36.0040 O 0 0 0 0 0 0 0 0 0 0 0 0 66.8190 73.2170 37.2120 C 0 0 0 0 0 0 0 0 0 0 0 0 66.0490 72.3370 37.8940 O 0 0 0 0 0 0 0 0 0 0 0 0 66.2290 70.9500 37.5970 C 0 0 0 0 0 0 0 0 0 0 0 0 67.2070 70.4140 38.6010 C 0 0 0 0 0 0 0 0 0 0 0 0 68.5140 70.1440 38.3980 C 0 0 0 0 0 0 0 0 0 0 0 0 69.2150 70.3400 37.0800 C 0 0 0 0 0 0 0 0 0 0 0 0 69.3810 69.5970 39.5210 C 0 0 0 0 0 0 0 0 0 0 0 0 68.7730 69.8280 40.9100 C 0 0 0 0 0 0 0 0 0 0 0 0 69.3750 71.0120 41.6220 C 0 0 0 0 0 0 0 0 0 0 0 0 68.7550 72.1570 41.9760 C 0 0 0 0 0 0 0 0 0 0 0 0 67.3280 72.4970 41.6430 C 0 0 0 0 0 0 0 0 0 0 0 0 69.4770 73.2270 42.7560 C 0 0 0 0 0 0 0 0 0 0 0 0 67.5570 74.0540 38.1920 C 0 0 0 0 0 0 0 0 0 0 0 0 66.9010 75.0480 38.9340 C 0 0 0 0 0 0 0 0 0 0 0 0 67.6300 75.8170 39.8300 C 0 0 0 0 0 0 0 0 0 0 0 0 68.9950 75.5990 39.9980 C 0 0 0 0 0 0 0 0 0 0 0 0 69.6510 74.6060 39.2850 C 0 0 0 0 0 0 0 0 0 0 0 0 68.9300 73.8240 38.3800 C 0 0 0 0 0 0 0 0 0 0 0 0 67.0450 76.8040 40.5760 O 0 0 0 0 0 0 0 0 0 0 0 0 67.5560 77.4980 40.9760 H 0 0 0 0 0 0 0 0 0 0 0 0 69.7010 76.3670 40.8780 O 0 0 0 0 0 0 0 0 0 0 0 0 69.2520 76.7930 41.5990 H 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 0 0 0 0 2 3 1 0 0 0 0 2 14 1 0 0 0 0 4 3 1 0 0 0 0 4 5 1 0 0 0 0 6 5 2 0 0 0 0 6 8 1 0 0 0 0 7 6 1 0 0 0 0 8 9 1 0 0 0 0 9 10 1 0 0 0 0 10 11 2 0 0 0 0 11 13 1 0 0 0 0 12 11 1 0 0 0 0 14 19 2 0 0 0 0 14 15 1 0 0 0 0 15 16 2 0 0 0 0 16 17 1 0 0 0 0 16 20 1 0 0 0 0 17 22 1 0 0 0 0 18 17 2 0 0 0 0 19 18 1 0 0 0 0 20 21 1 0 0 0 0 22 23 1 0 0 0 0 M END > <MODEL> 1 > <REMARK> VINA RESULT: 0.0 0.000 0.000 9 active torsions: status: ('A' for Active; 'I' for Inactive) 1 A between atoms: C_2 and O_3 2 A between atoms: C_2 and C_14 3 A between atoms: O_3 and C_4 4 A between atoms: C_4 and C_5 5 A between atoms: C_6 and C_8 6 A between atoms: C_8 and C_9 7 A between atoms: C_9 and C_10 8 A between atoms: C_16 and O_17 9 A between atoms: C_19 and O_20 > <TORSDO> F 9 > <SCORE> 0.0 > <RMSD_LB> 0.000 > <RMSD_UB> 0.000"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel/ctb_ob_svg_depiction/1.0.0	".. class:: infomark 
What this tool does
 Creates an .svg or .png image of a small set of molecules (few hundreds). Based on Open Babel PNG_/SVG_ 2D depiction. .. 
PNG: http://openbabel.org/docs/dev/FileFormats/PNG_2D_depiction.html .. _SVG: http://openbabel.org/docs/dev/FileFormats/SVG_2D_depiction.html ----- .. class:: warningmark 
Hint
 Use only libraries with at most a few hundred molecules. ----- .. class:: infomark 
Cite
 
Open Babel
 .. _Open Babel: http://openbabel.org/wiki/Main_Page"
toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_svg_depiction/openbabel_svg_depiction/3.1.1+galaxy1	".. class:: infomark 
What this tool does
 Creates an SVG or PNG image of a small set of molecules (not more than a few hundred). Based on Open Babel PNG_/SVG_ 2D depiction. Note that PNG is a poor choice for representing more than one molecule, as resolution will be low. .. _PNG: https://open-babel.readthedocs.io/en/latest/FileFormats/PNG_2D_depiction.html .. _SVG: https://open-babel.readthedocs.io/en/latest/FileFormats/SVG_2D_depiction.html ----- .. class:: warningmark 
Hint
 Use only libraries with at most a few hundred molecules."
toolshed.g2.bx.psu.edu/repos/bgruening/xchem_transfs_scoring/xchem_transfs_scoring/0.4.0	".. class:: infomark This tool performs scoring of docked ligand poses using deep learning. It uses the gnina and libmolgrid toolkits to perform the scoring to generate a prediction for how good the pose is. ----- .. class:: infomark 
Inputs
 1. The protein receptor to dock into as a file in PDB format. This should have the ligand removed but can retain the waters. This is specified by the 'receptor' parameter. 2. A set of ligand poses to score in SDF format. This is specified by the 'ligands' parameter. Other parameters: 'distance': Distance in Angstroms. Waters closer than this to any heavy atom in each ligand are removed. A discrete set of PDB files are created missing certain waters and the ligands corresponding to that PDB file are scored against it. If a distance of zero is specified then this process is skipped and all ligands are scored against the receptor 'as is'. The assumption is that all waters have already been removed in this case. Specifying a distance of 0 provides a significant boost in performance as the water checking step is avoided.. 'model': A number of models are provided: - weights.caffemodel - No threshold for distinction of actives and inactives (original model) - 10nm.0_iter_50000.caffemodel: actives are molecules from DUDE that have better than 10nM activity - 50nm.0_iter_50000.caffemodel: actives are molecules from DUDE that have better than 50nM activity - 200nm.0_iter_50000.caffemodel: actives are molecules from DUDE that have better than 200nM activity ----- .. class:: infomark 
Outputs
 An SDF file is produced as output. The predicted scores are contained within the SDF file as the TransFSScore property and the PDB file (with the waters that clash with the ligand removed) that was used for the scoring as the TransFSReceptor property. Values for the score range from 0 (poor binding) to 1 (good binding). The raw predictions (predictions.txt) is also provided as an output. A set of PDB files is also output, each one with different crystallographic waters removed. Each ligand is examined against input PDB structure and the with waters that clash (any heavy atom of the ligand closer than the 'distance' parameter being removed. The file names are encoded with the water numbers that are removed."
toolshed.g2.bx.psu.edu/repos/bgruening/ctb_im_xcos/ctb_im_xcos/1.1.4+galaxy0	".. class:: infomark 
What this tool does
 XCos is a reverse shape and feature overlap score for a pose compared to a set of ligands that identifies which ligands best overlap. The molecule is fragmented using the BRICS algorithm and each fragment (bit) is compared to the ligands using the SuCOS methodology. The resulting scores are the combined scores of the bits. XCos_RefMols - The fragments that the ligand overlays XCos_NumHits - the number of fragments the ligand overlays XCos_Score1 - The sum of each bit's feature and shape overlay score scaled by the number of heavy bit atoms. XCos was conceived and implemented by Warren Thompson <warren.thompson@diamond.ac.uk>. The original code can be found here: https://github.com/Waztom/xchem-XCOS ----- .. class:: infomark 
Input
 - poses: SD-file of ligand poses to score - fragments: SD-file of fragments to compare the poses ----- .. class:: infomark 
Output
 SD-file with the XCos scores added."
toolshed.g2.bx.psu.edu/repos/chemteam/biomd_neqgamma/biomd_neqgamma/0.1.5.2+galaxy1	".. class:: infomark 
What it does
 Perform dcTMD friction correction from an ensemble of XVG files generated by GROMACS targeted molecular dynamics (TMD) simulations. 
_
 .. class:: infomark 
Input
 - Ensemble of XVG files from TMD simulations - Optional: JSON file containing subgroups of trajectories from the ensemble. If this option is used, the dcTMD calculation will be performed separately for each sub-ensemble. 
___ .. class:: infomark 
Output
 - Tab-separated files containing free energy and friction data."
toolshed.g2.bx.psu.edu/repos/bgruening/fpocket/dpocket/4.0.0+galaxy0	"Calculate descriptors (i.e. featurize) 'pockets' in a protein structure, using the dpocket module of the fpocket software. To use, upload one or more protein structures in PDB format and select the type of pocket to detect. 'Custom options' can also be selected - this exposes all internal fpocket parameters. Using this option requires some knowledge of the fpocket prediction algorithm. Please consult the cited publications for more details. In addition, a list of ligand identifiers should be provided as a text file, one per line and one for each of the protein structures. ----- .. class:: infomark 
Input
 - One or more protein structures in PDB format. If using multiple structures, submitting them as a dataset collection is recommended to ensure ordering is preserved. - Text file with ligand identifiers, matching the ligand code in the PDB files. One identifier should be listed per line and one per PDB file. ----- .. class:: infomark 
Output
 - Tabular file with descriptors of pockets matching the positions of the input ligands. - Tabular file with descriptors of pockets which do NOT match the positions of the input ligands. - (Optional) Tabular file with descriptors of pockets which are explicitly defined by the positions of the input ligands. Each tabular file contains 57 columns (55 descriptors, plus protein and ligand names), a header line, and one row per pocket found."
toolshed.g2.bx.psu.edu/repos/chemteam/fastpca/fastpca/0.9.1	".. class:: infomark 
What it does
 Dimensionality reduction of molecular dynamics trajectories. Data can be input as tabular or GROMACS XTC files. In addition, data can be projected into a previously computed coordinate space by providing precomputed eigenvectors, statistics and a correlation/covariance matrix. Data can be normalized using the either the covariance or correlation matrix. Data can also be calculated on a torus, which is useful for periodic data, such as protein dihedral angles. 
_
 .. class:: infomark 
Input
 - Tabular or XTC file - If you want to project data into a previously calculated principal space, you can upload precomputed eigenvectors, statistics and correlation/covariance matrix. 
___ .. class:: infomark 
Output
 - Projected data (tabular file) with each column representing a principal component - Eigenvectors, statistics and covariance/correlation matrix"
toolshed.g2.bx.psu.edu/repos/bgruening/fpocket/fpocket/4.0.0+galaxy0	"Detect 'pockets' in a protein structure using the fpocket software. A potential use of this tool is locating potential binding sites in a protein prior to performing protein-ligand docking. To use, upload a protein structure in PDB format and select the type of pocket to detect. 'Custom options' can also be selected - this exposes all internal fpocket parameters. Using this option requires some knowledge of the fpocket prediction algorithm. Please consult the cited publications for more details. ----- .. class:: infomark 
Input
 A protein structure in PDB format. ----- .. class:: infomark 
Output
 Some or all of the following files: - A collection of PDB files, one for each pocket, each containing the atoms bordering the pocket. - A collection of PQR files, one for each pocket, each containing Voronoi vertices of the pocket. - A single PQR file containing all Voronoi vertices for all pockets. - A text file listing properties of all pockets detected."
toolshed.g2.bx.psu.edu/repos/iuc/goseq/goseq/1.50.0+galaxy0	".. class:: infomark 
What it does
 
Gene Ontology
 (GO) analysis is widely used to reduce complexity and highlight biological processes in genome-wide expression studies, but standard methods give biased results on RNA-seq data due to over-detection of differential expression for long and highly expressed transcripts. This tool provides methods for performing GO analysis of RNA-seq data, taking length bias into account. The methods and software used by goseq are equally applicable to other category based tests of RNA-seq data, such as KEGG
 pathway analysis. Options map closely to the excellent goseq manual_. ----- 
Inputs
 
Differentially expressed genes file
 goseq needs a tabular file containing information on differentially expressed genes. This should contain all genes assayed in the RNA-seq experiment. The file should have two columns with an optional header row. The first column should contain the Gene IDs, which must be unique within the file and not repeated. The second column should contain True or False. True means the gene should count as differentially expressed, False means it is not differentially expressed. You can use the ""Compute on rows"" tool to create a True / False column for your dataset. Example: =============== ===== ENSG00000236824 False ENSG00000162526 False ENSG00000090402 True ENSG00000169188 False ENSG00000124103 False =============== ===== 
Gene lengths file
 goseq needs information about the length of a gene to correct for potential length bias in differentially expressed genes using a Probability Weight Function (PWF). The PWF can be thought of, as a function which gives the probability that a gene will be differentially expressed, based on its length alone. The gene length file should have two columns with an optional header row. The first column should contain the Gene IDs, and the second column should contain the gene length in bp. If length data is unavailable for some genes, that entry should be set to NA. The goseq authors recommend using the gene lengths obtained from upstream summarization programs, such as 
featureCounts
, if provided. Alternatively, the 
Gene length and GC content
 tool can produce such a file. Example: =============== ===== ENSG00000236824 13458 ENSG00000162526 2191 ENSG00000090402 6138 ENSG00000169188 3245 ENSG00000124103 1137 =============== ===== 
Gene categories file
 This tool can get GO and KEGG categories for some genomes. The three GO categories are GO:MF (Molecular Function - molecular activities of gene products), GO:CC (Cellular Component - where gene products are active), GO:BP (Biological Process - pathways and larger processes made up of the activities of multiple gene products). If your genome is not available, you will also need a file describing the membership of genes in categories. The category file should have two columns with an optional header row. with Gene ID in the first column and category identifier in the second column. As the mapping between categories and genes is usually many-to-many, this table will usually have multiple rows with the same Gene ID and category identifier. Example: =============== =========== ENSG00000162526 GO\:0000003 ENSG00000198648 GO\:0000278 ENSG00000112312 GO\:0000278 ENSG00000174442 GO\:0000278 ENSG00000108953 GO\:0000278 =============== =========== ----- 
Outputs
 This tool outputs a tabular file containing a ranked list of gene categories, similar to below. The default output is the Wallenius method table. If the Sampling and/or Hypergeometric methods are also selected, additional tables are produced. Example: =========== =============== ================ ============ ========== ======================================== ========== =================== ==================== 
category
 
over_rep_pval
 
under_rep_pval
 
numDEInCat
 
numInCat
 
term
 
ontology
 
p_adjust_over_rep
 
p_adjust_under_rep
 ----------- --------------- ---------------- ------------ ---------- ---------------------------------------- ---------- ------------------- -------------------- GO\:0005576 0.000054 0.999975 56 142 extracellular region CC 0.394825 1 GO\:0005840 0.000143 0.999988 9 12 ribosome CC 0.394825 1 GO\:0044763 0.000252 0.999858 148 473 single-organism cellular process BP 0.394825 1 GO\:0044699 0.000279 0.999844 158 513 single-organism process BP 0.394825 1 GO\:0065010 0.000428 0.999808 43 108 extracellular membrane-bounded organelle CC 0.394825 1 GO\:0070062 0.000428 0.999808 43 108 extracellular exosome CC 0.394825 1 =========== =============== ================ ============ ========== ======================================== ========== =================== ==================== Optionally, this tool can also output: * a plot of the top 10 over-represented GO categories * some diagnostic plots * a tabular with the differentially expressed genes in categories (GO/KEGG terms) * an RData file ----- 
Method options
 3 methods, 
Wallenius
, 
Sampling
 and 
Hypergeometric
, can be used to calculate the p-values as follows. 
Wallenius
 approximates the true distribution of numbers of members of a category amongst DE genes by the Wallenius non-central hypergeometric distribution. This distribution assumes that within a category all genes have the same probability of being chosen. Therefore, this approximation works best when the range in probabilities obtained by the probability weighting function is small. This is the method used by default. 
Sampling
 uses random sampling to approximate the true distribution and uses it to calculate the p-values for over (and under) representation of categories. Although this is the most accurate method given a high enough value of sampling number, its use quickly becomes computationally prohibitive. It may sometimes be desirable to use random sampling to generate the null distribution for category membership. For example, to check consistency against results from the Wallenius approximation. This is easily accomplished by using the method option to additionally specify sampling and the number of samples to generate. 
Hypergeometric
 assumes there is no bias in power to detect differential expression at all and calculates the p-values using a standard hypergeometric distribution (no length bias correction is performed). Useful if you wish to test the effect of length bias on your results. Caution: Hypergeometric should NEVER be used for producing results for biological interpretation of RNA-seq data. If length bias is truly not present in your data, goseq will produce a nearly flat PWF plot, no length bias correction will be applied to your data, and all methods will produce the same results. ----- 
More Information
 In order to account for the length bias inherent to RNA-seq data when performing a GO analysis (or other category based tests), one cannot simply use the hypergeometric distribution as the null distribution for category membership, which is appropriate for data without DE length bias, such as microarray data. GO analysis of RNA-seq data requires the use of random sampling in order to generate a suitable null distribution for GO category membership and calculate each categories significance for over representation amongst DE genes. However, this random sampling is computationally expensive. In most cases, the Wallenius distribution can be used to approximate the true null distribution, without any significant loss in accuracy. The goseq package implements this approximation as its default option. The option to generate the null distribution using random sampling is also included as an option, but users should be aware that the default number of samples generated will not be enough to accurately call enrichment when there are a large number of go terms. Having established a null distribution, each category is then tested for over and under representation amongst the set of differentially expressed genes and the null is used to calculate a p-value for under and over representation. Having performed a GO analysis, you may now wish to interpret the results. If you wish to identify categories significantly enriched/unenriched below some p-value cutoff, it is necessary to first apply some kind of multiple hypothesis testing correction. For example, you can identify GO categories over enriched using a 0.05 FDR (p.adjust) cutoff [Benjamini and Hochberg, 1995]. Unless you are a machine, GO and KEGG category identifiers are probably not very meaningful to you. Information about each identifier can be obtained from the 
Gene Ontology
 and KEGG
 websites. .. _manual: https://bioconductor.org/packages/release/bioc/vignettes/goseq/inst/doc/goseq.pdf .. _Gene Ontology: http://www.geneontology.org .. _KEGG: http://www.genome.jp/kegg"
toolshed.g2.bx.psu.edu/repos/bgruening/rdock_rbcavity/rdock_rbcavity/2013.1-0+galaxy0	".. class:: infomark This tool generates the cavity definition for rDock docking (the receptor.as file) using the rbcavity program. See http://rdock.sourceforge.net/ for more details about rDock and associated programs. Only a subset of the parameters are currently exposed. Read the rDock docs for a full understanding. ----- .. class:: infomark 
Inputs
 1. The protein receptor to dock into as a file in Mol2 format. 2. A reference ligand used to define the location of the active site in Molfile or SDF format. 3. Various parameters for the mapper and cavity generation. Sensible defaults are provided for all. ----- .. class:: infomark 
Outputs
 An active site definition binary file (receptor.as) that is needed by the rDock docking program to guide the docking."
toolshed.g2.bx.psu.edu/repos/bgruening/rdock_rbdock/rdock_rbdock/2013.1-0+galaxy0	".. class:: infomark This tool performs protein-ligand docking using the rDock program. See http://rdock.sourceforge.net/ for more details about rDock and associated programs. ----- .. class:: infomark 
Inputs
 1. The protein receptor to dock into as a file in Mol2 format. 2. The active site definition as a file as generated by the rbcavity tool. 3. A set of ligands (collection or single file) to dock as a file in SDF format. 4. The number of docking poses to generate (integer). 5. The number of best scoring dockings to keep (integer). 6. The max score that is allowed. Poses with scores greater than this are excluded (float, optional). 7. The max normalised score (the score normalised by the number of heavy atoms) that is allowed. Poses with normalised scores greater than this are excluded (float, optional). 8. Optionally generate the name field in the ligands SDF (this is the first line in the entry). This field must be present and be unique for sorting and filtering to work. If your results contain only one record then the name field is probably absent and must be generated. You will need to perform some test dockings to establish suitable values for the score filters. The score is a number with lower values being better. Values can be negative. ----- .. class:: infomark 
Outputs
 An SDF file is produced as output. The binding affinity scores are contained within the SDF file.:: 1-pyrimethamine rDOCK(R) 3D libRbt.so/2013.1/901 2013/11/27 21 22 0 0 0 0 0 0 0 0999 V2000 -5.1897 17.8912 17.9590 N 0 0 0 0 0 0 -3.9121 17.9973 18.3210 C 0 0 0 0 0 0 -3.2404 19.1465 18.3804 N 0 0 0 0 0 0 -3.8989 20.2829 18.0453 C 0 0 0 0 0 0 -5.2389 20.2802 17.6553 C 0 0 0 0 0 0 -5.8448 19.0235 17.6464 C 0 0 0 0 0 0 -5.9601 21.5065 17.2850 C 0 0 0 0 0 0 -6.2108 22.5074 18.2382 C 0 0 0 0 0 0 -6.8903 23.6771 17.8851 C 0 0 0 0 0 0 -7.3267 23.8556 16.5746 C 0 0 0 0 0 0 -7.0903 22.8744 15.6151 C 0 0 0 0 0 0 -6.4107 21.7051 15.9695 C 0 0 0 0 0 0 -3.2455 16.8582 18.6507 N 0 0 0 0 0 0 -7.1550 18.8446 17.2393 N 0 0 0 0 0 0 -8.1626 25.2957 16.1391 Cl 0 0 0 0 0 0 -2.9891 22.1828 19.5033 C 0 0 0 0 0 0 -3.1112 21.5771 18.1096 C 0 0 0 0 0 0 -2.2766 16.9101 18.9273 H 0 0 0 0 0 0 -3.7237 15.9703 18.6154 H 0 0 0 0 0 0 -7.8809 19.3992 17.6807 H 0 0 0 0 0 0 -7.4159 17.8951 16.9940 H 0 0 0 0 0 0 1 2 2 0 0 0 1 6 1 0 0 0 2 3 1 0 0 0 2 13 1 0 0 0 3 4 2 0 0 0 4 5 1 0 0 0 4 17 1 0 0 0 5 6 2 0 0 0 5 7 1 0 0 0 6 14 1 0 0 0 7 8 2 0 0 0 7 12 1 0 0 0 8 9 1 0 0 0 9 10 2 0 0 0 10 11 1 0 0 0 10 15 1 0 0 0 11 12 2 0 0 0 13 18 1 0 0 0 13 19 1 0 0 0 14 20 1 0 0 0 14 21 1 0 0 0 16 17 1 0 0 0 M END > <CHROM.0> -177.71086620,1.45027861,170.39044546,46.02877151,68.76956623,70.55425150 > <CHROM.1> -81.34718191,-65.90186149,129.45748660,-5.61305786,21.23281353,17.50152835 0.96119776,0.49809360,-3.12917831 > <Rbt.Current_Directory> /home/timbo/github/im/docking-validation/targets/dhfr/expts/vs-simple-rdock > <Rbt.Executable> rbdock ($Id: //depot/dev/client3/rdock/2013.1/src/exe/rbdock.cxx#4 $) > <Rbt.Library> libRbt.so (2013.1, Build901 2013/11/27) > <Rbt.Parameter_File> /rDock_2013.1/data/scripts/dock.prm > <Rbt.Receptor> receptor.prm > <SCORE> 0.445364 > <SCORE.INTER> 8.4 > <SCORE.INTER.CONST> 1 > <SCORE.INTER.POLAR> 0 > <SCORE.INTER.REPUL> 0 > <SCORE.INTER.ROT> 3 > <SCORE.INTER.VDW> 0 > <SCORE.INTER.norm> 0.494118 > <SCORE.INTRA> -1.38672 > <SCORE.INTRA.DIHEDRAL> -0.818539 > <SCORE.INTRA.DIHEDRAL.0> 6.01924 > <SCORE.INTRA.POLAR> 0 > <SCORE.INTRA.POLAR.0> 0 > <SCORE.INTRA.REPUL> 0 > <SCORE.INTRA.REPUL.0> 0 > <SCORE.INTRA.VDW> -0.977448 > <SCORE.INTRA.VDW.0> -1.0079 > <SCORE.INTRA.norm> -0.0815716 > <SCORE.RESTR> > <SCORE.RESTR.norm> 0 > <SCORE.SYSTEM> -6.56792 > <SCORE.SYSTEM.CONST> 0 > <SCORE.SYSTEM.DIHEDRAL> 1.50415 > <SCORE.SYSTEM.POLAR> -2.3289 > <SCORE.SYSTEM.VDW> 0.59827 > <SCORE.SYSTEM.norm> -0.386348 > <SCORE.heavy> 17 > <SCORE.norm> 0.0261979 $$$$"
toolshed.g2.bx.psu.edu/repos/bgruening/rdock_sort_filter/rdock_sort_filter/2013.1-0+galaxy0	".. class:: infomark This tool sorts and filters SD files using the sdsort and sdfilter tools from the rDock suite. See http://rdock.sourceforge.net/ for more details about rDock and associated programs. The expected use is for filtering and sorting virtual screening results such as docking. ----- .. class:: infomark 
Inputs
 An SD-file, together with names of fields to filter, sort and group records by, and the number of records to appear in the output. An optional filter can be specified that is first applied to the records. This filter (the 'filter' parameter) must be specified as required by the 'sdfilter' application (see http://rdock.sourceforge.net/wp-content/uploads/2015/08/rDock_User_Guide.pdf) which is a Perl expression. As an example, if your SDF has a field name 'SCORE' which has numeric values then a valid filter expression would be '$SCORE > 90' (note the $ symbol). If you require to use multiple filters then you can combine them in a single expression like this: '$A < 5 and $B <7', or '$A < 5 or $B <7' The sorting is then performed on groups of molecules, with the groups being identified by a field in the SD-file (the 'name_field' parameter). Records from a group MUST be sequential in the input file. If 'name_field' is not specified then this grouping and sorting step is skipped. Sorting is performed by the rDock 'sdsort' application. The records within each group are sorted by the value of a field in the SD-file (the 'sort_field' parameter) and you can specify ascending or descending order (the 'descending' parameter). Then a number of top scoring (the 'top' parameter, typically having a value of 1) are retained. Finally, if the 'global_sort' parameter is set to 'True' then the all the records remaining are sorted according to the 'sort_field' and 'descending' parameters. Note: this step can use lots of memory if the files are very big. ----- .. class:: infomark 
Outputs
 An SD-file, containing molecules filtered and sorted according to the parameters."
toolshed.g2.bx.psu.edu/repos/bgruening/rxdock_rbcavity/rxdock_rbcavity/2013.1.1_148c5bd1+galaxy0	".. class:: infomark This tool generates the cavity definition for rxDock docking (the receptor.as file) using the rbcavity program. See https://www.rxdock.org for more details about rxDock and associated programs. Only a subset of the parameters are currently exposed. Read the rxDock docs for a full understanding. ----- .. class:: infomark 
Inputs
 1. The protein receptor to dock into as a file in Mol2 format. 2. A reference ligand used to define the location of the active site in Molfile or SDF format. 3. Various parameters for the mapper and cavity generation. Sensible defaults are provided for all. ----- .. class:: infomark 
Outputs
 An active site definition binary file (receptor.as) that is needed by the rxDock docking program to guide the docking."
toolshed.g2.bx.psu.edu/repos/bgruening/rxdock_rbdock/rxdock_rbdock/2013.1.1_148c5bd1+galaxy0	".. class:: infomark This tool performs protein-ligand docking using the rxDock program. See https://www.rxdock.org for more details about rxDock and associated programs. ----- .. class:: infomark 
Inputs
 1. The protein receptor to dock into as a file in Mol2 format. 2. The active site definition as a file as generated by the rbcavity tool. 3. A set of ligands (collection or single file) to dock as a file in SDF format. 4. The number of docking poses to generate (integer). 5. The number of best scoring dockings to keep (integer). 6. The max score that is allowed. Poses with scores greater than this are excluded (float, optional). 7. The max normalised score (the score normalised by the number of heavy atoms) that is allowed. Poses with normalised scores greater than this are excluded (float, optional). 8. Optionally generate the name field in the ligands SDF (this is the first line in the entry). This field must be present and be unique for sorting and filtering to work. If your results contain only one record then the name field is probably absent and must be generated. You will need to perform some test dockings to establish suitable values for the score filters. The score is a number with lower values being better. Values can be negative. ----- .. class:: infomark 
Outputs
 An SDF file is produced as output. The binding affinity scores are contained within the SDF file.:: 1-pyrimethamine rDOCK(R) 3D libRbt.so/2013.1/901 2013/11/27 21 22 0 0 0 0 0 0 0 0999 V2000 -5.1897 17.8912 17.9590 N 0 0 0 0 0 0 -3.9121 17.9973 18.3210 C 0 0 0 0 0 0 -3.2404 19.1465 18.3804 N 0 0 0 0 0 0 -3.8989 20.2829 18.0453 C 0 0 0 0 0 0 -5.2389 20.2802 17.6553 C 0 0 0 0 0 0 -5.8448 19.0235 17.6464 C 0 0 0 0 0 0 -5.9601 21.5065 17.2850 C 0 0 0 0 0 0 -6.2108 22.5074 18.2382 C 0 0 0 0 0 0 -6.8903 23.6771 17.8851 C 0 0 0 0 0 0 -7.3267 23.8556 16.5746 C 0 0 0 0 0 0 -7.0903 22.8744 15.6151 C 0 0 0 0 0 0 -6.4107 21.7051 15.9695 C 0 0 0 0 0 0 -3.2455 16.8582 18.6507 N 0 0 0 0 0 0 -7.1550 18.8446 17.2393 N 0 0 0 0 0 0 -8.1626 25.2957 16.1391 Cl 0 0 0 0 0 0 -2.9891 22.1828 19.5033 C 0 0 0 0 0 0 -3.1112 21.5771 18.1096 C 0 0 0 0 0 0 -2.2766 16.9101 18.9273 H 0 0 0 0 0 0 -3.7237 15.9703 18.6154 H 0 0 0 0 0 0 -7.8809 19.3992 17.6807 H 0 0 0 0 0 0 -7.4159 17.8951 16.9940 H 0 0 0 0 0 0 1 2 2 0 0 0 1 6 1 0 0 0 2 3 1 0 0 0 2 13 1 0 0 0 3 4 2 0 0 0 4 5 1 0 0 0 4 17 1 0 0 0 5 6 2 0 0 0 5 7 1 0 0 0 6 14 1 0 0 0 7 8 2 0 0 0 7 12 1 0 0 0 8 9 1 0 0 0 9 10 2 0 0 0 10 11 1 0 0 0 10 15 1 0 0 0 11 12 2 0 0 0 13 18 1 0 0 0 13 19 1 0 0 0 14 20 1 0 0 0 14 21 1 0 0 0 16 17 1 0 0 0 M END > <CHROM.0> -177.71086620,1.45027861,170.39044546,46.02877151,68.76956623,70.55425150 > <CHROM.1> -81.34718191,-65.90186149,129.45748660,-5.61305786,21.23281353,17.50152835 0.96119776,0.49809360,-3.12917831 > <Rbt.Current_Directory> /home/timbo/github/im/docking-validation/targets/dhfr/expts/vs-simple-rdock > <Rbt.Executable> rbdock ($Id: //depot/dev/client3/rdock/2013.1/src/exe/rbdock.cxx#4 $) > <Rbt.Library> libRbt.so (2013.1, Build901 2013/11/27) > <Rbt.Parameter_File> /rDock_2013.1/data/scripts/dock.prm > <Rbt.Receptor> receptor.prm > <SCORE> 0.445364 > <SCORE.INTER> 8.4 > <SCORE.INTER.CONST> 1 > <SCORE.INTER.POLAR> 0 > <SCORE.INTER.REPUL> 0 > <SCORE.INTER.ROT> 3 > <SCORE.INTER.VDW> 0 > <SCORE.INTER.norm> 0.494118 > <SCORE.INTRA> -1.38672 > <SCORE.INTRA.DIHEDRAL> -0.818539 > <SCORE.INTRA.DIHEDRAL.0> 6.01924 > <SCORE.INTRA.POLAR> 0 > <SCORE.INTRA.POLAR.0> 0 > <SCORE.INTRA.REPUL> 0 > <SCORE.INTRA.REPUL.0> 0 > <SCORE.INTRA.VDW> -0.977448 > <SCORE.INTRA.VDW.0> -1.0079 > <SCORE.INTRA.norm> -0.0815716 > <SCORE.RESTR> > <SCORE.RESTR.norm> 0 > <SCORE.SYSTEM> -6.56792 > <SCORE.SYSTEM.CONST> 0 > <SCORE.SYSTEM.DIHEDRAL> 1.50415 > <SCORE.SYSTEM.POLAR> -2.3289 > <SCORE.SYSTEM.VDW> 0.59827 > <SCORE.SYSTEM.norm> -0.386348 > <SCORE.heavy> 17 > <SCORE.norm> 0.0261979 $$$$"
toolshed.g2.bx.psu.edu/repos/earlhaminst/smina/smina/1.0	"smina &lt;https://sourceforge.net/projects/smina/&gt;
 is a fork of 
AutoDock Vina &lt;https://autodock-vina.readthedocs.io&gt;
 that is customized to better support scoring function development and high-performance energy minimization. smina is maintained by David Koes at the University of Pittsburgh and is not directly affiliated with the AutoDock project."
toolshed.g2.bx.psu.edu/repos/sauria/hifive/hifive/0.1.0	HiFive is a tool for handling, normalizing, and plotting HiC and 5C chromatin interaction data. It has numerous normalization approaches built in with a variety of options, allowing for fine-scale control and data processing. HiFive is broken down into a series of selectable commands, each with a HiC or 5C version. COMMANDS -------- Complete HiC analysis / Complete 5C analysis - this command creates a genome partion file, loads data into a dataset, creates a project file and performs data normalization on that project file. Create HiC fend set / Create 5C fragment set - this command takes a bed file containing restriction fragment information and creates a HiFive partition file that will be used for downstream data processing. Create HiC data set / Create 5C data set - this command loads data from BAM file pairs or a variety of other file formats, partitions reads according to the information in the HiFive genome partition file and creates a HiFive data file. Create HiC project / Create 5C project - this command creates a HiFive project, associates a specific data set with it, constructs a distance-dependence function and filters fragments based on coverage. Normalize HiC project / Normalize 5C project - this command provides a selection of normalization algorithms and finds correction values for data normalization. Create HiC heatmap set / Create 5C heatmap set - this command creates a set of heatmaps, one per chromosome and, if selected, one per chromosome pair (trans), in a compact HDF5 format. Heatmaps can also be plotted at the time of creation. Extract HiC interval / Extract 5C interval - this command returns a genomic interval files with data from a specified region. This data may also be plotted at the tie of extraction. Create HiC multi-resolution heatmap - this command returns a multi-resolution heatmap file with data heatmapped across resolutions from the smallest to largest specified binsizes in 2X steps. HiC genomic partitioning - fend file ---------------------------------------- A bed file containing either restriction enzyme cutopoints or fragment bounds is converted into an hdf5-type fragment file of fragment characteristics. In addition to coordinates, strand, and chromosome information, additional columns can be included containing other fragment characteristics, such as GC content. If additional columns are included, they must be labeled in the header with a label containing no spaces or commas. These names can be used with the binning algorithm to include the fragment characteristic in the model to be learned. Additional characteristics should be comma-separated pairs of values corresponding to the upstream and downstream sides of the cutsite or ends of the fragment, depending on the whether the bed file contains cutsites or fragment coordinates, respectively. HiC data -------- Reads are paired with the specified fend file, creating a HiFive dataset object. Data can be a series of paired-end bam files, a tabular format list of paired genomic positions (chromosome1 coordinate1 strand1 chromosome2 coordinate2 strand2), or a HiCPipe-style mat-formatted list of fend-pairs and observed read counts. HiC project ----------- Fends are filtered in an iterative manner using the minimum interaction cutoff and interaction size parameters specified to ensure that all valid fends have at least the minimum number of interactions with other valid fends. Subsequently, a distance dependence approximation curve is calculated piecewise using the number of bins specified. The first bin encompasses all interactions less than or equal to the minimum bin cutoff value. The remaining bins are evenly sized between log(minimum cutoff) and log(max possible interaction size). HiC normalization ----------------- Corrections values are learned for either each valid fend, ranges of fend characteristics, or both. The 'probability' and 'express' algorithms learn correction values associated with each fend while the 'binning' algorithm learns fend characteristic corrections. These can be chained together in either order to produce more robust corrections. Using the probability algorithm, observation of counts are assumed to be distributed according to a binomial distribution with an observation probability for each interaction equal to the product of the distance-dependence signal and the two fend correction parameters. Using the probability algorithm, learning is done using a backtracking line gradient descent approach. Learning proceeds for up to the maximum number of iterations but is terminated early if all of the absolute gradient values fall below the cutoff threshold. At each step, the learning rate is scaled down by the step value if the current learning rate does not produce sufficient improvement as measured by the Arjimo criterion. The express algorithm is a variant of matrix balancing and approximates the corrections through an iterative norm-2 adjustment to given all fragments a mean ratio of one for valid counts versus signal predicted from distance-dependence. This can be done using intra-regional interactions, inter-regional interactions, or all interactions. The binning algorithm divides each model parameter into some number of bins and based on a binomial distribution, correction values for each bin are learned, maximizing the log-likelihood of the data. Model parameters can be the fend lengths ('len'), fend GC content ('gc'), and any other characteristics passed as additional columns (with header labels) in the bed file used to create the HiFive fend file. Each parameter has a number of bins specified to divide it into and can be partitioned according to its type to contain approximately equal numbers of fends ('even'), or to cover equal portions of the range of parameter values ('fixed'). In addition, parameter types can include the '-const' suffix to denote a parameter that should not be optimized after seeding. HiC multi-resolution heatmapping -------------------------------- Multi-resolution heatmapping (MRH) allows multiple levels of resolution to be stored and accessed simultaneously using an intelligent binning scheme that only accepts bin with a number of observed reads meeting the minimum observation threshold. MRH files can be interactively explored through the MRH plugin in Galaxy. 5C genomic partitioning - fragment file --------------------------------------- A bed file containing targeted restriction enzyme fragment boundaries is converted into an hdf5-type fragment file of fragment characteristics. In addition to coordinates, strand, and chromosome information, additional columns can be included containing other fragment characteristics, such as GC content. If additional columns are included, they must be labeled in the header with a label containing no spaces or commas. These names can be used with the binning algorithm to include the fragment characteristic in the model to be learned. 5C data ------- Reads are loaded and paired with the specified fragment file, creating a HiFive dataset object. Data can be a series of paired-end bam files or a tabular format list of paired fragments and their observed read count (fragment1 fragment2 count). 5C project ---------- Fragments are filtered in an iterative manner using the minimum interaction cutoff and interaction size parameters specified to ensure that all valid fragments have at least the minimum number of interactions with other valid fragments. Subsequently, a distance dependence approximation line is calculated using a regression line to approximate the linear relationship between log(# reads) and log(distance). 5C normalization ---------------- Corrections values are learned for either each valid fragment, ranges of fragment characteristics, or both. The 'probability' and 'express' algorithms learn correction values associated with each fragment while the 'binning' algorithm learns fragment characteristic corrections. These can be chained together in either order to produce more robust corrections. The probability algorithm assumes non-zero counts to distributed according to a log-normal distribution with each interaction having a mean equal to the distance-depedence predicted signal times each of the interaction fragment correction parameters and a universal sigma value. Using the probability algorithm, learning is done using a backtracking line gradient descent approach. Learning proceeds for up to the maximum number of iterations but is terminated early if all of the absolute gradient values fall below the cutoff threshold. At each step, the learning rate is scaled down by the step value if the current learning rate does not produce sufficient improvement as measured by the Arjimo criterion. The express algorithm is a variant of matrix balancing and approximates the corrections through an iterative norm-2 adjustment to given all fragments a mean ratio of one for valid counts versus predicted signal from distance-dependence. This can be done using intra-regional interactions, inter-regional interactions, or all interactions. The binning algorithm divides each model parameter into some number of bins and based on a log-normal distribution, correction values for each bin are learned, maximizing the log-likelihood of the data. Model parameters can be the fragment lengths ('len') and any other characteristics passed as additional columns (with header labels) in the bed file used to create the HiFive fragment file. Each parameter has a number of bins specified to divide it into and can be partitioned according to its type to contain approximately equal numbers of fragments ('even'), or to cover equal portions of the range of parameter values ('fixed'). In addition, parameter types can include the '-const' suffix to denote a parameter that should not be optimized after seeding.
toolshed.g2.bx.psu.edu/repos/climate/ctsm_fates/ctsm_fates/2.0.1.1	"The Functionally Assembled Terrestrial Ecosystem Simulator (FATES)
 ========================================================================== This tool create and run CTSM-FATES EMERALD experiments from fates-emerald release. This version of FATES is maintained by the University of Oslo. Currently available resolution are as following: SeedClim Sites: ~~~~~~~~~~~~~~~ - ALP1,61.0243N,8.12343E - ALP2,60.8231N,7.27596E - ALP3,60.8328N,7.17561E - ALP4,60.9335N,6.41504E - SUB1,60.8203N,8.70466E - SUB2,60.8760N,7.17666E - SUB3,61.0866N,6.63028E - SUB4,60.5445N,6.51468E - BOR1,61.0355N,9.07876E - BOR2,60.8803N,7.16982E - BOR3,60.6652N,6.33738E - BOR4,60.6901N,5.96487E LandPress Sites: ~~~~~~~~~~~~~~~~ - LYG,60.70084N,5.092566E - BUO,65.83677N,12.224506E - HAV,64.779N,11.2193E - SKO,65.79602N,12.219299E Three-D Sites: ~~~~~~~~~~~~~~ - VIKE,60.88019N,7.16990E - JOAS,60.86183N,7.16800E - LIAH,60.85994N,7.19504E Finnmark Site: ~~~~~~~~~~~~~~ - FINN,69.341088N,25.293524E"
toolshed.g2.bx.psu.edu/repos/climate/c3s/c3s/0.3.0	"Copernicus Climate Data Store (C3S)
 ======================================= This tool is a wrapper to retrieve data from the Copernicus Climate Data Store. - It allows to retrieve data from the Copernicus climate Data Store. - Any user willing to use this tool needs to 
create a new account &lt;https://cds.climate.copernicus.eu/&gt;
. - Set your CDS API Key via: User -> Preferences -> Manage Information"" - Documentation on where to get the CDS API key can be found 
here &lt;https://cds.climate.copernicus.eu/how-to-api&gt;
. - Compose your request directly on C3S and extract the relevant information, which should be put in the input field ""Request"" or saved in a file. The format should be, for example:: import cdsapi c = cdsapi.Client() c.retrieve( 'reanalysis-era5-single-levels-monthly-means', { 'data_format': 'netcdf', 'product_type': 'monthly_averaged_reanalysis', 'variable': '2m_temperature', 'year': '2020', 'month': '12', 'time': '00:00', 'area': [60, 10, 59.5, 10.5], }, 'download.nc') - Be aware that for being able to download dataset from C3S, users also need to agree to licensing terms for each dataset of interest on the C3S website. License: ~~~~~~~~ Generated using Copernicus Climate Change Service information [2024] Neither the European Commission nor ECMWF is responsible for any use that may be made of the Copernicus information or data it contains."
toolshed.g2.bx.psu.edu/repos/climate/climate_stripes/climate_stripes/1.0.2	"Climate stripes
 ========================================= This tool generate stripes from timeseries and is often used to generate warming stripes. .. class:: infomark The wrappers aims at creating stripes from a timeseries. The input file must be in tabular format and must contain at least one column that is used for creating stripes. By default, no title and no axis are plotted. An additional column can be specified for date/time and its date and time format must then be specified with an additional options. 
What it does
 ---------------- This tools creates an image (png format) corresponding to the visualization of a timeseries as stripes (see https://www.climate-lab-book.ac.uk/2018/warming-stripes/). By default, the colormap is 
RdBu_r
 and no axis nor title are plotted. These settings can be changed in 
Advanced settings
."
toolshed.g2.bx.psu.edu/repos/gregory-minevich/check_snpeff_candidates/check_snpeff_candidates/1.0.0	"What it does:
 Indicates on a SnpEff output file which genes are found in a candidate list by comparing Gene IDs. For a description of the snpEff variant annotation and effect prediction tool: http://snpeff.sourceforge.net ------ 
Input:
 The candidate list should be in a tabular format with two columns: Gene ID and Gene Description (e.g. C55B7.12 and transcription_factor). The file should contain no headers. Useful candidate lists (e.g. transcription factors, genes expressed in neurons, transgene silencers, chromatin factors) are available on the CloudMap Galaxy page: http://usegalaxy.org/cloudmap ------ 
Citation:
 This tool is part of the CloudMap pipeline for analysis of mutant genome sequences. For further details, please see 
Gregory Minevich, Danny S. Park, Daniel Blankenberg, Richard J. Poole, and Oliver Hobert. CloudMap: A Cloud-based Pipeline for Analysis of Mutant Genome Sequences. (Genetics 2012 In Press)
 .. 
: http://biochemistry.hs.columbia.edu/labs/hobert/literature.html Correspondence to gm2123@columbia.edu (G.M.) or or38@columbia.edu (O.H.)"
toolshed.g2.bx.psu.edu/repos/gregory-minevich/ems_variant_density_mapping/ems_variant_density_mapping/1.0.0	"What it does:
 This tool is part of the CloudMap pipeline for analysis of mutant genome sequences. For further details, please see 
Gregory Minevich, Danny S. Park, Daniel Blankenberg, Richard J. Poole, and Oliver Hobert. CloudMap: A Cloud-based Pipeline for Analysis of Mutant Genome Sequences. (Genetics 2012 In Press)
 .. 
: http://biochemistry.hs.columbia.edu/labs/hobert/literature.html CloudMap workflows, shared histories and reference datasets are available at the 
CloudMap Galaxy page
 .. 
: http://usegalaxy.org/cloudmap Following the approach detailed in Zuryn et al., Genetics 2010, this tool plots histograms of variant density in a mutant C.elegans strain that has been backcrossed to its (pre-mutagenesis) starting strain. Common (i.e. non-phenotype causing) variants present in multiple WGS strains 
with the same background
 should first be subtracted using the GATK tool 
Select Variants
. Sample output where LG III shows linkage to the causal mutation is shown below. In this example, common variants from another strain have been subtracted and remaining variants have been filtered for most common EMS-induced mutations i.e. G/C --> A/T): .. image:: http://biochemistry.hs.columbia.edu/labs/hobert/CloudMap/EMS_Variant_Density_750px.png The experimental approach is detailed in Figure 1a from Zuryn et al., Genetics 2010: .. image:: http://biochemistry.hs.columbia.edu/labs/hobert/CloudMap/Zuryn_2010_Genetics_Fig1a.pdf Subtracting common (non-phenotype causing) variants from more whole genome sequenced strains (using GATK Tools 
Select Variants
) will result in less noise and a tighter mapping region. Additional backcrosses will also result in a smaller mapping region. ------ 
Settings:
 .. class:: infomark Supported colors for data points and loess regression line: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf http://research.stowers-institute.org/efg/R/Color/Chart/ColorChart.pdf .. class:: warningmark This tool requires that the statistical programming environment R has been installed on the system hosting Galaxy (http://www.r-project.org/). If you are accessing this tool on Galaxy via the Cloud, this does not apply to you. ------ 
Citation:
 This tool is part of the CloudMap package from the Hobert Lab. If you use this tool, please cite 
Gregory Minevich, Danny S. Park, Daniel Blankenberg, Richard J. Poole, and Oliver Hobert. CloudMap: A Cloud-based Pipeline for Analysis of Mutant Genome Sequences. (Genetics 2012 In Press)
 .. 
: http://biochemistry.hs.columbia.edu/labs/hobert/literature.html Correspondence to gm2123@columbia.edu (G.M.) or or38@columbia.edu (O.H.)"
toolshed.g2.bx.psu.edu/repos/gregory-minevich/snp_mapping_using_wgs/snp_mapping_using_wgs/1.0.0	"What it does:
 This tool is part of the CloudMap pipeline for analysis of mutant genome sequences. For further details, please see 
Gregory Minevich, Danny S. Park, Daniel Blankenberg, Richard J. Poole and Oliver Hobert. CloudMap: A Cloud-based Pipeline for Analysis of Mutant Genome Sequences. (Genetics 2012 In Press)
 .. 
: http://hobertlab.org/original-research/ CloudMap workflows, shared histories and reference datasets are available at the 
CloudMap Galaxy page
 .. 
: http://usegalaxy.org/cloudmap CloudMap video user guides and Frequently Asked Questions (FAQs) are available at the 
Hobert lab website
 .. 
: http://hobertlab.org/cloudmap This tool improves upon, and automates, the method described in Doitsidou et al., PLoS One 2010 for mapping causal mutations using whole genome sequencing data. Sample CloudMap output for a linked chromosome: .. image:: http://www.hobertlab.org/CloudMap/Linked_LG_500px.png The polymorphic Hawaiian strain CB4856 is used as a mapping strain in most cases but in principle any sequenced nematode strain that is significantly different from the mutant strain can be used for mapping. The tool plots the ratio of mapping strain (Hawaiian)/mutant strain (N2) nucleotides at all SNP positions, reflecting the number of recombinants in the sequenced pool of animals. Chromosomes which contain regions of linkage to the causal mutation will have regions where the ratio of mapping strain (Hawaiian)/total reads will be equal to 0. The scatter plots for such linked regions will have a high number of data points lying exactly on the X axis. A loess regression line is plotted through all the points on a given chromosome giving further accuracy to the linked region. Each scatter plot has a corresponding frequency plot that displays regions of linked chromosomes where pure parental (mutant strain) alleles are concentrated. 1Mb bins for the 0 ratio SNP positions are colored gray by default and .5Mb bins are colored in red. By default, frequency plots of pure parental alleles are normalized to remove false linkage caused by previously described (Seidel et al. 2008) patterns of genetic incompatibility between Bristol and Hawaiian strains. This normalization can be turned off via a checkbox input form setting. The experimental design required to generate data for the plots is described in the CloudMap paper (Fig.6A): .. image:: http://www.hobertlab.org/CloudMap/Doitsidou_2010_PLoS_Fig.1_500px.png ------ 
Input:
 This tool accepts as input a single VCF file containing reference (e.g. Bristol) and alternate (e.g. Hawaiian) mapping strain alleles calls at each of the mapping strain variant positions (e.g. 112,000 Hawaiian SNPs) in the pooled mutant sample. This input VCF is generated at an earlier analysis step by running the GATK Unified Genotyper on a BAM alignment file of the pooled mutant sample with a provided reference file of mapping strain variants (e.g. Hawaiian SNPs) in VCF format. The reader is referred to the user guide and online video for direction on this procedure. Default GATK Unified Genotyper parameters for mapping quality, base quality and coverage at each SNP position typically yield good results, though users may experiment with adjusting these parameters. In our testing, low threshold filtering on base pair quality (default settings) has been useful in improving accuracy of plots while high threshold filtering on coverage has skewed plot accuracy. The required VCF of mapping strain (e.g. Hawaiian) SNPs is a reference file that contains mapping strain SNP positions and reference base pairs at each position. It is available in the 
CloudMap Shared Data library
 .. 
: http://usegalaxy.org/library You may also make your own VCF of mapping strain variant positions following the steps described in the CloudMap paper. The CloudMap Hawaiian Variant Mapping with WGS Data tool supports data from any organism that has been crossed to a mapping strain for which variant information is available. C. elegans and Arabidopsis are natively supported. For all other organisms, users must provide a simple tab-delimited configuration file containing chromosome numbers and respective lengths (example configuration files for most major organisms provided at http://usegalaxy.org/cloudmap). Additional files required for other organisms are the same as described for C. elegans: a VCF file consisting of pooled F2 mutant progeny sequencing data, and a VCF file of the mapping strain variants. 
Output:
 The tool also provides a tabular output file that contains a count of the number of reference and alternate variants at each mapping strain variant position as well as the ratio of mapping strain (e.g. Hawaiian)/alternate SNPs. The position of each mapping strain SNP in map units and physical coordinates is also provided in the output file. ------ 
Settings:
 .. class:: infomark Information on loess regression and the loess span parameter: http://en.wikipedia.org/wiki/Local_regression .. class:: infomark Based on our testing, we've settled on .1 as a loess span default. Larger values result in smoothing of the line to reflect trends at a more macro level. Smaller values result in loess lines that more closely reflect local data fluctuations. .. class:: infomark Supported colors for data points and loess regression line: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf http://research.stowers-institute.org/efg/R/Color/Chart/ColorChart.pdf .. class:: warningmark This tool requires that the statistical programming environment R has been installed on the system hosting Galaxy (http://www.r-project.org/). If you are running this tool on Galaxy via the Cloud, this does not apply to you. ------ 
Citation:
 This tool is part of the CloudMap package from the Hobert Lab. If you use this tool, please cite 
Gregory Minevich, Danny S. Park, Daniel Blankenberg, Richard J. Poole, and Oliver Hobert. CloudMap: A Cloud-based Pipeline for Analysis of Mutant Genome Sequences. (Genetics 2012 In Press)
 .. 
: http://hobertlab.org/cloudmap Correspondence to gm2123@columbia.edu (Gregory Minevich) or r.poole@ucl.ac.uk (Richard J. Poole) or or38@columbia.edu (Oliver Hobert)"
toolshed.g2.bx.psu.edu/repos/gregory-minevich/cloudmap_variant_discovery_mapping/cloudmap_variant_discovery_mapping/1.0.0	"What it does:
 This tool is part of the CloudMap pipeline for analysis of mutant genome sequences. For further details, please see 
Gregory Minevich, Danny S. Park, Daniel Blankenberg, Richard J. Poole and Oliver Hobert. CloudMap: A Cloud-based Pipeline for Analysis of Mutant Genome Sequences. (Genetics 2012 In Press)
 .. 
: http://hobertlab.org/original-research/ CloudMap workflows, shared histories and reference datasets are available at the 
CloudMap Galaxy page
 .. 
: http://usegalaxy.org/cloudmap CloudMap video user guides and Frequently Asked Questions (FAQs) are available at the 
Hobert lab website
 .. 
: http://hobertlab.org/cloudmap Although Hawaiian Variant Mapping is the preferred method for mapping causal mutations in whole genome sequenced strains (see CloudMap Hawaiian Variant Mapping with WGS tool), there remain certain scenarios where alternate mapping approaches are useful. For instance, introducing tens of thousands of Hawaiian variants into a mutant strain may not be desirable for individuals concerned with the possibility that some of these Hawaiian variants may act as modifiers of a given phenotype. Behavioral mutants may be especially vulnerable in this regard. Furthermore, in the case of suppressor screens or other screens that have been performed in a mutant background, it is tedious to recover both the suppressor variant and the starting mutation when picking the F2 progeny required for the Hawaiian Variant Mapping technique. In these scenarios, it is useful to not have to rely on a polymorphic mapping strain like the Hawaiian strain. A recent study in plants (Abe et al. 2012), uses EMS-induced variants and bulk segregant analysis to map a phenotype-causing mutation. We have developed a similar method, which we call “Variant Discovery Mapping”. Our method makes use of background variants in addition to EMS-induced variants (including indels as well as SNPs), and also uses the bulk segregant approach. 
The conceptual strategy of variant discovery mapping is to perform in silico bulk segregant linkage analysis using variants that are already present in the mutant strain of interest, rather than examining those introduced by a cross to a polymorphic strain.
 Any individual mutant strain will contain a certain number of homozygous variants compared to the reference genome. These homozygous variants are of two types: 1) those directly induced during mutagenesis (one or more of which are responsible for the mutant phenotype) (Fig.11A red diamonds) and 2) those already present in the background of the parental strain, either because of genetic drift or because of the parental strain containing, for example, a transgene that was integrated into the genome by irradiation (Fig.11A pale blue diamonds). .. image:: http://www.hobertlab.org/CloudMap/Fig.11A_VDM.png Following an outcross to a non-parental strain and selection of a pool of F2-mutant recombinants, these homozygous variants will segregate according to their degree of linkage to the phenotype-inducing locus. The degree of linkage will be directly reflected in the allele frequency among the pool of recombinants and this can be represented as scatter plots of the ratio of variant reads/total reads present in the pool of sequenced recombinants (Fig.11A). We then plot a loess regression line through all the points on a given chromosome to give greater accuracy to the mapping region (Fig.11B). The loess lines on scatter plots for linked chromosomes approach 1, indicating retention of the original homozygous variants in the linked region. We also draw corresponding frequency plots that display regions of linked chromosomes where pure parental allele variant positions are concentrated (positions where the ratio of variant reads/total reads are equal to 1) (Fig.11B). 1Mb bins for the 0 ratio SNP positions are colored gray by default and .5Mb bins are colored in red. .. image:: http://www.hobertlab.org/CloudMap/Fig.11B-C_VDM_2.png 
We have tested the Variant Discovery Mapping method by crossing mutant strains to either N2 or Hawaiian CB4856 strains. In theory, it should be possible to cross the mutant strain to the original starting screening strain (as opposed to N2). For example, one might want to perform such a cross in the case of a suppressor screen where the screening strain has a second mutation that must be present for the mutant phenotype to be visible. However, crossing to the original starting screening strain will result in fewer mutations (relative to N2) that will be retained in the pooled sample that is sequenced. For this reason, mapping resulting from a cross to the original starting screening strain will not be as accurate as mapping from a cross to N2 and certainly not as accurate as mapping from a cross to the Hawaiian strain.
 ------ 
Input:
 This tool accepts as input a single VCF file containing reference (e.g. Bristol) and alternate (e.g. EMS, background, or crossing strain variant) alleles calls in the pooled mutant sample. This input VCF is generated at an earlier analysis step by running the GATK Unified Genotyper on a BAM alignment file of the pooled mutant sample and selecting only heterozygous or homozygous base positions as determined by the GATK Unified Genotyper (filtered for quality score > Q200). The reader is referred to the user guide and online video for direction on this procedure. The CloudMap Variant Discovery Mapping with WGS Data tool supports data from any organism. C. elegans and Arabidopsis are natively supported. For all other organisms, users must provide a simple tab-delimited configuration file containing chromosome numbers and respective lengths (example configuration files for most major organisms provided at http://usegalaxy.org/cloudmap). 
Output:
 The tool also provides a tabular output file that contains a count of the number of reference and alternate variants in the pooled mutant sample as well as the ratio of alternate alleles/total reads at each variant position. ------ 
Settings:
 .. class:: infomark Information on loess regression and the loess span parameter: http://en.wikipedia.org/wiki/Local_regression .. class:: infomark Based on our testing, we've settled on .4 as a loess span default. Larger values result in smoothing of the line to reflect trends at a more macro level. Smaller values result in loess lines that more closely reflect local data fluctuations. .. class:: infomark Supported colors for data points and loess regression line: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf http://research.stowers-institute.org/efg/R/Color/Chart/ColorChart.pdf .. class:: warningmark This tool requires that the statistical programming environment R has been installed on the system hosting Galaxy (http://www.r-project.org/). If you are running this tool on Galaxy via the Cloud, this does not apply to you. ------ 
Citation:
 This tool is part of the CloudMap package from the Hobert Lab. If you use this tool, please cite 
Gregory Minevich, Danny S Park, Daniel Blankenberg, Richard J. Poole, and Oliver Hobert. CloudMap: A Cloud-based Pipeline for Analysis of Mutant Genome Sequences. (Genetics 2012 In Press)
 .. 
: http://hobertlab.org/cloudmap Correspondence to gm2123@columbia.edu (Gregory Minevich) or r.poole@ucl.ac.uk (Richard J. Poole) or or38@columbia.edu (Oliver Hobert)"
toolshed.g2.bx.psu.edu/repos/gregory-minevich/cloudmap_in_silico_complementation/in_silico_complementation/1.0.0	".. class:: warningmark 
What it does
 This tool is part of the CloudMap pipeline for analysis of mutant genome sequences. For further details, please see 
Gregory Minevich, Danny S. Park, Daniel Blankenberg, Richard J. Poole and Oliver Hobert. CloudMap: A Cloud-based Pipeline for Analysis of Mutant Genome Sequences. (Genetics 2012 In Press)
 .. 
: http://biochemistry.hs.columbia.edu/labs/hobert/literature.html CloudMap workflows, shared histories and reference datasets are available at the 
CloudMap Galaxy page
. .. 
: http://usegalaxy.org/cloudmap If performed on a large scale, forward genetic screens usually yield multiple alleles of individual loci, which define specific complementation groups. The traditional way to identify such complementation groups is via complementation tests performed by genetic crosses. If screens have revealed dozens of mutants, comprehensive complementation testing can be time-consuming and labor-intensive. Moreover, complementation tests are impossible to perform with dominant alleles and are sometimes subject to misleading results (such as allelic complementation or non-allelic non- complementation). With the decreasing costs of whole genome sequencing, it is now possible to simply sequence many mutants that result from a screen and determine in silico which mutants carry variants in the same locus. To allow such analysis, we developed the CloudMap “in silico Complementation Test” tool to compare tabular lists of annotated variants from the program snpEff (which have been filtered for quality (see Materials and Methods) and had common variants subtracted) for shared gene hits (alleles). This tool creates two output files: 1 A summary file of the number of shared gene hits among the sequenced mutants sorted from most to fewest: .. image:: http://biochemistry.hs.columbia.edu/labs/hobert/CloudMap/Supp.Fig.1_in-silico_compSumm.png 2 A corresponding file of the snpEff annotated alleles from each sample also sorted from most to fewest: .. image:: http://biochemistry.hs.columbia.edu/labs/hobert/CloudMap/Supp.Fig.2_in-silico_compOut.png ------ 
Citation:
 This tool is part of the CloudMap package from the Hobert Lab. If you use this tool, please cite 
Gregory Minevich, Danny S Park, Daniel Blankenberg, Richard J. Poole, and Oliver Hobert. CloudMap: A Cloud-based Pipeline for Analysis of Mutant Genome Sequences. (Genetics 2012 In Press)
 .. 
: http://biochemistry.hs.columbia.edu/labs/hobert/literature.html Correspondence to gm2123@columbia.edu (G.M.) or or38@columbia.edu (O.H.) The annotated variant files used as input into this in silico complementation tool are generated by the snpEff program: CINGOLANI, P., A. PLATTS, L. WANG LE, M. COON, T. NGUYEN et al., 2012 A program for annotating and predicting the effects of single nucleotide polymorphisms, SnpEff: SNPs in the genome of Drosophila melanogaster strain w1118; iso-2; iso-3. Fly (Austin) 6: 80-92."
toolshed.g2.bx.psu.edu/repos/gregory-minevich/bcftools_view/bcftools_view/0.0.1	"What it does:
 This tool converts BCF files into VCF files using BCFtools view from the SAMtools set of utilities: http://samtools.sourceforge.net/samtools.shtml#4 ------ 
Citation:
 For the underlying tool, please cite 
Li H, Handsaker B, Wysoker A, Fennell T, Ruan J, Homer N, Marth G, Abecasis G, Durbin R; 1000 Genome Project Data Processing Subgroup. The Sequence Alignment/Map format and SAMtools. Bioinformatics. 2009 Aug 15;25(16):2078-9. &lt;http://www.ncbi.nlm.nih.gov/pubmed/19505943&gt;
_ If you use this tool within Galaxy, please cite 
Gregory Minevich, Danny S. Park, Daniel Blankenberg, Richard J. Poole, and Oliver Hobert. CloudMap: A Cloud-based Pipeline for Analysis of Mutant Genome Sequences. (Genetics 2012 In Press)
 .. 
: http://biochemistry.hs.columbia.edu/labs/hobert/literature.html Correspondence to gm2123@columbia.edu (G.M.) or or38@columbia.edu (O.H.)"
toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0	Combines a list collection into a single file dataset with option to include dataset names or merge common header line.
toolshed.g2.bx.psu.edu/repos/iuc/collection_column_join/collection_column_join/0.0.3	"Joins lists of tabular datasets together on a field. ----- 
Example
 To join three files, with headers, based on the first column: 
First file (in_1)
:: #KEY c2 c3 c4 one 1-1 1-2 1-3 two 1-4 1-5 1-6 three 1-7 1-8 1-9 
Second File (in_2)
:: #KEY c2 c3 c4 one 2-1 2-2 2-3 two 2-4 2-5 2-6 three 2-7 2-8 2-9 
Third file (in_3)
:: #KEY c2 c3 c4 one 3-3 3-2 3-3 two 3-4 3-5 3-6 three 3-7 3-8 3-9 
Joining
 the files, using 
identifier column of 1
 and a 
header lines of 1
, will return:: #KEY in_1_c2 in_1_c3 in_1_c4 in_2_c2 in_2_c3 in_2_c4 in_3_c2 in_3_c3 in_3_c4 one 1-1 1-2 1-3 2-1 2-2 2-3 3-3 3-2 3-3 three 1-7 1-8 1-9 2-7 2-8 2-9 3-7 3-8 3-9 two 1-4 1-5 1-6 2-4 2-5 2-6 3-4 3-5 3-6 
Joining
 the files, using 
identifier column of 1
 and a 
header lines of 1
, but disabling 
Add column name to header
, will return:: #KEY in_1 in_1 in_1 in_2 in_2 in_2 in_3 in_3 in_3 one 1-1 1-2 1-3 2-1 2-2 2-3 3-3 3-2 3-3 three 1-7 1-8 1-9 2-7 2-8 2-9 3-7 3-8 3-9 two 1-4 1-5 1-6 2-4 2-5 2-6 3-4 3-5 3-6"
toolshed.g2.bx.psu.edu/repos/iuc/collection_element_identifiers/collection_element_identifiers/0.0.2	This tool takes a list-type collection and produces a text dataset as output, containing the element identifiers of all datasets contained in the collection.
__SAMPLE_SHEET_TO_TABULAR__	"======== Synopsis ======== Takes a sample sheet dataset collection and converts the sample sheet metadata into a tabular dataset. The output is a tab-separated file where each row corresponds to an element in the sample sheet collection. The first column contains the element identifier, followed by columns for each metadata field defined in the sample sheet. When ""Include column headers"" is enabled, the first row will contain the column names, with ""element_identifier"" as the first column followed by the names from the sample sheet column definitions."
toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.2	"Split file into a dataset collection
 This tool splits a data set consisting of records into multiple data sets within a collection. A record can be for instance simply a line, a FASTA sequence (header + sequence), a FASTQ sequence (headers + sequence + qualities), etc. The important property is that the records either have a specific length (e.g. 4 lines for FASTQ) or that the beginning/end of a new record can be specified by a regular expression, e.g. "".
"" for lines or "">.
"" for FASTA. The tool has presets for text, tabular data sets (which are split after each line), FASTA (new records start with "">.*""), FASTQ (records consist of 4 lines), SDF (records start with ""^BEGIN IONS"") and MGF (records end with ""^$$$$""). For other data types the text delimiting records or the number of lines making up a record can be specified manually using the generic splitter. If the generic splitter is used, an option is also available to split records either before or after the separator. If a preset filetype is used, this is selected automatically (after for SDF, before for all others). If splitting by line (or by some other item, like a FASTA entry or an MGF record), the splitting can be either done alternatingly, in original record order, or at random. If t records are to be distributed to n new data sets, then the i-th record goes to data set * floor(i / t * n) (for batch), * i % n (for alternating), or * a random data set For instance, t=5 records are distributed as follows on n=2 data sets = === === ==== i bat alt rand = === === ==== 0 0 0 0 1 0 1 1 2 0 0 1 3 1 1 0 4 1 0 0 = === === ==== If the five records are distributed on n=3 data sets: = === === ==== i bat alt rand = === === ==== 0 0 0 0 1 0 1 1 2 1 2 2 3 1 0 0 4 2 1 1 = === === ==== Note that there are no guarantees when splitting at random that every result file will be non-empty, so downstream tools should be able to gracefully handle empty files. If a tabular file is used as input, you may choose to split by line or by column. If split by column, a new file is created for each unique value in the column. In addition, (Python) regular expressions may be used to transform the value in the column to a new value. Caution should be used with this feature, as it could transform all values to the same value, or other unexpected behavior. The default regular expression uses each value in the column without modifying it. Two modes are available for the tool. For the main mode, the number of output files is selected. In this case, records are shared out between this number of files. Alternatively, 'chunking mode' can be selected, which puts a fixed number of records (the 'chunk size') into each output file."
toolshed.g2.bx.psu.edu/repos/ecology/srs_global_indices/srs_global_indices/0.0.1	"========================================================================= Computes global biodiversity indices from satellite remote sensing data ========================================================================= 
What it does
 This tool aims to provide functions to apply Information Theory based diversity indexes on RasterLayer such as Shannon's entropy or Cumulative Residual Entropy (CRE). 
Input description
 It expects an image file as input, with a specific data format. ENVI HDR image with BIL interleave required. The image is an ENVI raster including : - A binary file (which has no extension here). - A header file (with .hdr extension). The header file is a text file including all necessary metadata which can be read with a text editor. It includes image dimensions, projection, and the name and central wavelength for each spectral band. In order to get such input we advise to use the tool preprocessing sentinel 2 data. If you did so you can directly enter the ""Reflectance"" output from this tool and thus select the otpion ""The data you are using are in a zip folder Reflectance"". ⚠️ If you do not use this Reflectance folder make sure that your data are respectively in bil and hdr format in the datatypes. - A number for the alpha indice which used to calculate the following indicators : Renyi, Hill and Prao. +--------------+----------+---------------+ | BIL | ENVI HDR | Number alpha | +==============+==========+===============+ | raster stack | Metadata | 1 | +--------------+----------+---------------+ | ... | ... | ... | +--------------+----------+---------------+ 
Output
 - One tabular with 9 columns longitude, latitude columns and one for each indices. - Seven png graph one for each indices."
toolshed.g2.bx.psu.edu/repos/ecology/srs_spectral_indices/srs_spectral_indices/0.0.1	"========================================================================= Computes biodiversity spectral indices from satellite remote sensing data ========================================================================= 
What it does
 This tool estimates vegetation properties (leaf and canopy) from sensor measurements. You can choose which vegetation property you want to compute. 
Input description
 It expects an image file as input, with a specific data format. ENVI HDR image with BIL interleave required. The image is an ENVI raster including : - A binary file (which has no extension here). - A header file (with .hdr extension). The header file is a text file including all necessary metadata which can be read with a text editor. It includes image dimensions, projection, and the name and central wavelength for each spectral band. In order to get such input we advise to use the tool preprocessing sentinel 2 data. If you did so you can directly enter the ""Reflectance"" output from this tool and thus select the otpion ""The data you are using are in a zip folder Reflectance"". ⚠️ If you do not use this Reflectance folder make sure that your data are respectively in bil and hdr format in the datatypes. Finally, you can choose whether or not you want to have the raster layer of your indice. +--------------+----------+---------------+ | BIL | HDR |Spectral indice| +==============+==========+===============+ | raster stack | Metadata | NDVI | +--------------+----------+---------------+ | ... | ... | ... | +--------------+----------+---------------+ 
Output
 - One tabular with 3 columns : longitude, latitude and the chosen indice. - One png plot for the vizualisation of the chosen indice. - Opionnnal, the raster layer in a zip file. 
Additionnal informations
 This tools allows you to compute one of GEO BON EBV 'Canopy Chlorophyll Content' (https://portal.geobon.org/ebv-detail?id=13). This EBV is computed by GEO BON on the Netherlands, here you can compute it on which ever Sentinel 2 data you want by chosing to calculate the indice CCI. When you chose your data you have to select a cloud cover smaller than 10% (you can selct this character directly on the different platforms wher you can download Sentinel 2 data). 
Indices description
 +-------------------+---------------------------------------------------------+ | Abbreviation | Name | +===================+=========================================================+ | ARI |Anthocyanin reflectance index | +-------------------+---------------------------------------------------------+ | ARVI |Atmospherically Resistant Vegetation Index | +-------------------+---------------------------------------------------------+ | BAI |Burn Area Index | +-------------------+---------------------------------------------------------+ | BAIS2 |Burned Area Index for Sentinel 2 | +-------------------+---------------------------------------------------------+ | CCCI |Canopy Chlorophyll Content Index | +-------------------+---------------------------------------------------------+ | CHL_RE |Chlorophyll Red-Edge ? | +-------------------+---------------------------------------------------------+ | EVI |Enhanced Vegetation Index | +-------------------+---------------------------------------------------------+ | GRVI1 |Green Ratio Vegetation Index | +-------------------+---------------------------------------------------------+ | GNDVI |Green Normalized Difference Vegetation Index | +-------------------+---------------------------------------------------------+ | IRECI |Inverted Red-Edge Chlorophyll Index | +-------------------+---------------------------------------------------------+ | LAI SAVI |Leaf Area Index Soil Adjusted Vegetation Index ? | +-------------------+---------------------------------------------------------+ | MCARI |Modified Chlorophyll Absorption in Reflectance Index | +-------------------+---------------------------------------------------------+ | mNDVI705 |Modified NDVI 705 | +-------------------+---------------------------------------------------------+ | MSAVI2 |Modified Soil Adjusted Vegetation Index 2 | +-------------------+---------------------------------------------------------+ | MSI |Moisture Stress Index | +-------------------+---------------------------------------------------------+ | mSR705 |Modified Simple Ratio 705 | +-------------------+---------------------------------------------------------+ | MTCI |MERIS Terrestrial Chlorophyll Index | +-------------------+---------------------------------------------------------+ | NBR_RAW |Normalized Burn Ratio RAW | +-------------------+---------------------------------------------------------+ | NDI_45 | | +-------------------+---------------------------------------------------------+ | NDII |Normalized Difference 819/1600 NDII | +-------------------+---------------------------------------------------------+ | NDSI |Normalized Difference Snow Index | +-------------------+---------------------------------------------------------+ | NDVI |Normalized Difference Vegetation Index | +-------------------+---------------------------------------------------------+ | NDVI_G | | +-------------------+---------------------------------------------------------+ | NDVI705 |NDVI 705 | +-------------------+---------------------------------------------------------+ | NDWI |Normalized Difference Water Index | +-------------------+---------------------------------------------------------+ | NDWI1 |NDMI ? Normalized Difference Moisture Index ? | +-------------------+---------------------------------------------------------+ | NDWI2 | | +-------------------+---------------------------------------------------------+ | PSRI |Plant Senescing Reflectance Index | +-------------------+---------------------------------------------------------+ | PSRI_NIR | | +-------------------+---------------------------------------------------------+ | RE_NDVI |Red Edge NDVI ? | +-------------------+---------------------------------------------------------+ | RE_NDWI |Red Edge NDWI | +-------------------+---------------------------------------------------------+ | S2REP |Sentinel 2 Red-Edge Position | +-------------------+---------------------------------------------------------+ | SAVI |Soil Adjusted Vegetation Index | +-------------------+---------------------------------------------------------+ | SIPI |Structure Intensive Pigment Index 1 | +-------------------+---------------------------------------------------------+ | SR |Simple Ratio ? | +-------------------+---------------------------------------------------------+ | CR_SWIR |CR short wave infrared | +-------------------+---------------------------------------------------------+"
toolshed.g2.bx.psu.edu/repos/ecology/srs_diversity_maps/srs_diversity_maps/0.0.1	"======================================================================== Process satellite remote sensing data to produce biodiversity indicators ======================================================================== 
What it does
 Féret and Asner (2014) developed a method for 
tropical forest
 diversity mapping based on very high spatial resolution airborne imaging spectroscopy. The goal of this tool using the package biodivMapR is to produce (spectral) diversity maps based on (optical) images. 
Input description
 It expects an image file as input, with a specific data format. ENVI HDR image with BIL interleave required. The image is an ENVI raster including : - A binary file (which has no extension here). - A header file (with .hdr extension). The header file is a text file including all necessary metadata which can be read with a text editor. It includes image dimensions, projection, and the name and central wavelength for each spectral band. In order to get such input we advise to use the tool preprocessing sentinel 2 data. +--------------+----------+ | BIL | ENVI HDR | +==============+==========+ | raster stack | Metadata | +--------------+----------+ | ... | ... | +--------------+----------+ 
Output
 - Three tabulars : alpha, beta, functionnal each of them with 3 colomns latitude, longitude and the indice. - Three png graph for each indice"
toolshed.g2.bx.psu.edu/repos/ecology/srs_preprocess_s2/srs_preprocess_s2/0.0.1	"========================== Preprocess Sentinel 2 data ========================== 
What it does
 The goal of this tool (using the package preprocS2) is to provide a common framework for the preprocessing of Level-2A Sentinel-2 images (does not work yet for Level 1C). Sentinel-2 L2A images can be produced or obtained from various data hubs or atmospheric correction methods. PreprocS2 provides a unique function to read, crop, resample the original image directory, and write it as a raster stack. 
Input description
 A zip folder with Sentinel 2 data. These data can be dowloaded on 3 different platforms: - Copernicus Open Access Hub, Scihub, which provides complete, free and open access to Sentinel-2 data : https://scihub.copernicus.eu/dhus/#/home - PEPS, la 'Plateforme d'Exploitation de Produits Sentinel' : https://peps.cnes.fr/rocket/#/search - Theia : https://theia.cnes.fr/atdistrib/rocket/#/search For each of those 3 platforms you will need to create an account. You need to select from where you dowloaded your data in order for the tool to know the format of the folder once it unzips it. +----------+----------+ | ZIP | Source | +==========+==========+ |folder.zip|Character | +----------+----------+ | ... | ... | +----------+----------+ 
Output
 A zip folder containing the refelctance data with a raster stack pile in BIL format (no extension), a header with the metadata (.hdr extension) and 2 other files in .xml. A zip folder containing the cloud mask with the 2 files for the raw data (.RAW) and 2 files containing the metadata (.hdr)."
toolshed.g2.bx.psu.edu/repos/iuc/bbgbigwig/bbgtobigwig/0.1	Estimates coverage of a reference genome for bam, bed or gff as a bigwig, suitable for viewing in JBrowse2 or other browser. A chromosome lengths file must be provided if the input has a missing dbkey (='?') on the pencil (edit attributes) form. The actual reference is not needed. The Compute sequence length tool can generate the lengths file. This can be useful in workflows with assemblies in progress before a stable reference is available for a custom or built in reference dbkey.
bed2gff1	"What it does
 This tool converts data from BED format to GFF format (scroll down for format description). -------- 
Example
 The following data in BED format:: chr28 346187 388197 BC114771 0 + 346187 388197 0 9 144,81,115,63,155,96,134,105,112, 0,24095,26190,31006,32131,33534,36994,41793,41898, Will be converted to GFF (
note
 that the start coordinate is incremented by 1):: ##gff-version 2 ##bed_to_gff_converter.py chr28 bed2gff mRNA 346188 388197 0 + . mRNA BC114771; chr28 bed2gff exon 346188 346331 0 + . exon BC114771; chr28 bed2gff exon 370283 370363 0 + . exon BC114771; chr28 bed2gff exon 372378 372492 0 + . exon BC114771; chr28 bed2gff exon 377194 377256 0 + . exon BC114771; chr28 bed2gff exon 378319 378473 0 + . exon BC114771; chr28 bed2gff exon 379722 379817 0 + . exon BC114771; chr28 bed2gff exon 383182 383315 0 + . exon BC114771; chr28 bed2gff exon 387981 388085 0 + . exon BC114771; chr28 bed2gff exon 388086 388197 0 + . exon BC114771; ------ .. class:: informark 
About formats
 
BED format
 Browser Extensible Data format was designed at UCSC for displaying data tracks in the Genome Browser. It has three required fields and several additional optional ones: The first three BED fields (required) are:: 1. chrom - The name of the chromosome (e.g. chr1, chrY_random). 2. chromStart - The starting position in the chromosome. (The first base in a chromosome is numbered 0.) 3. chromEnd - The ending position in the chromosome, plus 1 (i.e., a half-open interval). The additional BED fields (optional) are:: 4. name - The name of the BED line. 5. score - A score between 0 and 1000. 6. strand - Defines the strand - either '+' or '-'. 7. thickStart - The starting position where the feature is drawn thickly at the Genome Browser. 8. thickEnd - The ending position where the feature is drawn thickly at the Genome Browser. 9. reserved - This should always be set to zero. 10. blockCount - The number of blocks (exons) in the BED line. 11. blockSizes - A comma-separated list of the block sizes. The number of items in this list should correspond to blockCount. 12. blockStarts - A comma-separated list of block starts. All of the blockStart positions should be calculated relative to chromStart. The number of items in this list should correspond to blockCount. 13. expCount - The number of experiments. 14. expIds - A comma-separated list of experiment ids. The number of items in this list should correspond to expCount. 15. expScores - A comma-separated list of experiment scores. All of the expScores should be relative to expIds. The number of items in this list should correspond to expCount. 
GFF format
 General Feature Format is a format for describing genes and other features associated with DNA, RNA and Protein sequences. GFF lines have nine tab-separated fields:: 1. seqname - Must be a chromosome or scaffold. 2. source - The program that generated this feature. 3. feature - The name of this type of feature. Some examples of standard feature types are ""CDS"", ""start_codon"", ""stop_codon"", and ""exon"". 4. start - The starting position of the feature in the sequence. The first base is numbered 1. 5. end - The ending position of the feature (inclusive). 6. score - A score between 0 and 1000. If there is no score value, enter ""."". 7. strand - Valid entries include '+', '-', or '.' (for don't know/care). 8. frame - If the feature is a coding exon, frame should be a number between 0-2 that represents the reading frame of the first base. If the feature is not a coding exon, the value should be '.'. 9. group - All lines with the same group are linked together into a single item."
bed_to_bigBed	"This tool converts a 
sorted
 BED file into a bigBed file. Currently, the bedFields option to specify the number of non-standard fields is not supported as an AutoSQL file must be provided, which is a format currently not supported by Galaxy."
toolshed.g2.bx.psu.edu/repos/iuc/compress_file/compress_file/0.1.0	Compress files with gzip. If compressing a collection, all elements within that collection will be compressed, but the collection itself will not be.
toolshed.g2.bx.psu.edu/repos/iuc/bioext_bam2msa/bioext_bam2msa/0.21.10+galaxy0	Extract MSA from a BAM file
toolshed.g2.bx.psu.edu/repos/iuc/bam_to_scidx/bam_to_scidx/1.0.1	"What it does
 Converts BAM data to ScIdx, the Strand-specific coordinate count format, which is used by tools within the Chip-exo Galaxy flavor. ScIdx files are 1-based. The format consists of 5 columns: the chromosome, the position of the genomic coordinate, the number of tags on the forward strand, the number of tags on the reverse strand and the number of total tags on the position. With pair-end reads, only the 5’ end of READ1 will be used to create the ScIdx data file. Tools that use this format include GeneTrack and MultiGPS. ----- 
Options
 * 
Require proper mate-pairing?
 - Select 
Yes
 to require proper mate paring. Filtering by insert size parameters will not filter out single-end Read 1 unless proper mate-pairing is required. * 
Minimum insert size to output
 - Insert size below the minimum will be filtered from the results, but single-end Read 1 will not be filtered unless proper mate-pairing is required. * 
Maximum insert size to output
 - Insert size above the maximum will be filtered from the results, but single-end Read 1 will not be filtered unless proper mate-pairing is required."
toolshed.g2.bx.psu.edu/repos/iuc/gtftobed12/gtftobed12/357	"Converts a GTF_ file to a BED12_ formatted file using UCSC tools from Jim Kent. 
gtfToGenePred
, followed by 
genePredToBed
 .. _GTF: https://genome.ucsc.edu/FAQ/FAQformat.html#format4 .. _BED12: https://genome.ucsc.edu/FAQ/FAQformat.html#format1"
toolshed.g2.bx.psu.edu/repos/galaxyp/gffcompare_to_bed/gffcompare_to_bed/0.2.1	Convert a GFFCompare annotated GTF file to BED format. A typical workflow: .. image:: GTF2Bed_workflow.png :height: 308 :width: 750 usage: gffcompare_to_bed.py [-h] [-C CLASS_CODE] [-v] [-d] input output positional arguments: input GFFCompare annotated GTF file, (-) for stdin output BED file, (-) for stdout optional arguments: -h, --help show this help message and exit -C CLASS_CODE, --class_code CLASS_CODE Restrict output to gffcompare class codes -d, --debug Debug For class_code definitions see gffcompare documentation: http://ccb.jhu.edu/software/stringtie/gffcompare.shtml#transfrag-class-codes
toolshed.g2.bx.psu.edu/repos/iuc/intermine_galaxy_exchange/galaxy_intermine_exchange/0.0.1	"InterMine-Galaxy exchange format ================================ This file format exists as an intermediate step to import from Galaxy to InterMines and is not in itself a data format intended to be used for data storage or any purpose apart from Galaxy->InterMine interoperability. This file has two mandatory columns and one optional column Column 1 (mandatory) -------------------- 
Type of identifier
, e.g. 
Gene
 or 
Protein
. Must be a class in the 
InterMine data model
_. Gene or Protein is always a safe option. Column 2 (mandatory) -------------------- 
Identifier
 this could be, as an example, a gene symbol like 
GATA1
 or another other identifier, e.g. 
FBGN0000099
 or perhaps a protein accession, or some other identifier. Column 3 (optional) ------------------- 
Organism name
 - e.g. 
H. sapiens
 or 
Drosophila melanogaster
. This is optional and does not have to be precise, but is good to provide if it is known. ---- For additional information, please see: - https://github.com/intermine/galaxy-integration/blob/master/intermine-file-format/intermine-format-definition.md - https://raw.githubusercontent.com/intermine/galaxy-integration/master/intermine-file-format/intermine-format-example.tsv .. _InterMine data model: http://intermine.readthedocs.io/en/latest/data-model/model/#a-short-example"
toolshed.g2.bx.psu.edu/repos/iuc/crossmap_bam/crossmap_bam/0.7.3+galaxy0	"CrossMap -------- CrossMap is versatile tool to convert genome coordinates or annotation files between genome assemblies. It supports mostly commonly used file types, including BAM, BED,BigWig, GFF, GTF, SAM, Wiggle, and VCF formats. For large plain text file types, such as BED, GFF, GTF and VCF, reading from remote servers and file compression are supported. BAM --------- CrossMap updates chromosomes, genome coordinates, header sections, and all BAM flags accordingly. The program version (of CrossMap) is inserted into the header section, along with the names of the original BAM file and the chain file. For pair-end sequencing, insert size is also recalculated. 
Optional tags
 Q QC. QC failed. N Unmapped. Originally unmapped or originally mapped but failed to liftover to new assembly. M Multiple mapped. Alignment can be liftover to multiple places. U Unique mapped. Alignment can be liftover to only 1 place. 
Tags for pair-end sequencing include:
 QF QC failed NN both read1 and read2 unmapped NU read1 unmapped, read2 unique mapped NM read1 unmapped, multiple mapped UN read1 uniquely mapped, read2 unmap UU both read1 and read2 uniquely mapped UM read1 uniquely mapped, read2 multiple mapped MN read1 multiple mapped, read2 unmapped MU read1 multiple mapped, read2 unique mapped MM both read1 and read2 multiple mapped 
Tags for single-end sequencing include
 QF QC failed SN unmaped SM multiple mapped SU uniquely mapped"
toolshed.g2.bx.psu.edu/repos/iuc/crossmap_bed/crossmap_bed/0.7.3+galaxy0	CrossMap -------- CrossMap is versatile tool to convert genome coordinates or annotation files between genome assemblies. It supports mostly commonly used file types, including BAM, BED,BigWig, GFF, GTF, SAM, Wiggle, and VCF formats. For large plain text file types, such as BED, GFF, GTF and VCF, reading from remote servers and file compression are supported. BED --- BED format file must have at least 3 columns (chrom, start, end) and no more than 12 columns. BED format: http://genome.ucsc.edu/FAQ/FAQformat.html#format1 A BED (Browser Extensible Data) file is a tab-delimited text file describing genome regions or gene annotations. It is the standard file format used by UCSC. It consists of one line per feature, each containing 3-12 columns. CrossMap converts BED files with less than 12 columns to a different assembly by updating the chromosome and genome coordinates only; all other columns remain unchanged. Regions from old assembly mapping to multiple locations to the new assembly will be split. For 12-columns BED files, all columns will be updated accordingly except the 4th column (name of bed line), 5th column (score value) and 9th column (RGB value describing the display color). 12-column BED files usually define multiple blocks (eg. exon); if any of the exons fails to map to a new assembly, the whole BED line is skipped. Notes: 1. For BED-like formats mentioned above, CrossMap only updates “chrom (1st column)”, “start (2nd column) ”, “end (3rd column) ” and “strand” (if any). All other columns will keep AS-IS. 2. Lines starting with ‘#’, ‘browser’, ‘track’ will be skipped. 3. Lines will less than 3 columns will be skipped. 4. 2nd-column and 3-column must be integer, otherwise skipped. 5. “+” strand is assumed if no strand information was found. 6. For standard BED format (12 columns). If any of the defined exon blocks cannot be uniquely mapped to target assembly, the whole entry will be skipped. 7. If input region cannot be consecutively mapped target assembly, it will be split. 8. *.unmap file contains regions that cannot be unambiguously converted.
toolshed.g2.bx.psu.edu/repos/iuc/crossmap_gff/crossmap_gff/0.7.3+galaxy0	CrossMap -------- CrossMap is versatile tool to convert genome coordinates or annotation files between genome assemblies. It supports mostly commonly used file types, including BAM, BED,BigWig, GFF, GTF, SAM, Wiggle, and VCF formats. For large plain text file types, such as BED, GFF, GTF and VCF, reading from remote servers and file compression are supported. GFF / GTF --------- Your input data should be either GTF/GFF2.5 or GFF3 format. GFF (General Feature Format) is another plain text file used to describe gene structure. GTF (Gene Transfer Format) is a refined version of GTF. The first eight fields are the same as GFF. Only chromosome and genome coordinates are updated. The format of output is determined from the input. Notes: - Each feature (exon, intron, UTR, etc) is processed separately and independently, and we do NOT check if features originally belonging to the same gene were converted into the same gene. - If user want to liftover gene annotation files, use BED12 format.
toolshed.g2.bx.psu.edu/repos/iuc/crossmap_vcf/crossmap_vcf/0.7.3+galaxy0	CrossMap -------- CrossMap is versatile tool to convert genome coordinates or annotation files between genome assemblies. It supports mostly commonly used file types, including BAM, BED,BigWig, GFF, GTF, SAM, Wiggle, and VCF formats. For large plain text file types, such as BED, GFF, GTF and VCF, reading from remote servers and file compression are supported. VCF --- VCF (variant call format) is a flexible and extendable line-oriented text format developed by the 1000 Genome Project. It is useful for representing single nucleotide variants, indels, copy number variants, and structural variants. Chromosomes, coordinates, and reference alleles are updated to a new assembly, and all the other fields are not changed. Notes: - Genome coordinates and reference allele will be updated to target assembly. - Reference genome is genome sequence of target assembly. - If the reference genome sequence file (../database/genome/hg18.fa) was not indexed, CrossMap will automatically indexed it (only the first time you run CrossMap). - In the output VCF file, whether the chromosome IDs contain “chr” or not depends on the format of the input VCF file.
toolshed.g2.bx.psu.edu/repos/iuc/crossmap_wig/crossmap_wig/0.7.3+galaxy0	CrossMap -------- CrossMap is versatile tool to convert genome coordinates or annotation files between genome assemblies. It supports mostly commonly used file types, including BAM, BED,BigWig, GFF, GTF, SAM, Wiggle, and VCF formats. For large plain text file types, such as BED, GFF, GTF and VCF, reading from remote servers and file compression are supported. Wig --- Input wiggle data can be in variableStep (for data with irregular intervals) or fixedStep (for data with regular intervals). Regardless of the input, the output will always in bedGraph format. bedGraph format is similar to wiggle format and can be converted into BigWig format using UCSC wigToBigWig tool. We export files in bedGraph because it is usually much smaller than file in wiggle format, and more importantly, CrossMap internally transforms wiggle into bedGraph to increase running speed.
toolshed.g2.bx.psu.edu/repos/devteam/fasta_to_tabular/fasta2tab/1.1.0	"What it does
 This tool converts FASTA formatted sequences to TAB-delimited format. Many tools consider the first word of the FASTA "">"" title line to be an identifier, and any remaining text to be a free form description. It is therefore useful to split this text into two columns in Galaxy (identifier and any description) by setting 
How many columns to divide title string into?
 to 
2
. In some cases the description can be usefully broken up into more columns -- see the examples . The option 
How many characters to keep?
 allows to select a specified number of letters from the beginning of each FASTA entry. With the introduction of the 
How many columns to divide title string into?
 option this setting is of limited use, but does still allow you to truncate the identifier. ----- 
Example
 Suppose you have the following FASTA formatted sequences from a Roche (454) FLX sequencing run:: >EYKX4VC02EQLO5 length=108 xy=1826_0455 region=2 run=R_2007_11_07_16_15_57_ TCCGCGCCGAGCATGCCCATCTTGGATTCCGGCGCGATGACCATCGCCCGCTCCACCACG TTCGGCCGGCCCTTCTCGTCGAGGAATGACACCAGCGCTTCGCCCACG >EYKX4VC02D4GS2 length=60 xy=1573_3972 region=2 run=R_2007_11_07_16_15_57_ AATAAAACTAAATCAGCAAAGACTGGCAAATACTCACAGGCTTATACAATACAAATGTAA Running this tool with the default settings will produce this (2 column output): ========================================================================== ======================================= EYKX4VC02EQLO5 length=108 xy=1826_0455 region=2 run=R_2007_11_07_16_15_57_ TCCGCGCCGAGCATGCCCATCTTGGATTCCGGC...ACG EYKX4VC02D4GS2 length=60 xy=1573_3972 region=2 run=R_2007_11_07_16_15_57_ AATAAAACTAAATCAGCAAAGACTGGCAAATAC...TAA ========================================================================== ======================================= Having the full title line (the FASTA "">"" line text) as a column is not always ideal. The 
How many characters to keep?
 option is useful if your identifiers are all the same length. In this example the identifier is 14 characters, so setting 
How many characters to keep?
 to 
14
 (and leaving 
How many columns to divide title string into?
 as the default, 
1
) will produce this (2 column output): ============== ======================================= EYKX4VC02EQLO5 TCCGCGCCGAGCATGCCCATCTTGGATTCCGGC...ACG EYKX4VC02D4GS2 AATAAAACTAAATCAGCAAAGACTGGCAAATAC...TAA ============== ======================================= If however your FASTA file has identifiers of variable length, it is better to split the text into at least two columns. Running this tool with 
How many columns to divide title string into?
 to 
2
 will produce this (3 column output): ============== =========================================================== ======================================= EYKX4VC02EQLO5 length=108 xy=1826_0455 region=2 run=R_2007_11_07_16_15_57_ TCCGCGCCGAGCATGCCCATCTTGGATTCCGGC...ACG EYKX4VC02D4GS2 length=60 xy=1573_3972 region=2 run=R_2007_11_07_16_15_57_ AATAAAACTAAATCAGCAAAGACTGGCAAATAC...TAA ============== =========================================================== ======================================= Running this tool with 
How many columns to divide title string into?
 to 
5
 will produce this (5 column output): ============== ========== ============ ======== ========================== ======================================= EYKX4VC02EQLO5 length=108 xy=1826_0455 region=2 run=R_2007_11_07_16_15_57_ TCCGCGCCGAGCATGCCCATCTTGGATTCCGGC...ACG EYKX4VC02D4GS2 length=60 xy=1573_3972 region=2 run=R_2007_11_07_16_15_57_ AATAAAACTAAATCAGCAAAGACTGGCAAATAC...TAA ============== ========== ============ ======== ========================== ======================================= Running this tool with 
How many columns to divide title string into?
 to 
5
 and 
How many characters to keep?
 to 
10
 will produce this (5 column output). Notice that only the first column is truncated to 10 characters -- and be careful not to trim your sequence names too much (generally they should be unique): ========== ========== ============ ======== ========================== ======================================= EYKX4VC02E length=108 xy=1826_0455 region=2 run=R_2007_11_07_16_15_57_ TCCGCGCCGAGCATGCCCATCTTGGATTCCGGC...ACG EYKX4VC02D length=60 xy=1573_3972 region=2 run=R_2007_11_07_16_15_57_ AATAAAACTAAATCAGCAAAGACTGGCAAATAC...TAA ========== ========== ============ ======== ========================== ======================================= Note the sequences have been truncated for display purposes in the above tables."
toolshed.g2.bx.psu.edu/repos/devteam/fasta_to_tabular/fasta2tab/1.1.1	"What it does
 This tool converts FASTA formatted sequences to TAB-delimited format. Many tools consider the first word of the FASTA ""&gt;"" title line to be an identifier, and any remaining text to be a free form description. It is therefore useful to split this text into two columns in Galaxy (identifier and any description) by setting 
How many columns to divide title string into?
 to 
2
. In some cases the description can be usefully broken up into more columns -- see the examples . The option 
How many characters to keep?
 allows to select a specified number of letters from the beginning of each FASTA entry. With the introduction of the 
How many columns to divide title string into?
 option this setting is of limited use, but does still allow you to truncate the identifier. ----- 
Example
 Suppose you have the following FASTA formatted sequences from a Roche (454) FLX sequencing run:: >EYKX4VC02EQLO5 length=108 xy=1826_0455 region=2 run=R_2007_11_07_16_15_57_ TCCGCGCCGAGCATGCCCATCTTGGATTCCGGCGCGATGACCATCGCCCGCTCCACCACG TTCGGCCGGCCCTTCTCGTCGAGGAATGACACCAGCGCTTCGCCCACG >EYKX4VC02D4GS2 length=60 xy=1573_3972 region=2 run=R_2007_11_07_16_15_57_ AATAAAACTAAATCAGCAAAGACTGGCAAATACTCACAGGCTTATACAATACAAATGTAA Running this tool with the default settings will produce this (2 column output): ========================================================================== ======================================= EYKX4VC02EQLO5 length=108 xy=1826_0455 region=2 run=R_2007_11_07_16_15_57_ TCCGCGCCGAGCATGCCCATCTTGGATTCCGGC...ACG EYKX4VC02D4GS2 length=60 xy=1573_3972 region=2 run=R_2007_11_07_16_15_57_ AATAAAACTAAATCAGCAAAGACTGGCAAATAC...TAA ========================================================================== ======================================= Having the full title line (the FASTA ""&gt;"" line text) as a column is not always ideal. The 
How many characters to keep?
 option is useful if your identifiers are all the same length. In this example the identifier is 14 characters, so setting 
How many characters to keep?
 to 
14
 (and leaving 
How many columns to divide title string into?
 as the default, 
1
) will produce this (2 column output): ============== ======================================= EYKX4VC02EQLO5 TCCGCGCCGAGCATGCCCATCTTGGATTCCGGC...ACG EYKX4VC02D4GS2 AATAAAACTAAATCAGCAAAGACTGGCAAATAC...TAA ============== ======================================= If however your FASTA file has identifiers of variable length, it is better to split the text into at least two columns. Running this tool with 
How many columns to divide title string into?
 to 
2
 will produce this (3 column output): ============== =========================================================== ======================================= EYKX4VC02EQLO5 length=108 xy=1826_0455 region=2 run=R_2007_11_07_16_15_57_ TCCGCGCCGAGCATGCCCATCTTGGATTCCGGC...ACG EYKX4VC02D4GS2 length=60 xy=1573_3972 region=2 run=R_2007_11_07_16_15_57_ AATAAAACTAAATCAGCAAAGACTGGCAAATAC...TAA ============== =========================================================== ======================================= Running this tool with 
How many columns to divide title string into?
 to 
5
 will produce this (5 column output): ============== ========== ============ ======== ========================== ======================================= EYKX4VC02EQLO5 length=108 xy=1826_0455 region=2 run=R_2007_11_07_16_15_57_ TCCGCGCCGAGCATGCCCATCTTGGATTCCGGC...ACG EYKX4VC02D4GS2 length=60 xy=1573_3972 region=2 run=R_2007_11_07_16_15_57_ AATAAAACTAAATCAGCAAAGACTGGCAAATAC...TAA ============== ========== ============ ======== ========================== ======================================= Running this tool with 
How many columns to divide title string into?
 to 
5
 and 
How many characters to keep?
 to 
10
 will produce this (5 column output). Notice that only the first column is truncated to 10 characters -- and be careful not to trim your sequence names too much (generally they should be unique): ========== ========== ============ ======== ========================== ======================================= EYKX4VC02E length=108 xy=1826_0455 region=2 run=R_2007_11_07_16_15_57_ TCCGCGCCGAGCATGCCCATCTTGGATTCCGGC...ACG EYKX4VC02D length=60 xy=1573_3972 region=2 run=R_2007_11_07_16_15_57_ AATAAAACTAAATCAGCAAAGACTGGCAAATAC...TAA ========== ========== ============ ======== ========================== ======================================= Note the sequences have been truncated for display purposes in the above tables."
toolshed.g2.bx.psu.edu/repos/devteam/fastq_to_fasta/cshl_fastq_to_fasta/1.0.2+galaxy2	"What it does
 This tool converts data from Solexa format to FASTA format (scroll down for format description). -------- 
Example
 The following data in Solexa-FASTQ format:: @CSHL_4_FC042GAMMII_2_1_517_596 GGTCAATGATGAGTTGGCACTGTAGGCACCATCAAT +CSHL_4_FC042GAMMII_2_1_517_596 40 40 40 40 40 40 40 40 40 40 38 40 40 40 40 40 14 40 40 40 40 40 36 40 13 14 24 24 9 24 9 40 10 10 15 40 Will be converted to FASTA (with 'rename sequence names' = NO):: >CSHL_4_FC042GAMMII_2_1_517_596 GGTCAATGATGAGTTGGCACTGTAGGCACCATCAAT Will be converted to FASTA (with 'rename sequence names' = YES):: >1 GGTCAATGATGAGTTGGCACTGTAGGCACCATCAAT"
toolshed.g2.bx.psu.edu/repos/devteam/fastqtofasta/fastq_to_fasta_python/1.2+galaxy0	"What it does
 This tool converts FASTQ sequencing reads to FASTA sequences."
toolshed.g2.bx.psu.edu/repos/devteam/fastq_to_tabular/fastq_to_tabular/1.1.5+galaxy2	"What it does
 This tool converts FASTQ sequencing reads to a Tabular file. It is conventional to take the first word of the FASTQ ""@"" title line as the identifier, and any remaining text to be a free form description. It is therefore often useful to split this text into two columns in Galaxy (identifier and any description) by setting 
How many columns to divide title string into?
 to 
2
. In some cases the description can be usefully broken up into more columns -- see the examples . Tab characters, if present in the source FASTQ title, will be converted to spaces. ----- 
Example
 Consider the following two 454 reads in Sanger FASTQ format (using line wrapping for display, but do note not all tools will accept line wrapped FASTQ files):: @FSRRS4401BE7HA [length=395] [gc=36.46] [flows=800] [phred_min=0] [phred_max=40] [trimmed_length=95] tcagTTAAGATGGGATAATATCCTCAGATTGCGTGATGAACTTTGTTCTGGTGGAGGAGAAGGAAGTGCATTCGACGTAT GCCCGTTTGTCGATATTTGtatttaaagtaatccgtcacaaatcagtgacataaatattatttagatttcgggagcaact ttatttattccacaagcaggtttaaattttaaatttaaattattgcagaagactttaaattaacctcgttgtcggagtca tttgttcggttattggtcgaaagtaaccncgggaagtgccgaaaactaacaaacaaaagaagatagtgaaattttaatta aaanaaatagccaaacgtaactaactaaaacggacccgtcgaggaactgccaacggacgacacagggagtagnnn +FSRRS4401BE7HA [length=395] [gc=36.46] [flows=800] [phred_min=0] [phred_max=40] [trimmed_length=95] FFFDDDDDDDA666?688FFHGGIIIIIIIIIIIIIIIIIIHHHIIIIIIIIIGHGFFFFF====DFFFFFFFFFFFFFF D???:3104/76=:5...4.3,,,366////4<ABBAAA=CCFDDDDDDDD:666CDFFFF=<ABA=;:333111<===9 9;B889FFFFFFDDBDBDDD=8844231..,,,-,,,,,,,,1133..---17111,,,,,22555131121.--.,333 11,.,,3--,,.,,--,3511123..--!,,,,--,----9,,,,8=,,-,,,-,,,,---26:9:5-..1,,,,11//, ,,,!,,1917--,,,,-3.,--,,17,,,,---+11113.030000,,,044400036;96662.//;7><;!!! @FSRRS4401BRRTC [length=145] [gc=38.62] [flows=800] [phred_min=0] [phred_max=38] [trimmed_length=74] tcagCCAGCAATTCCGACTTAATTGTTCTTCTTCCATCATTCATCTCGACTAACAGTTCTACGATTAATGAGTTTGGCtt taatttgttgttcattattgtcacaattacactactgagactgccaaggcacncagggataggnn +FSRRS4401BRRTC [length=145] [gc=38.62] [flows=800] [phred_min=0] [phred_max=38] [trimmed_length=74] FFFFFFFFFDDDDFFFFGFDDDDBAAAAA=<4444@@B=555:BBBBB@@?8:8<?<89898<84442;==3,,,514,, ,11,,,.,,21777555513,..--1115758.//34488><<;;;;9944/!/4,,,57855!! By default this is converted into a 3 column tabular file, with the full FASTQ title used as column 1: =================================================================================================== ============== ============== FSRRS4401BE7HA [length=395] [gc=36.46] [flows=800] [phred_min=0] [phred_max=40] [trimmed_length=95] tcagTTAA...nnn FFFDDDDD...!!! FSRRS4401BRRTC [length=145] [gc=38.62] [flows=800] [phred_min=0] [phred_max=38] [trimmed_length=74] tcagCCAG...gnn FFFFFFFF...5!! =================================================================================================== ============== ============== If you specified the title should be turned into 2 columns, you'd get 4 columns in total: ============== ==================================================================================== ============== ============== FSRRS4401BE7HA [length=395] [gc=36.46] [flows=800] [phred_min=0] [phred_max=40] [trimmed_length=95] tcagTTAA...nnn FFFDDDDD...!!! FSRRS4401BRRTC [length=145] [gc=38.62] [flows=800] [phred_min=0] [phred_max=38] [trimmed_length=74] tcagCCAG...gnn FFFFFFFF...5!! ============== ==================================================================================== ============== ============== Similarly, for this example treating the title string as 7 columns makes sense: ============== ============ ========== =========== ============= ============== =================== ============== ============== FSRRS4401BE7HA [length=395] [gc=36.46] [flows=800] [phred_min=0] [phred_max=40] [trimmed_length=95] tcagTTAA...nnn FFFDDDDD...!!! FSRRS4401BRRTC [length=145] [gc=38.62] [flows=800] [phred_min=0] [phred_max=38] [trimmed_length=74] tcagCCAG...gnn FFFFFFFF...5!! ============== ============ ========== =========== ============= ============== =================== ============== ============== Note the sequences and quality strings have been truncated for display purposes in the above tables."
toolshed.g2.bx.psu.edu/repos/iuc/gfa_to_fa/gfa_to_fa/0.1.2	This tool converts a GFA file (Graphical Fragment Assembly) to a FASTA file. The second column from the GFA file is used as the sequence header. More information about the GFA file format can be found here_ .. _here: http://gfa-spec.github.io/GFA-spec/GFA1.html
gff2bed1	"What it does
 This tool converts data from GFF format to BED format (scroll down for format description). -------- 
Example
 The following data in GFF format:: chr22 GeneA enhancer 10000000 10001000 500 + . TGA chr22 GeneA promoter 10010000 10010100 900 + . TGA Will be converted to BED (
note
 that 1 is subtracted from the start coordinate):: chr22 9999999 10001000 enhancer 0 + chr22 10009999 10010100 promoter 0 + ------ .. class:: infomark 
About formats
 
BED format
 Browser Extensible Data format was designed at UCSC for displaying data tracks in the Genome Browser. It has three required fields and several additional optional ones: The first three BED fields (required) are:: 1. chrom - The name of the chromosome (e.g. chr1, chrY_random). 2. chromStart - The starting position in the chromosome. (The first base in a chromosome is numbered 0.) 3. chromEnd - The ending position in the chromosome, plus 1 (i.e., a half-open interval). The additional BED fields (optional) are:: 4. name - The name of the BED line. 5. score - A score between 0 and 1000. 6. strand - Defines the strand - either '+' or '-'. 7. thickStart - The starting position where the feature is drawn thickly at the Genome Browser. 8. thickEnd - The ending position where the feature is drawn thickly at the Genome Browser. 9. reserved - This should always be set to zero. 10. blockCount - The number of blocks (exons) in the BED line. 11. blockSizes - A comma-separated list of the block sizes. The number of items in this list should correspond to blockCount. 12. blockStarts - A comma-separated list of block starts. All of the blockStart positions should be calculated relative to chromStart. The number of items in this list should correspond to blockCount. 13. expCount - The number of experiments. 14. expIds - A comma-separated list of experiment ids. The number of items in this list should correspond to expCount. 15. expScores - A comma-separated list of experiment scores. All of the expScores should be relative to expIds. The number of items in this list should correspond to expCount. 
GFF format
 General Feature Format is a format for describing genes and other features associated with DNA, RNA and Protein sequences. GFF lines have nine tab-separated fields:: 1. seqname - Must be a chromosome or scaffold. 2. source - The program that generated this feature. 3. feature - The name of this type of feature. Some examples of standard feature types are ""CDS"", ""start_codon"", ""stop_codon"", and ""exon"". 4. start - The starting position of the feature in the sequence. The first base is numbered 1. 5. end - The ending position of the feature (inclusive). 6. score - A score between 0 and 1000. If there is no score value, enter ""."". 7. strand - Valid entries include '+', '-', or '.' (for don't know/care). 8. frame - If the feature is a coding exon, frame should be a number between 0-2 that represents the reading frame of the first base. If the feature is not a coding exon, the value should be '.'. 9. group - All lines with the same group are linked together into a single item."
toolshed.g2.bx.psu.edu/repos/bgruening/graphicsmagick_image_montage/graphicsmagick_image_montage/1.3.46+galaxy0	"What it does
 Montage composites multiple images into a single, larger image. You may need to resize large images before you attempt to montage them. The width parameter controls how many images wide the montage will be. With a width of 4, and 8 images selected, you will get 2 rows of 4 images. If you have 6 images selected, the first row will have 4 images, and the second will only have two."
MAF_To_BED1	"What it does
 This tool converts every MAF block to an interval line (in BED format; scroll down for description of MAF and BED formats) describing position of that alignment block within a corresponding genome. The interface for this tool contains two pages (steps): * 
Step 1 of 2
. Choose multiple alignments from history to be converted to BED format. * 
Step 2 of 2
. Choose species from the alignment to be included in the output and specify how to deal with alignment blocks that lack one or more species: * 
Choose species
 - the tool reads the alignment provided during Step 1 and generates a list of species contained within that alignment. Using checkboxes you can specify taxa to be included in the output (only reference genome, shown in 
bold
, is selected by default). If you select more than one species, then more than one history item will be created. * 
Choose to include/exclude blocks with missing species
 - if an alignment block does not contain any one of the species you selected within 
Choose species
 menu and this option is set to 
exclude blocks with missing species
, then coordinates of such a block 
will not
 be included in the output (see 
Example 2
 below). ----- 
Example 1
: 
Include only reference genome
 (hg18 in this case) and 
include blocks with missing species
: For the following alignment:: ##maf version=1 a score=68686.000000 s hg18.chr20 56827368 75 + 62435964 GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s panTro2.chr20 56528685 75 + 62293572 GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s rheMac2.chr10 89144112 69 - 94855758 GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- s mm8.chr2 173910832 61 + 181976762 AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- s canFam2.chr24 46551822 67 + 50763139 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C a score=10289.000000 s hg18.chr20 56827443 37 + 62435964 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s panTro2.chr20 56528760 37 + 62293572 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s rheMac2.chr10 89144181 37 - 94855758 ATGTGCGGAAAATGTGATACAGAAACCTGCAGAGCAG the tool will create 
a single
 history item containing the following (
note
 that field 4 is added to the output and is numbered iteratively: hg18_0, hg18_1 etc.):: chr20 56827368 56827443 hg18_0 0 + chr20 56827443 56827480 hg18_1 0 + ----- 
Example 2
: 
Include hg18 and mm8
 and 
exclude blocks with missing species
: For the following alignment:: ##maf version=1 a score=68686.000000 s hg18.chr20 56827368 75 + 62435964 GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s panTro2.chr20 56528685 75 + 62293572 GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s rheMac2.chr10 89144112 69 - 94855758 GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- s mm8.chr2 173910832 61 + 181976762 AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- s canFam2.chr24 46551822 67 + 50763139 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C a score=10289.000000 s hg18.chr20 56827443 37 + 62435964 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s panTro2.chr20 56528760 37 + 62293572 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s rheMac2.chr10 89144181 37 - 94855758 ATGTGCGGAAAATGTGATACAGAAACCTGCAGAGCAG the tool will create 
two
 history items (one for hg18 and one fopr mm8) containing the following (
note
 that both history items contain only one line describing the first alignment block. The second MAF block is not included in the output because it does not contain mm8): History item 
1
 (for hg18):: chr20 56827368 56827443 hg18_0 0 + History item 
2
 (for mm8):: chr2 173910832 173910893 mm8_0 0 + ------- .. class:: infomark 
About formats
 
MAF format
 multiple alignment format file. This format stores multiple alignments at the DNA level between entire genomes. - The .maf format is line-oriented. Each multiple alignment ends with a blank line. - Each sequence in an alignment is on a single line. - Lines starting with # are considered to be comments. - Each multiple alignment is in a separate paragraph that begins with an ""a"" line and contains an ""s"" line for each sequence in the multiple alignment. - Some MAF files may contain two optional line types: - An ""i"" line containing information about what is in the aligned species DNA before and after the immediately preceding ""s"" line; - An ""e"" line containing information about the size of the gap between the alignments that span the current block. 
BED format
 Browser Extensible Data format was designed at UCSC for displaying data tracks in the Genome Browser. It has three required fields and a number of additional optional ones: The first three BED fields (required) are:: 1. chrom - The name of the chromosome (e.g. chr1, chrY_random). 2. chromStart - The starting position in the chromosome. (The first base in a chromosome is numbered 0.) 3. chromEnd - The ending position in the chromosome, plus 1 (i.e., a half-open interval). Additional (optional) fields are:: 4. name - The name of the BED line. 5. score - A score between 0 and 1000. 6. strand - Defines the strand - either '+' or '-'."
MAF_To_Fasta1	"Types of MAF to FASTA conversion
 * 
Multiple Blocks
 converts a single MAF block to a single FASTA block. For example, if you have 6 MAF blocks, they will be converted to 6 FASTA blocks. * 
One Sequence per Species
 converts MAF blocks to a single aggregated FASTA block. For example, if you have 6 MAF blocks, they will be converted and concatenated into a single FASTA block. ------- 
What it does
 This tool converts MAF blocks to FASTA format and concatenates them into a single FASTA block or outputs multiple FASTA blocks separated by empty lines. The interface for this tool contains two pages (steps): * 
Step 1 of 2
. Choose multiple alignments from history to be converted to FASTA format. * 
Step 2 of 2
. Choose the type of output as well as the species from the alignment to be included in the output. Multiple Block output has additional options: * 
Choose species
 - the tool reads the alignment provided during Step 1 and generates a list of species contained within that alignment. Using checkboxes you can specify taxa to be included in the output (all species are selected by default). * 
Choose to include/exclude blocks with missing species
 - if an alignment block does not contain any one of the species you selected within 
Choose species
 menu and this option is set to 
exclude blocks with missing species
, then such a block 
will not
 be included in the output (see 
Example 2
 below). For example, if you want to extract human, mouse, and rat from a series of alignments and one of the blocks does not contain mouse sequence, then this block will not be converted to FASTA and will not be returned. ----- 
Example 1
: In the concatenated approach, the following alignment:: ##maf version=1 a score=68686.000000 s hg18.chr20 56827368 75 + 62435964 GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s panTro2.chr20 56528685 75 + 62293572 GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s rheMac2.chr10 89144112 69 - 94855758 GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- s mm8.chr2 173910832 61 + 181976762 AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- s canFam2.chr24 46551822 67 + 50763139 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C a score=10289.000000 s hg18.chr20 56827443 37 + 62435964 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s panTro2.chr20 56528760 37 + 62293572 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s rheMac2.chr10 89144181 37 - 94855758 ATGTGCGGAAAATGTGATACAGAAACCTGCAGAGCAG will be converted to (
note
 that because mm8 (mouse) and canFam2 (dog) are absent from the second block, they are replaced with gaps after concatenation):: >canFam2 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C------------------------------------- >hg18 GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC-ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG >mm8 AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC-------------------------------------------- >panTro2 GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC-ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG >rheMac2 GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA-------ATGTGCGGAAAATGTGATACAGAAACCTGCAGAGCAG ------ 
Example 2a
: Multiple Block Approach 
Include all species
 and 
include blocks with missing species
: The following alignment:: ##maf version=1 a score=68686.000000 s hg18.chr20 56827368 75 + 62435964 GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s panTro2.chr20 56528685 75 + 62293572 GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s rheMac2.chr10 89144112 69 - 94855758 GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- s mm8.chr2 173910832 61 + 181976762 AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- s canFam2.chr24 46551822 67 + 50763139 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C a score=10289.000000 s hg18.chr20 56827443 37 + 62435964 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s panTro2.chr20 56528760 37 + 62293572 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s rheMac2.chr10 89144181 37 - 94855758 ATGTGCGGAAAATGTGATACAGAAACCTGCAGAGCAG will be converted to:: >hg18.chr20(+):56827368-56827443|hg18_0 GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- >panTro2.chr20(+):56528685-56528760|panTro2_0 GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- >rheMac2.chr10(-):89144112-89144181|rheMac2_0 GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- >mm8.chr2(+):173910832-173910893|mm8_0 AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- >canFam2.chr24(+):46551822-46551889|canFam2_0 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C >hg18.chr20(+):56827443-56827480|hg18_1 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG >panTro2.chr20(+):56528760-56528797|panTro2_1 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG >rheMac2.chr10(-):89144181-89144218|rheMac2_1 ATGTGCGGAAAATGTGATACAGAAACCTGCAGAGCAG ----- 
Example 2b
: Multiple Block Approach 
Include hg18 and mm8
 and 
exclude blocks with missing species
: The following alignment:: ##maf version=1 a score=68686.000000 s hg18.chr20 56827368 75 + 62435964 GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s panTro2.chr20 56528685 75 + 62293572 GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s rheMac2.chr10 89144112 69 - 94855758 GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- s mm8.chr2 173910832 61 + 181976762 AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- s canFam2.chr24 46551822 67 + 50763139 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C a score=10289.000000 s hg18.chr20 56827443 37 + 62435964 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s panTro2.chr20 56528760 37 + 62293572 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s rheMac2.chr10 89144181 37 - 94855758 ATGTGCGGAAAATGTGATACAGAAACCTGCAGAGCAG will be converted to (
note
 that the second MAF block, which does not have mm8, is not included in the output):: >hg18.chr20(+):56827368-56827443|hg18_0 GACAGGGTGCATCTGGGAGGGCCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC >mm8.chr2(+):173910832-173910893|mm8_0 AGAAGGATCCACCT---------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------ ------ .. class:: infomark 
About formats
 
MAF format
 multiple alignment format file. This format stores multiple alignments at the DNA level between entire genomes. - The .maf format is line-oriented. Each multiple alignment ends with a blank line. - Each sequence in an alignment is on a single line. - Lines starting with # are considered to be comments. - Each multiple alignment is in a separate paragraph that begins with an ""a"" line and contains an ""s"" line for each sequence in the multiple alignment. - Some MAF files may contain two optional line types: - An ""i"" line containing information about what is in the aligned species DNA before and after the immediately preceding ""s"" line; - An ""e"" line containing information about the size of the gap between the alignments that span the current block."
MAF_To_Interval1	"What it does
 This tool converts every MAF block to a set of genomic intervals describing the position of that alignment block within a corresponding genome. Sequences from aligning species are also included in the output. The interface for this tool contains several options: * 
MAF file to convert
. Choose multiple alignments from history to be converted to BED format. * 
Choose species
. Choose additional species from the alignment to be included in the output * 
Exclude blocks which have a species missing
. if an alignment block does not contain any one of the species found in the alignment set and this option is set to 
exclude blocks with missing species
, then coordinates of such a block 
will not
 be included in the output (see 
Example 2
 below). * 
Remove Gap characters from sequences
. Gaps can be removed from sequences before they are output. ----- 
Example 1
: 
Include only reference genome
 (hg18 in this case) and 
include blocks with missing species
: For the following alignment:: ##maf version=1 a score=68686.000000 s hg18.chr20 56827368 75 + 62435964 GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s panTro2.chr20 56528685 75 + 62293572 GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s rheMac2.chr10 89144112 69 - 94855758 GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- s mm8.chr2 173910832 61 + 181976762 AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- s canFam2.chr24 46551822 67 + 50763139 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C a score=10289.000000 s hg18.chr20 56827443 37 + 62435964 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s panTro2.chr20 56528760 37 + 62293572 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s rheMac2.chr10 89144181 37 - 94855758 ATGTGCGGAAAATGTGATACAGAAACCTGCAGAGCAG the tool will create 
a single
 history item containing the following (
note
 the name field is numbered iteratively: hg18_0_0, hg18_1_0 etc. where the first number is the block number and the second number is the iteration through the block (if a species appears twice in a block, that interval will be repeated) and sequences for each species are included in the order specified in the header: the field is left empty when no sequence is available for that species):: #chrom start end strand score name canFam2 hg18 mm8 panTro2 rheMac2 chr20 56827368 56827443 + 68686.0 hg18_0_0 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- chr20 56827443 56827480 + 10289.0 hg18_1_0 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG ATGTGCGGAAAATGTGATACAGAAACCTGCAGAGCAG ----- 
Example 2
: 
Include hg18 and mm8
 and 
exclude blocks with missing species
: For the following alignment:: ##maf version=1 a score=68686.000000 s hg18.chr20 56827368 75 + 62435964 GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s panTro2.chr20 56528685 75 + 62293572 GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- s rheMac2.chr10 89144112 69 - 94855758 GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- s mm8.chr2 173910832 61 + 181976762 AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- s canFam2.chr24 46551822 67 + 50763139 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C a score=10289.000000 s hg18.chr20 56827443 37 + 62435964 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s panTro2.chr20 56528760 37 + 62293572 ATGTGCAGAAAATGTGATACAGAAACCTGCAGAGCAG s rheMac2.chr10 89144181 37 - 94855758 ATGTGCGGAAAATGTGATACAGAAACCTGCAGAGCAG the tool will create 
two
 history items (one for hg18 and one for mm8) containing the following (
note
 that both history items contain only one line describing the first alignment block. The second MAF block is not included in the output because it does not contain mm8): History item 
1
 (for hg18):: #chrom start end strand score name canFam2 hg18 mm8 panTro2 rheMac2 chr20 56827368 56827443 + 68686.0 hg18_0_0 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- History item 
2
 (for mm8):: #chrom start end strand score name canFam2 hg18 mm8 panTro2 rheMac2 chr2 173910832 173910893 + 68686.0 mm8_0_0 CG------GCGTCTGTAAGGGGCCACCGCCCGGCCTGTG-CTCAAAGCTACAAATGACTCAACTCCCAACCGA------C GACAGGGTGCATCTGGGAGGG---CCTGCCGGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- AGAAGGATCCACCT------------TGCTGGGCCTCTGCTCCAGCAAGACCCACCTCCCAACTCAAATGCCC------- GACAGGGTGCATCTGAGAGGG---CCTGCCAGGCCTTTA-TTCAACACTAGATACGCCCCATCTCCAATTCTAATGGAC- GACAGGGTGCATCTGAGAGGG---CCTGCTGGGCCTTTG-TTCAAAACTAGATATGCCCCAACTCCAATTCTA------- ------- .. class:: infomark 
About formats
 
MAF format
 multiple alignment format file. This format stores multiple alignments at the DNA level between entire genomes. - The .maf format is line-oriented. Each multiple alignment ends with a blank line. - Each sequence in an alignment is on a single line. - Lines starting with # are considered to be comments. - Each multiple alignment is in a separate paragraph that begins with an ""a"" line and contains an ""s"" line for each sequence in the multiple alignment. - Some MAF files may contain two optional line types: - An ""i"" line containing information about what is in the aligned species DNA before and after the immediately preceding ""s"" line; - An ""e"" line containing information about the size of the gap between the alignments that span the current block."
toolshed.g2.bx.psu.edu/repos/iuc/bam2fastx/bam2fastx/3.5.0+galaxy0	Convert a PacBio BAM file to Fastq or Fasta file.
Sff_extractor	"What it does
 This tool extracts data from the 454 Sequencer SFF format and creates three files containing the: Sequences (FASTA), Qualities (QUAL) and Clippings (XML)"
toolshed.g2.bx.psu.edu/repos/devteam/tabular_to_fastq/tabular_to_fastq/1.1.5+galaxy2	"What it does
 This tool attempts to convert a tabular file containing sequencing read data to a FASTQ formatted file. The FASTQ Groomer tool should always be used on the output of this tool."
toolshed.g2.bx.psu.edu/repos/devteam/tabular_to_fasta/tab2fasta/1.1.0	"What it does
 Converts tab delimited data into FASTA formatted sequences. ----------- 
Example
 Suppose this is a sequence file produced by Illumina (Solexa) sequencer:: 5 300 902 419 GACTCATGATTTCTTACCTATTAGTGGTTGAACATC 5 300 880 431 GTGATATGTATGTTGACGGCCATAAGGCTGCTTCTT Selecting 
c3
 and 
c4
 as the 
Title column(s)
 and 
c5
 as the 
Sequence column
 will result in:: >902_419 GACTCATGATTTCTTACCTATTAGTGGTTGAACATC >880_431 GTGATATGTATGTTGACGGCCATAAGGCTGCTTCTT"
toolshed.g2.bx.psu.edu/repos/devteam/tabular_to_fasta/tab2fasta/1.1.1	"What it does
 Converts tab delimited data into FASTA formatted sequences. ----------- 
Example
 Suppose this is a sequence file produced by Illumina (Solexa) sequencer:: 5 300 902 419 GACTCATGATTTCTTACCTATTAGTGGTTGAACATC 5 300 880 431 GTGATATGTATGTTGACGGCCATAAGGCTGCTTCTT Selecting 
c3
 and 
c4
 as the 
Title column(s)
 and 
c5
 as the 
Sequence column
 will result in:: >902_419 GACTCATGATTTCTTACCTATTAGTGGTTGAACATC >880_431 GTGATATGTATGTTGACGGCCATAAGGCTGCTTCTT"
toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator/tooldistillator/1.0.4+galaxy0	"What it does
 ToolDistillator is a tool to extract information from output files of specific tools, expose it as JSON files, and aggregate over several tools. This tool here is extracting information from output files of specific tools and exposing it as JSON files ** Tool input** A diverse set of tools is available, along with a generic one for tabular files with headers. There is also a command to aggregate JSON outputs. +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Tools | Version | Default input file | Optional files | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Abricate | 1.0.1 | output.tsv | | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | AMRFinderPlus | 3.11.26 | report.tsv | point_mutation_report.tsv, nucleotide_sequence.fasta | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | argNorm | 1.0.0 | output.tsv | | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Bakta | 1.7.0 | output.json | protein.faa, nucleotide.fna, annotation.gff3, summary.txt | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Bandage | 0.8.1 | info.txt | bandage_plot.svg | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Bracken | 2.8 | output.tsv | taxonomy.tsv | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | BWA | 2.2.1 | input1.bam | input2.bam | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | CheckM2 | 1.0.2 | quality_report.tsv | DIAMOND_RESULTS.tsv, protein_files.zip, checkm2.log | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Concoct | 1.1.0 | merge_cut_clusters.csv | fasta_bin_zip_folder.zip, contigs_cut_up.fasta, coordinates_cut_up.bed, coverage_table.tabular, log_file.txt | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | CoreProfiler | 1.1.1 | calling_results.tsv | profiles_w_tmp_alleles.json, new_alleles.fasta | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | CoverM | 0.7.0 | coverage_report.tsv | dereplication_cluster_definition.tsv, dereplication_representative_fasta_zip.zip | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | DasTool | 1.1.7 | summary_bins.tabular | fasta_bin_zip_folder.zip, contig_to_bin.tabular, quality_and_completness.tabular, protein_sequences.fasta, unbinned_sequences.fasta, log_file.txt | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | DeepARG | 1.0.4 | result_file.clean.deeparg.mapping.ARG | report_arg_merged.tsv, report_arg_merged_quant_subtype.tsv, report_arg_merged_quant_type.tsv, report_potential_arg.tsv, sequence_clean.fasta, bam_clean.bam, sam_clean.txt, bam_clean_sorted.bam, daa_clean_align.daa | +-----------------------------+----------------+---------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------+ | dRep | 3.4.2 | Widb.csv | fasta_dereplicated_bin_zip_folder.zip, bdb_file.csv, cdb_file.csv, chdb_file.csv, mdb_file.csv, ndb_file.csv, sdb_file.csv, wdb_file.csv, winning_genomes_pdf.pdf, cluster_scoring_pdf.pdf, clustering_scatterplots_pdf.pdf, primary_clustering_dendrogram_pdf.pdf, secondary_clustering_dendrogram_pdf.pdf, secondary_clustering_mds_pdf.pdf, log_file.txt | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Eggnog-Mapper | 2.1.12 | annotation_report.tsv | seed_orthologs_report.tsv, orthologs_report.tsv | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Fastp | 0.23.2 | output.json | | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Fastqc | 0.12.1 | report.txt | report.html | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Filtlong | 0.2.1 | input.fastq | | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Flye | 2.9.1 | contig.fasta | contig.gfa, infos.tsv | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Groot | 1.1.2 | report.tsv | bam_file.bam, groot_log.txt | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | GTDB-tk | 2.4.1 | taxonomy_summary.tsv | classify_directory.zip, align_directory.zip, identify_directory.zip, log_file.txt | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Integronfinder2 | 2.0.2 | output.integrons | output.summary | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | ISEScan | 1.7.2.3 | output.tsv | is.fna, orf.faa, orf.fna | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Kraken2 | 2.1.2 | taxonomy.tsv | reads_assignation.txt | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Maxbin2 | 2.2.7 | bin_summary.tabular | fasta_bin_zip_folder.zip, bin_predicted_markers_zip_folder.zip, too_short_sequences.fasta, unclassified_sequences.fasta, marker_gene_presence.tabular, marker_gene_presence_plot.pdf, log_file.txt | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Megahit | 1.2.9 | assembly.fasta | intermediate_contig_folder.zip, log_file.txt | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Metabat2 | 2.17 | cluster_membership.tsv | fasta_bin_zip_folder.zip, too_short_sequences.fasta, unbinned_sequences.fasta, low_depth_sequences.fasta | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | MMseqs2 linclust | 17-b804f | rep_seq.fasta | cluster_fasta_like.fasta, tsv_file.tsv | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | MMseqs2 taxonomy | 17-b804f | tax_output.tsv | kraken_report.txt, krona_report.html | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | MultiQC | 1.11 | report.html | | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PlasmidFinder | 2.1.6 | output.json | genome_hits.fasta, plasmid_hits.fasta | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Polypolish | 0.5.0 | contig.fasta | | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Prodigal | 2.6.3 | output.fnn | protein_translation.faa, potential_gene_start.start, gbk_genes_coordinate.gbk, gff_genes_coordinate.gff, sco_genes_coordinate.sco | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Quast | 5.2.0 | output.tsv | | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Recentrifuge | 1.10.0 | data.tsv | report.html, stat.tsv | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | RefSeqMasher | 0.1.2 | output.tsv | | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Shovill | 1.1.0 | contigs.fasta | alignment.bam, contigs.gfa | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Staramr | 0.9.1 | resfinder.tsv | mlst.tsv, pointfinder.tsv, plasmidfinder.tsv, settings.tsv | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Sylph | 0.8.1 | report.tsv | | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Sylph-tax | 1.2.0 | merge_report.tsv | taxonomic_profile_folder.zip | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | tabular_file | 0 | output.tsv | | +-----------------------------+----------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ** Options ** You can add a tool version and a database version for related tools. For some tools you can add optional files previously produced by the tool"
toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator_summarize/tooldistillator_summarize/1.0.4+galaxy0	"What it does
 ToolDistillator is a tool to extract information from output files of specific tools, expose it as JSON files, and aggregate over several tools. This tool here summarize JSON output file from ToolDistillator to only one JSON file"
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_axttomaf/ucsc_axtomaf/482+galaxy1	"What it does
 
axtToMaf
 is a tool to convert dataset from 
axt format
 to 
MAF format
. For implementation details see axtToMaf's 
source code
. .. _axtToMaf: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _axt format: https://genome.ucsc.edu/goldenPath/help/axt.html .. _MAF format: https://genome.ucsc.edu/FAQ/FAQformat.html#format5 .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/hg/mouseStuff/axtToMaf/axtToMaf.c"
toolshed.g2.bx.psu.edu/repos/iuc/bax2bam/bax2bam/0.0.11+galaxy0	".. class:: infomark 
What it does
 bax2bam converts the legacy PacBio basecall format (bax.h5) into the BAM basecall format. 
Input
 - Sample files that should be from the same movie (bax.h5) 
Output
 - Converted sample (BAM) .. class:: infomark 
References
 More information can be found on the GitHub repositories 
bax2bam &lt;https://github.com/pacificbiosciences/bax2bam/&gt;
 and 
PacBio Bioconda &lt;https://github.com/PacificBiosciences/pbbioconda&gt;
."
toolshed.g2.bx.psu.edu/repos/galaxyp/bed_to_protein_map/bed_to_protein_map/0.2.0	"Convert a BED format file of the proteins from a proteomics search database into a tabular format for the Multiomics Visualization Platform (MVP). Example input BED dataset:: X 276352 291629 ENST00000430923 20 + 284187 291629 80,80,80 5 42,148,137,129,131 0,7814,12380,14295,15146 X 304749 318819 ENST00000326153 20 - 305073 318787 80,80,80 10 448,153,149,209,159,68,131,71,138,381 0,2610,2982,6669,8016,9400,10140,10479,12164,13689 Output:: name chrom start end strand cds_start cds_end ENST00000430923 X 284187 284314 + 0 127 ENST00000430923 X 288732 288869 + 127 264 ENST00000430923 X 290647 290776 + 264 393 ENST00000430923 X 291498 291629 + 393 524 ENST00000326153 X 318438 318787 - 0 349 ENST00000326153 X 316913 317051 - 349 487 ENST00000326153 X 315228 315299 - 487 558 ENST00000326153 X 314889 315020 - 558 689 ENST00000326153 X 314149 314217 - 689 757 ENST00000326153 X 312765 312924 - 757 916 ENST00000326153 X 311418 311627 - 916 1125 ENST00000326153 X 307731 307880 - 1125 1274 ENST00000326153 X 307359 307512 - 1274 1427 ENST00000326153 X 305073 305197 - 1427 1551 The tabular output can be converted to a sqlite database using the Query_Tabular_ tool. The sqlite table should be named: feature_cds_map The names for the columns should be: name,chrom,start,end,strand,cds_start,cds_end This SQL query will return the genomic location for a peptide sequence in a protein (multiply the animo acid position by 3 for the cds location):: SELECT distinct chrom, CASE WHEN strand = '+' THEN start + cds_offset - cds_start ELSE end - cds_offset - cds_start END as ""pos"" FROM feature_cds_map WHERE name = acc_name AND cds_offset >= cds_start AND cds_offset < cds_end .. _Query_Tabular: https://toolshed.g2.bx.psu.edu/view/iuc/query_tabular/1ea4e668bf73"
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_fatovcf/fatovcf/482+galaxy0	"What it does
 
faToVcf
_ is a tool to extract a VCF from a multi-sequence FASTA alignment. .. _faToVcf: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt"
toolshed.g2.bx.psu.edu/repos/devteam/gffread/gffread/2.2.1.4+galaxy0	[['strand']'chr':]'start'..'end' <br> examples: <br> 1000..500000 <br> chr1:1000..500000 <br> +chr1:1000..500000 <br> -chr1:1000..500000
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_maftoaxt/maftoaxt/482+galaxy0	"What it does
 
mafToAxt
 is a tool to convert 
MAF
 dataset to 
axt
 format. Depending on parameters and the number of alignments in the MAF file this can be a full conversion or data extraction. For implementation details see mafToAxt's 
source code
. .. _mafToAxt: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _axt: https://genome.ucsc.edu/goldenPath/help/axt.html .. _MAF: https://genome.ucsc.edu/FAQ/FAQformat.html#format5 .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/hg/mouseStuff/mafToAxt/mafToAxt.c"
toolshed.g2.bx.psu.edu/repos/galaxyp/msconvert/msconvert/3.0.20287.6	"What it does
 Converts mass spectrometry (MS) files: proprietary MS vendor formats can be converted to open MS formats (mzML, mzXML, MGF, MS1/MS2) and open formats can be converted to other open formats. Additional options such as filtering and/or precursor recalculation are available. You can view the original documentation here_. .. _here: http://proteowizard.sourceforge.net/tools/msconvert.html"
toolshed.g2.bx.psu.edu/repos/galaxyp/mz_to_sqlite/mz_to_sqlite/2.1.1+galaxy0	** mz_to_sqlite converts proteomics file formats to a SQLite database**
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_nettoaxt/ucsc_nettoaxt/482+galaxy0	"What it does
 
netToAxt
 is a tool from the UCSC suite to convert dataset from 
ucsc.net
 format (and 
alignment chains
) to 
axt
 format. For implementation details see netToAxt's 
source code
_. .. _netToAxt: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _ucsc.net: https://genome.ucsc.edu/goldenPath/help/net.html .. _alignment chains: https://genome.ucsc.edu/goldenPath/help/chain.html .. _axt: https://genome.ucsc.edu/goldenPath/help/axt.html .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/hg/mouseStuff/netToAxt/netToAxt.c"
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_twobittofa/ucsc-twobittofa/482	"What it does
 
twoBitToFa
 is a tool to convert all or part of .2bit file to FASTA. For more information, check the 
user manual &lt;https://genome.ucsc.edu/goldenpath/help/twoBit.html&gt;
. For implementation details see twoBitToFa's 
source code
_. .. _twoBitToFa: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/utils/twoBitToFa/twoBitToFa.c"
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_wigtobigwig/ucsc_wigtobigwig/482+galaxy0	"Purpose
 This tool converts bedgraph or wiggle data into bigWig data for visualisation in browsers. The JBrowse tool can display these with other interesting tracks. 
Technical format details
 - 
Wiggle format
: The .wig format is line-oriented. Wiggle data is preceded by a UCSC track definition line. Following the track definition line is the track data, which can be entered in three different formats described below. - 
BED format
 with no declaration line and four columns of data:: chromA chromStartA chromEndA dataValueA chromB chromStartB chromEndB dataValueB - 
variableStep
 two column data; started by a declaration line and followed with chromosome positions and data values:: variableStep chrom=chrN [span=windowSize] chromStartA dataValueA chromStartB dataValueB - 
fixedStep
 single column data; started by a declaration line and followed with data values:: fixedStep chrom=chrN start=position step=stepInterval [span=windowSize] dataValue1 dataValue2 - The 
BedGraph format
 is described in detail at the 
UCSC Bioinformatics website
 For implementation details see wigToBigWig's 
source code
 and accompanying 
docs
_. .. _UCSC Bioinformatics website: http://genome.ucsc.edu/goldenPath/help/bedgraph.html .. _docs: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/utils/wigToBigWig/wigToBigWig.c"
toolshed.g2.bx.psu.edu/repos/iuc/datamash_ops/datamash_ops/1.9+galaxy0	".. class:: infomark 
TIP:
 Input data must be TAB delimited. If the desired dataset does not appear in the input list, use 
Text Manipulation->Convert
 to convert it to 
Tabular
 type. ----- 
Syntax
 This tools performs common operations (such as summing, counting, mean, standard-deviation) on input file, based on tabular data. The tool can also optionaly group the input based on a given field. ----- 
Example 1
 - Find the average score in statistics course of college students, grouped by their college major. The input file has three fields (Name,Major,Score) and a header line:: Name Major Score Bryan Arts 68 Gabriel Health-Medicine 100 Isaiah Arts 80 Tysza Business 92 Zackery Engineering 54 ... ... - Grouping the input by the second column (
Major
), sorting the input, and performing operations 
mean
 and 
sample standard deviation
 on the third column (
Score
), gives:: GroupBy(Major) mean(Score) sstdev(Score) Arts 68.9474 10.4215 Business 87.3636 5.18214 Engineering 66.5385 19.8814 Health-Medicine 90.6154 9.22441 Life-Sciences 55.3333 20.606 Social-Sciences 60.2667 17.2273 Note that input needs sorting here, since the column used for grouping (
Major
) is not sorted. This sample file is available at http://www.gnu.org/software/datamash . 
Example 2
 - Using the UCSC RefSeq Human Gene Track, available at: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/refGene.txt.gz - List the number and identifiers of isoforms per gene. The gene identifier is in column 13, the isoform/transcript identifier is in column 2. Grouping by column 13 and performing 
count
 and 
Combine all values
 on column 2, gives:: GroupBy(field-13) count(field-2) collapse(field-2) A1BG 1 NM_130786 A1BG-AS1 1 NR_015380 A1CF 6 NM_001198818,NM_001198819,NM_001198820,NM_014576,NM_138932,NM_138933 A2M 1 NM_000014 A2M-AS1 1 NR_026971 A2ML1 2 NM_001282424,NM_144670 ... - Count how many transcripts are listed for each chromosome and strand. Chromosome is on column 3, Strand is in column 4. Transcript identifiers are in column 2. Grouping by columns 
3,4
 and performing operation 
count
 on column 2, gives:: GroupBy(field-3) GroupBy(field-4) count(field-2) chr1 + 2456 chr1 - 2431 chr2 + 1599 chr2 - 1419 chr3 + 1287 chr3 - 1249 ... ----- 
GNU Datamash
 is a Free and Open Source Software, see more details on the Datamash_ Website. For more details about supported statistical operations, see https://www.gnu.org/software/datamash/download/ For more details about supported statistical operations, see Datamash_ website. .. _Datamash: https://www.gnu.org/software/datamash/"
toolshed.g2.bx.psu.edu/repos/iuc/datamash_reverse/datamash_reverse/1.9+galaxy0	".. class:: infomark 
TIP:
 Input data must be TAB delimited. If the desired dataset does not appear in the input list, use 
Text Manipulation->Convert
 to convert it to 
Tabular
 type. ----- 
Syntax
 This tools reverses the order of columns in a tabular input file. ----- 
Example
 Input file:: Genes Sample Counts NOX1 A1 514 DcP A2 542 HH B3 490 Output file:: Counts Sample Genes 514 A1 NOX1 542 A2 DcP 490 B3 HH ----- 
GNU Datamash
 is a Free and Open Source Software, see more details on the Datamash_ Website. For more details about supported statistical operations, see https://www.gnu.org/software/datamash/download/ For more details about supported statistical operations, see Datamash_ website. .. _Datamash: https://www.gnu.org/software/datamash/"
toolshed.g2.bx.psu.edu/repos/iuc/datamash_transpose/datamash_transpose/1.9+galaxy0	".. class:: infomark 
TIP:
 Input data must be TAB delimited. If the desired dataset does not appear in the input list, use 
Text Manipulation->Convert
 to convert it to 
Tabular
 type. ----- 
Syntax
 This tools transposes (swaps) rows/columns in a tabular input file. ----- 
Example
 Input file:: Genes NOX1 DcP HH Sample A1 A2 B3 Counts 514 542 490 Output file:: Genes Sample Counts NOX1 A1 514 DcP A2 542 HH B3 490 ----- 
GNU Datamash
 is a Free and Open Source Software, see more details on the Datamash_ Website. For more details about supported statistical operations, see https://www.gnu.org/software/datamash/download/ For more details about supported statistical operations, see Datamash_ website. .. _Datamash: https://www.gnu.org/software/datamash/"
toolshed.g2.bx.psu.edu/repos/iuc/variant_analyzer/read2mut/2.0.0	"What it does
 Takes a VCF file with mutations, a BAM file of aligned raw reads, and JSON files created by the tools 
DCS mutations to tags/reads
 and 
DCS mutations to SSCS stats
 as input and calculates frequencies and stats for DCS mutations based on information from the raw reads. 
Input
 
Dataset 1:
 VCF file with duplex consesus sequence (DCS) mutations. E.g. generated by the 
FreeBayes variant caller &lt;https://arxiv.org/abs/1207.3907&gt;
. 
Dataset 2:
 BAM file of aligned raw reads. This file can be obtained by the tool 
Map with BWA-MEM &lt;https://arxiv.org/abs/1303.3997&gt;
. 
Dataset 3:
 JSON file generated by the 
DCS mutations to tags/reads
 tool containing dictonaries of the tags of reads containing mutations in the DCS. 
Dataset 4:
 JSON file generated by the 
DCS mutations to SSCS stats
 tool stats of tags that carry a mutation in the SSCS at the same position a mutation is called in the DCS. 
Output
 The output is an XLSX file containing frequencies stats for DCS mutations based on information from the raw reads. In addition to that a tier based classification is provided based on the amout of support for a true variant call."
toolshed.g2.bx.psu.edu/repos/iuc/variant_analyzer/mut2sscs/2.0.0	"What it does
 Takes a VCF file with DCS mutations and a BAM file of aligned SSCS reads as input and writes statistics about tags of reads that carry a mutation in the SSCS at the same position a mutation is called in the DCS to a user specified output file.. 
Input
 
Dataset 1:
 VCF file with duplex consesus sequence (DCS) mutations. E.g. generated by the 
FreeBayes variant caller &lt;https://arxiv.org/abs/1207.3907&gt;
. 
Dataset 2:
 BAM file of aligned single stranded consensus sequence (SSCS) reads. This file can be obtained by the tool 
Map with BWA-MEM &lt;https://arxiv.org/abs/1303.3997&gt;
. 
Output
 The output is a json file containing dictonaries with stats of tags that carry a mutation in the SSCS at the same position a mutation is called in the DCS."
toolshed.g2.bx.psu.edu/repos/iuc/variant_analyzer/mut2read/2.0.0	"What it does
 Takes a VCF file with mutations, a BAM file of aligned DCS reads, and a tabular file with aligned families as input and prints all tags of reads that carry a mutation to a user specified output file and creates a fastq file of reads of tags with a mutation. 
Input
 
Dataset 1:
 VCF file with duplex consesus sequence (DCS) mutations. E.g. generated by the 
FreeBayes variant caller &lt;https://arxiv.org/abs/1207.3907&gt;
. 
Dataset 2:
 BAM file of aligned DCS reads. This file can be obtained by the tool 
Map with BWA-MEM &lt;https://arxiv.org/abs/1303.3997&gt;
. 
Dataset 3:
 Tabular file with reads as produced by the 
Du Novo: Align families
 tool of the 
Du Novo Analysis Pipeline &lt;https://doi.org/10.1186/s13059-016-1039-4&gt;
_ 
Output
 The output is a json file containing dictonaries of the tags of reads containing mutations in the DCS and a fastq file of all reads of these tags."
toolshed.g2.bx.psu.edu/repos/nick/dunovo/align_families/3.0.2	"What it does
 This is for processing duplex sequencing data. It does a multiple sequence alignment on each (single-stranded) family of reads. ----- 
Input
 This expects the output format of the ""Make families"" tool. ----- 
Output
 The output is a tabular file where each line corresponds to a (single) read. The columns are:: 1: barcode (both tags) 2: tag order in barcode (""ab"" or ""ba"") 3: read mate (""1"" or ""2"") 4: read name 5: read sequence, aligned (""-"" for gaps) 6: read quality scores, aligned ("" "" for gaps) ----- 
Alignments
 When ""MAFFT"" is selected as the multiple sequence aligner, the alignments are done with the command :: $ mafft --nuc --quiet family.fa > family.aligned.fa"
toolshed.g2.bx.psu.edu/repos/nick/dunovo/precheck/3.0.2	".. class:: infomark 
What it does
 This tool lets you check your input reads before running the Du Novo pipeline. It will tell you about how many unique barcodes are in your dataset, how many families have boths strands present, how many consensus sequences of each type it will be able to form, and more. .. class:: infomark 
Input
 The input must be in FASTQ format."
toolshed.g2.bx.psu.edu/repos/nick/dunovo/correct_barcodes/3.0.2	"What it does
 This is for processing duplex sequencing data. This will correct duplex barcodes and create new, larger families. Errors in barcodes normally prevent them from being recognized as the same as the other barcodes in their family. Correcting these errors allows the original, full families to be reconstructed, saving reads which would otherwise be lost. This tool accomplishes this by doing an all vs. all alignment between the barcodes with bowtie. This identifies ones which are identical except a few, small differences. ----- 
Input
 This expects the output format of the ""Make families"" tool. ----- 
Output
 The output format is the same as the input format, ready to be consumed by the ""Align families"" tool."
toolshed.g2.bx.psu.edu/repos/nick/dunovo/dunovo/3.0.2	"What it does
 This is for processing duplex sequencing data. It creates single-strand and duplex consensus reads from aligned read families. ----- 
Input
 This expects the output format of the ""Align families"" tool. ----- 
Output
 This will output final, duplex consensus reads in two FASTA files (first and second reads in the pairs). Optionally, you can save the single-strand reads too, in a separate FASTA file."
toolshed.g2.bx.psu.edu/repos/nick/dunovo/make_families/3.0.2	"What it does
 This tool is for processing raw duplex sequencing data, removing the barcodes and grouping by them into families of reads from the same fragment. ----- 
Output
 The output will be a tabular file where each line corresponds to a pair of input reads. The columns are:: 1: barcode (both tags joined and ordered) 2: tag order in barcode (""ab"" or ""ba"") 3: read1 name 4: read1 sequence (minus the tag and invariant sequences) 5: read1 quality scores (minus the same tag and invariant) 6: read2 name 7: read2 sequence (minus the tag and invariant sequences) 8: read2 quality scores (minus the same tag and invariant) ----- 
Barcode creation
 For each pair, the tool will remove the tag at the beginning of each read and create a barcode by concatenating the two tags. The order of the tags is determined by a string comparison so that it will make an identical barcode from pairs of either order. The original tag order will be noted in the second column. Since pairs from opposite strands will have the same tags, but in the reverse order, this produces the same barcode for reads from the same fragment, regardless of strand. Then a simple sort will group all reads from the same strand together, separated into strands by the different ""order"" values. Examples:: +---------------+-----------------+ | input tags | output | +-------+-------+-------+---------+ | read1 | read2 | order | barcode | +-------+-------+-------+---------+ | ATG | CCT | ab | ATGCCT | +-------+-------+-------+---------+ | CCT | ATG | ba | ATGCCT | +-------+-------+-------+---------+"
toolshed.g2.bx.psu.edu/repos/iuc/duplex_family_size_distribution/fsd_beforevsafter/1.0.2	"What it does
 This tool will create a distribution of family sizes from tags of various steps of the 
Du Novo Analysis Pipeline &lt;https://doi.org/10.1186/s13059-016-1039-4&gt;
. 
Input
 
Dataset 1:
 This tools expects a tabular file with the tags of all families, their sizes and information about forward (ab) and reverse (ba) strands.:: 1 2 3 ----------------------------- 1 AAAAAAAAAAAAAAAAATGGTATG ba 3 AAAAAAAAAAAAAATGGTATGGAC ab .. class:: infomark 
How to generate the input
 The first step of the 
Du Novo Analysis Pipeline &lt;https://doi.org/10.1186/s13059-016-1039-4&gt;
 is the 
Make Families
 tool or the 
Correct Barcodes
 tool that produces output in this form:: 1 2 3 4 ------------------------------------------------------ AAAAAAAAAAAAAAATAGCTCGAT ab read1 CGCTACGTGACTGGGTCATG AAAAAAAAAAAAAAATAGCTCGAT ab read2 CGCTACGTGACTGGGTCATG AAAAAAAAAAAAAAATAGCTCGAT ab read3 CGCTACGTGACTGGGTCATG AAAAAAAAAAAAAAAAATGGTATG ba read3 CGCTACGTGACTAAAACATG We only need columns 1 and 2. These two columns can be extracted from this dataset using the 
Cut
 tool:: 1 2 --------------------------- AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAAAATGGTATG ba Next, the tags are sorted in ascending or descending order using the 
Sort
 tool:: 1 2 --------------------------- AAAAAAAAAAAAAAAAATGGTATG ba AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab Finally, unique occurencies of each tag are counted. This is done using 
Unique lines
 tool that adds an additional column with the counts that also represent the family size (column 1):: 1 2 3 ----------------------------- 1 AAAAAAAAAAAAAAAAATGGTATG ba 3 AAAAAAAAAAAAAATGGTATGGAC ab These data can now be used in this tool. 
Dataset 2:
 A fasta file is required with all tags and the associated family size of both strands (forward and reverse) in the header and the read itself in the next line. This input file can be obtained by the 
Du Novo Analysis Pipeline &lt;https://doi.org/10.1186/s13059-016-1039-4&gt;
 with the tool 
Make consensus reads
. 
Dataset 3 (optional):
 In addition, the fasta file with all tags, which were not filtered out after trimming, can be included. This file can also be obtained by the 
Du Novo Analysis Pipeline
 with the tool 
Sequence Content Trimmer
. For both input files (dataset 2 and 3), only the DCSs of one mate are necessary these tools give information on both mates in the output file), since both files have the same tags and family sizes, but different DCSs, which are not required in this tool:: >AAAAAAAAATAGATCATAGACTCT 7-10 CTAGACTCACTGGCGTTACTGACTGCGAGACCCTCCACGTG >AAAAAAAAGGCAGAAGATATACGC 11-3 CNCNGGCCCCCCGCTCCGTGCACAGACGNNGCNACTGACAA 
Dataset 4 (optional):
 BAM file of the aligned reads. This file can be obtained by the tool 
Map with BWA-MEM &lt;https://arxiv.org/abs/1303.3997&gt;
. 
Output
 The output is a PDF file with a plot and a summary of the data of the plots. The tool compares various datasets from the 
Du Novo Analysis Pipeline &lt;https://doi.org/10.1186/s13059-016-1039-4&gt;
_ and helps in decision making of various parameters (e.g family size, minimum read length, etc). For example: Re-running trimming with different parameters allows to recover reads that would be lost due to stringent filtering by read length. This tool also allows to assess reads on target. The tool extracts the tags of reads and their family sizes before SSCS building, after DCS building, after trimming and finally after the alignment to the reference genome. In the plot, the family sizes for both SSCS-ab and SSCS-ba are shown; whereas the summary represents only counts either of SSCS-ab or of SSCS-ba."
toolshed.g2.bx.psu.edu/repos/iuc/duplex_family_size_distribution/fsd_regions/1.0.1	"What it does
 This tool provides a computationally very fast insight into the distribution of the family sizes of ALL tags from a Duplex Sequencing (DS) experiment that were aligned to different regions targeted in the reference genome. 
Input
 
Dataset 1:
 This tools expects a tabular file with the tags of all families, their sizes and information about forward (ab) and reverse (ba) strands:: 1 2 3 ----------------------------- 1 AAAAAAAAAAAAAAAAATGGTATG ba 3 AAAAAAAAAAAAAATGGTATGGAC ab .. class:: infomark 
How to generate the input
 The first step of the 
Du Novo Analysis Pipeline &lt;https://doi.org/10.1186/s13059-016-1039-4&gt;
 is the 
Make Families
 tool or the 
Correct Barcodes
 tool that produces output in this form:: 1 2 3 4 ------------------------------------------------------ AAAAAAAAAAAAAAATAGCTCGAT ab read1 CGCTACGTGACTGGGTCATG AAAAAAAAAAAAAAATAGCTCGAT ab read2 CGCTACGTGACTGGGTCATG AAAAAAAAAAAAAAATAGCTCGAT ab read3 CGCTACGTGACTGGGTCATG AAAAAAAAAAAAAAAAATGGTATG ba read3 CGCTACGTGACTAAAACATG We only need columns 1 and 2. These two columns can be extracted from this dataset using the 
Cut
 tool:: 1 2 --------------------------- AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAAAATGGTATG ba Next, the tags are sorted in ascending or descending order using the 
Sort
 tool:: 1 2 --------------------------- AAAAAAAAAAAAAAAAATGGTATG ba AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab Finally, unique occurencies of each tag are counted. This is done using 
Unique lines
 tool that adds an additional column with the counts that also represent the family size (column 1):: 1 2 3 ----------------------------- 1 AAAAAAAAAAAAAAAAATGGTATG ba 3 AAAAAAAAAAAAAATGGTATGGAC ab These data can now be used in this tool. 
Dataset 2:
 BAM file of the aligned reads. This file can be obtained by the tool 
Map with BWA-MEM &lt;https://arxiv.org/abs/1303.3997&gt;
. 
Dataset 3 (optional):
 BED file with start and stop positions of the regions. If it is not provided, then all aligned reads of the BAM file are used in the distribution of family sizes:: 1 2 3 --------------------- ACH_TDII 90 633 ACH_TDII 659 1140 ACH_TDII 1144 1561 
Output
 The output is a PDF file with the plot and the summarized data of the plot. The tool creates a distribution of family sizes of tags that were aligned to the reference genome. Note that tags that overlap different regions of the reference genome are counted for each region. This tool is useful to examine differences in coverage among targeted regions. The plot includes both complementary SSCS-ab and SSCS-ba that form a DCS; whereas the table shows only single counts of the tags per region."
toolshed.g2.bx.psu.edu/repos/iuc/duplex_family_size_distribution/fsd/1.0.2	"What it does
 This tool provides a computationally very fast insight into the distribution of the family sizes of ALL tags from a Duplex Sequencing experiment (DS) and gives a first assessment of the distribution of PE-reads in families with 1 member up to >20 members. This information is very useful in early decision steps of the analysis parameters, such as the minimum number of PE-reads to build the single stranded consensus sequence (SSCS). Moreover, this tool can compare several datasets or different steps in the analysis pipeline to monitor data loss or gain (e.g families re-united with barcode correction tool from the 
Du Novo Analysis Pipeline &lt;https://doi.org/10.1186/s13059-016-1039-4&gt;
). In an extension of this tool, each family is stratified into SSCS (ab/ba) and DSC and visualizes the allocation of DCSs respective to SSCS-ab and SSCS-ba. This is quite handy to better understand the relationship of SSCS to DCS per family and identify sources of bias (e.g. more SSCS to DCS in a particular family size, or more forward ab than reverse ba reads). 
Input
 This tools expects a tabular file with the tags of all families, their sizes and information about forward (ab) and reverse (ba) strands. At least one input file must be provided and up to 4 different datasets can be compared with this tool. All input files are produced in the same way (see section 
How to generate the input
):: 1 2 3 ----------------------------- 1 AAAAAAAAAAAAAAAAATGGTATG ba 3 AAAAAAAAAAAAAATGGTATGGAC ab .. class:: infomark 
How to generate the input
 The first step of the 
Du Novo Analysis Pipeline &lt;https://doi.org/10.1186/s13059-016-1039-4&gt;
 is the 
Make Families
 tool or the 
Correct Barcodes
 tool that produces output in this form:: 1 2 3 4 ------------------------------------------------------ AAAAAAAAAAAAAAATAGCTCGAT ab read1 CGCTACGTGACTGGGTCATG AAAAAAAAAAAAAAATAGCTCGAT ab read2 CGCTACGTGACTGGGTCATG AAAAAAAAAAAAAAATAGCTCGAT ab read3 CGCTACGTGACTGGGTCATG AAAAAAAAAAAAAAAAATGGTATG ba read3 CGCTACGTGACTAAAACATG We only need columns 1 and 2. These two columns can be extracted from this dataset using the 
Cut
 tool:: 1 2 --------------------------- AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAAAATGGTATG ba Next, the tags are sorted in ascending or descending order using the 
Sort
 tool:: 1 2 --------------------------- AAAAAAAAAAAAAAAAATGGTATG ba AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab Finally, unique occurencies of each tag are counted. This is done using 
Unique lines
 tool that adds an additional column with the counts that also represent the family size (column 1):: 1 2 3 ----------------------------- 1 AAAAAAAAAAAAAAAAATGGTATG ba 3 AAAAAAAAAAAAAATGGTATGGAC ab These data can now be used in this tool. 
Output
 The output is a PDF file with a plot and a summary of the data in the plot. The tool analyzes the family size distribution, that is the number of PE-reads per tag or family. This information is presented graphically as a histogram. The output can interpreted the following way: The 
first part
 of the output from this tool is “FSD after tags” and “FSD after PE reads” depending whether the data is compared to the total number of unique tags/families or the total number of PE-reads. The data is presented in a histogram showing the frequency of family sizes (FS) ranging from FS=1 to FS>20 members in a step-wise manner. Multiple datasets or steps of an analysis can be merged in one histogram (e.g. tags before and after barcode correction). The family size distribution gives the user an insight into the allocation of DCS in the libary; information that can be used to decide on the optimal family size parameter for building SSCSs. The 
second part
 of the output shows the family size distribution (FSD) for each of the datasets in more detail. The tags are categorized based on whether they form DCS (duplex = complementary tags in the ab and ba direction) or form only SSCS-ab or SSCS-ba with no matching complement."
toolshed.g2.bx.psu.edu/repos/nick/sequence_content_trimmer/sequence_content_trimmer/0.2.3	".. class:: infomark 
What it does
 This tool trims the 3' ends of reads based on the presence of the given bases. For instance, trim when N's are encountered or when the GC content exceeds a certain frequency. .. class:: infomark 
How it works
 This will slide along the read with a window, and trim once the frequency of filter bases exceeds the frequency threshold (unless ""Invert filter bases"" is enabled, in which case it will trim once non-filter bases exceed the threshold). The trim point will be just before the first (leftmost) filter base in the final window (the one where the frequency exceeded the threshold). .. class:: infomark 
Input
 The inputs can be in the following formats: fasta, fastq, fastqsanger, fastqillumina, and fastqsolexa. Both must be either a fasta or fastq type (no mixing fastq and fasta)."
toolshed.g2.bx.psu.edu/repos/iuc/duplex_family_size_distribution/td/1.0.3	"What it does
 Tags used in Duplex Sequencing (DS) are randomized barcodes, e.g 12 base pairs long. Since each DNA fragment is labeled by two tags at each end there are theoretically 4 to the power of (12+12) unique combinations. However, the input DNA in a typical DS experiment contains only ~1,000,000 molecules creating a large tag-to-input excess (4^24 ≫ 1,000,000). Because of such excess it is highly unlikely to tag distinct input DNA molecules with highly similar barcodes. This tool calculates the number of nucleotide differences among tags (tag distance), also known as 
Hamming distance &lt;https://en.wikipedia.org/wiki/Hamming_distance&gt;
. In this context the Hamming distance is simply the number of differences between two tags. The tool compares in a randomly selected subset of tags (default n=1000), the difference between each tag of the subset with the tags of the complete dataset. Each tag will differ by a certain number of nucleotides with the other tags; yet the tool uses the smallest difference observed with any other tag. 
Input
 This tools expects a tabular file with the tags of all families, the family sizes and information about forward (ab) and reverse (ba) strands:: 1 2 3 ----------------------------- 1 AAAAAAAAAAAAAAAAATGGTATG ba 3 AAAAAAAAAAAAAATGGTATGGAC ab .. class:: infomark 
How to generate the input
 The first step of the 
Du Novo Analysis Pipeline &lt;https://doi.org/10.1186/s13059-016-1039-4&gt;
 is the 
Make Families
 tool or the 
Correct Barcodes
 tool that produces output in this form:: 1 2 3 4 ------------------------------------------------------ AAAAAAAAAAAAAAATAGCTCGAT ab read1 CGCTACGTGACTGGGTCATG AAAAAAAAAAAAAAATAGCTCGAT ab read2 CGCTACGTGACTGGGTCATG AAAAAAAAAAAAAAATAGCTCGAT ab read3 CGCTACGTGACTGGGTCATG AAAAAAAAAAAAAAAAATGGTATG ba read3 CGCTACGTGACTAAAACATG We only need columns 1 and 2. These two columns can be extracted from this dataset using the 
Cut
 tool:: 1 2 --------------------------- AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAAAATGGTATG ba Next, the tags are sorted in ascending or descending order using the 
Sort
 tool:: 1 2 --------------------------- AAAAAAAAAAAAAAAAATGGTATG ba AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab AAAAAAAAAAAAAAATAGCTCGAT ab Finally, unique occurencies of each tag are counted. This is done using 
Unique lines
 tool that adds an additional column with the counts that also represent the family size (column 1):: 1 2 3 ----------------------------- 1 AAAAAAAAAAAAAAAAATGGTATG ba 3 AAAAAAAAAAAAAATGGTATGGAC ab These data can now be used in this tool. 
Output
 The output is one PDF file with various plots of the tag distance, a tabular file with the summarized data of the plots and a tabular file with the chimeras. The PDF file contains several pages: 1. This first page contains a graph representing the minimum tag distance (smallest number of differences) categorized after the family sizes. 2. The second page contains the same information as the first page, but plots the family size categorized by the minimum tag distance. 3. The third page contains the 
first step
 of the 
chimera analysis
, which examines the differences between the tags at both ends of a read (a/b). Chimeras can be distinguished by carrying the same tag at one end combined with multiple different tags at the other end of a read. Here, we describe the calculation of the TDs for only one tag in detail, but the process is repeated for each tag in the sample (default n=1000). First, the tool splits the tag into its upstream and downstream part (named a and b) and compares it with all other a parts of the families in the dataset. Next, the tool estimates the sequence differences (TD) among the a parts and extracts those tags with the smallest difference (TD a.min) and calculates the TD of the b part. The tags with the largest differences are extracted to estimate the maximum TD (TD b.max). The process is repeated starting with the b part instead and estimates TD a.max and TD b.min. Next, we calculate the sum of TD a.min and TD b.max. 4. The fourth page contains the 
second step
 of the 
chimera analysis
: the absolute difference (=delta TD) between the partial TDs (TD a.min & TD b.max and TD b.min & TD a.max). The partial TDs of chimeric tags are normally very different which means that multiple combinations of the same a part with different b parts are likely. But it is possible that small delta TDs occur due to a half of a tag that is identical to other halves in the data. For this purpose, the relative difference between the partial TDs is estimated in the next step. 5. The fifth page contains the 
third step
 of the 
chimera analysis
: the relative differences of the partial TDs (=relative delta TD). These are calculated as the absolute difference between TD a.min and TD b.max equal to TD delta. Since it is not known whether the absolute difference originates due to a low and a very large TD within a tag or an identical half (TD=0), the tool estimates the relative TD delta as the ratio of the difference to the sum of the partial TDs. In a chimera, it is expected that only one end of the tag contributes to the TD of the whole tag. In other words, if the same a part is observed in combination with several different b parts, then one end will have a TD = 0. Thus, the TD difference between the parts (TD a.min - TD b.max, TD a.max - TD b.min) is the same as the sum of the parts (TD a.min + TD b.max, TD a.max + TD b.min) or the ratio of the difference to the sum (relative delta TD = TD a.min - TD b.max / (TD a.min + TD b.max or TD a.max - TD b.min / (TD a.max + TD b.min)) will equal 1 in chimeric families. Here, the maximum value of the relative delta TD and the respective delta TD (in step 4) are selected and the plot can be interpreted as the following: - A low relative difference indicates that the total TD is equally distributed in the two partial TDs. This case would be expected, if all tags originate from different molecules. - A relative delta TD of 1 means that one part of the tags is identical. Since it is very unlikely that by chance two different tags have a TD of 0, the TDs in the other half are probably artificially introduced and represents chimeric families. 6. The sixth page is an analysis only of 
chimeric tags
 (relative delta TD =1) from step 5. 7. The last page is only generated when the parameter 
""Include only DCS in the analysis?""
 is set to 
False (NO)
. The graph represents the 
TD of the chimeric tags
 that form a DCS (complementary ab and ba). .. class:: infomark 
Note:
 Chimeras can be identical in the first or second part of the tag and can have an identical TD with mutliple tags. Therefore, the second column of the output file can have multiple tag entries. The file also contains the family sizes and the direction of the read (ab, ba). The asterisks mark the identical part of the tag:: 1 2 -------------------------------------------------------------------------------------------------- GAAAGGGAGG GCGCTTCACG 1 ba GCAATCGACG 
GCGCTTCACG
 1 ba CCCTCCCTGA GGTTCGTTAT 1 ba CGTCCTTTTC 
GGTTCGTTAT
 1 ba, GCACCTCCTT 
GGTTCGTTAT
 1 ba ATGCTGATCT CGAATGCATA 55 ba, 59 ab AGGTGCCGCC 
CGAATGCATA
 27 ba, 
ATGCTGATCT
 GAATGTTTAC 1 ba"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: antigenic1/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/antigenic.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: backtranseq2/6.6.0	".. class:: warningmark Back-translate a protein sequence to the nucleic acid sequence it is most likely to have come from. 
backtranseq
 uses a codon usage table which gives the frequency of usage of each codon for each amino acid. For each amino acid in the input sequence, the corresponding most frequently occuring codon is used in the nucleic acid sequence that is output. ----- You can view the original documentation here_. .. _here: http://emboss.open-bio.org/rel/rel6/apps/backtranseq.html"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: banana3/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/banana.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: biosed4/5.0.0	".. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/biosed.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: btwisted5/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/btwisted.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: cai6/5.0.0	".. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/cai.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: cai_custom6/5.0.0	".. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/cai_custom.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: chaos7/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/chaos.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: charge8/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/charge.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: checktrans9/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/checktrans.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: chips10/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/chips.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: cirdna11/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/cirdna.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: codcmp12/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/codcmp.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: coderet13/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/coderet.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: compseq14/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/compseq.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: cpgplot15/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/cpgplot.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: cpgreport16/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/cpgreport.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: cusp17/5.0.0	".. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/cusp.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: cutseq18/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/cutseq.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: dan19/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/dan.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: degapseq20/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/degapseq.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: descseq21/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/descseq.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: diffseq22/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/diffseq.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: digest23/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/digest.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: dotmatcher24/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/dotmatcher.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: dotpath25/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/dotpath.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: dottup26/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/dottup.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: dreg27/5.0.0+galaxy1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/dreg.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: einverted28/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/einverted.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: epestfind29/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/epestfind.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: equicktandem31/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/equicktandem.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: est2genome32/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/est2genome.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: etandem33/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/etandem.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: extractfeat34/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/extractfeat.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: extractseq35/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/extractseq.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: freak36/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/freak.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: fuzznuc37/5.0.3	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/fuzznuc.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: fuzzpro38/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/fuzzpro.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: fuzztran39/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/fuzztran.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: garnier40/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/4.0/emboss/apps/garnier.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: geecee41/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/geecee.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: getorf42/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/getorf.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: helixturnhelix43/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/helixturnhelix.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: hmoment44/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/hmoment.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: iep45/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/iep.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: infoseq46/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/infoseq.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: isochore47/5.0.0.1	".. class:: warningmark The input dataset needs to be sequences. ----- 
Syntax
 This application plots GC content over a sequence. It is intended for large sequences such as complete chromosomes or large genomic contigs, although interesting results can also be obtained from shorter sequences. You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/isochore.html - Both 
Window size
 and 
Shift increment
 are intergers. ----- 
Example
 - Input sequences:: >hg18_dna range=chrX:151073054-151073376 5'pad=0 3'pad=0 revComp=FALSE strand=? repeatMasking=none TTTATGTCTATAATCCTTACCAAAAGTTACCTTGGAATAAGAAGAAGTCA GTAAAAAGAAGGCTGTTGTTCCGTGAAATACTGTCTTTATGCCTCAGATT TGGAGTGCTCAGAGCCTCTGCAGCAAAGATTTGGCATGTGTCCTAGGCCT GCTCAGAGCAGCAAATCCCACCCTCTTGGAGAATGAGACTCATAGAGGGA CAGCTCCCTCCTCAGAGGCTTCTCTAATGGGACTCCAAAGAGCAAACACT CAGCCCCATGAGGACTGGCCAGGCCAAGTGGTGTGTGGGAACAGGGAGCA GCGGTTTCCAAGAGGATACAGTA - Output data file:: Position Percent G+C 1 .. 323 80 0.422 112 0.460 144 0.509 176 0.534 208 0.553 240 0.553 - Output graphics file: .. image:: ./static/emboss_icons/isochore.png"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: lindna48/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/lindna.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: marscan49/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/marscan.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: maskfeat50/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/maskfeat.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: maskseq51/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/maskseq.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: matcher52/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/matcher.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: megamerger53/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/megamerger.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: merger54/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/merger.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: msbar55/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/msbar.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: needle56/5.0.0.1	".. class:: warningmark needle reads any two sequences of the same type (DNA or protein). ----- 
Syntax
 This tool uses the Needleman-Wunsch global alignment algorithm to find the optimum alignment (including gaps) of two sequences when considering their entire length. - 
Optimal alignment:
 Dynamic programming methods ensure the optimal global alignment by exploring all possible alignments and choosing the best. - 
The Needleman-Wunsch algorithm
 is a member of the class of algorithms that can calculate the best score and alignment in the order of mn steps, (where 'n' and 'm' are the lengths of the two sequences). - 
Gap open penalty:
 [10.0 for any sequence] The gap open penalty is the score taken away when a gap is created. The best value depends on the choice of comparison matrix. The default value assumes you are using the EBLOSUM62 matrix for protein sequences, and the EDNAFULL matrix for nucleotide sequences. (Floating point number from 1.0 to 100.0) - 
Gap extension penalty:
 [0.5 for any sequence] The gap extension, penalty is added to the standard gap penalty for each base or residue in the gap. This is how long gaps are penalized. Usually you will expect a few long gaps rather than many short gaps, so the gap extension penalty should be lower than the gap penalty. An exception is where one or both sequences are single reads with possible sequencing errors in which case you would expect many single base gaps. You can get this result by setting the gap open penalty to zero (or very low) and using the gap extension penalty to control gap scoring. (Floating point number from 0.0 to 10.0) You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/needle.html ----- 
Example
 - Input File:: >hg18_dna range=chrX:151073054-151073136 5'pad=0 3'pad=0 revComp=FALSE strand=? repeatMasking=none TTTATGTCTATAATCCTTACCAAAAGTTACCTTGGAATAAGAAGAAGTCA GTAAAAAGAAGGCTGTTGTTCCGTGAAATACTG - If both Sequence1 and Sequence2 take the above file as input, Gap open penalty equals 10.0, Gap extension penalty equals 0.5, Brief identity and similarity is set to Yes, Output alignment file format is set to SRS pairs, the output file is:: ######################################## # Program: needle # Rundate: Mon Apr 02 2007 14:23:16 # Align_format: srspair # Report_file: ./database/files/dataset_7.dat ######################################## #======================================= # # Aligned_sequences: 2 # 1: hg18_dna # 2: hg18_dna # Matrix: EDNAFULL # Gap_penalty: 10.0 # Extend_penalty: 0.5 # # Length: 83 # Identity: 83/83 (100.0%) # Similarity: 83/83 (100.0%) # Gaps: 0/83 ( 0.0%) # Score: 415.0 # #======================================= hg18_dna 1 TTTATGTCTATAATCCTTACCAAAAGTTACCTTGGAATAAGAAGAAGTCA 50 |||||||||||||||||||||||||||||||||||||||||||||||||| hg18_dna 1 TTTATGTCTATAATCCTTACCAAAAGTTACCTTGGAATAAGAAGAAGTCA 50 hg18_dna 51 GTAAAAAGAAGGCTGTTGTTCCGTGAAATACTG 83 ||||||||||||||||||||||||||||||||| hg18_dna 51 GTAAAAAGAAGGCTGTTGTTCCGTGAAATACTG 83 #--------------------------------------- #---------------------------------------"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: newcpgreport57/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/newcpgreport.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: newcpgseek58/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/newcpgseek.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: newseq59/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/newseq.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: noreturn60/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/noreturn.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: notseq61/5.0.0	".. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/notseq.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: nthseq62/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/nthseq.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: octanol63/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/octanol.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: oddcomp64/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/oddcomp.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: palindrome65/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/palindrome.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: pasteseq66/5.0.0.1	.. class:: warningmark The input datasets need to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/pasteseq.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: patmatdb67/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/patmatdb.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: pepcoil68/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/pepcoil.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: pepinfo69/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/pepinfo.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: pepnet70/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/pepnet.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: pepstats71/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/pepstats.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: pepwheel72/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/pepwheel.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: pepwindow73/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/pepwindow.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: pepwindowall74/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/pepwindowall.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: plotcon75/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/plotcon.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: plotorf76/5.0.0	".. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/plotorf.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: polydot77/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/polydot.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: preg78/5.0.0+galaxy1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/preg.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: prettyplot79/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/prettyplot.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: prettyseq80/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/prettyseq.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: primersearch81/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/primersearch.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: revseq82/5.0.0	".. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/revseq.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: seqmatchall83/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/seqmatchall.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: seqret84/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/seqret.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: showfeat85/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/showfeat.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: shuffleseq87/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/shuffleseq.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: sigcleave88/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/sigcleave.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: sirna89/5.0.0	".. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/sirna.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: sixpack90/5.0.0+galaxy2	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/sixpack.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: skipseq91/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/skipseq.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: splitter92/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/splitter.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: supermatcher95/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/supermatcher.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: syco96/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/syco.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: tcode97/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/tcode.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: textsearch98/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/textsearch.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: tmap99/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/tmap.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: tranalign100/5.0.0	"You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/tranalign.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: transeq101/5.0.0	".. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/transeq.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: trimest102/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/trimest.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: trimseq103/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/trimseq.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: twofeat104/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/twofeat.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: union105/5.0.0	".. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. 
here: http://emboss.sourceforge.net/apps/release/5.0/emboss/apps/union.html ------ 
Citation
 For the underlying tool, please cite 
Rice P, Longden I, Bleasby A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 2000 Jun;16(6):276-7. &lt;http://www.ncbi.nlm.nih.gov/pubmed/10827456&gt;
 If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Schenck I, He J, Zhang Y, Ghent M, Veeraraghavan N, Albert I, Miller W, Makova KD, Hardison RC, Nekrutenko A. A framework for collaborative analysis of ENCODE data: making large-scale analyses biologist-friendly. Genome Res. 2007 Jun;17(6):960-4. &lt;http://www.ncbi.nlm.nih.gov/pubmed/17568012&gt;
_"
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: vectorstrip106/5.0.0.1	You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/vectorstrip.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: water107/5.0.0.1	.. class:: warningmark The input datasets need to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/water.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: wobble108/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/wobble.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: wordcount109/5.0.0.1	.. class:: warningmark The input dataset needs to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/wordcount.html
toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: wordmatch110/5.0.0.1	.. class:: warningmark The input datasets need to be sequences. ----- You can view the original documentation here_. .. _here: http://galaxy-iuc.github.io/emboss-5.0-docs/wordmatch.html
toolshed.g2.bx.psu.edu/repos/bgruening/hicup_deduplicator/hicup_deduplicator/0.9.2+galaxy0	For help please consult the documentation of HiCUP: https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html To get more information about the deduplicator visit: https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html#hicup-deduplicator
toolshed.g2.bx.psu.edu/repos/bgruening/hicup_digester/hicup_digester/0.9.2+galaxy0	For help please consult the documentation of HiCUP: https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html To get more information about the digister visit:https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html#hicup-digester
toolshed.g2.bx.psu.edu/repos/bgruening/hicup_filter/hicup_filter/0.9.2+galaxy0	For help please consult the documentation of HiCUP: https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html To get more information about the filter visit: https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html#hicup-filter
toolshed.g2.bx.psu.edu/repos/bgruening/hicup_mapper/hicup_mapper/0.9.2+galaxy0	For help please consult the documentation of HiCUP: https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html To get more information about the mapper visit: https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html#hicup-mapper
toolshed.g2.bx.psu.edu/repos/bgruening/hicup_hicup/hicup_hicup/0.9.2+galaxy0	For help please consult the documentation of HiCUP: https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html To get more information about the pipeline visit: https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html#hicup-pipeline-control-script
toolshed.g2.bx.psu.edu/repos/bgruening/hicup_truncater/hicup_truncater/0.9.2+galaxy0	For help please consult the documentation of HiCUP: https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html To get more information about the truncater visit: https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html#hicup-truncater
toolshed.g2.bx.psu.edu/repos/bgruening/hicup2juicer/hicup2juicer/0.9.2+galaxy0	"HiCUP homepage: www.bioinformatics.babraham.ac.uk/projects/hicup The hicup2juicer script converts HiCUP BAM/SAM files to a format compatible with Juicer and JuiceBox( https://github.com/aidenlab/juicer ). Outputfiles generated by this script may be converted to Juicer "".hic"" files using the ""pre"" command as described at: https://github.com/aidenlab/juicer/wiki/Pre The script does not use restriction site coordinates when generating output. FUNCTION HiCUP generates SAM/BAM files of mapped, filtered paired-end reads constituting the sequenced valid Hi-C di-tags. These may then be analysed by a variety of specialised tools, but before this is possible the datasets will need parsing into an appropriate format. The hicup2juicer script converts HiCUP BAM/SAM files to a tab-delimited format comprising 7 columns, with read pairs on the same line: <readname> <str1> <chr1> <pos1> <frag1> <str2> <chr2> <pos2> <frag2> <mapq1> <mapq2> str = strand (0 for forward, anything else for reverse) chr = chromosome (must be a chromosome in the genome) pos = position frag = restriction site fragment mapq = mapping quality score Column1: Readpair index number (starting at 1) Column2: forward read strand (0 = positive strand, 1 = negative strand) Column3: forward read chromosome name Column4: forward read position Column5: forward read fragment id (set to the dummy value 0) Column6: reverse read strand (0 = positive strand, 1 = negative strand) Column7: reverse read chromosome name Column8: reverse read position Column9: reverse read fragment id (set to the dummy value 1) Column10: forward read MAPQ score Column11: reverse read MAPQ score COMMAND LINE OPTIONS --help Print help message and exit --version Print the program version and exit --zip Write output to a gzip file Full instructions on running the pipeline can be found at: www.bioinformatics.babraham.ac.uk/projects/hicup Steven Wingett, Babraham Institute, Cambridge, UK"
toolshed.g2.bx.psu.edu/repos/lldelisle/cooler_csort_tabix/cooler_csort_tabix/0.9.3+galaxy1	"cooler csort
 tool developped by mirnylab see https://github.com/open2c/cooler and https://cooler.readthedocs.io/en/latest/cli.html#cooler-csort command-line:: cooler csort -i tabix -c1 c1 -c2 c2 -p1 p1 -p2 p2 -o output input chromosomeSize"
toolshed.g2.bx.psu.edu/repos/lldelisle/cooler_balance/cooler_balance/0.9.3+galaxy0	"cooler balance
 tool developped by mirnylab see https://github.com/mirnylab/cooler and https://cooler.readthedocs.io/en/latest/cli.html#cooler-balance command-line:: cp $input $output cooler balance -p \${GALAXY_SLOTS:-1} --mad-max madmax --min-nnz minnnz --min-count mincount --ignore-diags ignorediags --ignore-dist ignoredist --tol tol --max-iters maxiters cistrans -f --blacklist 'blacklist'"
toolshed.g2.bx.psu.edu/repos/lldelisle/cooler_cload_tabix/cooler_cload_tabix/0.9.3+galaxy1	"cooler cload
 tool developped by mirnylab see https://github.com/open2c/cooler and https://cooler.readthedocs.io/en/latest/cli.html#cooler-cload-tabix command-line:: cooler cload tabix --assembly assembly -c2 c2 -p2 p2 input_bed input_pairs output"
toolshed.g2.bx.psu.edu/repos/lldelisle/cooler_makebins/cooler_makebins/0.9.3+galaxy0	"cooler makebins
 tool developped by mirnylab see https://github.com/open2c/cooler and https://cooler.readthedocs.io/en/latest/cli.html#cooler-makebins"
toolshed.g2.bx.psu.edu/repos/iuc/tasmanian_mismatch/tasmanian_mismatch/1.0.9+galaxy1	"What it does
 This tool counts the number/proportion of mismatches per position along the read, for each read (see figure below). .. image:: ${static_path}/images/snapshot_good.jpg :height: 350 :width: 650 ----- 
What is special
 By providing a BED file, tasmanian-mismatch will count mismatches from all regions depicted in the figure below, and will report them separately. Also, a parameter defined as 
""confidence""
 allows including reads with >= bases in the boundary region in a separate group. This is useful when the BED refers to repeat regions. Since these regions might not have been correctly placed in the assembly or are not the same in different individuals, we can include this 
confidence
 repeat regions where we have high confidence on the reference genome to which we mapped the reads. .. image:: ${static_path}/images/intersections_tasmanian.jpg :height: 150 :width: 650 Softclips are critical in FFPE (Formalin-fixed paraffin-embedded) experiments as mismatches tend to accumulate at the ends of the reads. Most often, softclips are all accepted during the analysis and many real mismatches are indirectly excluded from the analysis. Hence, this tool provides different ways to deal with this: The 
softclips
 field allows for 3 different ways at treating softclips: 0) Exclude these region if there is less than 2/3 identity with the reference genome 1) Exclude all softclipped bases 2) Include all softclipped bases .. class:: warningmark BAM/SAM file must be 
sorted
 if not using a BED file."
toolshed.g2.bx.psu.edu/repos/iuc/fasttree/fasttree/2.1.10+galaxy1	"=========== Description =========== .. class:: infomark FastTree_ infers approximately-maximum-likelihood phylogenetic trees from alignments of nucleotide or protein sequences. FastTree can handle alignments with up to a million of sequences in a reasonable amount of time and memory. For large alignments, FastTree is 100-1,000 times faster than PhyML 3.0 or RAxML 7. FastTree is open-source software. .. 
FastTree: http://meta.microbesonline.org/fasttree ----- ----- Input ----- A) Aligned sequences file in FASTA format (e.g. the alignment from the 'PyNAST' tool) or PHYLIP format. B) Newick file to set the starting tree(s) (optional) ---------- Parameters ---------- Nucleotide or protein alignment To specify the type of sequence alignment (nucleotide or protein) you are providing. Nucleotide evolution model Specify GTR+CAT (Generalized Time Reversable Model) or Jukes-Cantor + CAT. Protein evolution model Specify JTT (Jones-Taylor-Thornton 1992 model) + CAT, WAG (Whelan-And-Goldman 2001 model) + CAT, or LG (Le-Gascuel 2008 model) + CAT. Allow spaces and other restricted characters (but not ') in sequence and quote names in the output tree. (fasta only) FastTree will not be able to read these trees back in. (-quote) Number of multiple alignments (phylip only) (-n) ------------------- Advanced Parameters ------------------- Number of rates of sites Defaults to 20. Specify whether you wish to choose constant rates. (-cat/-nocat) Use Gamma20 After optimizing the tree under the CAT approximation, rescale the lengths to optimize the Gamma20 likelihood. (-gamma) Eliminates the support value computation. Support value computation is not recommended for wide alignments. (-nosupport) Turn off maximum-likelihood. (-noml) Speed up the neighbor joining phase and reduce memory usage Recommended for >50,000 sequences. (-fastest) Use pseudocounts Recommended if you have many fragmentary sequences. (-pseudo) Optimize branch lengths without ML NNIs. (-mllen) ------ Output ------ This tool produces two output files, one of which is optional (the log file). (A) The phylogenetic tree in Newick
 format (nhx). (B) 
(optional)
 The log file, containing information of the FastTree run to build the phylogenetic tree. .. class:: infomark The placement of the root is not biologically meaningful. The local support values are given as names for the internal nodes, and range from 0 to 1, not from 0 to 100 or 0 to 1,000. If all sequences are unique, then the tree will be fully resolved (the root will have three children and other internal nodes will have two children). If there are multiple sequences that are identical to each other, then there will be a multifurcation. Also, there are no support values for the parent nodes of redundant sequences. .. 
Newick: http://en.wikipedia.org/wiki/Newick_format ----- ========= Resources ========= FastTree_v2.1.10
 .. _FastTree_v2.1.10: http://meta.microbesonline.org/fasttree/#Install"
toolshed.g2.bx.psu.edu/repos/iuc/iqtree/iqtree/2.4.0+galaxy1	Note that --seqtype CODON is always necessary when using codon models and you also need to specify a genetic code like this if differed from the standard genetic code. <br/><i>-st</i> NT2AA tells IQ-TREE to translate protein-coding DNA into AA sequences and then subsequent analysis will work on the AA sequences. You can also use a genetic code like --seqtype NT2AA5 for the Invertebrate Mitochondrial Code (see genetic code table).
toolshed.g2.bx.psu.edu/repos/iuc/rapidnj/rapidnj/2.3.2	============ RapidNJ ============ Especially useful for large datasets where maximum-likelihood based phylogenetic inference becomes intractable, RapidNJ reduces the computing time of canonical neighbour-joining for phylogenetic tree inference. RapidNJ accepts either matrices in phylip format or alignments in stockholm or FASTA format.
toolshed.g2.bx.psu.edu/repos/iuc/tn93_readreduce/tn93_readreduce/1.0.15+galaxy0	"readreduce ---------- Merge matching reads into clusters using the 
Tamura Nei 93 distance
 algorithm."
toolshed.g2.bx.psu.edu/repos/iuc/tn93/tn93/1.0.15+galaxy0	"TN93 ---- Compute pairwise distances between aligned nucleotide sequences in sequential FASTA format using the 
Tamura Nei 93 distance
. NOTES ----- All sequences must be aligned and have the same length. Only IUPAC characters are recognized (e.g. no ~). Sequence names can include copy number as in >seqname:10 ':' can be replaced with another character using 
-d
, and sequences that have no explicit copy number are assumed to be a single copy. Copy numbers only affect histogram and mean calculations."
toolshed.g2.bx.psu.edu/repos/iuc/tn93_cluster/tn93_cluster/1.0.15+galaxy0	TN93-Cluster ============ Clusters sequences within a minimum distance of each other using the 1993 Tamura-Nei distance calculation.
toolshed.g2.bx.psu.edu/repos/iuc/tn93_filter/tn93_filter/1.0.15+galaxy0	"TN93 ---- This is a simple program meant to compute pairwise distances between aligned nucleotide sequences in sequential FASTA format using the 
Tamura Nei 93 distance
. USAGE ----- usage: tn93 [-h] [-o OUTPUT] [-t THRESHOLD] [-a AMBIGS] [-l OVERLAP][-d COUNTS_IN_NAME] [-f FORMAT] [-s SECOND_FASTA] [-b] [-c] [-q] [FASTA] Try it from using the example file in 'data' by typing tn93 -t 0.05 -o data/test.dst data/test.fas Output (diagnostics written to stderr, histogram written to stdout so can be redirected) Example: Read 8 sequences of length 1320 Will perform 28 pairwise distance calculations Progress: 100% ( 7 links found, inf evals/sec) { ""Actual comparisons performed"" :28, ""Total comparisons possible"" : 28, ""Links found"" : 7, ""Maximum distance"" : 0.0955213, ""Mean distance"" : 0.0644451, ""Histogram"" : [[0.005,0],[0.01,0],[0.015,0],[0.02,0],[0.025,0],[0.03,2],[0.035,1],[0.04,0],[0.045,1],[0.05,3],[0.055,1],[0.06,2],[0.065,2],[0.07,3],[0.075,4],[0.08,3],[0.085,3],[0.09,2],[0.095,0],[0.1,1],[0.105,0],[0.11,0],[0.115,0],[0.12,0],[0.125,0],[0.13,0],[0.135,0],[0.14,0],[0.145,0],[0.15,0],[0.155,0],[0.16,0],[0.165,0],[0.17,0],[0.175,0],[0.18,0],[0.185,0],[0.19,0],[0.195,0],[0.2,0],[0.205,0],[0.21,0],[0.215,0],[0.22,0],[0.225,0],[0.23,0],[0.235,0],[0.24,0],[0.245,0],[0.25,0],[0.255,0],[0.26,0],[0.265,0],[0.27,0],[0.275,0],[0.28,0],[0.285,0],[0.29,0],[0.295,0],[0.3,0],[0.305,0],[0.31,0],[0.315,0],[0.32,0],[0.325,0],[0.33,0],[0.335,0],[0.34,0],[0.345,0],[0.35,0],[0.355,0],[0.36,0],[0.365,0],[0.37,0],[0.375,0],[0.38,0],[0.385,0],[0.39,0],[0.395,0],[0.4,0],[0.405,0],[0.41,0],[0.415,0],[0.42,0],[0.425,0],[0.43,0],[0.435,0],[0.44,0],[0.445,0],[0.45,0],[0.455,0],[0.46,0],[0.465,0],[0.47,0],[0.475,0],[0.48,0],[0.485,0],[0.49,0],[0.495,0],[0.5,0],[0.505,0],[0.51,0],[0.515,0],[0.52,0],[0.525,0],[0.53,0],[0.535,0],[0.54,0],[0.545,0],[0.55,0],[0.555,0],[0.56,0],[0.565,0],[0.57,0],[0.575,0],[0.58,0],[0.585,0],[0.59,0],[0.595,0],[0.6,0],[0.605,0],[0.61,0],[0.615,0],[0.62,0],[0.625,0],[0.63,0],[0.635,0],[0.64,0],[0.645,0],[0.65,0],[0.655,0],[0.66,0],[0.665,0],[0.67,0],[0.675,0],[0.68,0],[0.685,0],[0.69,0],[0.695,0],[0.7,0],[0.705,0],[0.71,0],[0.715,0],[0.72,0],[0.725,0],[0.73,0],[0.735,0],[0.74,0],[0.745,0],[0.75,0],[0.755,0],[0.76,0],[0.765,0],[0.77,0],[0.775,0],[0.78,0],[0.785,0],[0.79,0],[0.795,0],[0.8,0],[0.805,0],[0.81,0],[0.815,0],[0.82,0],[0.825,0],[0.83,0],[0.835,0],[0.84,0],[0.845,0],[0.85,0],[0.855,0],[0.86,0],[0.865,0],[0.87,0],[0.875,0],[0.88,0],[0.885,0],[0.89,0],[0.895,0],[0.9,0],[0.905,0],[0.91,0],[0.915,0],[0.92,0],[0.925,0],[0.93,0],[0.935,0],[0.94,0],[0.945,0],[0.95,0],[0.955,0],[0.96,0],[0.965,0],[0.97,0],[0.975,0],[0.98,0],[0.985,0],[0.99,0],[0.995,0],[1,0]] } NOTES ----- All sequences must be aligned and have the same length. Only IUPAC characters are recognized (e.g. no ~). Sequence names can include copy number as in >seqname:10 ':' can be replaced with another character using 
-d
, and sequences that have no explicit copy number are assumed to be a single copy. Copy numbers only affect histogram and mean calculations."
toolshed.g2.bx.psu.edu/repos/devteam/fastx_barcode_splitter/cshl_fastx_barcode_splitter/1.0.1+galaxy2	"What it does
 This tool splits a Solexa library (FASTQ file) or a regular FASTA file into several files, using barcodes as the split criteria. -------- 
Barcode file Format
 Barcode files are simple text files. Each line should contain an identifier (descriptive name for the barcode), and the barcode itself (A/C/G/T), separated by a TAB character. Example:: #This line is a comment (starts with a 'number' sign) BC1 GATCT BC2 ATCGT BC3 GTGAT BC4 TGTCT For each barcode, a new FASTQ file will be created (with the barcode's identifier as part of the file name). Sequences matching the barcode will be stored in the appropriate file. One additional FASTQ file will be created (the 'unmatched' file), where sequences not matching any barcode will be stored. The output of this tool is an HTML file, displaying the split counts and the file locations. 
Output Example
 .. image:: barcode_splitter_output_example.png"
toolshed.g2.bx.psu.edu/repos/devteam/fastx_clipper/cshl_fastx_clipper/1.0.3+galaxy2	"What it does
 This tool clips adapters from the 3'-end of the sequences in a FASTA/FASTQ file. -------- 
Clipping Illustration:
 .. image:: fastx_clipper_illustration.png 
Clipping Example:
 .. image:: fastx_clipper_example.png 
In the above example:
 * Sequence no. 1 was discarded since it wasn't clipped (i.e. didn't contain the adapter sequence). (
Output
 parameter). * Sequence no. 5 was discarded --- it's length (after clipping) was shorter than 15 nt (
Minimum Sequence Length
 parameter)."
toolshed.g2.bx.psu.edu/repos/devteam/fastx_collapser/cshl_fastx_collapser/1.0.1+galaxy2	"What it does
 This tool collapses identical sequences in a FASTA file into a single sequence. -------- 
Example
 Example Input File (Sequence ""ATAT"" appears multiple times):: >CSHL_2_FC0042AGLLOO_1_1_605_414 TGCG >CSHL_2_FC0042AGLLOO_1_1_537_759 ATAT >CSHL_2_FC0042AGLLOO_1_1_774_520 TGGC >CSHL_2_FC0042AGLLOO_1_1_742_502 ATAT >CSHL_2_FC0042AGLLOO_1_1_781_514 TGAG >CSHL_2_FC0042AGLLOO_1_1_757_487 TTCA >CSHL_2_FC0042AGLLOO_1_1_903_769 ATAT >CSHL_2_FC0042AGLLOO_1_1_724_499 ATAT Example Output file:: >1-1 TGCG >2-4 ATAT >3-1 TGGC >4-1 TGAG >5-1 TTCA .. class:: infomark Original Sequence Names / Lane descriptions (e.g. ""CSHL_2_FC0042AGLLOO_1_1_742_502"") are discarded. The output sequence name is composed of two numbers: the first is the sequence's number, the second is the multiplicity value. The following output:: >2-4 ATAT means that the sequence ""ATAT"" is the second sequence in the file, and it appeared 4 times in the input FASTA file."
toolshed.g2.bx.psu.edu/repos/devteam/fastq_combiner/fastq_combiner/1.1.5+galaxy2	"What it does
 This tool joins a FASTA file to a Quality Score file, creating a single FASTQ block for each read. Specifying a set of quality scores is optional; when not provided, the output will be fastqsanger or fastqcssanger (when a csfasta is provided) with each quality score being the maximal allowed value (93). Use this tool, for example, to convert 454-type output to FASTQ."
toolshed.g2.bx.psu.edu/repos/devteam/fasta_compute_length/fasta_compute_length/1.0.4	"What it does
 This tool counts the length of each fasta sequence in the file. The output file has two columns per line (separated by tab): fasta titles and lengths of the sequences. The option 
How many characters to keep?
 allows to select a specified number of letters from the beginning of each FASTA entry. ----- 
Example
 Suppose you have the following FASTA formatted sequences from a Roche (454) FLX sequencing run:: >EYKX4VC02EQLO5 length=108 xy=1826_0455 region=2 run=R_2007_11_07_16_15_57_ TCCGCGCCGAGCATGCCCATCTTGGATTCCGGCGCGATGACCATCGCCCGCTCCACCACG TTCGGCCGGCCCTTCTCGTCGAGGAATGACACCAGCGCTTCGCCCACG &gt;EYKX4VC02D4GS2 length=60 xy=1573_3972 region=2 run=R_2007_11_07_16_15_57_ AATAAAACTAAATCAGCAAAGACTGGCAAATACTCACAGGCTTATACAATACAAATGTAAfa Running this tool while setting 
How many characters to keep?
 to 
14
 will produce this:: EYKX4VC02EQLO5 108 EYKX4VC02D4GS2 60 However, if your IDs are not all the same length, you may wish to just keep the fasta ID, and not the description:: >EYKX4VC02EQLO5 length=108 xy=1826_0455 region=2 run=R_2007_11_07_16_15_57_ TCCGCGCCGAGCATGCCCATCTTGGATTCCGGCGCGATGACCATCGCCCGCTCCACCACG TTCGGCCGGCCCTTCTCGTCGAGGAATGACACCAGCGCTTCGCCCACG >EYKX4VC length=60 xy=1573_3972 region=2 run=R_2007_11_07_16_15_57_ AATAAAACTAAATCAGCAAAGACTGGCAAATACTCACAGGCTTATACAATACAAATGTAAfa Running this tool with 
Strip fasta description from header
 set to 
True
 and 
How many characters to keep?
 set to 
0
 will produce:: EYKX4VC02EQLO5 108 EYKX4VC 60"
toolshed.g2.bx.psu.edu/repos/devteam/fasta_concatenate_by_species/fasta_concatenate0/0.0.1	"What it does
 This tools attempts to parse FASTA headers to determine the species for each sequence in a multiple FASTA alignment. It then linearly concatenates the sequences for each species in the file, creating one sequence per determined species. ------- 
Example
 Starting FASTA:: >hg18.chr1(+):10016339-10016341|hg18_0 GT >panTro2.chr1(+):10195380-10195382|panTro2_0 GT >rheMac2.chr1(+):13119747-13119749|rheMac2_0 GT >mm8.chr4(-):148269679-148269681|mm8_0 GT >canFam2.chr5(+):66213635-66213637|canFam2_0 GT >hg18.chr1(-):100323677-100323679|hg18_1 GT >panTro2.chr1(-):101678671-101678673|panTro2_1 GT >rheMac2.chr1(-):103154011-103154013|rheMac2_1 GT >mm8.chr3(+):116620616-116620618|mm8_1 GT >canFam2.chr6(+):52954092-52954094|canFam2_1 GT becomes:: >hg18 GTGT >panTro2 GTGT >rheMac2 GTGT >mm8 GTGT >canFam2 GTGT .. class:: warningmark This tool will only work properly on files with Galaxy style FASTA headers."
toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/5.2+galaxy0	".. class:: infomark 
What it does
 
Cutadapt
 finds and removes adapter sequences, primers, poly-A tails and other types of unwanted sequence from your high-throughput sequencing reads. Cleaning your data in this way is often required: Reads from small-RNA sequencing contain the 3’ sequencing adapter because the read is longer than the molecule that is sequenced, such as in microRNA, or CRISPR data, or Poly-A tails that are useful for pulling out RNA from your sample but often you don’t want them to be in your reads. Cutadapt helps with these trimming tasks by finding the adapter or primer sequences in an error-tolerant way. It can also modify and filter reads in various ways. Cutadapt searches for the adapter in all reads and removes it when it finds it. Unless you use a filtering option, all reads that were present in the input file will also be present in the output file, some of them trimmed, some of them not. Even reads that were trimmed entirely (because the adapter was found in the very beginning) are output. All of this can be changed with options in the tool form above. See the complete 
Cutadapt documentation
 for additional details. If you use Cutadapt, please cite 
Marcel, 2011
 under 
Citations
 below. ----- Input Sequences 
*
*
 Accepted input formats for the tool are: - FASTQ.GZ - FASTQ.BZ2 - FASTQ or - FASTA ----- Specifying Adapters 
*
*
* To trim an adapter, input the ADAPTER sequence in plain text or in a FASTA file e.g. AACCGGTT (with the characters: 
*$
, 
^
, 
...
, if anchored or linked). ============================================= =================== 
Option
 
Sequence
 --------------------------------------------- ------------------- 3’ (End) Adapter ADAPTER Anchored 3’ Adapter ADAPTER$ 5’ (Front) Adapter ADAPTER Anchored 5’ Adapter ^ADAPTER 5’ or 3’ (Both possible) ADAPTER Linked Adapter - 3' (End) only ADAPTER1...ADAPTER2 Non-anchored Linked Adapter - 5' (Front) only ADAPTER1...ADAPTER2 ============================================= =================== Below is an illustration of the allowed adapter locations relative to the read and depending on the adapter type: .. image:: $PATH_TO_IMAGES/adapters.svg ------------------- 
Example: Illumina TruSeq Adapters
 ------------------- If you have reads containing Illumina TruSeq adapters, for example, follow these steps. For Single-end reads as well as the first reads of Paired-end data: 
Read 1
 In the 
3' (End) Adapters
 option above, insert A + the “TruSeq Indexed Adapter” prefix that is common to all Indexed Adapter sequences, e.g insert: AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC For the second reads of Paired-end data: 
Read 2
 In the 
3' (End) Adapters
 option above, insert the reverse complement of the “TruSeq Universal Adapter”: AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT The adapter sequences can be found in the document 
Illumina TruSeq Adapters De-Mystified
. ----------- 
Paired Adapters
 ----------- Normally, the tool looks for adapters on R1 and R2 reads independently. That is, the best matching R1 adapter of each type (3' End, 5' End, Anywhere) is removed from R1 and the best matching R2 adapter of each type is removed from R2. To change this, you can use the 
Pairwise adapter search
 (--pair-adapters) option, which causes each R1 adapter to be paired up with its corresponding R2 adapter. The first R1 adapter of a given type that you specify will be paired up with the first R2 adapter of that type, and so on. The adapters are then always removed in pairs from a read pair. For example, if you specify the following two 3'-end adapters for the R1 reads: - 
AAAAA
 - 
GGGGG
 and these two 3'-end adapters for the R2 reads: - 
CCCC
 - 
TTTT
 then, with this option enabled, the tool will trim a pair of reads only if: - either 
AAAAA
 is found in R1 and 
CCCCC
 is found in R2, - or 
GGGG
 is found in R1 and 
TTTT
 is found in R2. Two limitations exist in this mode: 1. You need to provide equal numbers of R1 and R2 adapters of each type to allow pair formation, or the tool run will fail. 2. The algorithm identifies the best-matching R1 adapter first and then checks whether it can find its corresponding R2 adapter. If not, the read pair remains unchanged, even though it is, in theory, possible that a different R1 adapter that does not fit as well would have had a corresponding R2 adapter present, i.e., some legitimate adapter pairs might remain unhandled. This mode is useful, for example, for 
demultiplexing Illumina unique dual indices (UDIs)
. ----- Outputs 
*
 - Trimmed reads Optionally, under 
Output Options
 you can choose to output * Report * Info file ----------- 
Report
 ----------- Cutadapt can output per-adapter statistics if you select to generate the report above. Example: :: This is cutadapt 3.4 with Python 3.9.2 Command line parameters: -j=1 -a AGATCGGAAGAGC -A AGATCGGAAGAGC --output=out1.fq.gz --paired-output=out2.fq.gz --error-rate=0.1 --times=1 --overlap=3 --action=trim --minimum-length=30:40 --pair-filter=both --cut=0 bwa-mem-fastq1_assimetric_fq_gz.fq.gz bwa-mem-fastq2_assimetric_fq_gz.fq.gz Processing reads on 1 core in paired-end mode ... Finished in 0.01 s (129 µs/read; 0.46 M reads/minute). === Summary === Total read pairs processed: 99 Read 1 with adapter: 2 (2.0%) Read 2 with adapter: 4 (4.0%) Pairs that were too short: 3 (3.0%) Pairs written (passing filters): 96 (97.0%) Total basepairs processed: 48,291 bp Read 1: 24,147 bp Read 2: 24,144 bp Total written (filtered): 48,171 bp (99.8%) Read 1: 24,090 bp Read 2: 24,081 bp ----------- 
Info file
 ----------- The info file contains information about the found adapters. The output is a tab-separated text file. Each line corresponds to one read of the input file. Columns contain the following data: * 
1st
: Read name * 
2nd
: Number of errors * 
3rd
: 0-based start coordinate of the adapter match * 
4th
: 0-based end coordinate of the adapter match * 
5th
: Sequence of the read to the left of the adapter match (can be empty) * 
6th
: Sequence of the read that was matched to the adapter * 
7th
: Sequence of the read to the right of the adapter match (can be empty) * 
8th
: Name of the found adapter * 
9th
: Quality values corresponding to sequence left of the adapter match (can be empty) * 
10th
: Quality values corresponding to sequence matched to the adapter (can be empty) * 
11th
: Quality values corresponding to sequence to the right of the adapter (can be empty) The concatenation of columns 5-7 yields the full read sequence. Column 8 identifies the found adapter. Adapters without a name are numbered starting from 1. Fields 9-11 are empty if quality values are not available. Concatenating them yields the full sequence of quality values. If no adapter was found, the format is as follows: #. Read name #. The value -1 #. The read sequence #. Quality values When parsing the file, be aware that additional columns may be added in the future. Note also that some fields can be empty, resulting in consecutive tabs within a line. If the --times option is used and greater than 1, each read can appear more than once in the info file. There will be one line for each found adapter, all with identical read names. Only for the first of those lines will the concatenation of columns 5-7 be identical to the original read sequence (and accordingly for columns 9-11). For subsequent lines, the shown sequence are the ones that were used in subsequent rounds of adapter trimming, that is, they get successively shorter. -------------------- Renaming Reads 
*
*
 The --rename option expects a template string such as {id} extra_info {adapter_name} as a parameter. It can contain regular text and placeholders that consist of a name enclosed in curly braces ({placeholdername}). The read name will be set to the template string in which the placeholders are replaced with the actual values relevant for the current read. The following placeholders are currently available for single-end reads: * {header} – the full, unchanged header * {id} – the read ID, that is, the part of the header before the first whitespace * {comment} – the part of the header after the whitespace following the ID * {adapter_name} – the name of adapter that was found in this read or no_adapter if there was none adapter match. If you use --times to do multiple rounds of adapter matching, this is the name of the last found adapter. * {match_sequence} – the sequence of the read that matched the adapter (including errors). If there was no adapter match, this is set to an empty string. If you use a linked adapter, this is to the two matching strings, separated by a comma. * {cut_prefix} – the prefix removed by the --cut (or -u) option (that is, when used with a positive length argument) * {cut_suffix} – the suffix removed by the --cut (or -u) option (that is, when used with a negative length argument) * {rc} – this is replaced with the string rc if the read was reverse complemented. This only applies when reverse complementing was requested If the --rename option is used with paired-end data, the template is applied separately to both R1 and R2. That is, for R1, the placeholders are replaced with values from R1, and for R2, the placeholders are replaced with values from R2. For example, {comment} becomes R1’s comment in R1 and it becomes R2’s comment in R2. For paired-end data, the placeholder {rn} is available (“read number”), and it is replaced with 1 in R1 and with 2 in R2. In addition, it is possible to write a placeholder as {r1.placeholdername} or {r2.placeholdername}, which always takes the replacement value from R1 or R2, respectively. The {r1.placeholder} and {r2.placeholder} notation is available for all placeholders except {rn} and {id} because the read ID needs to be identical for both reads. ----- 
Galaxy Wrapper Development
 Original author: Lance Parsons <lparsons@princeton.edu> ----- .. 
Cutadapt documentation
: https://cutadapt.readthedocs.io .. 
Illumina TruSeq Adapters De-Mystified
: http://tucf-genomics.tufts.edu/documents/protocols/TUCF_Understanding_Illumina_TruSeq_Adapters.pdf .. 
demultiplexing Illumina unique dual indices (UDIs)
: https://cutadapt.readthedocs.io/en/stable/guide.html#unique-dual-indices"
toolshed.g2.bx.psu.edu/repos/galaxyp/fasta_merge_files_and_filter_unique_sequences/fasta_merge_files_and_filter_unique_sequences/1.2.0	"What it does
 Concatenate FASTA database files together. If the uniqueness criterion is ""Accession and Sequence"", only the first appearence of each unique sequence will appear in the output. Otherwise, duplicate sequences are allowed, but only the first appearance of each accession will appear in the output. The default accession parser will treat everything in the header before the first space as the accession. ------ 
Citation
 If you use this tool in Galaxy, please the GalaxyP developers at: https://github.com/galaxyproteomics/"
toolshed.g2.bx.psu.edu/repos/devteam/fasta_formatter/cshl_fasta_formatter/1.0.1+galaxy2	"What it does
 This tool re-formats a FASTA file, changing the width of the nucleotides lines. 
TIP:
 Outputting a single line (with 
width = 0
) can be useful for scripting (with 
grep
, 
awk
, and 
perl
). Every odd line is a sequence identifier, and every even line is a nucleotides line. -------- 
Example
 Input FASTA file (each nucleotides line is 50 characters long):: >Scaffold3648 AGGAATGATGACTACAATGATCAACTTAACCTATCTATTTAATTTAGTTC CCTAATGTCAGGGACCTACCTGTTTTTGTTATGTTTGGGTTTTGTTGTTG TTGTTTTTTTAATCTGAAGGTATTGTGCATTATATGACCTGTAATACACA ATTAAAGTCAATTTTAATGAACATGTAGTAAAAACT >Scaffold9299 CAGCATCTACATAATATGATCGCTATTAAACTTAAATCTCCTTGACGGAG TCTTCGGTCATAACACAAACCCAGACCTACGTATATGACAAAGCTAATAG aactggtctttacctTTAAGTTG Output FASTA file (with width=80):: >Scaffold3648 AGGAATGATGACTACAATGATCAACTTAACCTATCTATTTAATTTAGTTCCCTAATGTCAGGGACCTACCTGTTTTTGTT ATGTTTGGGTTTTGTTGTTGTTGTTTTTTTAATCTGAAGGTATTGTGCATTATATGACCTGTAATACACAATTAAAGTCA ATTTTAATGAACATGTAGTAAAAACT >Scaffold9299 CAGCATCTACATAATATGATCGCTATTAAACTTAAATCTCCTTGACGGAGTCTTCGGTCATAACACAAACCCAGACCTAC GTATATGACAAAGCTAATAGaactggtctttacctTTAAGTTG Output FASTA file (with width=0 => single line):: >Scaffold3648 AGGAATGATGACTACAATGATCAACTTAACCTATCTATTTAATTTAGTTCCCTAATGTCAGGGACCTACCTGTTTTTGTTATGTTTGGGTTTTGTTGTTGTTGTTTTTTTAATCTGAAGGTATTGTGCATTATATGACCTGTAATACACAATTAAAGTCAATTTTAATGAACATGTAGTAAAAACT >Scaffold9299 CAGCATCTACATAATATGATCGCTATTAAACTTAAATCTCCTTGACGGAGTCTTCGGTCATAACACAAACCCAGACCTACGTATATGACAAAGCTAATAGaactggtctttacctTTAAGTTG"
toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.2+galaxy0	"What it does
 This tool offers several conversions options relating to the FASTQ format. When using 
Basic
 options, the output will be 
sanger
 formatted or 
cssanger
 formatted (when the input is Color Space Sanger). Inconsistent identifiers are fixed by default. When converting, if a quality score falls outside of the target score range, it will be coerced to the closest available value (i.e. the minimum or maximum). When converting between Solexa and the other formats, quality scores are mapped between Solexa and PHRED scales using the equations found in 
Cock PJ, Fields CJ, Goto N, Heuer ML, Rice PM. The Sanger FASTQ file format for sequences with quality scores, and the Solexa/Illumina FASTQ variants. Nucleic Acids Res. 2009 Dec 16.
 When converting between color space (csSanger) and base/sequence space (Sanger, Illumina, Solexa) formats, adapter bases are lost or gained; if gained, the base 'G' is used as the adapter. You cannot convert a color space read to base space if there is no adapter present in the color space sequence. Any masked or ambiguous nucleotides in base space will be converted to 'N's when determining color space encoding. ----- 
Quality Score Comparison
 :: SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS ...............................IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII ..........................XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX !""#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^
`abcdefghijklmnopqrstuvwxyz{|}~ | | | | | | 33 59 64 73 104 126 S - Sanger Phred+33, 93 values (0, 93) (0 to 60 expected in raw reads) I - Illumina 1.3 Phred+64, 62 values (0, 62) (0 to 40 expected in raw reads) X - Solexa Solexa+64, 67 values (-5, 62) (-5 to 40 expected in raw reads) Diagram adapted from http://en.wikipedia.org/wiki/FASTQ_format .. class:: infomark Output from Illumina 1.8+ pipelines are Sanger encoded. ------ .. _Cock PJ, Fields CJ, Goto N, Heuer ML, Rice PM. The Sanger FASTQ file format for sequences with quality scores, and the Solexa/Illumina FASTQ variants. Nucleic Acids Res. 2009 Dec 16.: https://doi.org/10.1093/nar/gkp1137"
toolshed.g2.bx.psu.edu/repos/devteam/fastq_masker_by_quality/fastq_masker_by_quality/1.1.5+galaxy2	"What it does
 This tool allows masking base characters in FASTQ format files dependent upon user specified quality score value and comparison method. This tool is not available for use on color space (csSanger) formats."
toolshed.g2.bx.psu.edu/repos/devteam/fastq_trimmer_by_quality/fastq_quality_trimmer/1.1.5	"What it does
 This tool allows you to trim the ends of reads based upon the aggregate value of quality scores found within a sliding window; a sliding window of size 1 is equivalent to 'simple' trimming of the ends. The user specifies the aggregating action (min, max, sum, mean) to perform on the quality score values found within the sliding window to be used with the user defined comparison operation and comparison value. The user can provide a maximum count of bases that can be excluded from the aggregation within the window. When set, this tool will first check the aggregation of the entire window, then after removing 1 value, then after removing 2 values, up to the number declared. Setting this value to be equal to or greater than the window size will cause no trimming to occur. ----- .. class:: warningmark Trimming a color space read will cause any adapter base to be lost."
toolshed.g2.bx.psu.edu/repos/devteam/fastq_trimmer/fastq_trimmer/1.2+galaxy0	"What is does
 This tool allows you to trim the ends of reads. You can specify either absolute or percent-based offsets. Offsets are calculated, starting at 0, from the respective end to be trimmed. When using the percent-based method, offsets are rounded to the nearest integer. For example, if you have a read of length 36:: @Some FASTQ Sanger Read CAATATGTNCTCACTGATAAGTGGATATNAGCNCCA + =@@.@;B-%?8>CBA@>7@7BBCA4-48%<;;%<B@ And you set absolute offsets of 2 and 9:: @Some FASTQ Sanger Read ATATGTNCTCACTGATAAGTGGATA + @.@;B-%?8>CBA@>7@7BBCA4-4 Or you set percent offsets of 6% and 20% (corresponds to absolute offsets of 2,7 for a read length of 36):: @Some FASTQ Sanger Read ATATGTNCTCACTGATAAGTGGATATN + @.@;B-%?8>CBA@>7@7BBCA4-48% ----- .. class:: warningmark Trimming a color space read will cause any adapter base to be lost."
toolshed.g2.bx.psu.edu/repos/devteam/fastq_paired_end_deinterlacer/fastq_paired_end_deinterlacer/1.2+galaxy0	"What it does
 De-interlaces a single fastq dataset representing paired-end run into two fastq datasets containing only the first or second mate read. Reads without mate are saved in separate output files. Sequence identifiers for paired-end reads must follow the /1 and /2 convention. ----- 
Input
 A multiple-fastq file containing paired-end reads, for example:: @1539:931/1 ACTTCCCGCGCGTGAAGGCGCCGGCAAACGAGGCTCGGGAAGGGGCTCCCG +1539:931/1 BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB @1539:931/2 CGCCATTCCGAATCGTAGTTGTCGGCGTCTTCCAGTGCGGCAAGGCATCGT +1539:931/2 WNUUZ\P^
BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB ----- **Output** Multi-fastq file with left-hand mate only:: @1539:931/1 ACTTCCCGCGCGTGAAGGCGCCGGCAAACGAGGCTCGGGAAGGGGCTCCCG +1539:931/1 BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB Multi-fastq file with right-hand mate only:: @1539:931/2 CGCCATTCCGAATCGTAGTTGTCGGCGTCTTCCAGTGCGGCAAGGCATCGT +1539:931/2 WNUUZ\P^
BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
toolshed.g2.bx.psu.edu/repos/devteam/fastq_paired_end_interlacer/fastq_paired_end_interlacer/1.2.0.1+galaxy0	"What it does
 This tool joins paired end FASTQ reads from two separate files, one with the left mates and one with the right mates, into a single files where left mates alternate with their right mates. The join is performed using sequence identifiers, allowing the two files to contain differing ordering. If a sequence identifier does not appear in both files, it is included in a separate file. Sequence identifiers with /1 and /2 appended override the left-hand and right-hand designation; i.e. if the reads end with /1 and /2, the read containing /1 will be used as the left-hand read and the read containing /2 will be used as the right-hand read. Sequences without this designation will follow the left-hand and right-hand settings set by the user. ----- 
Input
 Left-hand mates, for example:: @1539:931/1 ACTTCCCGCGCGTGAAGGCGCCGGCAAACGAGGCTCGGGAAGGGGCTCCCG +1539:931/1 BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB Right-hand mates, for example:: @1539:931/2 CGCCATTCCGAATCGTAGTTGTCGGCGTCTTCCAGTGCGGCAAGGCATCGT +1539:931/2 WNUUZ\P^
BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB ----- **Output** A multiple-fastq file containing interlaced left and right paired reads:: @1539:931/1 ACTTCCCGCGCGTGAAGGCGCCGGCAAACGAGGCTCGGGAAGGGGCTCCCG +1539:931/1 BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB @1539:931/2 CGCCATTCCGAATCGTAGTTGTCGGCGTCTTCCAGTGCGGCAAGGCATCGT +1539:931/2 WNUUZ\P^
BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB A multiple-fastq file containing reads that have no mate is also produced."
toolshed.g2.bx.psu.edu/repos/devteam/fastq_paired_end_joiner/fastq_paired_end_joiner/2.0.1.2+galaxy0	"What it does
 This tool joins paired end FASTQ reads from two separate files into a single read in one file. The join is performed using sequence identifiers, allowing the two files to contain differing ordering. If a sequence identifier does not appear in both files, it is excluded from the output. ----- 
Input formats
 Both old and new (from recent Illumina software) style FASTQ headers are supported. The following example uses the ""old"" style. Left-hand Read:: @HWI-EAS91_1_30788AAXX:7:21:1542:1758/1 GTCAATTGTACTGGTCAATACTAAAAGAATAGGATC +HWI-EAS91_1_30788AAXX:7:21:1542:1758/1 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Right-hand Read:: @HWI-EAS91_1_30788AAXX:7:21:1542:1758/2 GCTCCTAGCATCTGGAGTCTCTATCACCTGAGCCCA +HWI-EAS91_1_30788AAXX:7:21:1542:1758/2 hhhhhhhhhhhhhhhhhhhhhhhh
hfhhVZSWehR ----- **Output** A multiple-fastq file, for example:: @HWI-EAS91_1_30788AAXX:7:21:1542:1758 GTCAATTGTACTGGTCAATACTAAAAGAATAGGATCGCTCCTAGCATCTGGAGTCTCTATCACCTGAGCCCA +HWI-EAS91_1_30788AAXX:7:21:1542:1758 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
hfhhVZSWehR ------ 
The ""new"" style
 Recent Illumina FASTQ headers are structured as follows:: @COORDS FLAGS COORDS = INSTRUMENT:RUN_#:FLOWCELL_ID:LANE:TILE:X:Y FLAGS = READ:IS_FILTERED:CONTROL_NUMBER:INDEX_SEQUENCE where the whitespace character between COORDS and FLAGS can be either a space or a tab. ------ 
Credits
 New style header support added by Simone Leo <simone.leo@crs4.it> ."
toolshed.g2.bx.psu.edu/repos/devteam/fastq_paired_end_splitter/fastq_paired_end_splitter/1.2+galaxy0	"What it does
 Splits a single fastq dataset representing paired-end run into two datasets (one for each end). This tool works only for datasets where both ends have 
the same
 length. Sequence identifiers will have /1 or /2 appended for the split forward and reverse reads, respectively. ----- 
Input format
 A multiple-fastq file, for example:: @HWI-EAS91_1_30788AAXX:7:21:1542:1758 GTCAATTGTACTGGTCAATACTAAAAGAATAGGATCGCTCCTAGCATCTGGAGTCTCTATCACCTGAGCCCA +HWI-EAS91_1_30788AAXX:7:21:1542:1758 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
hfhhVZSWehR ----- **Outputs** Forward Read:: @HWI-EAS91_1_30788AAXX:7:21:1542:1758/1 GTCAATTGTACTGGTCAATACTAAAAGAATAGGATC +HWI-EAS91_1_30788AAXX:7:21:1542:1758/1 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Reverse Read:: @HWI-EAS91_1_30788AAXX:7:21:1542:1758/2 GCTCCTAGCATCTGGAGTCTCTATCACCTGAGCCCA +HWI-EAS91_1_30788AAXX:7:21:1542:1758/2 hhhhhhhhhhhhhhhhhhhhhhhh
hfhhVZSWehR"
toolshed.g2.bx.psu.edu/repos/iuc/flash/flash/1.2.11.4	FLASH (Fast Length Adjustment of SHort reads) is an accurate and fast tool to merge paired-end reads that were generated from DNA fragments whose lengths are shorter than twice the length of reads. Merged read pairs result in unpaired longer reads, which are generally more desired in genome assembly and genome analysis processes. Briefly, the FLASH algorithm considers all possible overlaps at or above a minimum length between the reads in a pair and chooses the overlap that results in the lowest mismatch density (proportion of mismatched bases in the overlapped region). Ties between multiple overlaps are broken by considering quality scores at mismatch sites. When building the merged sequence, FLASH computes a consensus sequence in the overlapped region.
toolshed.g2.bx.psu.edu/repos/iuc/fasta_stats/fasta-stats/2.0	".. class:: infomark 
Purpose
 Displays the summary statistics for a FASTA file. ------ .. class:: infomark 
Outputs
 This tool generates two outputs: a general summary and an optional gap stats file. The general summary includes the following information: - Lengths: n50, min, max, median and average - Number of base pairs: A, C, G, T, N, Total and Total_not_N - Number of sequences - GC content In addition the optional gap stats BED file includes the information about gaps localization."
toolshed.g2.bx.psu.edu/repos/mbernt/fasta_regex_finder/fasta_regex_finder/0.1.0	DESCRIPTION Search a fasta file for matches to a regular expression and return a bed file with the coordinates of the match and the matched sequence itself. Output bed file has columns: 1. Name of fasta sequence (e.g. chromosome) 2. Start of the match 3. End of the match 4. ID of the match 5. Length of the match 6. Strand 7. Matched sequence as it appears on the forward strand For matches on the reverse strand it is reported the start and end position on the forward strand and the matched string on the forward strand (so the G4 'GGGAGGGT' present on the reverse strand is reported as ACCCTCCC). Note: Fasta sequences (chroms) are read in memory one at a time along with the matches for that chromosome. The order of the output is: chroms as they are found in the inut fasta, matches sorted within chroms by positions. ARGUMENTS: - regex Regex to be searched in the fasta input. Matches to the reverse complement will have - strand. The default regex is '([gG]{3,}\w{1,7}){3,}[gG]{3,}' which searches for G-quadruplexes. - matchcase Match case while searching for matches. Default is to ignore case (I.e. 'ACTG' will match 'actg'). - noreverse Do not search the reverse complement of the input fasta. Use this flag to search protein sequences. - maxstr Maximum length of the match to report in the 7th column of the output. Default is to report up to 10000nt. Truncated matches are reported as <ACTG...ACTG>[<maxstr>,<tot length>] - seqnames List of fasta sequences in the input to search. E.g. use --seqnames chr1 chr2 chrM to search only these crhomosomes. Default is to search all the sequences in input. EXAMPLE: Test data:: >mychr ACTGnACTGnACTGnTGAC Example1 regex=ACTG:: mychr 0 4 mychr_0_4_for 4 + ACTG mychr 5 9 mychr_5_9_for 4 + ACTG mychr 10 14 mychr_10_14_for 4 + ACTG Example2 regex=ACTG maxstr=3:: mychr 0 4 mychr_0_4_for 4 + ACT[3,4] mychr 5 9 mychr_5_9_for 4 + ACT[3,4] mychr 10 14 mychr_10_14_for 4 + ACT[3,4] Example3 regex=A\w\wG:: mychr 0 5 mychr_0_5_for 5 + ACTGn mychr 5 10 mychr_5_10_for 5 + ACTGn mychr 10 15 mychr_10_15_for 5 + ACTGn
toolshed.g2.bx.psu.edu/repos/galaxyp/filter_by_fasta_ids/filter_by_fasta_ids/2.3	"What it does
 Filter entries of a FASTA file on the headers and/or the sequences based on various criteria."
toolshed.g2.bx.psu.edu/repos/devteam/fastq_filter/fastq_filter/1.2+galaxy0	"This tool allows you to build complex filters to be applied to each read in a FASTQ file. 
Basic Options:
 * You can specify a minimum and maximum read lengths. * You can specify minimum and maximum per base quality scores, with optionally specifying the number of bases that are allowed to deviate from this range (default of 0 deviant bases). * If your data is paired-end, select the proper checkbox; this will cause each read to be internally split down the middle and filters applied to each half using the offsets specified. 
Advance Options:
 * You can specify any number of advanced filters. * 5' and 3' offsets are defined, starting at zero, increasing from the respective end of the reads. For example, a quality string of ""ABCDEFG"", with 5' and 3' offsets of 1 and 1, respectively, specified will yield ""BCDEF"". * You can specify either absolute offset values, or percentage offset values. 
Absolute Values
 based offsets are useful for fixed length reads (e.g. Illumina or SOLiD data). 
Percentage of Read Length
 based offsets are useful for variable length reads (e.g. 454 data). When using the percent-based method, offsets are rounded to the nearest integer. * The user specifies the aggregating action (min, max, sum, mean) to perform on the quality score values found between the specified offsets to be used with the user defined comparison operation and comparison value. * If a set of offsets is specified that causes the remaining quality score list to be of length zero, then the read will 
pass
 the quality filter unless the size range filter is used to remove these reads. ----- .. class:: warningmark Adapter bases in color space reads are excluded from filtering."
toolshed.g2.bx.psu.edu/repos/devteam/fastq_quality_filter/cshl_fastq_quality_filter/1.0.2+galaxy2	"What it does
 This tool filters reads based on quality scores. .. class:: infomark Using 
percent = 100
 requires all cycles of all reads to be at least the quality cut-off value. .. class:: infomark Using 
percent = 50
 requires the median quality of the cycles (in each read) to be at least the quality cut-off value. -------- Quality score distribution (of all cycles) is calculated for each read. If it is lower than the quality cut-off value - the read is discarded. 
Example
:: @CSHL_4_FC042AGOOII:1:2:214:584 GACAATAAAC +CSHL_4_FC042AGOOII:1:2:214:584 30 30 30 30 30 30 30 30 20 10 Using 
percent = 50
 and 
cut-off = 30
 - This read will not be discarded (the median quality is higher than 30). Using 
percent = 90
 and 
cut-off = 30
 - This read will be discarded (90% of the cycles do no have quality equal to / higher than 30). Using 
percent = 100
 and 
cut-off = 20
 - This read will be discarded (not all cycles have quality equal to / higher than 20)."
toolshed.g2.bx.psu.edu/repos/peterjc/seq_filter_by_id/seq_filter_by_id/0.2.9	"What it does
 By default it divides a FASTA, FASTQ or Standard Flowgram Format (SFF) file in two, those sequences with or without an ID present in the tabular file column(s) specified. You can opt to have a single output file of just the matching records, or just the non-matching ones. Instead of providing the identifiers in a tabular file, you can alternatively provide them as a parameter (type or paste them into the text box). This is a useful shortcut for extracting a few sequences of interest without first having to prepare a tabular file. Note that the order of sequences in the original sequence file is preserved, as is any Roche XML Manifest in an SFF file. Also, if any sequences share an identifier (which would be very unusual in SFF files), duplicates are not removed. 
Example Usage
 You may have performed some kind of contamination search, for example running BLASTN against a database of cloning vectors or bacteria, giving you a tabular file containing read identifiers. You could use this tool to extract only the reads without BLAST matches (i.e. those which do not match your contaminant database). You may have a file of FASTA sequences which has been used with some analysis tool giving tabular output, which has then been filtered on some criteria. You can then use this tool to divide the original FASTA file into those entries matching or not matching your criteria (those with or without their identifier in the filtered tabular file). 
References
 If you use this Galaxy tool in work leading to a scientific publication please cite the following papers: Peter J.A. Cock, Björn A. Grüning, Konrad Paszkiewicz and Leighton Pritchard (2013). Galaxy tools and workflows for sequence analysis with applications in molecular plant pathology. PeerJ 1:e167 https://doi.org/10.7717/peerj.167 This tool uses Biopython to read and write SFF files, so you may also wish to cite the Biopython application note (and Galaxy too of course): Cock et al (2009). Biopython: freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics 25(11) 1422-3. https://doi.org/10.1093/bioinformatics/btp163 pmid:19304878. This tool is available to install into other Galaxy Instances via the Galaxy Tool Shed at http://toolshed.g2.bx.psu.edu/view/peterjc/seq_filter_by_id"
toolshed.g2.bx.psu.edu/repos/devteam/fasta_filter_by_length/fasta_filter_by_length/1.2	".. class:: infomark 
TIP
. To return sequences longer than a certain length, set 
Minimal length
 to desired value and leave 
Maximum length
 set to '0'. ----- 
What it does
 Outputs sequences greater than or equal to 
Minimal length
 and less than or equal to 
Maximum length
. ----- 
Example
 Suppose you have the following FASTA formatted sequences:: >seq1 TCATTTAATGAC >seq2 ATGGC >seq3 TCACATGATGCCG >seq4 ATGGAAGC Setting the 
Minimal length
 to 
10
, and the 
Maximum length
 to 
0
 will return all sequences longer than 10 bp:: >seq1 TCATTTAATGAC >seq3 TCACATGATGCCG"
toolshed.g2.bx.psu.edu/repos/iuc/length_and_gc_content/length_and_gc_content/0.1.2	"What it does
 .. class:: infomark This tool calculates the length and/or GC content for the genes in a GTF file. For the GC content, it requires a FASTA file that is the same genome version as the GTF. ----- 
Inputs
 - a GTF file - a FASTA file (if GC content is requested) ----- 
Outputs
 - a tabular file with Gene ID and length - a tabular file with Gene ID and GC content ----- 
More Information
 To calculate gene length, this tool counts the number of bases in all exons of a gene, after merging any overlapping exons from different transcripts."
toolshed.g2.bx.psu.edu/repos/devteam/fastq_manipulation/fastq_manipulation/1.2+galaxy0	"This tool allows you to build complex manipulations to be applied to each matching read in a FASTQ file. A read must match all matching directives in order for it to be manipulated; if a read does not match, it is output in a non-modified manner. All reads matching will have each of the specified manipulations performed upon them, in the order specified. Regular Expression Matches are made using re.search, see http://docs.python.org/library/re.html for more information. All matching is performed on a single line string, regardless if e.g. the sequence or quality score spans multiple lines in the original file. String translations are performed using string.translate, see http://docs.python.org/library/string.html#string.translate and http://docs.python.org/library/string.html#string.maketrans for more information. .. class:: warningmark Only color space reads can have adapter bases substituted. ----- 
Example
 Suppose you have a color space sanger formatted sequence (fastqcssanger) and you want to double-encode the color space into psuedo-nucleotide space (this is different from converting) to allow these reads to be used in tools which do not natively support it (using specially designed indexes). This tool can handle this manipulation, however, this is generally not recommended as results tend to be poorer than those produced from tools which are specially designed to handle color space data. Steps: 1. Click 
Add new Match Reads
 and leave the matching options set to the default (Matching by sequence name/identifier using the regular expression ""*.""; thereby matching all reads). 2. Click 
Add new Manipulate Reads
, change 
Manipulate Reads on
 to ""Sequence Content"", set 
Sequence Manipulation Type
 to ""Change Adapter Base"" and set 
New Adapter
 to """" (an empty text field). 3. Click 
Add new Manipulate Reads
, change 
Manipulate Reads on
 to ""Sequence Content"", set 
Sequence Manipulation Type
 to ""String Translate"" and set 
From
 to ""0123."" and 
To
 to ""ACGTN"". 4. Click Execute. The new history item will contained double-encoded psuedo-nucleotide space reads."
toolshed.g2.bx.psu.edu/repos/bgruening/find_subsequences/bg_find_subsequences/0.2	"What it does
 Searches for a subsequence in a larger nucleotide sequence. For example to get all restriction enzymes for BamHI. You can use ambiguous values using the standard 
nucleotide ambiguity code &lt;http://www.dnabaser.com/articles/IUPAC%20ambiguity%20codes.html&gt;
_. This tool is searching on both strands."
toolshed.g2.bx.psu.edu/repos/iuc/pear/iuc_pear/0.9.6.4	"What it does
 PEAR_ is an ultrafast, memory-efficient and highly accurate pair-end read merger. PEAR evaluates all possible paired-end read overlaps and without requiring the target fragment size as input. In addition, it implements a statistical test for minimizing false-positive results. Together with a highly optimized implementation, it can merge millions of paired end reads within a couple of minutes on a standard desktop computer. For more information please look at the documentation_ and 
github repository
_. .. _PEAR: https://sco.h-its.org/exelixis/web/software/pear/ .. _documentation: https://sco.h-its.org/exelixis/web/software/pear/doc.html .. _github repository: https://github.com/tseemann/PEAR Please note that PEAR is released under the 
CC Attribution-NonCommercial-ShareAlike
 license and that commercial partners should obtain a license."
toolshed.g2.bx.psu.edu/repos/devteam/fastq_quality_converter/cshl_fastq_quality_converter/1.0.1+galaxy2	"What it does
 Converts a Solexa FASTQ file to/from numeric or ASCII quality format. .. class:: warningmark Re-scaling is 
not
 performed. (e.g. conversion from Phred scale to Solexa scale). ----- FASTQ with Numeric quality scores:: @CSHL__2_FC042AGWWWXX:8:1:120:202 ACGATAGATCGGAAGAGCTAGTATGCCGTTTTCTGC +CSHL__2_FC042AGWWWXX:8:1:120:202 40 40 40 40 20 40 40 40 40 6 40 40 28 40 40 25 40 20 40 -1 30 40 14 27 40 8 1 3 7 -1 11 10 -1 21 10 8 @CSHL__2_FC042AGWWWXX:8:1:103:1185 ATCACGATAGATCGGCAGAGCTCGTTTACCGTCTTC +CSHL__2_FC042AGWWWXX:8:1:103:1185 40 40 40 40 40 35 33 31 40 40 40 32 30 22 40 -0 9 22 17 14 8 36 15 34 22 12 23 3 10 -0 8 2 4 25 30 2 FASTQ with ASCII quality scores:: @CSHL__2_FC042AGWWWXX:8:1:120:202 ACGATAGATCGGAAGAGCTAGTATGCCGTTTTCTGC +CSHL__2_FC042AGWWWXX:8:1:120:202 hhhhThhhhFhh\hhYhTh?^hN[hHACG?KJ?UJH @CSHL__2_FC042AGWWWXX:8:1:103:1185 ATCACGATAGATCGGCAGAGCTCGTTTACCGTCTTC +CSHL__2_FC042AGWWWXX:8:1:103:1185 hhhhhca_hhh`^Vh@IVQNHdObVLWCJ@HBDY^B"
toolshed.g2.bx.psu.edu/repos/devteam/fasta_nucleotide_changer/cshl_fasta_nucleotides_changer/1.0.2+galaxy2	"What it does
 This tool converts RNA FASTA files to DNA (and vice-versa). In 
RNA-to-DNA
 mode, U's are changed into T's. In 
DNA-to-RNA
 mode, T's are changed into U's. -------- 
Example
 Input RNA FASTA file ( from Sanger's mirBase ):: >cel-let-7 MIMAT0000001 Caenorhabditis elegans let-7 UGAGGUAGUAGGUUGUAUAGUU >cel-lin-4 MIMAT0000002 Caenorhabditis elegans lin-4 UCCCUGAGACCUCAAGUGUGA >cel-miR-1 MIMAT0000003 Caenorhabditis elegans miR-1 UGGAAUGUAAAGAAGUAUGUA Output DNA FASTA file (with RNA-to-DNA mode):: >cel-let-7 MIMAT0000001 Caenorhabditis elegans let-7 TGAGGTAGTAGGTTGTATAGTT >cel-lin-4 MIMAT0000002 Caenorhabditis elegans lin-4 TCCCTGAGACCTCAAGTGTGA >cel-miR-1 MIMAT0000003 Caenorhabditis elegans miR-1 TGGAATGTAAAGAAGTATGTA"
toolshed.g2.bx.psu.edu/repos/devteam/fastx_artifacts_filter/cshl_fastx_artifacts_filter/1.0.1+galaxy2	"What it does
 This tool filters sequencing artifacts (reads with all but 3 identical bases). -------- 
The following is an example of sequences which will be filtered out
:: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAACACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAACACAAAAAAAAAAAAAAAAAAAAAAAAAAAAACACAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC AAAAACACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAACACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAACACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAA AAAAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAA AAAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAA AAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAA"
toolshed.g2.bx.psu.edu/repos/devteam/fastx_renamer/cshl_fastx_renamer/0.0.14+galaxy2	"What it does
 This tool renames the sequence identifiers in a FASTQ/A file. .. class:: infomark Use this tool at the beginning of your workflow, as a way to keep the original sequence (before trimming, clipping, barcode-removal, etc). -------- 
Example
 The following Solexa-FASTQ file:: @CSHL_4_FC042GAMMII_2_1_517_596 GGTCAATGATGAGTTGGCACTGTAGGCACCATCAAT +CSHL_4_FC042GAMMII_2_1_517_596 40 40 40 40 40 40 40 40 40 40 38 40 40 40 40 40 14 40 40 40 40 40 36 40 13 14 24 24 9 24 9 40 10 10 15 40 Renamed to 
nucleotides sequence
:: @GGTCAATGATGAGTTGGCACTGTAGGCACCATCAAT GGTCAATGATGAGTTGGCACTGTAGGCACCATCAAT +GGTCAATGATGAGTTGGCACTGTAGGCACCATCAAT 40 40 40 40 40 40 40 40 40 40 38 40 40 40 40 40 14 40 40 40 40 40 36 40 13 14 24 24 9 24 9 40 10 10 15 40 Renamed to 
numeric counter
:: @1 GGTCAATGATGAGTTGGCACTGTAGGCACCATCAAT +1 40 40 40 40 40 40 40 40 40 40 38 40 40 40 40 40 14 40 40 40 40 40 36 40 13 14 24 24 9 24 9 40 10 10 15 40"
toolshed.g2.bx.psu.edu/repos/devteam/fastx_reverse_complement/cshl_fastx_reverse_complement/1.0.2+galaxy2	"What it does
 This tool reverse-complements each sequence in a library. If the library is a FASTQ, the quality-scores are also reversed. -------- 
Example
 Input FASTQ file:: @CSHL_1_FC42AGWWWXX:8:1:3:740 TGTCTGTAGCCTCNTCCTTGTAATTCAAAGNNGGTA +CSHL_1_FC42AGWWWXX:8:1:3:740 33 33 33 34 33 33 33 33 33 33 33 33 27 5 27 33 33 33 33 33 33 27 21 27 33 32 31 29 26 24 5 5 15 17 27 26 Output FASTQ file:: @CSHL_1_FC42AGWWWXX:8:1:3:740 TACCNNCTTTGAATTACAAGGANGAGGCTACAGACA +CSHL_1_FC42AGWWWXX:8:1:3:740 26 27 17 15 5 5 24 26 29 31 32 33 27 21 27 33 33 33 33 33 33 27 5 27 33 33 33 33 33 33 33 33 34 33 33 33"
toolshed.g2.bx.psu.edu/repos/devteam/short_reads_trim_seq/trim_reads/1.0.0	".. class:: warningmark To use this tool, your dataset needs to be in the 
Quality Score
 format. Click the pencil icon next to your dataset to set the datatype to 
Quality Score
 (see below for examples). ----- 
What it does
 This tool finds high quality segments within sequencing reads generated by by Roche (454), Illumina (Solexa), or ABI SOLiD machines. ----- 
Example
 Suppose this is your sequencing read:: 5'---------
-------------
------
----3' where 
dashes
 (-) are HIGH quality bases (above 20) and 
asterisks
 (*) are LOW quality bases (below 20). If the 
Minimal length of contiguous segment
 is set to 
5
 (of course, only for the purposes of this example), the tool will return:: 5'--------- ------------- ------- you can see that the tool simply splits the read on low quality bases and then returns all segments longer than 5. 
Note
, that the output of this tool will likely contain higher number of shorter sequences compared to the original input. If we set the 
Minimal length of contiguous segment
 to 
0**, the tool will only return the single longest segment:: -------------"
toolshed.g2.bx.psu.edu/repos/iuc/seqkit_grep/seqkit_grep/2.12.0+galaxy0	".. class:: infomark 
What it does
 search sequences by ID/name/sequence/sequence motifs, mismatch allowed ------ .. class:: infomark 
Attention
 0. By default, we match sequence ID with patterns, use ""-n/--by-name"" for matching full name instead of just ID. 1. Unlike POSIX/GNU grep, we compare the pattern to the whole target (ID/full header) by default. Please switch ""-r/--use-regexp"" on for partly matching. 2. When searching by sequences, it's partly matching, and both positive and negative strands are searched. Please switch on ""-P/--only-positive-strand"" if you would like to search only on the positive strand. Mismatch is allowed using flag ""-m/--max-mismatch"", you can increase the value of ""-j/--threads"" to accelerate processing. 3. Degenerate bases/residues like ""RYMM.."" are also supported by flag -d. But do not use degenerate bases/residues in regular expression, you need convert them to regular expression, e.g., change ""N"" or ""X"" to ""."". 4. When providing search patterns (motifs) via flag '-p', please use double quotation marks for patterns containing comma, e.g., -p '""A{2,}""' or -p ""\""A{2,}\"""". Because the command line argument parser accepts comma-separated-values (CSV) for multiple values (motifs). Patterns in file do not follow this rule. 5. The order of sequences in result is consistent with that in original file, not the order of the query patterns. But for FASTA file, you can use: seqkit faidx seqs.fasta --infile-list IDs.txt 6. For multiple patterns, you can either set ""-p"" multiple times, i.e., -p pattern1 -p pattern2, or give a file of patterns via ""-f/--pattern-file""."
toolshed.g2.bx.psu.edu/repos/iuc/seqkit_translate/seqkit_translate/2.12.0+galaxy0	"What it does
 Translate DNA/RNA to protein sequence (supporting ambiguous bases)"
toolshed.g2.bx.psu.edu/repos/iuc/seqkit_split2/seqkit_split2/2.12.0+galaxy1	"Seqkit Split2
 This tool splits FASTA or FASTQ files (single-end or paired-end) into multiple files based on the number of parts, sequences per part, or sequence length. It supports low memory usage and fast processing. 
Input type
: Choose between single-end FASTA/FASTQ or paired-end FASTQ files. 
Split sequences by
: - 
Number of parts
: Split into N parts using round-robin distribution. - 
Number of sequences per part
: Split into parts with N sequences each. - 
Length of sequences
: Split into chunks of >=N bases (supports K/M/G suffix, e.g., 10K, 1M). 
Outputs
 - A collection of split FASTA/FASTQ files For more details, see the Seqkit Split2 documentation_ .. _documentation: https://bioinf.shenwei.me/seqkit/usage/#split2"
toolshed.g2.bx.psu.edu/repos/iuc/sickle/sickle/1.33.3	"What it does
 Most modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well. Incorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon the length threshold. It takes the quality values and slides a window across them whose length is 0.1 times the length of the read. If this length is less than 1, then the window is set to be equal to the length of the read. Otherwise, the window slides along the quality values until the average quality in the window rises above the threshold, at which point the algorithm determines where within the window the rise occurs and cuts the read and quality there for the 5'-end cut. Then when the average quality in the window drops below the threshold, the algorithm determines where in the window the drop occurs and cuts both the read and quality strings there for the 3'-end cut. However, if the length of the remaining sequence is less than the minimum length threshold, then the read is discarded entirely (or replaced with an ""N"" record). 5'-end trimming can be disabled. Sickle also has an option to truncate reads with Ns at the first N position. Sickle supports three types of quality values: Illumina, Solexa, and Sanger. Note that the Solexa quality setting is an approximation (the actual conversion is a non-linear transformation). The end approximation is close. Illumina quality refers to qualities encoded with the CASAVA pipeline between versions 1.3 and 1.7. Illumina quality using CASAVA >= 1.8 is Sanger encoded. The quality value will be determined from the datatype of the data, i.e. a fastqsanger datatype is assumed to be Sanger encoded. Note that Sickle will remove the 2nd FASTQ record header (on the ""+"" line) and replace it with simply a ""+"". This is the default format for CASAVA >= 1.8. ----- 
Options
 
Single-end
 This option takes one single-end input file and outputs one single-end output file of reads that passed the filters. 
Paired-End (one interleaved input file)
 This option takes as input one interleaved paired-end file. If you then check the ""Output only one file with all reads"" checkbox, it will output one interleaved file where any read that did not pass filter will be replaced with a FASTQ record where the sequence is a single ""N"" and the quality is the lowest quality possible for that quality type. This will preserve the paired nature of the data. If you leave the checkbox unchecked, it will output two files, one interleaved file with all the passed pairs and one singletons file where only one of the pair passed filter. 
Paired-End (two separate input files)
 This option takes two separate (forward and reverse) paired-end files as input. The output is three files: Two paired-end files with pairs that passed filter and a singletons file where only one of the pair passed filter. 
Quality threshold
 Input your desired quality threshold. This threshold is phred-scaled, which is typically values between 0-41 for FASTQ data. 
Length threshold
 Input your desired length threshold. This is the threshold to determine if a read is kept after all the trimming steps are done. 
Disable 5-prime trimming
 An option to disable trimming the read on the 5-prime end. This trimming trims the read if the average quality values dip below the quality threshold at the 5-prime end. 
Truncate sequences with Ns
 This option will trim a read at the first ""N"" base in the read after doing quality trimming. It is then still subject to the length threshold. ----- Copyright: Nikhil Joshi http://github.com/najoshi/sickle"
toolshed.g2.bx.psu.edu/repos/rnateam/splitfasta/rbc_splitfasta/0.5.1	Takes an input FASTA file and writes entries (i.e. sequences) to separate datasets, which are organized in a dataset collection. There are two modes: 1) each sequence is written to its own data set which is named by the ID of the sequence or 2) The file is split into a given number of chunks which are numbered.
toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.10+galaxy0	Instructs Trim Galore! to remove N bp from the 3' end of read 1 after adapter/quality trimming has been performed. This may remove some unwanted bias from the 3' end that is not directly related to adapter sequence or basecall quality. (--three_prime_clip_R1)
toolshed.g2.bx.psu.edu/repos/devteam/fastx_trimmer/cshl_fastx_trimmer/1.0.2+galaxy2	"What it does
 This tool trims (cut bases from) sequences in a FASTA/Q file. -------- 
Example
 Input Fasta file (with 36 bases in each sequences):: >1-1 TATGGTCAGAAACCATATGCAGAGCCTGTAGGCACC >2-1 CAGCGAGGCTTTAATGCCATTTGGCTGTAGGCACCA Trimming with First=1 and Last=21, we get a FASTA file with 21 bases in each sequences (starting from the first base):: >1-1 TATGGTCAGAAACCATATGCA >2-1 CAGCGAGGCTTTAATGCCATT Trimming with First=6 and Last=10, will generate a FASTA file with 5 bases (bases 6,7,8,9,10) in each sequences:: >1-1 TCAGA >2-1 AGGCT"
toolshed.g2.bx.psu.edu/repos/iuc/umi_tools_count/umi_tools_count/1.1.6+galaxy0	"count - Count reads per gene from BAM using UMIs and mapping coordinates ======================================================================== This tool is only designed to work with library preparation methods where the fragmentation occurs after amplification, as per most single cell RNA-Seq methods (e.g 10x, inDrop, Drop-seq, SCRB-seq and CEL-seq2). Since the precise mapping co-ordinate is not longer informative for such library preparations, it is simplified to the gene. This is a reasonable approach providing the number of available UMIs is sufficiently high and the sequencing depth is sufficiently low that the probability of two reads from the same gene having the same UMIs is acceptably low. If you want to count reads per gene for library preparations which fragment prior to amplification (e.g bulk RNA-Seq), please use 
umi_tools dedup
 to remove the duplicate reads as this will use the full information from the mapping co-ordinate. Then use a read counting tool such as FeatureCounts or HTSeq to count the reads per gene. In the rare case of bulk RNA-Seq using a library preparation method with fragmentation after amplification, one can still use 
count
 but note that it has not been tested on bulk RNA-Seq. This tool deviates from group and dedup in that the 
--per-gene
 option is hardcoded on. Extracting barcodes ------------------- It is assumed that the FASTQ files were processed with 
umi_tools extract
 before mapping and thus the UMI is the last word of the read name. e.g: @HISEQ:87:00000000_AATT where 
AATT
 is the UMI sequeuence. If you have used an alternative method which does not separate the read id and UMI with a ""_"", such as bcl2fastq which uses "":"", you can specify the separator with the option 
--umi-separator=&lt;sep&gt;
, replacing <sep> with e.g "":"". Alternatively, if your UMIs are encoded in a tag, you can specify this by setting the option --extract-umi-method=tag and set the tag name with the --umi-tag option. For example, if your UMIs are encoded in the 'UM' tag, provide the following options: 
--extract-umi-method=tag
 
--umi-tag=UM
 Finally, if you have used umis to extract the UMI +/- cell barcode, you can specify 
--extract-umi-method=umis
 The start position of a read is considered to be the start of its alignment minus any soft clipped bases. A read aligned at position 500 with cigar 2S98M will be assumed to start at position 498. UMI grouping options -------------------- Grouping Method ............... What method to use to identify group of reads with the same (or similar) UMI(s)? All methods start by identifying the reads with the same mapping position. The simplest methods, unique and percentile, group reads with the exact same UMI. The network-based methods, cluster, adjacency and directional, build networks where nodes are UMIs and edges connect UMIs with an edit distance <= threshold (usually 1). The groups of reads are then defined from the network in a method-specific manner. For all the network-based methods, each read group is equivalent to one read count for the gene. - unique Reads group share the exact same UMI - percentile Reads group share the exact same UMI. UMIs with counts < 1% of the median counts for UMIs at the same position are ignored. - cluster Identify clusters of connected UMIs (based on hamming distance threshold). Each network is a read group - adjacency Cluster UMIs as above. For each cluster, select the node (UMI) with the highest counts. Visit all nodes one edge away. If all nodes have been visited, stop. Otherwise, repeat with remaining nodes until all nodes have been visted. Each step defines a read group. - directional (default) Identify clusters of connected UMIs (based on hamming distance threshold) and umi A counts >= (2* umi B counts) - 1. Each network is a read group."
toolshed.g2.bx.psu.edu/repos/iuc/umi_tools_dedup/umi_tools_dedup/1.1.6+galaxy0	"umi_tools dedup - Deduplicate reads based on their UMI and mapping coordinates ============================================================================== Purpose ------- The purpose of this command is to deduplicate BAM files based on the first mapping co-ordinate and the UMI attached to the read. Extracting barcodes ------------------- It is assumed that the FASTQ files were processed with 
umi_tools extract
 before mapping and thus the UMI is the last word of the read name. e.g: @HISEQ:87:00000000_AATT where 
AATT
 is the UMI sequeuence. If you have used an alternative method which does not separate the read id and UMI with a ""_"", such as bcl2fastq which uses "":"", you can specify the separator with the option 
--umi-separator=&lt;sep&gt;
, replacing <sep> with e.g "":"". Alternatively, if your UMIs are encoded in a tag, you can specify this by setting the option --extract-umi-method=tag and set the tag name with the --umi-tag option. For example, if your UMIs are encoded in the 'UM' tag, provide the following options: 
--extract-umi-method=tag
 
--umi-tag=UM
 Finally, if you have used umis to extract the UMI +/- cell barcode, you can specify 
--extract-umi-method=umis
 The start position of a read is considered to be the start of its alignment minus any soft clipped bases. A read aligned at position 500 with cigar 2S98M will be assumed to start at position 498. UMI grouping options -------------------- Grouping Method ............... What method to use to identify group of reads with the same (or similar) UMI(s)? All methods start by identifying the reads with the same mapping position. The simplest methods, unique and percentile, group reads with the exact same UMI. The network-based methods, cluster, adjacency and directional, build networks where nodes are UMIs and edges connect UMIs with an edit distance <= threshold (usually 1). The groups of reads are then defined from the network in a method-specific manner. For all the network-based methods, each read group is equivalent to one read count for the gene. - unique Reads group share the exact same UMI - percentile Reads group share the exact same UMI. UMIs with counts < 1% of the median counts for UMIs at the same position are ignored. - cluster Identify clusters of connected UMIs (based on hamming distance threshold). Each network is a read group - adjacency Cluster UMIs as above. For each cluster, select the node (UMI) with the highest counts. Visit all nodes one edge away. If all nodes have been visited, stop. Otherwise, repeat with remaining nodes until all nodes have been visted. Each step defines a read group. - directional (default) Identify clusters of connected UMIs (based on hamming distance threshold) and umi A counts >= (2
 umi B counts) - 1. Each network is a read group. Selecting the representative read --------------------------------- For every group of duplicate reads, a single representative read is retained.The following criteria are applied to select the read that will be retained from a group of duplicated reads: 1. The read with the lowest number of mapping coordinates (see 
--multimapping-detection-method
 option) 2. The read with the highest mapping quality. Note that this is not the read sequencing quality and that if two reads have the same mapping quality then one will be picked at random regardless of the read quality. Otherwise a read is chosen at random. Optional statistics output -------------------------- One can use the edit distance between UMIs at the same position as an quality control for the deduplication process by comparing with a null expectation of random sampling. For the random sampling, the observed frequency of UMIs is used to more reasonably model the null expectation. Use the option 
Output UMI related statistics files?
 generate stats outfiles: edit_distance Reports the (binned) average edit distance between the UMIs at each position. Positions with a single UMI are reported seperately. The edit distances are reported pre- and post-deduplication alongside the null expectation from random sampling of UMIs from the UMIs observed across all positions. Note that separate null distributions are reported since the null depends on the observed frequency of each UMI which is different pre- and post-deduplication. The post-duplication values should be closer to their respective null than the pre-deduplication vs null comparison In addition, this option will trigger reporting of further summary statistics for the UMIs which may be informative for selecting the optimal deduplication method or debugging. Each unique UMI sequence may be observed [0-many] times at multiple positions in the BAM. The following files report the distribution for the frequencies of each UMI. per_umi_per_position The 
_stats_per_umi_per_position.tsv
 file simply tabulates the counts for unique combinations of UMI and position. E.g if prior to deduplication, we have two positions in the BAM (POSa, POSb), at POSa we have observed 2
UMIa, 1
UMIb and at POSb: 1
UMIc, 3
UMId, then the stats file is populated thus: ====== ============= counts instances_pre ------ ------------- 1 2 2 1 3 1 ====== ============= If post deduplication, UMIb is grouped with UMIa such that POSa: 3
UMIa, then the 
instances_post
 column is populated thus: ====== ============= ============== counts instances_pre instances_post ------ ------------- -------------- 1 2 1 2 1 0 3 1 2 ====== ============= ============== per_umi_per The 
_stats_per_umi_per.tsv
 table provides UMI-level summary statistics. Keeping in mind that each unique UMI sequence can be observed at [0-many] times across multiple positions in the BAM, :times_observed: How many positions the UMI was observed at :total_counts: The total number of times the UMI was observed across all positions :median_counts: The median for the distribution of how often the UMI was observed at each position (excluding zeros) Hence, whenever times_observed=1, total_counts==median_counts."
toolshed.g2.bx.psu.edu/repos/iuc/umi_tools_extract/umi_tools_extract/1.1.6+galaxy0	"extract - Extract UMI from fastq ================================ Extract UMI barcode from a read and add it to the read name, leaving any sample barcode in place Can deal with paired end reads and UMIs split across the paired ends. Can also optionally extract cell barcodes and append these to the read name also. See the section below for an explanation for how to encode the barcode pattern(s) to specficy the position of the UMI +/- cell barcode. Filtering and correcting cell barcodes -------------------------------------- 
umi_tools extract
 can optionally filter cell barcodes against a user-supplied whitelist (
--whitelist
). If a whitelist is not available for your data, e.g if you have performed droplet-based scRNA-Seq, you can use the whitelist tool. Cell barcodes which do not match the whitelist (user-generated or automatically generated) can also be optionally corrected using the 
--error-correct-cell
 option. The whitelist should be in the following format (tab-separated):: AAAAAA AGAAAA AAAATC AAACAT AAACTA AAACTN,GAACTA AAATAC AAATCA GAATCA AAATGT AAAGGT,CAATGT Where column 1 is the whitelisted cell barcodes and column 2 is the list (comma-separated) of other cell barcodes which should be corrected to the barcode in column 1. If the 
--error-correct-cell
 option is not used, this column will be ignored. Any additional columns in the whitelist input, such as the counts columns from the output of umi_tools whitelist, will be ignored. There are two methods enabled to extract the umi barcode (+/- cell barcode). For both methods, the patterns should be provided using the 
--bc-pattern
 and 
--bc-pattern2
 options.x - 
string
 This should be used where the barcodes are always in the same place in the read. - N = UMI position (required) - C = cell barcode position (optional) - X = sample position (optional) Bases with Ns and Cs will be extracted and added to the read name. The corresponding sequence qualities will be removed from the read. Bases with an X will be reattached to the read. E.g. If the pattern is 
NNNNCC
, Then the read:: @HISEQ:87:00000000 read1 AAGGTTGCTGATTGGATGGGCTAG + DA1AEBFGGCG01DFH00B1FF0B will become:: @HISEQ:87:00000000_TT_AAGG read1 GCTGATTGGATGGGCTAG + 1AFGGCG01DFH00B1FF0B where 'TT' is the cell barcode and 'AAGG' is the UMI. - 
regex
 This method allows for more flexible barcode extraction and should be used where the cell barcodes are variable in length. Alternatively, the regex option can also be used to filter out reads which do not contain an expected adapter sequence. UMI-tools uses the regex module rather than the more standard re module since the former also enables fuzzy matching The regex must contain groups to define how the barcodes are encoded in the read. The expected groups in the regex are: umi_n = UMI positions, where n can be any value (required) cell_n = cell barcode positions, where n can be any value (optional) discard_n = positions to discard, where n can be any value (optional) UMI positions and cell barcode positions will be extracted and added to the read name. The corresponding sequence qualities will be removed from the read. Discard bases and the corresponding quality scores will be removed from the read. All bases matched by other groups or components of the regex will be reattached to the read sequence For example, the following regex can be used to extract reads from the Klein et al inDrop data:: (?P<cell_1>.{8,12})(?P<discard_1>GAGTGATTGCTTGTGACGCCTT)(?P<cell_2>.{8})(?P<umi_1>.{6})T{3}.* Where only reads with a 3' T-tail and 
GAGTGATTGCTTGTGACGCCTT
 in the correct position to yield two cell barcodes of 8-12 and 8bp respectively, and a 6bp UMI will be retained. You can also specify fuzzy matching to allow errors. For example if the discard group above was specified as below this would enable matches with up to 2 errors in the discard_1 group. :: (?P<discard_1>GAGTGATTGCTTGTGACGCCTT){s<=2} Note that all UMIs must be the same length for downstream processing with dedup, group or count commands"
toolshed.g2.bx.psu.edu/repos/iuc/umi_tools_group/umi_tools_group/1.1.6+galaxy0	"umi_tools group - Group reads based on their UMI ================================================ Purpose ------- The purpose of this command is to identify groups of reads based on their genomic coordinate and UMI. The group command can be used to create two types of outfile: a tagged BAM or a flatfile describing the read groups To generate the tagged-BAM file, use the option --output-bam and provide a filename with the -S option. Alternatively, if you do not provide a filename, the bam file will be outputted to the stdout. If you have provided the --log/-L option to send the logging output elsewhere, you can pipe the output from the group command directly to e.g samtools sort like so: 
umi_tools group -I inf.bam --group-out=grouped.tsv --output-bam --log=group.log --paired | samtools sort - -o grouped_sorted.bam
 The tagged-BAM file will have two tagged per read: - UG = Unique_id. 0-indexed unique id number for each group of reads with the same genomic position and UMI or UMIs inferred to be from the same true UMI + errors - BX = Final UMI. The inferred true UMI for the group To generate the flatfile describing the read groups, include the --group-out=<filename> option. The columns of the read groups file are below. The first five columns relate to the read. The final 3 columns relate to the group. - read_id read identifier - contig alignment contig - position Alignment position. Note that this position is not the start position of the read in the BAM file but the start of the read taking into account the read strand and cigar - umi The read UMI - umi_count The number of times this UMI is observed for reads at the same position - final_umi The inferred true UMI for the group - final_umi_count The total number of reads within the group - unique_id The unique id for the group Extracting barcodes ------------------- It is assumed that the FASTQ files were processed with 
umi_tools extract
 before mapping and thus the UMI is the last word of the read name. e.g: @HISEQ:87:00000000_AATT where 
AATT
 is the UMI sequeuence. If you have used an alternative method which does not separate the read id and UMI with a ""_"", such as bcl2fastq which uses "":"", you can specify the separator with the option 
--umi-separator=&lt;sep&gt;
, replacing <sep> with e.g "":"". Alternatively, if your UMIs are encoded in a tag, you can specify this by setting the option --extract-umi-method=tag and set the tag name with the --umi-tag option. For example, if your UMIs are encoded in the 'UM' tag, provide the following options: 
--extract-umi-method=tag
 
--umi-tag=UM
 Finally, if you have used umis to extract the UMI +/- cell barcode, you can specify 
--extract-umi-method=umis
 The start position of a read is considered to be the start of its alignment minus any soft clipped bases. A read aligned at position 500 with cigar 2S98M will be assumed to start at position 498. UMI grouping options -------------------- Grouping Method ............... What method to use to identify group of reads with the same (or similar) UMI(s)? All methods start by identifying the reads with the same mapping position. The simplest methods, unique and percentile, group reads with the exact same UMI. The network-based methods, cluster, adjacency and directional, build networks where nodes are UMIs and edges connect UMIs with an edit distance <= threshold (usually 1). The groups of reads are then defined from the network in a method-specific manner. For all the network-based methods, each read group is equivalent to one read count for the gene. - unique Reads group share the exact same UMI - percentile Reads group share the exact same UMI. UMIs with counts < 1% of the median counts for UMIs at the same position are ignored. - cluster Identify clusters of connected UMIs (based on hamming distance threshold). Each network is a read group - adjacency Cluster UMIs as above. For each cluster, select the node (UMI) with the highest counts. Visit all nodes one edge away. If all nodes have been visited, stop. Otherwise, repeat with remaining nodes until all nodes have been visted. Each step defines a read group. - directional (default) Identify clusters of connected UMIs (based on hamming distance threshold) and umi A counts >= (2* umi B counts) - 1. Each network is a read group."
toolshed.g2.bx.psu.edu/repos/iuc/umi_tools_whitelist/umi_tools_whitelist/1.1.6+galaxy0	"UMI-tools whitelist - Extract barcodes from fastq ================================================== Purpose ------- Extract cell barcodes and identify the most likely true barcodes using the 'knee' method. There are two methods enabled to extract the umi barcode (+/- cell barcode). For both methods, the patterns should be provided using the 
--bc-pattern
 and 
--bc-pattern2
 options.x - 
string
 This should be used where the barcodes are always in the same place in the read. - N = UMI position (required) - C = cell barcode position (optional) - X = sample position (optional) Bases with Ns and Cs will be extracted and added to the read name. The corresponding sequence qualities will be removed from the read. Bases with an X will be reattached to the read. E.g. If the pattern is 
NNNNCC
, Then the read:: @HISEQ:87:00000000 read1 AAGGTTGCTGATTGGATGGGCTAG + DA1AEBFGGCG01DFH00B1FF0B will become:: @HISEQ:87:00000000_TT_AAGG read1 GCTGATTGGATGGGCTAG + 1AFGGCG01DFH00B1FF0B where 'TT' is the cell barcode and 'AAGG' is the UMI. - 
regex
 This method allows for more flexible barcode extraction and should be used where the cell barcodes are variable in length. Alternatively, the regex option can also be used to filter out reads which do not contain an expected adapter sequence. UMI-tools uses the regex module rather than the more standard re module since the former also enables fuzzy matching The regex must contain groups to define how the barcodes are encoded in the read. The expected groups in the regex are: umi_n = UMI positions, where n can be any value (required) cell_n = cell barcode positions, where n can be any value (optional) discard_n = positions to discard, where n can be any value (optional) UMI positions and cell barcode positions will be extracted and added to the read name. The corresponding sequence qualities will be removed from the read. Discard bases and the corresponding quality scores will be removed from the read. All bases matched by other groups or components of the regex will be reattached to the read sequence For example, the following regex can be used to extract reads from the Klein et al inDrop data:: (?P<cell_1>.{8,12})(?P<discard_1>GAGTGATTGCTTGTGACGCCTT)(?P<cell_2>.{8})(?P<umi_1>.{6})T{3}.* Where only reads with a 3' T-tail and 
GAGTGATTGCTTGTGACGCCTT
 in the correct position to yield two cell barcodes of 8-12 and 8bp respectively, and a 6bp UMI will be retained. You can also specify fuzzy matching to allow errors. For example if the discard group above was specified as below this would enable matches with up to 2 errors in the discard_1 group. :: (?P<discard_1>GAGTGATTGCTTGTGACGCCTT){s<=2} Note that all UMIs must be the same length for downstream processing with dedup, group or count commands Output: ------- The whitelist is outputted as 4 tab-separated columns: 1. whitelisted cell barcode 2. Other cell barcode(s) (comma-separated) to correct to the whitelisted barcode 3. Count for whitelisted cell barcodes 4. Count(s) for the other cell barcode(s) (comma-separated) example output: AAAAAA AGAAAA 146 1 AAAATC 22 AAACAT 21 AAACTA AAACTN,GAACTA 27 1,1 AAATAC 72 AAATCA GAATCA 37 3 AAATGT AAAGGT,CAATGT 41 1,1 AAATTG CAATTG 36 1 AACAAT 18 AACATA 24 If --error-correct-threshold is set to 0, columns 2 and 4 will be empty."
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_fasplit/fasplit/482	"What it does
 
faSplit
 is a tool to split a single FASTA file into several files. For implementation details see faSplit's 
source code
. .. _faSplit: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/utils/faSplit/faSplit.c"
toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/1.1.0+galaxy0	".. class:: infomark 
What it does
 fastp_ is a tool designed to provide fast all-in-one preprocessing for FASTQ files. This tool is developed in C++ with multithreading supported to afford high performance. 
Features
 1. Filter out bad (too low quality, too short, or too many N...) and/or duplicate reads 2. Cut low quality bases for per read in its 5' and 3' by evaluating the mean quality from a sliding window (like Trimmomatic but faster) 3. Trim all reads in front and tail 4. Cut adapters. Adapter sequences can be automatically detected, which means you don't have to input the adapter sequences to trim them. 5. Correct mismatched base pairs in overlapped regions of paired end reads, if one base is with high quality while the other is with ultra-low quality 6. Trim polyG in 3' ends, which is commonly seen in NovaSeq/NextSeq data. Trim polyX in 3' ends to remove unwanted polyX tailing (i.e. polyA tailing for mRNA-Seq data) 7. Preprocess unique molecular identifier (UMI) enabled data, shift UMI to sequence name 8. Report JSON format result for further interpreting 9. Visualize quality control and filtering results on a single HTML page (like FASTQC but faster and more informative) 10. Split the output to multiple files (0001.R1.gz, 0002.R1.gz...) to support parallel processing. Two modes can be used, limiting the total split file number, or limiting the lines of each split file (
Not enabled in this Galaxy tool
) 11. Support long reads (data from PacBio / Nanopore devices) ----- 
Inputs
 Single-end or Paired-end (compressed) fastqsanger files ----- 
Outputs
 * Processed reads * Merged reads * Unmerged filtered reads, reads that cannot be merged successfully, but both pass all the filters. * Unmerged unfiltered reads, reads that cannot be merged, i.e. 
forward
 passes filters but 
reverse
 doesn't. Optionally, under 
Output Options
 you can choose to output * HTML report (default is Yes) * JSON report (compatible with MultiQC) .. _fastp: https://github.com/OpenGene/fastp"
toolshed.g2.bx.psu.edu/repos/lparsons/fastq_join/fastq_join/1.1.2-806.1	Overview -------- fastq-join joins two paired-end reads on the overlapping ends. Split read ids character: Verifies that the 2 files probe id's match up to char C. Use ' ' for Illumina reads. Maximum difference is the maximum allowed percentage of bases that differ in the matching region. Minimum overlap is the minimum number of bases that must overlap (with no more than the maximum difference) for reads to be joined. Verbose stitch length report is a report for each joined paired of reads showing how large the overlapping section was. This tool uses sqr(distance)/len for anchored alignment quality algorithm. It's a good measure of anchored alignment quality, akin to squared-deviation for means. This tool uses the fastq-join program that is part of the ea-utils suite. See http://code.google.com/p/ea-utils/wiki/FastqJoin for details.
toolshed.g2.bx.psu.edu/repos/iuc/filtlong/filtlong/0.3.1+galaxy0	Filtlong is a tool for filtering long reads by quality. It can take a set of long reads and produce a smaller, better subset. It uses both read length (longer is better) and read identity (higher is better) when choosing which reads pass the filter.
toolshed.g2.bx.psu.edu/repos/devteam/fastx_quality_statistics/cshl_fastx_quality_statistics/1.0.1+galaxy2	"What it does
 Creates quality statistics report for the given Solexa/FASTQ library. .. class:: infomark 
TIP:
 This statistics report can be used as input for 
Quality Score
 and 
Nucleotides Distribution
 tools. ----- 
The output file will contain the following fields:
 * column = column number (1 to 36 for a 36-cycles read Solexa file) * count = number of bases found in this column. * min = Lowest quality score value found in this column. * max = Highest quality score value found in this column. * sum = Sum of quality score values for this column. * mean = Mean quality score value for this column. * Q1 = 1st quartile quality score. * med = Median quality score. * Q3 = 3rd quartile quality score. * IQR = Inter-Quartile range (Q3-Q1). * lW = 'Left-Whisker' value (for boxplotting). * rW = 'Right-Whisker' value (for boxplotting). * A_Count = Count of 'A' nucleotides found in this column. * C_Count = Count of 'C' nucleotides found in this column. * G_Count = Count of 'G' nucleotides found in this column. * T_Count = Count of 'T' nucleotides found in this column. * N_Count = Count of 'N' nucleotides found in this column. For example:: 1 6362991 -4 40 250734117 39.41 40 40 40 0 40 40 1396976 1329101 678730 2958184 0 2 6362991 -5 40 250531036 39.37 40 40 40 0 40 40 1786786 1055766 1738025 1782414 0 3 6362991 -5 40 248722469 39.09 40 40 40 0 40 40 2296384 984875 1443989 1637743 0 4 6362991 -4 40 248214827 39.01 40 40 40 0 40 40 2536861 1167423 1248968 1409739 0 36 6362991 -5 40 117158566 18.41 7 15 30 23 -5 40 4074444 1402980 63287 822035 245"
toolshed.g2.bx.psu.edu/repos/devteam/fastx_nucleotides_distribution/cshl_fastx_nucleotides_distribution/1.0.1+galaxy2	"What it does
 Creates a stacked-histogram graph for the nucleotide distribution in the Solexa library. .. class:: infomark 
TIP:
 Use the 
FASTQ Statistics
 tool to generate the report file needed for this tool. ----- 
Output Examples
 The following chart clearly shows the barcode used at the 5'-end of the library: 
GATCT
 .. image:: fastq_nucleotides_distribution_1.png In the following chart, one can almost 'read' the most abundant sequence by looking at the dominant values: 
TGATA TCGTA TTGAT GACTG AA...
 .. image:: fastq_nucleotides_distribution_2.png The following chart shows a growing number of unknown (N) nucleotides towards later cycles (which might indicate a sequencing problem): .. image:: fastq_nucleotides_distribution_3.png But most of the time, the chart will look rather random: .. image:: fastq_nucleotides_distribution_4.png"
toolshed.g2.bx.psu.edu/repos/devteam/fastq_quality_boxplot/cshl_fastq_quality_boxplot/1.0.1+galaxy2	"What it does
 Creates a boxplot graph for the quality scores in the library. .. class:: infomark 
TIP:
 Use the 
FASTQ Statistics
 tool to generate the report file needed for this tool. ----- 
Output Examples
 * Black horizontal lines are medians * Rectangular red boxes show the Inter-quartile Range (IQR) (top value is Q3, bottom value is Q1) * Whiskers show outlier at max. 1.5*IQR An excellent quality library (median quality is 40 for almost all 36 cycles): .. image:: fastq_quality_boxplot_1.png A relatively good quality library (median quality degrades towards later cycles): .. image:: fastq_quality_boxplot_2.png A low quality library (median drops quickly): .. image:: fastq_quality_boxplot_3.png"
toolshed.g2.bx.psu.edu/repos/devteam/fastq_stats/fastq_stats/1.1.5+galaxy2	"What is does
 This tool creates summary statistics on a FASTQ file. .. class:: infomark 
TIP:
 This statistics report can be used as input for the 
Boxplot
 tools. ----- 
The output file will contain the following fields:
 * column = column number (1 to 36 for a 36-cycles read Solexa file) * count = number of bases found in this column. * min = Lowest quality score value found in this column. * max = Highest quality score value found in this column. * sum = Sum of quality score values for this column. * mean = Mean quality score value for this column. * Q1 = 1st quartile quality score. * med = Median quality score. * Q3 = 3rd quartile quality score. * IQR = Inter-Quartile range (Q3-Q1). * lW = 'Left-Whisker' value (for boxplotting). * rW = 'Right-Whisker' value (for boxplotting). * outliers = Scores falling beyond the left and right whiskers (comma separated list). * A_Count = Count of 'A' nucleotides found in this column. * C_Count = Count of 'C' nucleotides found in this column. * G_Count = Count of 'G' nucleotides found in this column. * T_Count = Count of 'T' nucleotides found in this column. * N_Count = Count of 'N' nucleotides found in this column. * Other_Nucs = Comma separated list of other nucleotides found in this column. * Other_Count = Comma separated count of other nucleotides found in this column. For example:: #column count min max sum mean Q1 med Q3 IQR lW rW outliers A_Count C_Count G_Count T_Count N_Count other_bases other_base_count 1 14336356 2 33 450600675 31.4306281875 32.0 33.0 33.0 1.0 31 33 2,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30 4482314 2199633 4425957 3208745 19707 2 14336356 2 34 441135033 30.7703737965 30.0 33.0 33.0 3.0 26 34 2,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 4419184 2170537 4627987 3118567 81 3 14336356 2 34 433659182 30.2489127642 29.0 32.0 33.0 4.0 23 34 2,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22 4310988 2941988 3437467 3645784 129 4 14336356 2 34 433635331 30.2472490917 29.0 32.0 33.0 4.0 23 34 2,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22 4110637 3007028 3671749 3546839 103 5 14336356 2 34 432498583 30.167957813 29.0 32.0 33.0 4.0 23 34 2,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22 4348275 2935903 3293025 3759029 124 ----- .. class:: warningmark Adapter bases in color space reads are excluded from statistics."
toolshed.g2.bx.psu.edu/repos/bgruening/fastq_info/fastq_info/0.25.1+Galaxy0	".. class:: infomark 
Purpose
 FASTQ info is part of 
FASTQ utils &lt;https://github.com/nunofonseca/fastq_utils&gt;
_, a set of Linux utilities to validate and manipulate fastq files."
toolshed.g2.bx.psu.edu/repos/iuc/fastqe/fastqe/0.3.1+galaxy0	FASTQ + Emoji = FASTQE 🤔 ========================= Compute quality stats for FASTQ files and print those stats as emoji... for some reason. Scores can also be binned: +-------+-------+ | Bin | Emoji | +=======+=======+ | N | 🚫 | +-------+-------+ | 2-9 | 💀 | +-------+-------+ | 10–19 | 💩 | +-------+-------+ | 20–24 | ⚠️ | +-------+-------+ | 25–29 | 😄 | +-------+-------+ | 30–34 | 😆 | +-------+-------+ | 35–39 | 😎 | +-------+-------+ | ≥ 40 | 😍 | +-------+-------+
toolshed.g2.bx.psu.edu/repos/iuc/falco/falco/1.2.4+galaxy0	"What it does
 Falco_ is a high-speed emulation of the popular FastQC software for quality control of sequencing data. 💚️ With its superior performance Falco saves computational resources and gives you back results faster than FastQC. We recommend it for most use cases (but see below for rare exceptions). 💚️ The main functions of Falco are very similar to those of FastQC: - Import of data from BAM, SAM or FastQ/FastQ.gz files (any variant), - Providing a quick overview to tell you in which areas there may be problems - Summary graphs and tables to quickly assess your data - Export of results to an HTML-based report .. class:: infomark The plain text report generated by Falco can be used as a ""FastQC"" report in MultiQC and its data is very similar though not 100% identical to that generated by FastQC on the same inputs. .. class:: Warning mark In the following situations, FastQC is still a better solution than this version of Falco: - your input is bz2-compressed fastq Falco doesn't currently support fastq.bz2 as input format meaning Galaxy has to perform a relatively slow format conversion before running the tool, which together makes the analysis slower than with FastQC. - you need the HTML report to be viewable offline The current version of Falco relies on plotly to generate the graphs in the HTML report dynamically each time it's viewed. MultiQC plots generated from Falco's raw data output are, of course, viewable offline just like the ones generated from FastQC output. ----- 
Inputs and outputs
 The Falco_ development repo includes very good documentation. A summary of it follows below for those in a tearing hurry. This wrapper will accept a Galaxy fastq, fastq.gz, sam or bam as the input read file to check. It will also take an optional file containing a list of contaminants information, in the form of a tab-delimited file with 2 columns, name and sequence. As another option the tool takes a custom limits.txt file that allows setting the warning thresholds for the different modules and also specifies which modules to include in the output. The tool produces a basic text and a HTML output file that contain all of the results, including the following: - Basic Statistics - Per base sequence quality - Per sequence quality scores - Per base sequence content - Per base GC content - Per sequence GC content - Per base N content - Sequence Length Distribution - Sequence Duplication Levels - Overrepresented sequences - Adapter Content All except Basic Statistics and Overrepresented sequences are plots. .. _Falco: https://github.com/smithlabcode/falco/"
toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1	".. class:: infomark 
Purpose
 FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines. It provides a set of analyses which you can use to get a quick impression of whether your data has any problems of which you should be aware before doing any further analysis. The main functions of FastQC are: - Import of data from BAM, SAM or FastQ/FastQ.gz files (any variant), - Providing a quick overview to tell you in which areas there may be problems - Summary graphs and tables to quickly assess your data - Export of results to an HTML based permanent report - Offline operation to allow automated generation of reports without running the interactive application ----- .. class:: infomark 
FastQC
 This is a Galaxy wrapper. It merely exposes the external package FastQC_ which is documented at FastQC_ Kindly acknowledge it as well as this tool if you use it. FastQC incorporates the Picard-tools_ libraries for SAM/BAM processing. The contaminants file parameter was borrowed from the independently developed fastqcwrapper contributed to the Galaxy Community Tool Shed by J. Johnson. Adaption to version 0.11.2 by T. McGowan. ----- .. class:: infomark 
Inputs and outputs
 FastQC_ is the best place to look for documentation - it's very good. A summary follows below for those in a tearing hurry. This wrapper will accept a Galaxy fastq, fastq.gz, sam or bam as the input read file to check. It will also take an optional file containing a list of contaminants information, in the form of a tab-delimited file with 2 columns, name and sequence. As another option the tool takes a custom limits.txt file that allows setting the warning thresholds for the different modules and also specifies which modules to include in the output. The tool produces a basic text and a HTML output file that contain all of the results, including the following: - Basic Statistics - Per base sequence quality - Per sequence quality scores - Per base sequence content - Per base GC content - Per sequence GC content - Per base N content - Sequence Length Distribution - Sequence Duplication Levels - Overrepresented sequences - Kmer Content All except Basic Statistics and Overrepresented sequences are plots. .. _FastQC: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/ .. _Picard-tools: https://broadinstitute.github.io/picard/"
toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.33+galaxy0	"What it does
 
MultiQC &lt;http://multiqc.info/&gt;
_ aggregates results from bioinformatics analyses across many samples into a single report. It takes results of multiple analyses and creates a report that can be viewed as a single beautiful web-page. It's a general use tool, perfect for summarizing the output from numerous bioinformatics tools. 
Inputs
 MultiQC takes software output summaries/logs and creates a single report from them. You need to tell the tool which software was used to generate the report. This is done using the 
Software name
 dropdown. At present only the Galaxy tools found in the ToolShed produce logs that can used with MultiQC"
toolshed.g2.bx.psu.edu/repos/iuc/prinseq/prinseq/0.20.4+galaxy2	"What it does
 PRINSEQ is a tool for easy and rapid quality control and data processing of metagenomic and metatranscriptomic datasets. This tool allow to process the sequences with filtering and trimming. More information on 
PRINSEQ manual &lt;http://prinseq.sourceforge.net/manual.html&gt;
_. ----- 
Input
 The input file is sequence file in fastq format (sequences and quality):: @HWI-M00234:263:000000000-ADM55:1:1101:7508:4067 1:N:0:ATCACG GGTGCACTAGGATCGTAGTTGGCTACTTTCCCGTTTTCAATGTATACGCAAGGTACACGGTCAGCGGT + CCCCCGFGED8DDCAFDAEE9DFGGGG9CFAFFCC@@CFGFGGCGFGG>GGGFFGDGEFFEFG8>4GF ----- 
Parameters
 The parameters are numerous in PRINSEQ given the wanted treatments. Several filter treatments are proposed: - Filters based on sequence length - Filters based on quality score - Filters based on base content And several trimming treatments eliminate sequence parts: - Trim of ends - Trim of tails - Trim based quality score All these treaments can be customed using proposed parameters. ----- 
Output
 The output file is a sequence file with sequences and quality from input file which have undergone filter and trimming."
toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2	".. class:: infomark 
What it does
 Trimmomatic performs a variety of useful trimming tasks for illumina paired-end and single ended data. This tool allows the following trimming steps to be performed: * 
ILLUMINACLIP:
 Cut adapter and other illumina-specific sequences from the read * If 
Always keep both reads (PE specific/palindrome mode)
 is True, the reverse read will also be retained in palindrome mode. After read-though has been detected by palindrome mode, and the adapter sequence removed, the reverse read contains the same sequence information as the forward read, albeit in reverse complement. For this reason, the default behaviour is to entirely drop the reverse read. Retaining the reverse read may be useful e.g. if the downstream tools cannot handle a combination of paired and unpaired reads. * 
SLIDINGWINDOW:
 Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold * 
MINLEN:
 Drop the read if it is below a specified length * 
LEADING:
 Cut bases off the start of a read, if below a threshold quality * 
TRAILING:
 Cut bases off the end of a read, if below a threshold quality * 
CROP:
 Cut the read to a specified length * 
HEADCROP:
 Cut the specified number of bases from the start of the read * 
AVGQUAL:
 Drop the read if the average quality is below a specified value * 
MAXINFO:
 Trim reads adaptively, balancing read length and error rate to maximise the value of each read If ILLUMINACLIP is requested then it is always performed first; subsequent options can be mixed and matched and will be performed in the order that they have been specified. .. class:: warningmark Note that trimming operation order is important. ------------- .. class:: infomark 
Inputs
 For single-end data this Trimmomatic tool accepts a single FASTQ file; for paired-end data it will accept either two FASTQ files (R1 and R2), or a dataset collection containing the R1/R2 FASTQ pair. .. class:: infomark 
Outputs
 For paired-end data a particular strength of Trimmomatic is that it retains the pairing of reads (from R1 and R2) in the filtered output files: * Two FASTQ files (R1-paired and R2-paired) contain one read from each pair where both have survived filtering. * Additionally two FASTQ files (R1-unpaired and R2-unpaired) contain reads where one of the pair failed the filtering steps. .. class:: warningmark If the input consists of a dataset collection with the R1/R2 FASTQ pair then the outputs will also inclue two dataset collections: one for the 'paired' outputs and one for the 'unpaired' (as described above) Retaining the same order and number of reads in the filtered output fastq files is essential for many downstream analysis tools. For single-end data the output is a single FASTQ file containing just the filtered reads. ------------- .. class:: infomark 
Credits
 This Galaxy tool was originally developed within the Bioinformatics Core Facility at the University of Manchester, with contributions from Peter van Heusden, Marius van den Beek, Jelle Scholtalbers, Charles Girardot, Matthias Bernt and Cristóbal Gallardo. It is now maintained as part of the IUC tool collection. It runs the Trimmomatic program which has been developed within Bjorn Usadel's group at RWTH Aachen university. Trimmomatic website (including documentation): * http://www.usadellab.org/cms/index.php?page=trimmomatic The reference for Trimmomatic is: * Bolger, A.M., Lohse, M., &amp; Usadel, B. (2014). Trimmomatic: A flexible trimmer for Illumina Sequence Data. Bioinformatics, btu170. Please kindly acknowledge both this Galaxy tool and the Trimmomatic program if you use it."
toolshed.g2.bx.psu.edu/repos/iuc/extract_genomic_dna/Extract genomic DNA 1/3.0.3+galaxy3	".. class:: warningmark This tool requires interval or gff (special tabular formatted data). If your data is not TAB delimited, first use 
Text Manipulation->Convert
. .. class:: warningmark Make sure that the genome build is specified for the dataset from which you are extracting sequences (click the pencil icon in the history item if it is not specified). .. class:: warningmark All of the following will cause a line from the input dataset to be skipped and a warning generated. The number of warnings and skipped lines is documented in the resulting history item. - Any lines that do not contain at least 3 columns, a chromosome and numerical start and end coordinates. - Sequences that fall outside of the range of a line's start and end coordinates. - Chromosome, start or end coordinates that are invalid for the specified build. - Any lines whose data columns are not separated by a 
TAB
 character ( other white-space characters are invalid ). ----- 
What it does
 This tool uses coordinate, strand, and build information to fetch genomic DNAs in FASTA or interval format. If the output format is FASTA, the header format can be specified. Selecting the 
bedtools getfasta default
 option produces a FASTA header formatted like the default header produced the the bedtools getfasta tool, and the ""force strandedness"" option is assumed. If the input data includes a strand column and the strand is '+' or '-', it is included in the header. If the input data includes a strand column and the value is anything but '+' or '-', a '.' is included in the header. If the input data does not include a strand column, a '.' is included in the header. An example FASTA header produced by selecting this option is: >chr7:127475281-127475310(+) Selecing the 
character delimited field values
 option allows selection of a character delimiter that is used when generating the FASTA header with fields genome, chrom, start, end, strand (name) delimited by the selected character. For example, selecting an underscore will produce a FASTA header like this: >mm9_53_550_+ test_chromosome while selecting a vertical bar will produce a FASTA header like this: >mm9|53|550|+ test_chromosome If strand is not defined, the default value is ""+"". ----- 
Example
 If the input dataset is:: chr7 127475281 127475310 NM_000230 0 + chr7 127485994 127486166 NM_000230 0 + chr7 127486011 127486166 D49487 0 + Extracting sequences with 
FASTA
 output data type, 
character delimited field values
 as header format and 
header field delimiter
 set to the underscore character returns:: >hg17_chr7_127475281_127475310_+ NM_000230 GTAGGAATCGCAGCGCCAGCGGTTGCAAG >hg17_chr7_127485994_127486166_+ NM_000230 GCCCAAGAAGCCCATCCTGGGAAGGAAAATGCATTGGGGAACCCTGTGCG GATTCTTGTGGCTTTGGCCCTATCTTTTCTATGTCCAAGCTGTGCCCATC CAAAAAGTCCAAGATGACACCAAAACCCTCATCAAGACAATTGTCACCAG GATCAATGACATTTCACACACG >hg17_chr7_127486011_127486166_+ D49487 TGGGAAGGAAAATGCATTGGGGAACCCTGTGCGGATTCTTGTGGCTTTGG CCCTATCTTTTCTATGTCCAAGCTGTGCCCATCCAAAAAGTCCAAGATGA CACCAAAACCCTCATCAAGACAATTGTCACCAGGATCAATGACATTTCAC ACACG Extracting sequences with 
Interval
 output data type returns:: chr7 127475281 127475310 NM_000230 0 + GTAGGAATCGCAGCGCCAGCGGTTGCAAG chr7 127485994 127486166 NM_000230 0 + GCCCAAGAAGCCCATCCTGGGAAGGAAAATGCATTGGGGAACCCTGTGCGGATTCTTGTGGCTTTGGCCCTATCTTTTCTATGTCCAAGCTGTGCCCATCCAAAAAGTCCAAGATGACACCAAAACCCTCATCAAGACAATTGTCACCAGGATCAATGACATTTCACACACG chr7 127486011 127486166 D49487 0 + TGGGAAGGAAAATGCATTGGGGAACCCTGTGCGGATTCTTGTGGCTTTGGCCCTATCTTTTCTATGTCCAAGCTGTGCCCATCCAAAAAGTCCAAGATGACACCAAAACCCTCATCAAGACAATTGTCACCAGGATCAATGACATTTCACACACG"
maf_by_block_number1	"What it does
 This tool takes a list of block numbers, one per line, and extracts the corresponding MAF blocks from the provided file. Block numbers start at 0."
Interval2Maf_pairwise1	"What it does
 This tool takes genomic coordinates, superimposes them on pairwise alignments (in MAF format) stored on the Galaxy site, and excises alignment blocks corresponding to each set of coordinates. Alignment blocks that extend past START and/or END positions of an interval are trimmed. Note that a single genomic interval may correspond to two or more alignment blocks. ----- 
Example
 Here a single interval is superimposed on three MAF blocks. Blocks 1 and 3 are trimmed because they extend beyond boundaries of the interval: .. image:: ${static_path}/images/maf_icons/interval2maf.png"
MAF_filter	"This tool allows you to build complex filters to be applied to each alignment block of a MAF file. You can define restraints on species based upon chromosome and strand. You can specify comma separated lists of chromosomes where appropriate. .. class:: infomark For example, this tool is useful to restrict a set of alignments to only those blocks which contain alignments between chromosomes that are considered homologous. ----- .. class:: warningmark If a species is not found in a particular block, all filters on that species are ignored. ----- This tool allows the user to remove any undesired species from a MAF file. If no species are specified then all species will be kept. If species are specified, columns which contain only gaps are removed. The options for this are: * 
Exclude blocks which have missing species
 - suppose you want to restrict an 8-way alignment to human, mouse, and rat. The tool will first remove all other species. Next, if this option is set to 
YES
 the tool WILL NOT return MAF blocks, which do not include human, mouse, or rat. This means that all alignment blocks returned by the tool will have exactly three sequences in this example. * 
Exclude blocks which have only one species
 - if this option is set to 
YES
 all single sequence alignment blocks WILL NOT be returned. ----- You can also provide a size range and limit your output to the MAF blocks which fall within the specified range."
MAF_Limit_To_Species1	"What It Does
 This tool allows the user to remove any undesired species from a MAF file. Columns which contain only gaps are removed. The options for this tool are: * 
Exclude blocks which have missing species
 - suppose you want to restrict an 8-way alignment to human, mouse, and rat. The tool will first remove all other species. Next, if this option is set to 
YES
 the tool WILL NOT return MAF blocks, which do not include human, mouse, or rat. This means that all alignment blocks returned by the tool will have exactly three sequences in this example. * 
Exclude blocks with have only one species
 - if this option is set to 
YES
 all single sequence alignment blocks WILL NOT be returned."
maf_limit_size1	"What it does
 This tool takes a MAF file and a size range and extracts the MAF blocks which fall within the specified range."
MAF_Thread_For_Species1	"What it does
 This tool allows the user to merge MAF blocks which are adjoining in each specified species from a MAF file. Columns which contain only gaps are removed. Species which are not desired are removed from the output. 
Example
 Specifying the desired species as hg17 and panTro1 with this MAF file:: ##maf version=1 a score=60426.000000 s hg17.chr7 127471195 331 + 158628139 gtttgccatcttttgctgctctagggaatccagcagctgtcaccatgtaaacaagcccaggctagaccaGTTACCCTCATCATCTTAGCTGATAGCCAGCCAGCCACCACAGGCAtgagtcaggccatattgctggacccacagaattatgagctaaataaatagtcttgggttaagccactaagttttaggcatagtgtgttatgtaTCTCACAAACATATAAGACTGTGTGTTTGTTGACTGGAGGAAGAGATGCTATAAAGACCACCTTTTAAAACTTCCC-------------------------------AAATACT-GCCACTGATGTCCTG-----ATGGAGGTA-------TGAA-------------------AACATCCACTAA s panTro1.chr6 129885076 331 + 161576975 gtttgccatcttttgctgctcttgggaatccagcagctgtcaccatgtaaacaagcccaggctagaccaGTTACCCTCATCATCTTAGCTGATAGCCAGCCAGCCACCACAGGCAtgagtcaggccatattgctggacccacagaattatgagctaaataaatagtcttgggttaagccactaagttttaggcatagtgtgttatgtaTCTCACAAACATATAAGACTGTGTGTTTGTTGACTGGAGGAAGAGATGCTATAAAGACCACCTTTTGAAACTTCCC-------------------------------AAATACT-GCCACTGATGTCCTG-----ATGGAGGTA-------TGAA-------------------AACATCCACTAA s mm5.chr6 28904571 357 + 149721531 CTCCACTCTCGTTTGCTGTT----------------CTGTCACCATGGAAACAAA-CGAGGGTGGTCCAGTTACTATCTTGACTGCAGCTGGCAGTCAGTT-GCCACT-----CAGGAATAAGGCTATGCCATT-GATCCACTGAACCGTGATCTGGAAACCTGGCTGTTGTTT-------CAAGCCTTGGGGCCAGTTTGCGGTGTTACTCATGA--CTCTAAGATCGTGTGCTTG----CTGCAGGAAGAGACAGCAAGGGGGTTACATTTAAAAAGCCCCCAGTTTAGCTATAGGCAGGCCAACAGGTGTAAAAATACTCACTAGTAATGGGCTGAACTCATGGAGGTAGCATTAGTGAGACACTGTAACTGTTTTTTTAAAAATCACTAA s rn3.chr4 56178191 282 + 187371129 CTTCACTCTCATTTGCTGTT----------------CTGTCACTATGGAGACAAACACAGGCTAGCCCAGTTACTATCTTGATCACAGCAGCT-GTCAGCTAGCTGCCACTCACAGGAATAAGGCCATACCATT-GATCCACTGAACCTTGATCTAGGAATTTGGC----------------------TGGGGCCAGTTTGCGGTGTCACTCATGA--CTCTAAGATTGTGTGTTTG----CTCCAGGAAGAGACGGCAAGAGGATTACCTTTAAAAGGTTC---------------------------------GGAGTCTAGCTGTAGACAGCCCA-----ATG--GGTA-------TAAC-------------------AATACTCACTAA a score=8157.000000 s hg17.chr7 127471526 58 + 158628139 AATTTGTGGTTTATTCATTTTTCATTATTTTGTTTAAGGAGGTCTATAGTGGAAGAGG s panTro1.chr6 129885407 58 + 161576975 AATTTGTGGTTTATTCGTTTTTCATTATTTTGTTTAAGGAGGTCTATAGTGGAAGAGG s mm5.chr6 28904928 54 + 149721531 AA----CGTTTCATTGATTGCTCATCATTTAAAAAAAGAAATTCCTCAGTGGAAGAGG results in:: ##maf version=1 a score=0.0 s hg17.chr7 127471195 389 + 158628139 gtttgccatcttttgctgctctagggaatccagcagctgtcaccatgtaaacaagcccaggctagaccaGTTACCCTCATCATCTTAGCTGATAGCCAGCCAGCCACCACAGGCAtgagtcaggccatattgctggacccacagaattatgagctaaataaatagtcttgggttaagccactaagttttaggcatagtgtgttatgtaTCTCACAAACATATAAGACTGTGTGTTTGTTGACTGGAGGAAGAGATGCTATAAAGACCACCTTTTAAAACTTCCCAAATACTGCCACTGATGTCCTGATGGAGGTATGAAAACATCCACTAAAATTTGTGGTTTATTCATTTTTCATTATTTTGTTTAAGGAGGTCTATAGTGGAAGAGG s panTro1.chr6 129885076 389 + 161576975 gtttgccatcttttgctgctcttgggaatccagcagctgtcaccatgtaaacaagcccaggctagaccaGTTACCCTCATCATCTTAGCTGATAGCCAGCCAGCCACCACAGGCAtgagtcaggccatattgctggacccacagaattatgagctaaataaatagtcttgggttaagccactaagttttaggcatagtgtgttatgtaTCTCACAAACATATAAGACTGTGTGTTTGTTGACTGGAGGAAGAGATGCTATAAAGACCACCTTTTGAAACTTCCCAAATACTGCCACTGATGTCCTGATGGAGGTATGAAAACATCCACTAAAATTTGTGGTTTATTCGTTTTTCATTATTTTGTTTAAGGAGGTCTATAGTGGAAGAGG"
maf_stats1	"What it does
 This tool takes a MAF file and an interval file and relates coverage information by interval for each species. If a column does not exist in the reference genome, it is not included in the output. Consider the interval: ""chrX 1000 1100 myInterval"" Let's suppose we want to do stats on three way alignments for H, M, and R. The result look like this: chrX 1000 1100 myInterval H XXX YYY chrX 1000 1100 myInterval M XXX YYY chrX 1000 1100 myInterval R XXX YYY where XXX and YYY are: XXX = number of nucleotides YYY = number of gaps ---- Alternatively, you can request only summary information for a set of intervals: ======== =========== ======== #species nucleotides coverage ======== =========== ======== hg18 30639 0.2372 rheMac2 7524 0.0582 panTro2 30390 0.2353 ======== =========== ======== where 
coverage
 is the number of nucleotides divided by the total length of the provided intervals."
MAF_Reverse_Complement_1	"What it does
 This tool takes a MAF file and creates a new MAF file, where each block has been reversed complemented. 
Example
 This MAF Block:: a score=8157.000000 s hg17.chr7 127471526 58 + 158628139 AATTTGTGGTTTATTCATTTTTCATTATTTTGTTTAAGGAGGTCTATAGTGGAAGAGG s panTro1.chr6 129885407 58 + 161576975 AATTTGTGGTTTATTCGTTTTTCATTATTTTGTTTAAGGAGGTCTATAGTGGAAGAGG s mm5.chr6 28904928 54 + 149721531 AA----CGTTTCATTGATTGCTCATCATTTAAAAAAAGAAATTCCTCAGTGGAAGAGG becomes:: a score=8157.000000 s hg17.chr7 31156555 58 - 158628139 CCTCTTCCACTATAGACCTCCTTAAACAAAATAATGAAAAATGAATAAACCACAAATT s panTro1.chr6 31691510 58 - 161576975 CCTCTTCCACTATAGACCTCCTTAAACAAAATAATGAAAAACGAATAAACCACAAATT s mm5.chr6 120816549 54 - 149721531 CCTCTTCCACTGAGGAATTTCTTTTTTTAAATGATGAGCAATCAATGAAACG----TT"
GeneBed_Maf_Fasta2	"What it does
 The coding sequence of genes are usually composed of several coding exons. Each of these coding exons is an individual genomic region, which when concatenated with each other constitutes the coding sequence. A single genomic region can be covered by multiple alignment blocks. In many cases it is desirable to stitch these alignment blocks together. This tool accepts a list of gene-based intervals, in the Gene BED format. For every interval it performs the following: * finds all MAF blocks that overlap the coding regions; * sorts MAF blocks by alignment score; * stitches blocks together and resolves overlaps based on alignment score; * outputs alignments in FASTA format."
Interval_Maf_Merged_Fasta2	"What it does
 A single genomic region can be covered by multiple alignment blocks. In many cases it is desirable to stitch these alignment blocks together. This tool accepts a list of genomic intervals. For every interval it performs the following: * finds all MAF blocks that overlap the interval; * sorts MAF blocks by alignment score; * stitches blocks together and resolves overlaps based on alignment score; * outputs alignments in FASTA format. ------ 
Example
 Here three MAF blocks overlapping a single interval are stitched together. Space between blocks 2 and 3 is filled with gaps: .. image:: ${static_path}/images/maf_icons/stitchMaf.png"
toolshed.g2.bx.psu.edu/repos/iuc/bigwig_outlier_bed/bigwig_outlier_bed/0.2.4+galaxy0	"Purpose
 
Combine bigwig outlier regions into bed files
 Bigwigs allow quantative tracks to be viewed in an interactive genome browser like JBrowse2. Peaks are easy to see. Unusually low regions can be harder to spot, even if they are relatively large, unless the view is zoomed right in. Automated methods for combining evidence from multiple bigwigs can be useful for constructing browseable 
issues
 or other kinds of summary bed format tracks. For example, combining coverage outlier regions, with the frequency of specific dicnucleotide short tandem repeats, for evaluating technical sequencing technology effects in the evaluation of a genome assembly described at https://github.com/arangrhie/T2T-Polish 
What does it produce?
 Bed format results are output, containing each continuous segment of at least 
minwin
 base pairs above a cut point, or below another cut point. These can be viewed as features on the reference genome using a genome browser tool like JBrowse2. Three kinds of bed files can be created depending on the values included. Both high and low regions in one bed output is the default. This can be displayed in JBrowse2 with colour indicating the high or low status, one less track and a little easier to understand. High and low features can be output as separate bed files. 
How is it controlled?
 The cut points are calculated using a user supplied quantile, from each chromosome's bigwig value distribution. The defaults are 0.99 and 0.01 and the default 
minwin
 is 10. The probability of 10 values at or below the 1st percentile purely by chance is about 0.01
10, so false positives should be rare, even in a 3GB genome. This data driven and non-parametric method is preferred for the asymmetrical distributions found in typical bigwigs, such as depth of coverage for genome sequencing reads. Coverage values are truncated at zero, and regions with very high values often form a long sparse right tail. 
How do I choose the input data?
 One or more bigwigs and can be selected as inputs. Multiple bigwigs will be combined in bed files, so must share the reference genome to display using JBrowse2. .. class:: warningmark 
Lower quantile may not behave as expected in bigwigs with large fractions of zero values
 The lower cut point may be problematic for integer values like coverage if many values are zero. For example, if 5% of bases have zero coverage, the 1st percentile is also zero, but that cut point will include the entire 5% 
at or below 0"
Extract_features1	"What it does
 This tool extracts selected features from GFF data. ----- 
Example
 Selecting 
promoter
 from the following GFF data:: chr22 GeneA enhancer 10000000 10001000 500 + . TGA chr22 GeneA promoter 10010000 10010100 900 + . TGA chr22 GeneB promoter 10020000 10025000 400 - . TGB chr22 GeneB CCDS2220 10030000 10065000 800 - . TGB will produce the following output:: chr22 GeneA promoter 10010000 10010100 900 + . TGA chr22 GeneB promoter 10020000 10025000 400 - . TGB ---- .. class:: infomark 
About formats
 
GFF format
 General Feature Format is a format for describing genes and other features associated with DNA, RNA and Protein sequences. GFF lines have nine tab-separated fields:: 1. seqname - Must be a chromosome or scaffold. 2. source - The program that generated this feature. 3. feature - The name of this type of feature. Some examples of standard feature types are ""CDS"", ""start_codon"", ""stop_codon"", and ""exon"". 4. start - The starting position of the feature in the sequence. The first base is numbered 1. 5. end - The ending position of the feature (inclusive). 6. score - A score between 0 and 1000. If there is no score value, enter ""."". 7. strand - Valid entries include '+', '-', or '.' (for don't know/care). 8. frame - If the feature is a coding exon, frame should be a number between 0-2 that represents the reading frame of the first base. If the feature is not a coding exon, the value should be '.'. 9. group - All lines with the same group are linked together into a single item."
Filter1	".. class:: warningmark Double equal signs, ==, must be used as 
""equal to""
 (e.g., 
c1 == 'chr22'
) .. class:: infomark 
TIP:
 Attempting to apply a filtering condition may throw exceptions if the data type (e.g., string, integer) in every line of the columns being filtered is not appropriate for the condition (e.g., attempting certain numerical calculations on strings). If an exception is thrown when applying the condition to a line, that line is skipped as invalid for the filter condition. The number of invalid skipped lines is documented in the resulting history item as a ""Condition/data issue"". .. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
Syntax
 The filter tool allows you to restrict the dataset using simple conditional statements. - Columns are referenced with 
c
 and a 
number
. For example, 
c1
 refers to the first column of a tab-delimited file - Make sure that multi-character operators contain no white space ( e.g., 
<=
 is valid while 
< =
 is not valid ) - When using 'equal-to' operator 
double equal sign '==' must be used
 ( e.g., 
c1=='chr1'
 ) - Non-numerical values must be included in single or double quotes ( e.g., 
c6=='+'
 ) - Filtering condition can include logical operators, but 
make sure operators are all lower case
 ( e.g., 
(c1!='chrX' and c1!='chrY') or not c6=='+'
 ) ----- 
Example
 - 
c1=='chr1'
 selects lines in which the first column is chr1 - 
c3-c2<100*c4
 selects lines where subtracting column 3 from column 2 is less than the value of column 4 times 100 - 
len(c2.split(',')) < 4
 will select lines where the second column has less than four comma separated elements - 
c2>=1
 selects lines in which the value of column 2 is greater than or equal to 1 - Numbers should not contain commas - 
c2<=44,554,350
 will not work, but 
c2<=44554350
 will - Some words in the data can be used, but must be single or double quoted ( e.g., 
c3=='exon'
 )"
gff_filter_by_attribute	".. class:: warningmark Double equal signs, ==, must be used as 
""equal to""
 (e.g., 
c1 == 'chr22'
) .. class:: infomark 
TIP:
 Attempting to apply a filtering condition may throw exceptions if the data type (e.g., string, integer) in every line of the attribute being filtered is not appropriate for the condition (e.g., attempting certain numerical calculations on strings). If an exception is thrown when applying the condition to a line, that line is skipped as invalid for the filter condition. The number of invalid skipped lines is documented in the resulting history item as a ""Condition/data issue"". .. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
Syntax
 The filter tool allows you to restrict the dataset using simple conditional statements. - Make sure that multi-character operators contain no white space ( e.g., 
<=
 is valid while 
< =
 is not valid ) - When using 'equal-to' operator 
double equal sign '==' must be used
 ( e.g., 
attribute_name=='chr1'
 ) - Non-numerical values must be included in single or double quotes ( e.g., 
attribute_name=='XX22'
 ) - You can combine multiple conditional statements using 
and
 or 
or
 ( e.g., 
attribute_name=='XX22' or attribute_name=='XX21'
 )"
gff_filter_by_feature_count	".. class:: infomark Valid comparison operators are: > < >=, <=, !=, and == ----- 
Syntax
 The filter tool allows you to restrict the dataset based on transcripts' feature counts."
gtf_filter_by_attribute_values_list	This tool filters a GTF file using a list of attribute values. The attribute values are taken from the first column in the file; additional columns in the file are ignored. An example use of this tool is to filter a GTF file using a list of transcript_ids or gene_ids obtained from Cuffdiff.
Grep1	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
Syntax
 The select tool searches the data for lines containing or not containing a match to the given pattern. Regular Expression is introduced in this tool. A Regular Expression is a pattern describing a certain amount of text. - 
( ) { } [ ] . * ? + \ ^ $
 are all special characters. 
\
 can be used to ""escape"" a special character, allowing that special character to be searched for. - 
\A
 matches the beginning of a string(but not an internal line). - 
\d
 matches a digit, same as [0-9]. - 
\D
 matches a non-digit. - 
\s
 matches a whitespace character. - 
\S
 matches anything BUT a whitespace. - 
\t
 matches a tab. - 
\w
 matches an alphanumeric character. - 
\W
 matches anything but an alphanumeric character. - 
(
 .. 
)
 groups a particular pattern. - 
\Z
 matches the end of a string(but not a internal line). - 
{
 n or n, or n,m 
}
 specifies an expected number of repetitions of the preceding pattern. - 
{n}
 The preceding item is matched exactly n times. - 
{n,}
 The preceding item is matched n or more times. - 
{n,m}
 The preceding item is matched at least n times but not more than m times. - 
[
 ... 
]
 creates a character class. Within the brackets, single characters can be placed. A dash (-) may be used to indicate a range such as 
a-z
. - 
.
 Matches any single character except a newline. - 
*
 The preceding item will be matched zero or more times. - 
?
 The preceding item is optional and matched at most once. - 
+
 The preceding item will be matched one or more times. - 
^
 has two meaning: - matches the beginning of a line or string. - indicates negation in a character class. For example, [^...] matches every character except the ones inside brackets. - 
$
 matches the end of a line or string. - 
\|
 Separates alternate possibilities. ----- 
Example
 - 
^chr([0-9A-Za-z])+
 would match lines that begin with chromosomes, such as lines in a BED format file. - 
(ACGT){1,5}
 would match at least 1 ""ACGT"" and at most 5 ""ACGT"" consecutively. - 
([^,][0-9]{1,3})(,[0-9]{3})*
 would match a large integer that is properly separated with commas such as 23,078,651. - 
(abc)|(def)
 would match either ""abc"" or ""def"". - 
^\W+#** would match any line that is a comment."
toolshed.g2.bx.psu.edu/repos/peterjc/sample_seqs/sample_seqs/0.2.6	"What it does
 Takes an input file of sequences (typically FASTA or FASTQ, but also Standard Flowgram Format (SFF) is supported), and returns a new sequence file sub-sampling uniformly from this (in the same format, preserving the input order and selecting sequencing evenly though the input file). Several sampling modes are supported, all designed to do non-random uniform sampling (i.e. evenly through the input file). This allows reproducibility, and also works on paired sequence files (run the tool twice, once on each file using the same settings). By sampling uniformly (evenly) through the file, this avoids any bias should reads in any part of the file be of lesser quality (e.g. for high throughput sequencing the reads at the start and end of the file can be of lower quality). The simplest mode is to take every 
N
-th sequence, for example taking every 2nd sequence would sample half the file - while taking every 5th sequence would take 20% of the file. The target count method picks 
N
 sequences from the input file, which again will be distributed uniformly (evenly) though the file. This works by first counting the number of records, then calculating the desired percentage of sequences to take. Note if your input file has exactly 
N
 sequences this selects them all (effectively copying the input file). If your input file has less than 
N
 sequences, this is treated as an error. If you tick the interleaved option, the file is processed as pairs of records to ensure your read pairs are not separated by sampling. For example using 20% would take every 5th pair of records, or you could request 1000 read pairs. If instead of interleaved paired reads you have two matched files (one for each pair), run the tool twice with the same sampling options to make to matched smaller files. .. class:: warningmark Note interleaved/pair mode does 
not
 actually check your read names match a known pair naming scheme! 
Example Usage
 Suppose you have some Illumina paired end data as files 
R1.fastq
 and 
R2.fastq
 which give an estimated x200 coverage, and you wish to do a 
de novo
 assembly with a tool like MIRA which recommends lower coverage. Running the tool twice (on 
R1.fastq
 and 
R2.fastq
) taking every 3rd read would reduce the estimated coverage to about x66, and would preserve the pairing as well (as two smaller FASTQ files). Similarly, if you had some Illumina paired end data interleaved into one file with an estimated x200 coverage, you would run this tool in interleaved mode, taking every 3rd read pair. This would again reduce the estimated coverage to about x66, while preserving the read pairing. Suppose you have a transcriptome assembly, and wish to look at the species distribution of the top BLAST hits for an initial quality check. Rather than using all your sequences, you could pick 1000 only for this. 
Citation
 This tool uses Biopython, so if you use this Galaxy tool in work leading to a scientific publication please cite the following paper: Cock et al (2009). Biopython: freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics 25(11) 1422-3. https://doi.org/10.1093/bioinformatics/btp163 pmid:19304878. This tool is available to install into other Galaxy Instances via the Galaxy Tool Shed at http://toolshed.g2.bx.psu.edu/view/peterjc/sample_seqs"
toolshed.g2.bx.psu.edu/repos/ecology/xarray_coords_info/xarray_coords_info/2022.3.0+galaxy0	"What it does
 The tool will generate a collection containing one file per coordinate. Each file contains the values of the corresponding coordinate. The output of this tool is usually used as input to other tools. 
Input
 A netcdf file (xxx.nc). 
Outputs
 An output file is generated for each coordinate and each file contains the value of the corresponding coordinate. -------------------------------- Run this tool before considering using Netcdf Xarray operation."
toolshed.g2.bx.psu.edu/repos/ecology/xarray_metadata_info/xarray_metadata_info/2022.3.0+galaxy0	"What it does
 First the tool will give general information about the input in a 'info file' output. (command $ncdump -h inputfile) Then, a general tabular 'variables' summarize dimensions details inside each available variable. The summary tabular file has the general structure : Variable1 Var1_Number_of_Dim Dim1 Dim1_size ... DimN DimN_size VariableX VarX_Number_of_Dim DimX1 DimX1_size ... DimXN DimXN_size ... 
Input
 A netcdf file (xxx.nc). 
Outputs
 An Information file. A summary tabular file. -------------------------------- The Netcdf Info tool use the netcdf functions : https://www.unidata.ucar.edu/software/netcdf/docs/index.html Run this tool before considering using Netcdf Read."
toolshed.g2.bx.psu.edu/repos/ecology/xarray_select/xarray_select/2022.3.0+galaxy0	"What it does
 This tool extracts variable values with custom conditions on dimensions. It can use manualy given coordinates or automaticaly take them from a tabular file to filter informations. If no values are availables at a coordinate X, the tool will search the closest coordinate with a non NA value. Filter can be set on every dimension. Available filtering operations are : =, >, <, >=, <=, [interval], ]interval[. 
Input
 A netcdf file (.nc). Variable tabular file from 'Netcdf Metadate Info'. Tabular file with coordinates (only coordinates, no header!) and the following structure : 'lat' 'lon'. 
Outputs
 A single output with values for the wanted variable if there is only one coordinate. A data collection where one file is created for every coordinate, if multiple coordinates from tabular file. ------------------------------------------------- The xarray select tool can be used after the xarray Info."
toolshed.g2.bx.psu.edu/repos/ecology/xarray_mapplot/xarray_mapplot/2022.3.0+galaxy0	"What it does
 This tool plots a variable on a geographical map. It must be a 2D variable (latitude, longitude) and eventually with and additional time dimension (specific time to plot should then be selected). The appearance of plots can be customized with options as well at the projection. The projection needs to be given as a PROJ dictionary such as: - {""proj"":""EquidistantConic"", ""central_longitude"": 20.0, ""central_latitude"": 70.0 } - {""proj"":""AlbersEqualArea"", ""central_longitude"": 20.0, ""central_latitude"": 70.0 } - {""proj"":""EuroPP""} The output is a collection of plots (png format); one per selected times. ------------------------------------------------- The xarray select tool can be used after the xarray Info and xarray coord."
toolshed.g2.bx.psu.edu/repos/ecology/xarray_netcdf2netcdf/xarray_netcdf2netcdf/2022.3.0+galaxy0	"What it does
 Select a variable ans can restrict over any of its dimension and apply a scaling (1 by default). the result is stored in a new netCDF file. One can also select the range of time (for timeseries) to apply these operations over the range only when a range of time is selected and when scaling, one can choose to save the entire timeseries or the selected range only. when scaling, one can add additional filters on dimensions (typically used to filter over latitudes and longitudes) ------------------------------------------------- The xarray select tool can be used after the xarray Info and xarray Coord."
toolshed.g2.bx.psu.edu/repos/iuc/gemini_annotate/gemini_annotate/0.20.1+galaxy2	"What it does
 Given an existing GEMINI database and an annotation source in BED or VCF format, the annotate tool will, for each variant in the variants table of the database, screen for overlapping regions defined in the annotation source and update one or more new columns of the variant record in the database based on the result and the annotation found."
toolshed.g2.bx.psu.edu/repos/iuc/gemini_inheritance/gemini_inheritance/0.20.1	"What it does
 Assuming you have defined the familial relationships between samples when loading your VCF into GEMINI, you can use this tool to identify candidate genes and variants that explain the inheritance pattern of a phenotype of interest. 
Inheritance pattern detection rules
 
Autosomal recessive
 Criteria: - all affected must be hom_alt - [affected] no unaffected can be hom_alt (can be unknown) - [default] if parents exist they must be unaffected and het for all affected kids - [default] if there are no affecteds that have a parent, a warning is issued. If 
--lenient
 is specified, the 2 criteria prefixed with “[default]” are not applied. If 
--allow-unaffected
 is specified, the criterion prefixed with “[affected]” is not enforced. ---- 
Autosomal dominant
 Criteria: - All affecteds must be het - [affected] No unaffected can be het or homalt (can be unknown) - de_novo mutations are not auto_dom (at least not in the first generation) - At least 1 affected must have 1 affected parent (or have no parents). - If no affected has a parent, a warning is issued. - [default] All affecteds must have parents with known phenotype. - [default] All affected kids must have at least 1 affected parent If 
--lenient
 is specified, the criteria prefixed with “[default]” are not enforced. If 
--allow-unaffected
 is specified, the criterion prefixed with “[affected]” is not enforced. Note that, for autosomal dominant, 
--lenient
 allows singleton affecteds to be used to meet the 
--min-kindreds
 requirement if they are HET. If there is incomplete penetrance in the kindred (unaffected obligate carriers), these individuals currently must be coded as having unknown phenotype or as being affected. ---- 
X-linked recessive
 Criteria: - Affected females must be HOM_ALT - Unaffected females are HET or HOM_REF - Affected males are not HOM_REF - Unaffected males are HOM_REF Note: Pseudo-autosomal regions are not accounted for by the tool. ---- 
X-linked dominant
 Criteria: - Affected males are HET or HOM_ALT - Affected females must be HET - Unaffecteds must be HOM_REF - girls of affected dad must be affected - boys of affected dad must be unaffected - mothers of affected males must be het (and affected) - at least 1 parent of affected females must be het (and affected). Note: Pseudo-autosomal regions are not accounted for by the tool. ---- 
De-novo mutations
 Criteria: - all affected must be het - [affected] all unaffected must be homref or homalt - at least 1 affected kid must have unaffected parents - [default] if an affected has affected parents, it’s not de_novo - [default] all affected kids must have unaffected (or no) parents - [default] warning if none of the affected samples have parents. The last 3 items, prefixed with [default] can be turned off with 
--lenient
. If 
--allow-unaffected
 is specified, then the criterion prefixed [affected] is not enforced. ---- 
X-linked de-novo mutations
 Criteria: - affected female child must be het - affected male child must be hom_alt (or het) - parents should be unaffected and hom_ref Note: Pseudo-autosomal regions are not accounted for by the tool. ---- 
Compound heterozygosity
 Unlike canonical recessive sites where the same recessive allele is inherited from both parents at the 
same
 site in the gene, compound heterozygosity occurs when the individual’s phenotype is caused by two heterozygous recessive alleles at 
different
 sites in a particular gene. To detect compound heterozygosity, the tool looks for two heterozygous variants impacting the same gene at different loci. The complicating factor is that this is a case of 
recessive
 inheritance and as such, we must also require that the consequential alleles at each heterozygous site were inherited on different chromosomes (one from each parent). Hence, where possible, the tool will phase by transmission. Criteria (default): - All affected individuals must be heterozygous at both sites. - No unaffected can be homozygous alterate at either site. - Neither parent of an affected sample can be homozygous reference at both sites. - If any unphased-unaffected is het at both sites, the site will be given lower priority. - No phased-unaffected can be heterozygous at both sites. a. 
--allow-unaffected
 keeps sites where a phased unaffected shares the het-pair b. unphased, unaffected that share the het pair are counted and reported for each candidate pair. - Candidates where an affected from the same family does NOT share the same het pair are removed. - Sites are automatically phased by transmission when parents are present in order to remove false positive candidates. If data from one or both parents are unavailable and the child’s data was not phased prior to loading into GEMINI, all comp_het variant pairs will automatically be given at most priority == 2. If there’s only a single parent and both the parent and the affected are HET at both sites, the candidate will have priority 3. Criteria (
--pattern-only
): - Kid must be HET at both sites. - Kid must have alts on different chromosomes. - Neither parent can be HOM_ALT at either site. - If either parent is phased at both sites and matches the kid, it’s excluded. - When the above criteria are met, and both parents and kid are phased or parents are HET at different sites, the priority is 1. - If either parent is HET at both sites, priority is reduced. - If both parents are not phased, the priority is 2. - For every parent that’s a het at both sites, the priority is incremented by 1. - The priority in a family is the minimum found among all kids. ---- 
Violation of Mendelian laws
 The tool can be used to detect the following kinds of non-Mendelian patterns: - loss of heterozygosity (LOH) events - de-novo mutations - implausible de-novo mutations - potential cases of uniparental disomy Criteria: - LOH: child and one parent are opposite homozygotes; other parent is HET - plausible de novo: kid is het. parents are same homozygotes - implausible de novo: kid is homozygote. parents are same homozygotes and opposite to kid. - uniparental disomy: parents are opposite homozygotes; kid is homozygote"
toolshed.g2.bx.psu.edu/repos/iuc/gemini_load/gemini_load/0.20.1+galaxy2	".. class:: Warning mark 
CADD scores licensing
 CADD scores are freely available for non-commercial applications only. Make sure you 
contact the developers &lt;https://cadd.gs.washington.edu/contact&gt;
__ before using them in any commercial application. ----- 
What it does
 Before we can use GEMINI to explore genetic variation, we must first load the variant information stored in VCF format into the GEMINI database framework. To fully leverage the power of GEMINI, you should first 
annotate your VCF dataset
 with the functional consequences of the variants using either 
VEP
 or 
snpEff
. .. class:: Warning mark To avoid problems during annotation, but also during later variant queries with GEMINI tools, it is good practice to preprocess your VCF dataset even before annoation to split records with multiple alternate alleles, and to left-align and trim indels. The authors of GEMINI recommend the tool 
vt
 for this purpose, an equivalently good option is 
bcftools norm
, and Galaxy wrappers exist for both tools. In addition, you are encouraged to provide 
family and sample phenotype information in PED format
, if you are planning to use GEMINI for any kind of variant identification based on inheritance patterns. A PED file is simply a tabular text file (columns can be separated by either spaces or TABs, but not a mixture of the two within the same file) with the header:: #family_id name paternal_id maternal_id sex phenotype and optional additional columns. The actual column names in the header are not fixed, but there have to be at least six columns that are interpreted as detailed next. Subsequent lines describe one sample from the VCF input dataset each, where - 
family_id
 is an alphanumeric identifier of a family If the family, to which the sample belongs, is unknown, a placeholder of 
0
, 
-9
 or 
None
 can be used to indicate this fact. - 
name
 is the identifier of the sample described by the line - 
paternal_id
 is the identifier of the sample's father If the sample's father is not available in the VCF, a placeholder of 
0
, 
-9
 or 
None
 can be used to indicate this fact. - 
maternal_id
 is the identifier of the sample's mother If the sample's mother is not available in the VCF, a placeholder of 
0
, 
-9
 or 
None
 can be used to indicate this fact. - 
sex
 is a numeric code for the sample's sex (1=male, 2=female, any other number=unknown sex) - 
phenotype
 is a numeric code for the sample's phenotypic affection status (1=unaffected, 2=affected) If the sample's phenotype is unknown, a placeholder of 
0
 or 
-9
 can be used to indicate this fact. - Optional additional columns can have any column name you like, and accept any per-sample value. The data from such extra columns will be added to the samples table of the GEMINI database so you can use them in queries. Extra columns can be used, 
e.g.
, to describe additional phenotypes. - If no extra columns are present in a PED file, then the header line is optional. Here are two examples of valid PED file contents:: #family_id name paternal_id maternal_id sex phenotype hair_color 1 M10475 -9 -9 1 1 brown 1 M10478 M10475 M10500 2 2 brown 1 M10500 -9 -9 2 2 black 1 M128215 M10475 M10500 1 1 blue This describes a family with two kids, in which mother and daughter, but not father and son are phenotypically affected. The file also stores the hair color of all family members. :: #family_id name paternal_id maternal_id sex phenotype 0 M10475 0 0 -1 1 0 M10478 0 0 -1 2 0 M10500 0 0 -1 2 0 M128215 0 0 -1 1 This describes the same samples as above, but without recording family structure, sex or additional traits. Only the sample phenotypes are provided. In this case (no extra columns), the header line could be omitted."
toolshed.g2.bx.psu.edu/repos/iuc/gemini_query/gemini_query/0.20.1+galaxy2	"What it does
 The real power in the GEMINI framework lies in the fact that all of your genetic variants have been stored in a convenient database in the context of a wealth of genome annotations that facilitate variant interpretation. The expressive power of SQL allows one to pose intricate questions of one’s variation data. This tool offers you a flexible, yet relatively easy way to query your variants! ----- 
Building your variant query with the Basic variant query constructor
 This mode tries to break down the complexity of formulating GEMINI queries into more easily digestable parts. In this mode, the tool also prevents you from combining options that are incompatible or not meaningful. 
Genotype filters
 These are discussed 
here &lt;https://gemini.readthedocs.io/en/latest/content/querying.html#gt-filter-filtering-on-genotypes&gt;
 in the GEMINI documentation. The tool supports regular genotype filters like:: gt.sample1 == HET and gt_depths.sample1 >= 15 , which would keep only variants for which sample 1 is a heterozygous carrier and if the genomic position in sample1 is covered by at least 15 sequencing reads, as well as GEMINI wildcard filters of the general form 
(COLUMN).(SAMPLE_FILTER).(RULE).(RULE_ENFORCEMENT)
 like:: (gt_types).(phenotype==2).(!=HOM_REF).(all) , which keeps only variants for which all phenotypic samples are homozygous. 
Sample filters
 Sample filters have the same format as the second component of the genotype wildcard filters above, so:: phenotype == 2 would filter for phenotypically affected samples. In this case, however, the filter determines, from which samples variants should be reported, i.e., here, only variants found in phenotypically affected samples become analyzed. You can use the 
--in
 filter to adjust the exact meaning of the sample filter. 
Region filters
 They let you restrict your analysis to parts of the genome, which can be useful if you have prior knowledge of the approximate location of a variant of interest. If you specify more then one region filter, they get combined with a logical 
OR
, meaning variants and genes falling in 
any
 of the regions are reported. 
Additional constraints on variants
 These get translated directly into the WHERE clause of an SQL query and, thus, have to be expressed in valid SQL syntax. As an example you could use:: is_exonic = 1 and impact_severity != 'LOW' to indicate that you are only interested in exonic variants that are not of 
LOW
 impact severity, 
i.e.
, not silent mutations. Note that in SQL syntax tests for equality use a single 
=
, while genotype filters (discussed above) are following Python syntax and use 
==
 for the same purpose. Also note that non-numerical values need to be enclosed in single-quotes, 
e.g.
 
'LOW'
, but numerical values must 
NOT
 be. ----- 
Building your query with the Advanced query constructor
 For the sake of simplicity, the basic mode of the tool limits your queries to the variants table of the underlying database. While this still allows many useful queries to be formulated, it prevents you from joining information from other tables (in particular, the gene_detailed table) or to query a different table directly. In advanced mode, you take responsibility for formulating the complete SQL query in correct syntax, which allows you to do anything you could do with the command line tool. Beyond querying other tables, this includes changing output column names, deriving simple statistics on columns using the SQL Min, Max, Count, Avg and Sum functions, and more. The price you pay for this extra flexibility is that you will have to make sure that any other tool options you set are compatible with the result of your particular query. For example, most output formats except the tabular default output of GEMINI are incompatible with non-standard queries. Choosing non-compatible options can result in them getting ignored silently, but also in tool errors, or in problems with downstream tools. The chapter 
Querying the GEMINI database &lt;http://gemini.readthedocs.org/en/latest/content/querying.html&gt;
 of the GEMINI documentation can get you started with formulating your own queries. Note that genotype filters and sample filters cannot be expressed as genuine SQL queries, so even the Advanced query constructor is offering them. Region filters and sort order of rows and columns on the other hand can be controlled through SQL queries, like in this example:: SELECT gene, chrom, start, end, ref, alt FROM variants WHERE chrom = 'chr1' AND start >= 10000000 and stop <= 20000000 and is_lof = 1 ORDER BY chrom, start , which would report all loss-of-function variants between 10,000,000 and 20,000,000 on chr1 and report the selected columns sorted on chromosome, then position."
toolshed.g2.bx.psu.edu/repos/iuc/raxml/raxml/8.2.12+galaxy1	"RAxML_ (Randomized Axelerated Maximum Likelihood) is a program for Maximum Likelihood-based inference of large phylogenetic trees. The program is explicitly being developed to efficiently infer trees for extremely large datasets, either in terms of the number of taxa and/or the sequence length. .. _RAxML: http://www.exelixis-lab.org/web/software/raxml/ 
Tool development
: Oleksandr Moskalenko with adaptations from Tiago Antao."
toolshed.g2.bx.psu.edu/repos/iuc/mageck_count/mageck_count/0.5.9.2.4	".. class:: infomark 
What it does
 Model-based Analysis of Genome-wide CRISPR-Cas9 Knockout (MAGeCK_) is a computational tool to identify important genes from the recent genome-scale CRISPR-Cas9 knockout screens (or GeCKO) technology. MAGeCK can be used for prioritizing single-guide RNAs, genes and pathways in genome-scale CRISPR/Cas9 knockout screens. MAGeCK identifies both positively and negatively selected genes simultaneously and reports robust results across different experimental conditions. MAGeCK is developed and maintained by Wei Li and Han Xu from 
Prof. Xiaole Shirley Liu's lab
 at the Department of Biostatistics and Computational Biology, Dana-Farber Cancer Institute and Harvard School of Public Health. MAGeCK has been used to identify functional lncRNAs from screens with close to 
100% validation rate
. ----- 
Inputs
 
Read files
 
MAGeCK count
 accepts one or more FASTQ.GZ, FASTQ or BAM files as input. Since version 0.5.5, MAGeCK count module supports collecting read counts from BAM files. This will allow you to use a third-party aligner to map reads to the library with mismatches, providing more usable reads for the analysis. However, it is still recommended to directly use the fastq file in the count module (which does not allow any mismatches), because: * Some mismatches in the sgRNAs may have unwanted behaviors (have no on-target cleavages or have other off-target cleavages); * In most cases the read counts are enough if we allow no mismatches; * The mapping procedure is more complicated; for example, you need to know the exact length of 3' adapter sequence. It is also possible to input a Count Table to normalize counts and get statistics. 
sgRNA library file
 When starting from FASTQ, FASTQ.GZ or BAM files, MAGeCK needs to know the sgRNA sequences and targeting genes. Such information is provided in the sgRNA library file and can be specified in the tool form above. The sgRNA library file can be provided in .tsv or .csv format. There are three columns in the library file: the sgRNA ID, the sequence, and the gene it is targeting. Example: ============ ==================== ======== 
sgRNA ID
 
Sequence
 
Gene
 ------------ -------------------- -------- s_10007 TGTTCACAGTATAGTTTGCC CCNA1 s_10008 TTCTCCCTAATTGCTTGCTG CCNA1 s_10027 ACATGTTGCTTCCCCTTGCA CCNC ============ ==================== ======== 
Control sgRNA file
 The optional Control sgRNAs file is used to generate null distribution when calculating the p values. If this option is not specified, MAGeCK generates the null distribution of RRA scores by assuming all of the genes in the library are non-essential, see 
More Information
 below. This approach is sometimes over-conservative, and you can improve this if you know some genes are not essential. By providing the corresponding sgRNA IDs in this option, MAGeCK will have a better estimation of p values. To use this option, you need to prepare a text file specifying the IDs of control sgRNAs, one line for one sgRNA ID. ----- 
Outputs
 This tool outputs * an sgRNA Counts table Optionally, under 
Output Options
 you can choose to output * a Count Summary file * a PDF report * a Normalized Counts table * an Unmapped reads file * the .R and .Rnw files used to generate the plots and PDF * a Log file of the analysis 
sgRNA Count file
 An example of the sgRNA count output file is shown below. This file can be used with 
MAGeCK test
. Example: ============== ======== =========== =========== 
sgRNA
 
Gene
 
Sample1
 
Sample2
 -------------- -------- ----------- ----------- A1CF_m52595977 A1CF 213 199 A1CF_m52596017 A1CF 294 164 A1CF_m52596056 A1CF 421 378 A1CF_m52603842 A1CF 274 281 A1CF_m52603847 A1CF 0 0 ============== ======== =========== =========== 
Count Summary
 MAGeCK can produce a 
Count Summary
 file containing statistics of the input files (the statistics of fastq files are also in the PDF report). An example count summary file is shown below. Example: ========== ===== ===== ====== ========== =========== ========== ========= ======== ============ ======================= ========================== ============ File Label Reads Mapped Percentage TotalsgRNAs Zerocounts GiniIndex NegSelQC NegSelQCPval NegSelQCPvalPermutation NegSelQCPvalPermutationFDR NegSelQCGene ========== ===== ===== ====== ========== =========== ========== ========= ======== ============ ======================= ========================== ============ InputFile1 L1 2500 1453 0.5812 2550 1276 0.5267 0 1 1 1 0.0 ========== ===== ===== ====== ========== =========== ========== ========= ======== ============ ======================= ========================== ============ ----- 
More Information
 
Overview of the MAGeCK algorithm
 Briefly, read counts from different samples are first median-normalized to adjust for the effect of library sizes and read count distributions. Then the variance of read counts is estimated by sharing information across features, and a negative binomial (NB) model is used to test whether sgRNA abundance differs significantly between treatments and controls. This approach is similar to those used for differential RNA-Seq analysis. We rank sgRNAs based on P-values calculated from the NB model, and use a modified robust ranking aggregation (RRA) algorithm named α-RRA to identify positively or negatively selected genes. More specifically, α-RRA assumes that if a gene has no effect on selection, then sgRNAs targeting this gene should be uniformly distributed across the ranked list of all the sgRNAs. α-RRA ranks genes by comparing the skew in rankings to the uniform null model, and prioritizes genes whose sgRNA rankings are consistently higher than expected. α-RRA calculates the statistical significance of the skew by permutation, and a detailed description of the algorithm is presented in the Materials and methods section of the MAGeCK paper. Finally, MAGeCK reports positively and negatively selected pathways by applying α-RRA to the rankings of genes in a pathway. 
MAGeCK FAQs
 
The 5' trim length option can only trim a fixed length of nucleotides before sgRNA, but what if the trimming length is different in different reads?
 MAGeCK enables automatically determining trimming length, even the length may be different within the same fastq files. Alternatively, you can use 
cutadapt
 to trim the adaptor sequences of variable length before running MAGeCK. 
How do I get the simple statistics of my input files?
 MAGeCK produces a 
Count Summary
 file containing the statistics of the input files, the statistics are also in the PDF report. The statistics can also be found in the log file for 
MAGeCK
 count. 
How do I know the quality of my samples?
 For simple QC terms, you can just take a look at the sample statistics. Generally in a good negative selection sample: #. the mapped reads should be over 60 percent of the total number reads #. the number of zero-count sgRNAs should be few (<5%, and prefered <1%). One exception is in positive selection experiments, where the number of zero-count sgRNAs may be much higher, but the percentage of mapped reads should be reasonably high. For more information on using MAGeCK, see the 
MAGeCK website here
. .. _MAGeCK: https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0554-4 .. 
100% validation rate
: https://sourceforge.net/p/mageck/wiki/Home/ .. 
Prof. Xiaole Shirley Liu's lab
: http://liulab.dfci.harvard.edu/ .. 
MAGeCK website here
: https://sourceforge.net/p/mageck/wiki/QA/#using-mageck"
toolshed.g2.bx.psu.edu/repos/iuc/mageck_mle/mageck_mle/0.5.9.2.1	".. class:: infomark 
What it does
 
MAGeCK mle
 calculates gene essentiality from CRISPR screens. Compared with the original algorithm in 
MAGeCK test
, MAGeCK mle uses a measurement called beta score to call gene essentialities: a positive beta score means a gene is positively selected, and a negative beta score means a gene is negatively selected. It is similar to the term log-fold change in differential expression, and compared with the original robust ranking aggregation (RRA) algorithm, this measurement has the following advantages: * It has only one score for one gene, instead of two scores in RRA: one for positive selection, one for negative selection; * It allows a direct comparison across multiple conditions, or even experiments; * It is able to incorporate sgRNA efficiency information. ----- 
Inputs
 
sgRNA count file
 The sgRNA read count file will be used in -k parameter in the mle command. The read count file should list the names of the sgRNA, the gene it is targeting, followed by the read counts in each sample. Each item should be separated by the tab ('\t'). A header line is optional. For example in the studies of T. Wang et al. Science 2014, there are 4 CRISPR screening samples, and they are labeled as: HL60.initial, KBM7.initial, HL60.final, KBM7.final. Here are a few lines of the read count file: ============== ======== ================ ================ ============== ============== 
sgRNA
 
gene
 
HL60.initial
 
KBM7.initial
 
HL60.final
 
KBM7.final
 -------------- -------- ---------------- ---------------- -------------- -------------- A1CF_m52595977 A1CF 213 274 883 175 A1CF_m52596017 A1CF 294 412 1554 1891 A1CF_m52596056 A1CF 421 368 566 759 A1CF_m52603842 A1CF 274 243 314 855 A1CF_m52603847 A1CF 0 50 145 266 ============== ======== ================ ================ ============== ============== 
Design matrix file
 Either the sample labels can be specified in the tool form above, or alternatively, a 
design matrix
 file can be provided. The design matrix indicates which sample is affected by which condition. It is generally a binary matrix indicating which sample (indicated by the first column) is affected by which condition (indicated by the first row). For the meanings of the design matrix, check the input file format page. ============ ======== ==== ==== 
Samples
 baseline HL60 KBM7 ------------ -------- ---- ---- HL60.initial 1 0 0 KBM7.initial 1 0 0 HL60.final 1 1 0 KBM7.final 1 0 1 ============ ======== ==== ==== The following are the rules for the design matrix file: * The design matrix file must include a header line of condition labels * The first column is the sample labels that must match sample labels in read count file * The second column must be a ""baseline"" column that sets all values to ""1"" * The element in the design matrix is either ""0"" or ""1"" * You must have at least one sample of ""initial state"" (e.g., day 0 or plasmid) that has only one ""1"" in the corresponding row. That only ""1"" must be in the baseline column. * In the design matrix above, there are four samples, two corresponding to the initial states of two cell lines, and two corresponding to the final states of two cell lines. We design two conditions (HL60 and KBM7) that model the cell type-specific effects. 
Control sgRNA file
 The optional Control sgRNAs file is used to generate null distribution when calculating the p values. If this option is not specified, MAGeCK generates the null distribution of RRA scores by assuming all of the genes in the library are non-essential, see 
More Information
 below. This approach is sometimes over-conservative, and you can improve this if you know some genes are not essential. By providing the corresponding sgRNA IDs in this option, MAGeCK will have a better estimation of p values. To use this option, you need to prepare a text file specifying the IDs of control sgRNAs, one line for one sgRNA ID. 
Outputs
 This tool outputs * a ranked sgRNA Summary file * a ranked Gene Summary file Optionally, under 
Output Options
 you can choose to output * a Log file of the analysis If successful, MAGeCK mle will generate two files, the Gene Summary file (including gene beta scores), and the sgRNA Summary file (including sgRNA efficiency probability predictions). 
Gene Summary file (including beta scores)
 An example of the gene summary output file is below. This file includes the beta scores in two conditions specified in the design matrix (HL60|beta and KBM7|beta), and the associated statistics. For more information, check the output format specification of the 
mageck test
 
Gene Summary
 file. ======== ========= ============= ========== ================ ============ ===================== ================= ============= =========== ================ ============ ===================== ================= 
Gene
 
sgRNA
 
HL60|beta
 
HL60|z
 
HL60|p-value
 
HL60|fdr
 
HL60|wald-p-value
 
HL60|wald-fdr
 
KBM7|beta
 
KBM7|ze
 
KBM7|p-value
 
KBM7|fdr
 
KBM7|wald-p-value
 
KBM7|wald-fdr
 -------- --------- ------------- ---------- ---------------- ------------ --------------------- ----------------- ------------- ----------- ---------------- ------------ --------------------- ----------------- RNF14 10 0.24927 0.72077 0.36256 0.75648 0.47105 0.9999 0.57276 1.6565 0.06468 0.32386 0.097625 0.73193 RNF10 10 0.10159 0.29373 0.92087 0.98235 0.76896 0.9999 0.11341 0.32794 0.90145 0.97365 0.74296 0.98421 RNF11 10 3.6354 10.513 0.00028 0.021739 7.5197e-26 1.3376e-22 2.5928 7.4925 0.0014898 0.032024 6.7577e-14 1.33e-11 ======== ========= ============= ========== ================ ============ ===================== ================= ============= =========== ================ ============ ===================== ================= 
sgRNA Summary file (including sgRNA efficiency probability predictions)
 An example of the sgRNA ranking output is as follows: ================ ======== ================= =================== ================ ============== ======= =============== =========== ========= ========= =========== ============== =========== ===================== 
sgrna
 
Gene
 
control_count
 
treatment_count
 
control_mean
 
treat_mean
 
LFC
 
control_var
 
adj_var
 
score
 
p.low
 
p.high
 
p.twosided
 
FDR
 
high_in_treatment
 ---------------- -------- ----------------- ------------------- ---------------- -------------- ------- --------------- ----------- --------- --------- ----------- -------------- ----------- --------------------- INO80B_m74682554 INO80B 0.0/0.0 1220.15/1476.14 0.810860 1348.15 10.70 0.0 19.0767 308.478 1.0 1.11022e-16 2.22044e-16 1.57651e-14 True NHS_p17705966 NHS 1.62172/3.90887 2327.09/1849.95 2.76529 2088.52 9.54 2.61554 68.2450 252.480 1.0 1.11022e-16 2.22044e-16 1.57651e-14 True ================ ======== ================= =================== ================ ============== ======= =============== =========== ========= ========= =========== ============== =========== ===================== The contents of each column are as follows: * 
sgrna
 sgRNA ID * 
Gene
 The targeting gene * 
control_count
 Normalized read counts in control samples * 
treatment_count
 Normalized read counts in treatment samples * 
control_mean
 Mean read counts in control samples * 
treat_mean
 Mean read counts in treatment samples * 
LFC
 The log fold change of sgRNA * 
control_var
 The raw variance in control samples * 
adj_var
 The adjusted variance in control samples * 
score
 The score of this sgRNA * 
p.low
 p-value (lower tail) * 
p.high
 p-value (higher tail) * 
p.twosided
 p-value (two sided) * 
FDR
 false discovery rate * 
high_in_treatment
 Whether the abundance is higher in treatment samples ----- 
More Information
 
Overview of the MAGeCK algorithm
 Briefly, read counts from different samples are first median-normalized to adjust for the effect of library sizes and read count distributions. Then the variance of read counts is estimated by sharing information across features, and a negative binomial (NB) model is used to test whether sgRNA abundance differs significantly between treatments and controls. This approach is similar to those used for differential RNA-Seq analysis. We rank sgRNAs based on P-values calculated from the NB model, and use a modified robust ranking aggregation (RRA) algorithm named α-RRA to identify positively or negatively selected genes. More specifically, α-RRA assumes that if a gene has no effect on selection, then sgRNAs targeting this gene should be uniformly distributed across the ranked list of all the sgRNAs. α-RRA ranks genes by comparing the skew in rankings to the uniform null model, and prioritizes genes whose sgRNA rankings are consistently higher than expected. α-RRA calculates the statistical significance of the skew by permutation, and a detailed description of the algorithm is presented in the Materials and methods section of the 
MAGeCK paper
. Finally, MAGeCK reports positively and negatively selected pathways by applying α-RRA to the rankings of genes in a pathway. For more information on using MAGeCK, see the 
MAGeCK website here
. .. 
design matrix
: https://sourceforge.net/p/mageck/wiki/input/#design-matrix-file .. 
MAGeCK paper
: https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0554-4 .. _
MAGeCK website here
: https://sourceforge.net/p/mageck/wiki/QA/#using-mageck"
toolshed.g2.bx.psu.edu/repos/iuc/mageck_test/mageck_test/0.5.9.2.1	".. class:: infomark 
What it does
 
Model-based Analysis of Genome-wide CRISPR-Cas9 Knockout
 (MAGeCK) is a computational tool to identify important genes from the recent genome-scale CRISPR-Cas9 knockout screens (or GeCKO) technology. MAGeCK can be used for prioritizing single-guide RNAs, genes and pathways in genome-scale CRISPR/Cas9 knockout screens. MAGeCK identifies both positively and negatively selected genes simultaneously and reports robust results across different experimental conditions. MAGeCK is developed and maintained by Wei Li and Han Xu from 
Prof. Xiaole Shirley Liu's lab
 at the Department of Biostatistics and Computational Biology, Dana-Farber Cancer Institute and Harvard School of Public Health. MAGeCK has been used to identify functional lncRNAs from screens with close to 
100% validation rate
. .. 
Model-based Analysis of Genome-wide CRISPR-Cas9 Knockout
: https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0554-4 .. 
100% validation rate
: https://sourceforge.net/p/mageck/wiki/Home/ .. 
Prof. Xiaole Shirley Liu's lab
: http://liulab.dfci.harvard.edu/ ----- 
mageck test
 This tests and ranks sgRNAs and genes based on the table provided. 
Inputs
 
sgRNA count file
 The input sgRNA count file be tab-delimited and list the names of the sgRNA, the gene it is targeting, followed by the read counts in each sample. A header line is optional. For example in the studies of T. Wang et al. Science 2014, there are 4 CRISPR screening samples, and they are labeled as: HL60.initial, KBM7.initial, HL60.final, KBM7.final, see below. Example: ============== ======== ================ ================ ============== ============== 
sgRNA
 
gene
 
HL60.initial
 
KBM7.initial
 
HL60.final
 
KBM7.final
 -------------- -------- ---------------- ---------------- -------------- -------------- A1CF_m52595977 A1CF 213 274 883 175 A1CF_m52596017 A1CF 294 412 1554 1891 A1CF_m52596056 A1CF 421 368 566 759 A1CF_m52603842 A1CF 274 243 314 855 A1CF_m52603847 A1CF 0 50 145 266 ============== ======== ================ ================ ============== ============== 
Sample Labels
 In the Treatment and Control inputs above, you can use either Sample Label or Sample Index to specify samples. If sample label is used, the labels MUST match the sample labels in the first line of the count table. For example, ""HL60.final,KBM7.final"". You can also use sample index to specify samples. The index of the sample is the order it appears in the sgRNA read count file, starting from 0. The index is used in the Treatment and Control inputs. In the example above, there are four samples, and the index of each sample is as follows: ============ ======= 
sample
 
index
 ------------ ------- HL60.initial 0 KBM7.initial 1 HL60.final 2 KBM7.final 3 ============ ======= 
Control sgRNA file
 The optional Control sgRNA file is used to generate null distribution when calculating the p values. If this option is not specified, MAGeCK generates the null distribution of RRA scores by assuming all of the genes in the library are non-essential. This approach is sometimes over-conservative, and you can improve this if you know some genes are not essential. By providing the corresponding sgRNA IDs in this option, MAGeCK will have a better estimation of p values. To use this option, you need to prepare a text file specifying the IDs of control sgRNAs, one line for one sgRNA ID. ----- 
Outputs
 This tool outputs * a ranked sgRNA Summary file * a ranked Gene Summary file Optionally, under 
Output Options
 you can choose to output * a Normalized Counts table * a PDF of the plots * the .R and .Rnw files to generate the report * a Log file of the analysis 
sgRNA Summary file
 An example of the sgRNA ranking output is as follows: ================ ======== ================= =================== ================ ============== ======= =============== =========== ========= ========= =========== ============== =========== ===================== 
sgrna
 
Gene
 
control_count
 
treatment_count
 
control_mean
 
treat_mean
 
LFC
 
control_var
 
adj_var
 
score
 
p.low
 
p.high
 
p.twosided
 
FDR
 
high_in_treatment
 ---------------- -------- ----------------- ------------------- ---------------- -------------- ------- --------------- ----------- --------- --------- ----------- -------------- ----------- --------------------- INO80B_m74682554 INO80B 0.0/0.0 1220.15/1476.14 0.810860 1348.15 10.70 0.0 19.0767 308.478 1.0 1.11022e-16 2.22044e-16 1.57651e-14 True NHS_p17705966 NHS 1.62172/3.90887 2327.09/1849.95 2.76529 2088.52 9.54 2.61554 68.2450 252.480 1.0 1.11022e-16 2.22044e-16 1.57651e-14 True ================ ======== ================= =================== ================ ============== ======= =============== =========== ========= ========= =========== ============== =========== ===================== The contents of each column are as follows: * 
sgrna
 sgRNA ID * 
Gene
 The targeting gene * 
control_count
 Normalized read counts in control samples * 
treatment_count
 Normalized read counts in treatment samples * 
control_mean
 Mean read counts in control samples * 
treat_mean
 Mean read counts in treatment samples * 
LFC
 The log fold change of sgRNA * 
control_var
 The raw variance in control samples * 
adj_var
 The adjusted variance in control samples * 
score
 The score of this sgRNA * 
p.low
 p-value (lower tail) * 
p.high
 p-value (higher tail) * 
p.twosided
 p-value (two sided) * 
FDR
 false discovery rate * 
high_in_treatment
 Whether the abundance is higher in treatment samples 
Gene Summary file
 An example of the gene summary output file is as follows: ======= ======= ============= =============== =========== ============ ================= =========== ============= =============== =========== ============ ================= =========== 
id
 
num
 
neg|score
 
neg|p-value
 
neg|fdr
 
neg|rank
 
neg|goodsgrna
 
neg|lfc
 
pos|score
 
pos|p-value
 
pos|fdr
 
pos|rank
 
pos|goodsgrna
 
pos|lfc
 ------- ------- ------------- --------------- ----------- ------------ ----------------- ----------- ------------- --------------- ----------- ------------ ----------------- ----------- ESPL1 12 6.4327e-10 7.558e-06 7.9e-05 1 -2.35 11 0.99725 0.99981 0.999992 615 0 -0.07 RPL18 12 6.4671e-10 7.558e-06 7.9e-05 2 -2.12 11 0.99799 0.99989 0.999992 620 0 -0.32 CDK1 12 2.6439e-09 7.558e-06 7.9e-05 3 -1.93 12 1.0 0.99999 0.999992 655 0 -0.12 ======= ======= ============= =============== =========== ============ ================= =========== ============= =============== =========== ============ ================= =========== The contents of each column is as follows: * 
id
 Gene ID * 
num
 The number of targeting sgRNAs for each gene * 
neg|score
 The RRA lo value of this gene in negative selection * 
neg|p-value
 The raw p-value (using permutation) of this gene in negative selection * 
neg|fdr
 The false discovery rate of this gene in negative selection * 
neg|rank
 The ranking of this gene in negative selection * 
neg|goodsgrna
 The number of ""good"" sgRNAs, i.e., sgRNAs whose ranking is below the alpha cutoff (determined by the --gene-test-fdr-threshold option), in negative selection. * 
neg|lfc
 The log fold change of this gene in negative selection * 
pos|score
 The number of targeting sgRNAs for each gene in positive selection (usually the same as num.neg) * 
pos|score
 The RRA lo value of this gene in negative selection * 
pos|p-value
 The raw p-value of this gene in positive selection * 
pos|fdr
 The false discovery rate of this gene in positive selection * 
pos|rank
 The ranking of this gene in positive selection * 
pos|goodsgrna
 The number of ""good"" sgRNAs, i.e., sgRNAs whose ranking is below the alpha cutoff (determined by the --gene-test-fdr-threshold option), in positive selection. * 
pos|lfc
 The log fold change of this gene in positive selection Genes are ranked by the p.neg field (by default). If you need a ranking by the p.pos, you can use the --sort-criteria option. ----- 
More Information
 
Overview of the MAGeCK algorithm
 Briefly, read counts from different samples are first median-normalized to adjust for the effect of library sizes and read count distributions. Then the variance of read counts is estimated by sharing information across features, and a negative binomial (NB) model is used to test whether sgRNA abundance differs significantly between treatments and controls. This approach is similar to those used for differential RNA-Seq analysis. We rank sgRNAs based on P-values calculated from the NB model, and use a modified robust ranking aggregation (RRA) algorithm named α-RRA to identify positively or negatively selected genes. More specifically, α-RRA assumes that if a gene has no effect on selection, then sgRNAs targeting this gene should be uniformly distributed across the ranked list of all the sgRNAs. α-RRA ranks genes by comparing the skew in rankings to the uniform null model, and prioritizes genes whose sgRNA rankings are consistently higher than expected. α-RRA calculates the statistical significance of the skew by permutation, and a detailed description of the algorithm is presented in the Materials and methods section of the 
MAGeCK paper
. Finally, MAGeCK reports positively and negatively selected pathways by applying α-RRA to the rankings of genes in a pathway. For more information on using MAGeCK, see the 
MAGeCK website here
. .. 
MAGeCK paper
: https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0554-4 .. _
MAGeCK website here
: https://sourceforge.net/p/mageck/wiki/QA/#using-mageck"
toolshed.g2.bx.psu.edu/repos/iuc/sra_tools/sam_dump/3.1.1+galaxy1	"What it does?
 This tool extracts data (in BAM_ format) from the Short Read Archive (SRA) at the National Center for Biotechnology Information (NCBI). It is based on the sam-dump_ utility of the SRA Toolkit and returns a collection of NGS data containing one file for each accession number provided. 
How to use it?
 There are three ways in which you can download data: 1. Plain text input of accession number(s) 2. Providing a list of accessions from file 3. Extracting data from an already uploaded SRA dataset Below we discuss each in detail. ------ 
Plain text input of accession number(s)
 When you type an accession number (e.g., 
SRR1582967
) into 
Accession
 box and click 
Execute
 the tool will fetch the data for you. You can also provide a list of multiple accession numbers (e.g. 
SRR3141592, SRR271828, SRR112358
). ----- 
Providing a list of accessions from file
 A more realistic scenario is when you want to upload a number of datasets at once. To do this you need a list of accession, where there is only one accession per line (see below for information on how to generate such a file). Once you have this file: 1. Upload it into your history using Galaxy's upload tool 2. Once the list of accessions is uploaded choose 
List of SRA accessions, one per line
 from 
select input type
 dropdown 3. Choose uploaded file within the 
sra accession list
 field 4. Click 
Execute
 ----- 
Extract data from an already uploaded SRA dataset
 If an SRA dataset is already present in the history, the sequencing data can be extracted in a human-readable data format (fastq, sam, bam) by setting 
select input type
 drop-down to 
SRA archive in current history
. ----- 
How to generate accession lists
 1. Go to 
SRA Run Selector
 by clicking this link_ 2. Find the study you are interested in by typing a search term within the 
Search
 box. This can be a word (e.g., 
mitochondria
) or an accession you have gotten from a paper (e.g., 
SRR1582967
). 3. Once you click on the study of interest you will see the number of datasets in this study within the 
Related SRA data
 box 4. Click on the Runs number 5. On the page that would open you will see 
Accession List
 button 6. Clicking of this button will produce a file that you will need to upload into Galaxy and use as the input to this tool. ----- .. _sam-dump: https://github.com/ncbi/sra-tools .. _BAM: https://samtools.github.io/hts-specs/SAMv1.pdf .. _collection: https://galaxyproject.org/tutorials/collections/ .. _link: https://trace.ncbi.nlm.nih.gov/Traces/index.html?view=run_browser&display=reads For credits, information, support and bug reports, please refer ato https://github.com/galaxyproject/tools-iuc."
toolshed.g2.bx.psu.edu/repos/iuc/sra_tools/fastq_dump/3.1.1+galaxy1	"A string of characters and/or variables. The variables can be one of: $ac: accession, $si: spot id, $sn: spot name, $sg: spot group (barcode), $sl: spot length in bases, $ri: read number, $rn: read name, $rl: read length in bases. '[]' could be used for an optional output: if all vars in [] yield empty values whole group is not printed. Empty value is empty string or for numeric variables. Ex: @$sn[
$rn]/$ri '
$rn' is omitted if name is empty"""
toolshed.g2.bx.psu.edu/repos/iuc/ebi_metagenomics_run_downloader/ebi_metagenomics_run_downloader/0.1.0	"What it does
 The European Bioinformatics Institute (EMBL-EBI) maintains the world’s most comprehensive range of freely available and up-to-date molecular databases This tool download data related to a run in EBI Metagenomics database."
toolshed.g2.bx.psu.edu/repos/iuc/ebi_search_rest_results/ebi_search_rest_results/0.1.1	"What it does
 The European Bioinformatics Institute (EMBL-EBI) maintains the world’s most comprehensive range of freely available and up-to-date molecular databases. EBI Search, also named as 'EB-eye', is a scalable search engine that: - provides text search functionality and uniform access to resources and services hosted at the European Bioinformatics Institute (EMBL-EBI) - is based on the consolidated Apache Lucene technology - exposes both a Web and RESTful Web Services interfaces - provides inter-domain navigation via a network of cross-references Here, sample clients provided by EBI is used"
toolshed.g2.bx.psu.edu/repos/iuc/ega_download_client/pyega3/5.0.2+galaxy0	The pyEGA3 download client is a python-based tool for viewing and downloading files from authorized EGA datasets. .. class:: Warning mark Data is stored unencrypted on the user's Galaxy account. Confidential data from the EGA could be left unprotected when uploaded to a public Galaxy server. Make sure to read the EGA Data Access Agreement (DAA_) before uploading any data to Galaxy from the EGA. If this applies to you, we recommend you follow the GalaxySensitiveData-ELIXIR-IS_ page for updates on encrypted data at rest. If you have an EGA account, you can set your EGA credentials in the user preferences menu of Galaxy. Otherwise, default EGA credentials with access to an example dataset will be used. pyEGA3 uses the EGA Data API and has several key features: - Files are transferred over secure https connections and received unencrypted, so no need for decryption after download. - Downloads resume from where they left off in the event that the connection is interrupted. - pyEGA3 supports file segmenting and parallelized download of segments, improving overall performance. - After download completes, file integrity is verified using checksums. - pyEGA3 implements the GA4GH-compliant htsget protocol for download of genomic ranges for data files with accompanying index files. .. _DAA: https://ega-archive.org/files/Example_DAA.doc .. _GalaxySensitiveData-ELIXIR-IS: https://github.com/elixir-europe/GalaxySensitiveData-ELIXIR_IS
toolshed.g2.bx.psu.edu/repos/iuc/sra_tools/fasterq_dump/3.1.1+galaxy1	"A string of characters and/or variables. The variables can be one of: $ac: accession, $si: spot id, $sn: spot name, $sg: spot group (barcode), $sl: spot length in bases, $ri: read number, $rn: read name, $rl: read length in bases. '[]' could be used for an optional output: if all vars in [] yield empty values whole group is not printed. Empty value is empty string or for numeric variables. Ex: @$sn[
$rn]/$ri '
$rn' is omitted if name is empty"""
toolshed.g2.bx.psu.edu/repos/iuc/iedb_api/iedb_api/2.15.3+galaxy1	"The dataset should have on allele per line. The allele may be followed by an optional comma-separated list of peptide lengths, e.g.: HLA-A
03:01,8,9,10 HLA-B
07:02,9"
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0	"What it does
 Given a file containing a list of NCBI accession numbers or a direct entry of accession numbers in the tool text input box, this tool will download the corresponding sequence records via the NCBI API. 
Limitations
 - For protein sequence downloads, only fasta format is supported - To avoid rate-limits imposed by the NCBI API, records are downloaded sequentially with a delay between requests. This may make it impractical to use this tool to download many (>100) records. 
Output
 A collection of sequence records in the desired format."
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_datasets/datasets_download_gene/18.14.0+galaxy0	".. class:: infomark 
What it does
 Downloads gene data from NCBI using the 
datasets
_ command-line tool. Retrieve gene sequences, transcripts, proteins, and annotation reports. 
Query Options
 ============= ================================================================ Method Description ============= ================================================================ Gene ID NCBI Gene ID (e.g., 672 for BRCA1) Symbol Gene symbol with taxon (e.g., TP53 in human) Accession RefSeq nucleotide (NM_) or protein (NP_/WP_) accession Taxon All genes for a taxon (large downloads) ============= ================================================================ ---- 
Key Options
 - 
Ortholog retrieval
: Get orthologous genes across taxa (vertebrates/insects) - 
Taxon filter
: Limit WP_ accession results to specific organisms - 
Flanking sequence
: Include nucleotides upstream/downstream (WP_ only) - 
FASTA filter
: Subset output to specific accessions 
Outputs (Eukaryote)
 - 
Gene Data Report
: Tabular metadata (ID, symbol, description, coordinates) - 
Gene Product Report
: Detailed transcript/protein information - 
Sequences
: Gene, RNA, protein, CDS, 5'/3' UTR FASTA files 
Outputs (Prokaryote)
 Prokaryotic genes (WP_ accessions) use a different report format with: accession, description, EC number, gene symbol, protein info. 
Examples
 Download human BRCA1:: Query by: Gene ID Gene ID: 672 Download TP53 orthologs in rodents:: Query by: Symbol Symbol: tp53 Ortholog: rodentia .. _datasets: https://www.ncbi.nlm.nih.gov/datasets/"
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_datasets/datasets_download_genome/18.14.0+galaxy0	".. class:: infomark 
What it does
 Downloads genome assemblies from NCBI using the 
datasets
_ command-line tool. Retrieve genome sequences, annotations, and metadata by accession or taxon. 
Query Options
 - 
By Accession
: NCBI Assembly (GCF_/GCA_) or BioProject accession - 
By Taxon
: Taxonomy ID, scientific name, or common name 
Filters
 ==================== =============================================== Filter Description ==================== =============================================== Reference only Limit to reference/representative assemblies Annotated only Include only genomes with annotations Assembly level Chromosome, complete, contig, or scaffold Assembly source RefSeq (GCF_) or GenBank (GCA_) Exclude atypical Remove atypical assemblies (e.g., partial) MAG filter Include/exclude metagenome-assembled genomes Date range Filter by release date ==================== =============================================== ---- .. class:: warningmark 
Note
: The ""Reference only"" filter returns only RefSeq (GCF_) assemblies. If a taxon has only GenBank (GCA_) assemblies, this filter will return no results with a misleading error message. It is a NCBI datasets bug (not a Galaxy bug). 
Outputs
 - 
Data Report
: Tabular metadata for matching assemblies - 
Genome FASTA
: Genomic sequences (nested collection by accession) - 
Annotation files
: GFF3, GTF, GenBank flat files - 
Protein/RNA/CDS
: Amino acid and nucleotide sequences - 
Sequence Report
: Per-sequence metadata (chromosome, length, etc.) .. _datasets: https://www.ncbi.nlm.nih.gov/datasets/"
toolshed.g2.bx.psu.edu/repos/galaxyp/dbbuilder/dbbuilder/0.3.4	"Output
 Creates a FASTA file of specified protein sequences for comparison with experimental MS/MS data in search algorithm. 
External Links
 - Galaxy-P_101_ shows usage Protein Database Downloader tool in the creation of a workflow - UniProtKB_ provides additional information about the UniProt Knowledgebase .. _Galaxy-P_101: http://msi-galaxy-p.readthedocs.org/en/latest/sections/galaxyp_101.html .. _UniProtKB: http://www.uniprot.org/help/uniprotkb 
Additional Protein Fasta URLs
 
HUMAN GUT METAPROTEOME:
 * 61MB gzip http://www.bork.embl.de/~arumugam/Qin_et_al_2010/frequent_microbe_proteins.fasta.gz 
MOUSE GUT MICROBIOTA:
 * See: http://gigadb.org/dataset/view/id/100114/token/mZlMYJIF04LshpgP"
toolshed.g2.bx.psu.edu/repos/galaxyp/uniprotxml_downloader/uniprotxml_downloader/2.5.0	UniProtKB/TrEMBL (unreviewed)is a large, automatically annotated database that may contain redundant sequences, but there is a higher chance peptides will be identified. UniProtKB/Swiss-Prot (reviewed) is a smaller, manually annotated database with less of a chance peptides will be identified but less sequence redundancy
toolshed.g2.bx.psu.edu/repos/galaxyp/unipept/unipept/6.2.4+galaxy1	isoleucine (I) and leucine (L) are equated when matching tryptic peptides to UniProt records
upload1	"Auto-detect
 The system will attempt to detect Axt, Fasta, Fastqsolexa, Gff, Gff3, Html, Lav, Maf, Tabular, Wiggle, Bed and Interval (Bed with headers) formats. If your file is not detected properly as one of the known formats, it most likely means that it has some format problems (e.g., different number of columns on different rows). You can still coerce the system to set your data to the format you think it should be. You can also upload compressed files, which will automatically be decompressed. ----- 
Ab1
 A binary sequence file in 'ab1' format with a '.ab1' file extension. You must manually select this 'File Format' when uploading the file. ----- 
Axt
 blastz pairwise alignment format. Each alignment block in an axt file contains three lines: a summary line and 2 sequence lines. Blocks are separated from one another by blank lines. The summary line contains chromosomal position and size information about the alignment. It consists of 9 required fields. ----- 
Bam
 A binary file compressed in the BGZF format with a '.bam' file extension. ----- 
Bed
 * Tab delimited format (tabular) * Does not require header line * Contains 3 required fields: - chrom - The name of the chromosome (e.g. chr3, chrY, chr2_random) or contig (e.g. ctgY1). - chromStart - The starting position of the feature in the chromosome or contig. The first base in a chromosome is numbered 0. - chromEnd - The ending position of the feature in the chromosome or contig. The chromEnd base is not included in the display of the feature. For example, the first 100 bases of a chromosome are defined as chromStart=0, chromEnd=100, and span the bases numbered 0-99. * May contain 9 additional optional BED fields: - name - Defines the name of the BED line. This label is displayed to the left of the BED line in the Genome Browser window when the track is open to full display mode or directly to the left of the item in pack mode. - score - A score between 0 and 1000. If the track line useScore attribute is set to 1 for this annotation data set, the score value will determine the level of gray in which this feature is displayed (higher numbers = darker gray). - strand - Defines the strand - either '+' or '-'. - thickStart - The starting position at which the feature is drawn thickly (for example, the start codon in gene displays). - thickEnd - The ending position at which the feature is drawn thickly (for example, the stop codon in gene displays). - itemRgb - An RGB value of the form R,G,B (e.g. 255,0,0). If the track line itemRgb attribute is set to ""On"", this RBG value will determine the display color of the data contained in this BED line. NOTE: It is recommended that a simple color scheme (eight colors or less) be used with this attribute to avoid overwhelming the color resources of the Genome Browser and your Internet browser. - blockCount - The number of blocks (exons) in the BED line. - blockSizes - A comma-separated list of the block sizes. The number of items in this list should correspond to blockCount. - blockStarts - A comma-separated list of block starts. All of the blockStart positions should be calculated relative to chromStart. The number of items in this list should correspond to blockCount. * Example:: chr22 1000 5000 cloneA 960 + 1000 5000 0 2 567,488, 0,3512 chr22 2000 6000 cloneB 900 - 2000 6000 0 2 433,399, 0,3601 ----- 
Fasta
 A sequence in FASTA format consists of a single-line description, followed by lines of sequence data. The first character of the description line is a greater-than ("">"") symbol in the first column. All lines should be shorter than 80 characters:: >sequence1 atgcgtttgcgtgc gtcggtttcgttgc >sequence2 tttcgtgcgtatag tggcgcggtga ----- 
FastqSolexa
 FastqSolexa is the Illumina (Solexa) variant of the Fastq format, which stores sequences and quality scores in a single file:: @seq1 GACAGCTTGGTTTTTAGTGAGTTGTTCCTTTCTTT +seq1 hhhhhhhhhhhhhhhhhhhhhhhhhhPW@hhhhhh @seq2 GCAATGACGGCAGCAATAAACTCAACAGGTGCTGG +seq2 hhhhhhhhhhhhhhYhhahhhhWhAhFhSIJGChO Or:: @seq1 GAATTGATCAGGACATAGGACAACTGTAGGCACCAT +seq1 40 40 40 40 35 40 40 40 25 40 40 26 40 9 33 11 40 35 17 40 40 33 40 7 9 15 3 22 15 30 11 17 9 4 9 4 @seq2 GAGTTCTCGTCGCCTGTAGGCACCATCAATCGTATG +seq2 40 15 40 17 6 36 40 40 40 25 40 9 35 33 40 14 14 18 15 17 19 28 31 4 24 18 27 14 15 18 2 8 12 8 11 9 ----- 
Gff
 GFF lines have nine required fields that must be tab-separated. ----- 
Gff3
 The GFF3 format addresses the most common extensions to GFF, while preserving backward compatibility with previous formats. ----- 
Interval (Genomic Intervals)
 - Tab delimited format (tabular) - File must start with definition line in the following format (columns may be in any order).:: #CHROM START END STRAND - CHROM - The name of the chromosome (e.g. chr3, chrY, chr2_random) or contig (e.g. ctgY1). - START - The starting position of the feature in the chromosome or contig. The first base in a chromosome is numbered 0. - END - The ending position of the feature in the chromosome or contig. The chromEnd base is not included in the display of the feature. For example, the first 100 bases of a chromosome are defined as chromStart=0, chromEnd=100, and span the bases numbered 0-99. - STRAND - Defines the strand - either '+' or '-'. - Example:: #CHROM START END STRAND NAME COMMENT chr1 10 100 + exon myExon chrX 1000 10050 - gene myGene ----- 
Lav
 Lav is the primary output format for BLASTZ. The first line of a .lav file begins with #:lav.. ----- 
MAF
 TBA and multiz multiple alignment format. The first line of a .maf file begins with ##maf. This word is followed by white-space-separated ""variable=value"" pairs. There should be no white space surrounding the ""="". ----- 
Scf
 A binary sequence file in 'scf' format with a '.scf' file extension. You must manually select this 'File Format' when uploading the file. ----- 
Sff
 A binary file in 'Standard Flowgram Format' with a '.sff' file extension. ----- 
Tabular (tab delimited)
 Any data in tab delimited format (tabular) ----- 
Table (delimiter-separated)
 Any delimiter-separated tabular data (CSV or TSV). ----- 
Wig
 The wiggle format is line-oriented. Wiggle data is preceded by a track definition line, which adds a number of options for controlling the default display of this track. ----- 
Other text type
 Any text file"
toolshed.g2.bx.psu.edu/repos/iuc/fastq_dl/fastq_dl/3.0.1+galaxy1	"This tool downloads FASTQ files from the European Nucleotide Archive (ENA) based on a list of ENA accession IDs. You can provide either accession IDs in text format or upload a file containing accession IDs (one per line). The tool also allows you to group downloaded data by experiment or sample and can optionally retrieve only metadata without downloading the FASTQ files. Input Types ----------- You can select from two types of inputs: 1. 
ENA Accession IDs (Text Input)
: - Provide a list of ENA accession IDs (e.g., Study, Sample, Experiment, or Run accessions) separated by whitespace. 2. 
Accession IDs File
: - Provide a file containing a list of ENA accession IDs, one per line. Parameters ---------- - 
Group by Experiment
: This option groups the downloaded runs by the experiment accession, which can be useful if you need to process data related to a specific experiment. - 
Group by Sample
: This option groups the downloaded runs by the sample accession. - 
Only Download Metadata
: Select this option if you only want to retrieve metadata without downloading the actual FASTQ files. This is useful if you need information about the runs but do not need the raw sequence data. Outputs ------- The tool generates three types of outputs: 1. 
Metadata Files
: This collection contains metadata files for each accession, in 
.tsv
 format, which provide details about the corresponding run. 2. 
Single-End Data
: If the input FASTQ files contain single-end reads, those files will be placed into a separate collection. In 
.fastq.gz
 format. 3. 
Paired-End Data
: If the input FASTQ files contain paired-end reads, those files will be grouped into pairs (forward and reverse). The paired files will also be placed in a separate collection and will be in 
.fastq.gz
 format."
toolshed.g2.bx.psu.edu/repos/iuc/pysradb_search/pysradb_search/1.4.2+galaxy2	".. class:: infomark 
Purpose
 pysradb allows to retrieve metadata, such as run accession numbers, from SRA and ENA based on multiple criteria: - Database: SRA or ENA - Query keywords - Accession number: a relevant study/experiment/sample/run accession number - Organism: scientific name of the sample organism - Library layout: paired or single-end reads - Sample size: rounded to the nearest megabase - Publication date - Sequencing platform: Illumina, Nanopore or PacBio - Library selection: method used to select and/or enrich the material being sequenced - Library source: Type of source material that is being sequenced - Library preparation strategy: sequencing technique intended for the library ------ .. class:: infomark 
Outputs
 pysradb generates three different output types: - Raw metadata file - Statistics for the search query - Graphs to illustrate the search results ------ .. class:: infomark 
Sequencing instruments
 
Comparisons between HiSeq instruments
 HiSeq 3000/4000 provides some improvements with respect the previous model HiSeq 2500: - HiSeq 3000/4000 genere up to 1.5 Tb and 5 Tb reads per run. - HiSeq 3000/4000 use patterned flow cell technology originally developed for HiSeq X platforms. - HiSeq 3000/4000 run 3 times faster and yield 65% more reads per lane. - HiSeq 3000/4000 patterned flow cells contain billions of nanowells at fixed, known positions on the flow cell. The structured organization enables clustering at higher densities compared to non-pattern HiSeq designs. However, the HiSeq 3000/4000 also have some also some limitations with respect to HiSeq 2500: - HiSeq 3000/4000 are not recommended for low complexity sequencing. Applications such as non-unique amplicons, 16S, are currently not recommended. - Libraries with low complexity within the first 25 bases of a read are not expected to produce high quality data. - Library size restrictions. Libraries that are too long can result in polyclonal clusters that span more than 1 well, these will not pass filter. Smaller libraries will preferentially amplify with Illumina's new kinetic exclusion amplification so tight library distributions ranging from 300-500 bp are recommended. - Very low tolerance for adapter dimers. Even as little as 1% adapter dimer can take up ~6% of sequencing reads, 10% contamination will take up 84% of reads. Illumina recommends you keep adapter contamination below 0.5% of your entire library. - Higher duplication rates as compared to HiSeq 2500. - Low quality read 2 (entire HiSeq 3000 install base is affected). HiSeq 3000/4000 support DNA-seq, RNA-seq , ChIP-Seq, mate-pair, small RNA and exome library preparation. Any library preparation where there is enough sequence diversity is currently supported. Amplicon, 16S and applications with low sequencing diversity are currently not supported on the HiSeq 3000 / 4000. HiSeq 2500 is considered the most reliable model according to different sources. 
What type of read quality is expected from the HiSeq 3000/4000 ?
 - 2 x 50bp ≥85% bases > Q30 - 2 x 75bp ≥80% bases > Q30 - 2 x 150bp ≥75% of bases >Q30 
What is the difference between MiSeq and HiSeq?
 HiSeq and MiSeq platforms are among the most widely used platform to study microbial communities. But the two platforms differ in the length and amount of reads. MiSeq can run 600 cycles to produce 200 million 300 bp reads, on the other hand, HiSeq 2500 can run 500 cycles to produce 120 million 250 bp. 
What are the differences between HiSeq and NovaSeq?
 The Illumina NovaSeq provides a massive upgrade in sequencing throughput compared to the HiSeq 4000. There are more stringent library requirements and requires a larger sample size. Due to the vast amount of data produced by the NovaSeq and the known issue of index swapping, unique dual-indexed libraries are required. 
What are the characteristics of HiSeq X instruments?
 - HiSeq X is recommended for whole genome sequencing only (including whole bisulfite sequencing). This means that it is not adequate for RNA-seq, exome, ChIP-seq or small RNA-seq applications. - Plant and animal samples can be sequenced on the HiSeq X. - Expect coverate is over 30x or approximately 375 million reads per lane by loading one sample per lane. - Hiseq X Ten generates utilize 2x150 base pair read configurations and has slightly better GC coverage than the HiSeq 2500. 
What are the differences between MiSeq and Nextseq?
 The NextSeq Series of systems delivers the power of high-throughput sequencing with the simplicity of a desktop sequencer. NextSeq instruments represent an improvement when compared with Miseq, despite generating sorter reads (150bp, compared to MiSeq 250bp). NextSeq is recommended in the following applications & methods: - Exome & large panel sequencing (enrichment-based) - Single-cell profiling (scRNA-Seq, scDNA-Seq, oligo tagging assays) - Transcriptome sequencing (total RNA-Seq, mRNA-Seq, gene expression profiling) - Methylation sequencing - Metagenomic profiling (shotgun metagenomics, metatranscriptomics) - Cell-free sequencing & liquid biopsy analysis Regarding the maximum number of reads per ran, MiSeq can generate 25 million, vs 400 million generated by the Nextseq 550 instrument. MiSeq recommended for sequencing samples of low diversity. 
What are the differences between HiSeq and NextSeq?
 The main technical difference between HiSeq and NextSeq will be the number of dyes each machines use. HiSeq uses traditional color coding with four different dyes, while NextSeq uses two dyes. This does not give any practical differences in terms of the data quality, but the trend in illumina sequencers are more into the direction of reducing the number of dyes. 
What is the difference between Nextseq and NovaSeq?
 The NovaSeq 6000 system offers deep and broad coverage and is recommended for large whole-genome sequencing (human, plant, animal) projects. It generates 250 bp reads, with 20 billion maximum reads per run. NovaSeq 6000 instruments have not application based restrictions. 
Illumina maximum read-length summary
 - MiSeq: between 300 and 600 bp - NextSeq: 300 bp - HiSeq 2500: between 250 and 500 bp (depending of the sofware) - HiSeq 4000: 150 bp - HiSeq X: 150 bp 
Nanopore models - single-molecule ultra-long-read sequencing
 Nanopore sequencing provides the longest read lengths, from 500 bp to the current record of 2.3 Mb, with 10-30-kb genomic libraries being common. Even after error correction, sequencing error rates of corrected nanopore reads (1.5-9%) are still higher than those of corrected PacBio reads (<1%). 
PacBio SMRT instruments - single-molecule long-read low-error rate sequencing
 PacBio Sequel II CLR sequencing represents a major advancement in sequencing throughput over previous PacBio platforms with the production of more sequencing data and longer reads versus RS II and the Sequel I. The PacBio HiFi sequencing method yields highly accurate long-read sequencing datasets with read lengths averaging 10-25 kb and accuracies greater than 99.5%."
toolshed.g2.bx.psu.edu/repos/iuc/jvarkit_wgscoverageplotter/jvarkit_wgscoverageplotter/20201223+galaxy0	"WGSCoveragePlotter from the jvarkit toolkit
 WGSCoveragePlotter_ is a tool to plot the coverage of aligned reads across a genomic contig. It takes as input a BAM file and a genomic contig in FASTA format and produces an image in either PNG or SVG format showing the depth of read coverage across the contig. .. _WGSCoveragePlotter: http://lindenb.github.io/jvarkit/WGSCoveragePlotter.html"
toolshed.g2.bx.psu.edu/repos/devteam/ucsc_custom_track/build_ucsc_custom_track_1/1.0.1	".. class:: infomark This tool allows you to build custom tracks using datasets in your history for the UCSC genome browser. You can view these custom tracks on the UCSC genome browser by clicking on 
display at UCSC main/test
 link in the history panel of the output dataset. ----- .. class:: warningmark Please note that this tool requires 
all input datasets(tracks) to have the same genome build
. The tool throws an error when this requirement is not met. You may then have to choose a valid dataset or remove invalid tracks."
toolshed.g2.bx.psu.edu/repos/iuc/circos/circos/0.69.8+galaxy7	"Circos ====== .. class:: infomark Made a nice plot? Share it with us in the 
training materials &lt;https://github.com/galaxyproject/training-material/issues/1867&gt;
, we're looking for more cool plots with scientifically relevant stories to share with the students who want to learn how to use Circos. Have a feature request? 
Share it with us! &lt;https://github.com/galaxyproject/training-material/issues/1867&gt;
 .. class:: warningmark No data or plot crashes? Check that your chromosome naming scheme matches between your Karyotype file and your datasets. Circos is a software package for visualizing data and information. It visualizes data in a circular layout — this makes Circos ideal for exploring relationships between objects or positions. There are other reasons why a circular layout is advantageous, not the least being the fact that it is attractive. Circos is ideal for creating publication-quality infographics and illustrations with a high data-to-ink ratio, richly layered data and pleasant symmetries. You have fine control each element in the figure to tailor its focus points and detail to your audience. .. image:: $PATH_TO_IMAGES/circos-sample-panel.png :alt: several example circos plots For more information see the Circos documentation_. .. _documentation: http://circos.ca/documentation/"
toolshed.g2.bx.psu.edu/repos/iuc/circos/circgraph/0.9-RC2	Circos ====== Circos is a software package for visualizing data and information. It visualizes data in a circular layout — this makes Circos ideal for exploring relationships between objects or positions. There are other reasons why a circular layout is advantageous, not the least being the fact that it is attractive. Circos is ideal for creating publication-quality infographics and illustrations with a high data-to-ink ratio, richly layered data and pleasant symmetries. You have fine control each element in the figure to tailor its focus points and detail to your audience. .. image:: $PATH_TO_IMAGES/circos-sample-panel.png :alt: several example circos plots For more information see the Circos documentation_. .. _documentation: http://circos.ca/documentation/
toolshed.g2.bx.psu.edu/repos/iuc/circos/circos_aln_to_links/0.69.8+galaxy7	Converts several standard alignment formats into a format appropriate for Circos plots.
toolshed.g2.bx.psu.edu/repos/iuc/circos/circos_bundlelinks/0.69.8+galaxy7	"From the 
official documentation &lt;http://circos.ca/documentation/tutorials/utilities/bundling_links/lesson&gt;
__ The purpose of the bundlelinks tool is to reduce the number of links in a dataset by merging (or bundling) adjacent links together. By merging links, you can distill a visually complex representation into one which effectively summarizes the link structure of your data. A bundle is encoded as a new link with a single start and end position, which are formed by the boundaries of the merged links. Bundles are best shown using the ribbon feature."
toolshed.g2.bx.psu.edu/repos/iuc/circos/circos_interval_to_text/0.69.8+galaxy7	Converts standard BED6+ and GFF3 files into a format appropriate for Circos data tracks, especially text tracks. BED3 files cannot be used as they lack the name field.
toolshed.g2.bx.psu.edu/repos/iuc/circos/circos_interval_to_tile/0.69.8+galaxy7	Converts standard BED3+ and GFF3 files into a format appropriate for Circos data tracks, especially tile tracks. BED3 files do not need conversion, Circos can accept these files directly.
toolshed.g2.bx.psu.edu/repos/iuc/circos/circos_binlinks/0.69.8+galaxy7	"From the 
official documentation &lt;http://circos.ca/documentation/tutorials/utilities/density_tracks/&gt;
__ The purpose of this script is to generate data for histogram and highlight tracks that stores the number, size and consensus position of links. All chromosomes in the input link file are divided into bins (controlled by -bin_size), and link statistics are calculated on a bin-by-bin basis."
toolshed.g2.bx.psu.edu/repos/iuc/circos/circos_resample/0.69.8+galaxy7	"From the script's documentation: The data resolution in a figure is limited by the output print, or screen, resolution and our own visual acuity. To read more about how these limits affect figure design, see 
this page &lt;http://mkweb.bcgsc.ca/images/resolution/visual-acuity-sequence-visualization.pdf&gt;
__ This script is designed to convert very high-resolution data tracks to low-resolution equivalents which are easier to interpret and faster to draw."
toolshed.g2.bx.psu.edu/repos/iuc/circos/circos_wiggle_to_stacked/0.69.8+galaxy7	Converts standard bigWig files into a format appropriate for Circos stacked histogram plots If you need to process bedgraph, please convert those to bigwig first.
toolshed.g2.bx.psu.edu/repos/iuc/circos/circos_tableviewer/0.69.8+galaxy7	"The 
official documentation &lt;http://mkweb.bcgsc.ca/tableviewer/docs/&gt;
__ has a lot of useful information. You should provide a table that looks like: :: labels A B C A 10 15 20 D 15 20 25 E 20 30 50 The header column and row are strictly required."
toolshed.g2.bx.psu.edu/repos/iuc/circos/circos_wiggle_to_scatter/0.69.8+galaxy7	Converts standard bigWig files into a format appropriate for Circos scatter/line/histogram plot tracks. If you need to process bedgraph, please convert those to bigwig first.
toolshed.g2.bx.psu.edu/repos/iuc/export2graphlan/export2graphlan/0.20+galaxy0	"What it does
 export2graphlan is a conversion software tool to produce both annotation and tree file for GraPhlAn. It can convert MetaPhlAn, LEfSe, and/or HUMAnN output to GraPhlAn input format In particular, the annotation file tries to highlight specific sub-trees deriving automatically from input file what nodes are important. For more information, check the 
user manual &lt;https://github.com/SegataLab/export2graphlan/&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/circos/circos_gc_skew/0.69.8+galaxy7	Calculate GC skew (G-C)/(G+C) for multiple windows along the sequence. Returns a list of ratios (floats), controlled by the length of the sequence and the size of the window. Returns 0 for windows without any G/C by handling zero division errors. Does NOT look at any ambiguous nucleotides.
toolshed.g2.bx.psu.edu/repos/devteam/gmaj/gmaj_1/2.0.1	".. class:: infomark 
Reference Sequence:
 The default option, ""First sequence in each block"", is the correct choice for the vast majority of MAF alignments. The alternative, ""Any sequence"", will allow you to flip the blocks to view them with any of the MAF sequences as the reference, but this is only appropriate if the file was generated by a sequence-symmetric alignment program such as TBA_. Using ""Any sequence"" with an ordinary MAF will 
not
 give the same results as if that alignment had been run with a different reference sequence. .. class:: infomark 
Annotation Style:
 The default style, ""Galaxy"", specifies one set of annotations for each species in the MAF file; it assumes that if you have, say, exons for several chromosomes of one species, they are all together in one file. The other style, ""Basic"", is more flexible but cumbersome: a separate set of files is specified for each sequence (e.g. chromosome), and you must fill in the full sequence name as it appears in the MAF. The Basic style also allows you to provide a display offset that GMAJ will add to all of the position labels for that sequence. With either style, specifying more than one set of annotations for the same sequence will result in an error message from GMAJ. ---- 
What it does
 GMAJ is an interactive viewer for MAF alignments, with support for optional annotation data. In addition to browsing the alignments, you can select and export them according to a variety of criteria and send the output back to your Galaxy history. For detailed information on GMAJ, click here_. ------ 
Citation
 If you use GMAJ, please cite 
Blanchette M, Kent WJ, Riemer C, Elnitski L, Smit AF, Roskin KM, Baertsch R, Rosenbloom K, Clawson H, Green ED, Haussler D, Miller W. Aligning multiple genomic sequences with the threaded blockset aligner. Genome Res. 2004 Apr;14(4):708-15. &lt;http://www.ncbi.nlm.nih.gov/pubmed/15060014&gt;
 and http://globin.cse.psu.edu/dist/gmaj/. If you use this tool in Galaxy, please cite 
Blankenberg D, Taylor J, Nekrutenko A; The Galaxy Team. Making whole genome multiple alignments usable for biologists. Bioinformatics. 2011 Sep 1;27(17):2426-2428. &lt;http://www.ncbi.nlm.nih.gov/pubmed/21775304&gt;
 .. _here: /static/gmaj/docs/gmaj_readme.html .. _TBA: http://www.bx.psu.edu/miller_lab/"
toolshed.g2.bx.psu.edu/repos/iuc/graphlan_annotate/graphlan_annotate/1.1.3	"What it does
 GraPhlAn is a software tool for producing high-quality circular representations of taxonomic and phylogenetic trees. GraPhlAn focuses on concise, integrative, informative, and publication-ready representations of phylogenetically- and taxonomically-driven investigation. 
graphlan_annotate
 modifies any input tree (in any of the three standard format) adding additional information regarding structural or graphical aspects of the tree (like colors and style of the taxa, labels, shadows, heatmaps, ...). The annotation file is a tab-delimited file listing the graphical options for clades. Usually each line has three fields: the name of the clade, the name of the option, and the value to assign to the option. Lines can however have two fields (typically for ""global"" option not referred to a specific clade) or four fields when the external rings (a sort of circular heatmap) is specified. For more information, check the 
user manual &lt;https://github.com/biobakery/graphlan&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/graphlan/graphlan/1.1.3	"What it does
 GraPhlAn is a software tool for producing high-quality circular representations of taxonomic and phylogenetic trees. GraPhlAn focuses on concise, integrative, informative, and publication-ready representations of phylogenetically- and taxonomically-driven investigation. For more information, check the 
user manual &lt;https://github.com/biobakery/graphlan&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_heatmap/ggplot2_heatmap/3.4.0+galaxy0	"This tool will generate a clustered heatmap of your data. More customization options will be added, for now the heatmap uses a red coloring scheme and clustering is performed using the ""maximum"" similarity measure and the ""complete"" hierarchical clustering measure. Input data should have row labels in the first column and column labels. For example, the row labels (the first column) should represent gene IDs and the column labels should represent sample IDs."
toolshed.g2.bx.psu.edu/repos/devteam/histogram/histogram_rpy/1.0.4	".. class:: infomark 
TIP:
 To remove comment lines that do not begin with a 
#
 character, use 
Text Manipulation->Remove beginning
 .. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
Syntax
 This tool computes a histogram of the numerical values in a column of a dataset. - All invalid, blank and comment lines in the dataset are skipped. The number of skipped lines is displayed in the resulting history item. - 
Column for x axis
 - only numerical columns are possible. - 
Number of breaks(bars)
 - breakpoints between histogram cells. Value of '0' will determine breaks automatically. - 
Plot title
 - the histogram title. - 
Label for x axis
 - the label of the x axis for the histogram. - 
Include smoothed density
 - if checked, the resulting graph will join the given corresponding points with line segments. ----- 
Example
 - Input file:: 1 68 4.1 2 71 4.6 3 62 3.8 4 75 4.4 5 58 3.2 6 60 3.1 7 67 3.8 8 68 4.1 9 71 4.3 10 69 3.7 - Create a histogram on column 2 of the above dataset. .. image:: histogram2.png"
toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_histogram/ggplot2_histogram/3.4.0+galaxy0	This tool will generate a histogram representing the distributions of each numerical column. Each column should have a descriptive header with no spaces, which will be used in the plot legend to represent the corresponding column (group). Input data example: ID Cond_A Cond_B gene_A 10 15 gene_B 8 12 gene_C 10 15 gene_D 6 9 gene_E 9 13.5 gene_F 8 12
toolshed.g2.bx.psu.edu/repos/iuc/jbrowse/jbrowse/1.16.11+galaxy1	"JBrowse-in-Galaxy ================= JBrowse-in-Galaxy offers a highly configurable, workflow-compatible alternative to Trackster. Overview -------- JBrowse is a fast, embeddable genome browser built completely with JavaScript and HTML5. The JBrowse-in-Galaxy (JiG) tool was written to help build complex JBrowse installations straight from Galaxy, taking advantage of the latest Galaxy features such as dataset collections, sections, and colour pickers. It allows you to build up a JBrowse instance without worrying about how to run the command line tools to format your data, and which options need to be supplied and where. Additionally it comes with many javascript functions to handle colouring of features which would be nearly impossible to write without the assistance of this tool. The JBrowse-in-Galaxy tool is maintained by 
the Galaxy IUC &lt;https://github.com/galaxyproject/tools-iuc/issues&gt;
, who you can help you with missing features or bugs in the tool. Options ------- The first option you encounter is the 
Fasta Sequence(s)
. This option now accepts multiple fasta files, allowing you to build JBrowse instances that contain data for multiple genomes or chrosomomes (generally known as ""landmark features"" in gff3 terminology.) Up to 30 will be shown from the dropdown selector within JBrowse, this is a known issue. 
Standalone Instances
 enable you to have either a complete JBrowse instance in a dataset, or just the data directory without JBrowse (e.g. for Apollo). Currently Galaxy copies the entire JBrowse directory in order to have a complete, downloadable file that contains a ready-to-go JBrowse instance. This is obviously an anti-feature because users don't want a complete copy of JBrowse (12Mb) that's duplicated for every JBrowse dataset in their history, and admins don't want useless copies of JBrowse on disk. Unfortunately we have not come up with the perfect solution just yet, but we're working on it! In the meantime, users have been given the option to produce just the 
data/
 directory. For those unfamiliar with JBrowse, the 
data/
 directory contains processed data files, but no way to view them. This feature is additionally implemented for upcoming 
Apollo &lt;https://github.com/gmod/apollo&gt;
 integration. 
Genetic Code
 is a new feature in v0.4 of JiG / v1.12.0 of JBrowse, which allows users to specify a non standard genetic code, and have JBrowse highlight the correct start and stop codons. 
Track Groups
 represent a set of tracks in a single category. These can be used to let your users understand relationships between large groups of tracks. .. image:: sections.png Annotation Tracks ----------------- Within Track Groups, you have one or more 
Annotation Tracks
. Each Annotation Track is a groups of datasets which have similar styling. This allows you to rapidly build up JBrowse instances without having to configure tracks individually. A massive improvement over previous versions. For example, if you have five different GFF3 files from various gene callers that you wish to display, you can take advantage of this feature to style all of them similarly. There are a few different types of tracks supported, each with their own set of options: GFF3/BED ~~~~~~~~ These are your standard feature tracks. They usually highlight genes, mRNAs and other features of interest along a genomic region. The underlying tool and this help documentation focus primarily on GFF3 data, and have not been tested extensively with other formats. Automatic min/max detection will fail under BED datasets. The data may be of a subclass we call 
match/match part
 data. This consists of top level 
match
 features, with a child 
match_part
 feature, and is often used in displaying alignments. (See ""Alignments"" section on the 
GFF3 specification &lt;https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md&gt;
__ for more information). If the data is match/match part, you will need to specify the top level match feature name, as it can be one of a few different SO terms, and JiG does not yet have the ability to understand SO terms. Next up is the 
Styling Options
 section, which lets you control a few properties on how the track is styled. Most of these you will not need to configure and can safely leave on defaults. Occasionally you will want to change what information is shown in the end product. .. image:: styling.png In the above image you can see some black text, and some blue text. The source of the black text is configured with the 
style.label
 option, and the source of the blue text is configured with the 
style.description
 option. Feature Score Scaling & Colouring Options ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ First, you need to choose between ignoring the score attribute of GFF3 files, or using it. If you choose to ignore it, all features will be coloured with a solid colour. If you choose to use it, features will have slightly different colours based on their scores. .. image:: opacity.png If you choose 
Ignore score
, you may choose between automatically choosing a colour, or manually specifying one. The automatically chosen colours vary along a brewer palette and generally look quite nice with no human intervention required. The manual colour choice is somewhat self explanatory. Clicking on the small coloured square will bring up a colour palette. If you choose 
Base on score
, you're faced with a dizzying array of options. First is the function to map the colour choices to colour values. JiG comes with a few functions built in such as linear scaling, logarithmic scaling, and blast scaling. The 
linear scaling
 method says ""take these values, and they map directly to a range of output values"". 
Logarithmic scaling
 says ""please take the log of the score before mapping"", and 
Blast scaling
 is further specialised to handle blast data more nicely. These are convenience functions to help transform the wide array of possible values in the GFF3 score attribute to more meaningful numbers. If you need more comprehensive score scaling, it is recommended that you pre-process your GFF3 files somehow. Once you've selected a scaling method, you can choose to manually specify the minimum and maximum expected values, or you can let JiG determine them for you automatically. Finally, opacity is the only mapping we currently provide. Future iterations will attempt to improve upon this and provide more colour scales. The Opacity option maps the highest scoring features to full opacity, and everything else to lower ones. BAM Pileups ~~~~~~~~~~~ We support BAM files and can automatically generate SNP tracks based on that bam data. .. image:: bam.png This is 
strongly discouraged
 for high coverage density datasets. Unfortunately there are no other configuration options exposed for bam files. BlastXML ~~~~~~~~ .. image:: blast.png JiG now supports both blastn and blastp datasets. JiG internally uses a blastXML to gapped GFF3 tool to convert your blastxml datasets into a format amenable to visualization in JBrowse. This tool is also available separately from the IUC on the toolshed. 
Minimum Gap Size
 reflects how long a gap must be before it becomes a real gap in the processed gff3 file. In the picture above, various sizes of gaps can be seen. If the minimum gap size was set much higher, say 100nt, many of the smaller gaps would disappear, and the features on both sides would be merged into one, longer feature. This setting is inversely proportional to runtime and output file size. 
Do not set this to a low value for large datasets
. By setting this number lower, you will have extremely large outputs and extremely long runtimes. The default was configured based off of the author's experience, but the author only works on small viruses. It is 
strongly
 recommended that you filter your blast results before display, e.g. picking out the top 10 hits or so. 
Protein blast search
 option merely informs underlying tools that they should adjust feature locations by 3x. Styling Options ^^^^^^^^^^^^^^^ Please see the styling options for GFF3 datasets, they are identical. Feature Score Scaling & Coloring Options ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Please see the score scaling and colouring options for GFF3 datasets, they are identical. Remember to set your score scaling to ""blast"" method if you do use it. Bigwig XY ~~~~~~~~~ .. image:: bigwig.png 
XYPlot
 BigWig tracks can be displayed as a ""density"" plot which is continuous line which varies in colour, or as an ""XYplot."" XYplots are preferable for users to visually identify specific features in a bigwig track, however density tracks are more visually compact. 
Variance Band
 is an option available to XYPlots, and can be seen in the third and fourth tracks in the above picture. This overlays a mean line, and 1 and 2 standard deviation areas. 
Track Scaling
 is different from colour scaling, instead it configures how the track behaves inside of JBrowse. 
Autoscaling globally
 means that JBrowse will determine the minimum and maximum for the track, and fix the bounds of the viewport to that. E.g. if your track ranges from 1-1000, and the region you're currently zoomed to only goes from 0-50, then the viewport range will still show 1-1000. This is good for global genomic context. However you may wish to consider 
autoscaling locally
 instead. In the example of a region which varies from 0-50, autoscaling locally would cause the individual track's viewport to re-adjust and show just the 0-50 region. If neither of these options are palatable, you may manually hardcode the minimum and maximums for the track to scale to. Colour Options ^^^^^^^^^^^^^^ BigWig tracks have two colours in JBrowse, a positive and a negative colour. As always you may manually choose a colour, or let JiG choose for you. One of the more interesting options is the 
Bicolor pivot
. This option allows you to control the point at which JBrowse switches from the positive colour to the negative. In the above graphic, you can see this has been configured to ""mean"" for the first two (orange and blue) tracks. VCFs/SNPs ~~~~~~~~~ These tracks do not support any special configuration. Known Issues ------------ - More than 30 landmark features cannot be listed in the manual selector. - Non GFF3 likely has issue with automatically determined min/max scores. Manually specify minimum and maximum score attributes, or do not use varied colours based on scores to avoid this issue. 
Attribution
 This Galaxy tool relies on the JBrowse, maintained by the GMOD Community. The Galaxy wrapper is developed by the IUC"
toolshed.g2.bx.psu.edu/repos/iuc/jbrowse/jbrowse_to_standalone/1.16.11+galaxy1	"Upgrades an existing bare JBrowse ""data"" directory into a full-fledged JBrowse instance. 
Attribution
 This Galaxy tool relies on the JBrowse, maintained by the GMOD Community. The Galaxy wrapper is developed by the IUC"
toolshed.g2.bx.psu.edu/repos/fubar/jbrowse2/jbrowse2/2.13.0+galaxy0	"JBrowse2-in-Galaxy ================== JBrowse2-in-Galaxy offers a highly configurable, workflow-compatible Genome viewer. Use and local viewing ===================== Browser tracks need a coordinate system, based on the reference genome, so that must be chosen before adding groups of tracks. Many Galaxy datatypes can be turned into a track for display - in all cases, the selected reference genome must have been used to generate the data: bam bed bigwig blastxml cram gff3 hic maf paf vcf Note that cram and bam will be large, so very slow amd are only recommended if you need the cigar annotation. Otherwise conversion to bed is recommended to slim them down. Unfortunately if you have millions of rows in a bed, it will also be very slow - in which case a bigwig is recommended. A JBrowse2 history item can be opened by viewing it (the ""eye"" icon). Overview -------- JBrowse is a fast, embeddable genome browser built completely with JavaScript and HTML5. The JBrowse-in-Galaxy (JiG) tool was written to help build complex JBrowse installations straight from Galaxy. It allows you to build up a JBrowse instance without worrying about how to run the command line tools to format your data, and which options need to be supplied and where. Options ------- 
Reference or Assembly
 Choose either a built-in or select one from your history. Track coordinates and contig names 
must
 match this reference precisely or they will not display. 
Track Groups
 represent a set of tracks in a single category. Annotation Tracks ----------------- BED ~~~ Bed feature annotation usually requires clicking on the feature. Bed files created by the 
Bigwig extremes to bed features
 tool have the score in column 5 set to 
1
 for regions above the quantile and 
-1
 for regions below. It is possible to set an advanced option for each bed track, to use the bed score in column 5 for the feature colour, giving red and blue using a JBrowse2 plugin, so they are easy to distinguish in a combined track. MAF ~~~ For history references, the name of the reference genome dataset in your history is very important for MAF tracks, because it becomes the ""dbkey"" for that reference. So, it must be exactly the same as the genome name (dbkey) used when making the MAF, typically in the Lastz tool. If you used ""hg38"" as the reference in Lastz, that is exactly what the reference fasta should be named for any MAF track. Change it using the ""pencil"" icon on the reference data in your history. Any other name, such as ""hg38.fasta"" will cause the MAF track to show no data since there are no matches with that reference dbkey. GFF3/BED ~~~~~~~~ Standard feature tracks. They usually highlight genes, mRNAs and other features of interest along a genomic region. When these contain tens of millions of features, such as repeat regions from a VGP assembly, displaying one at a time leads to extremely slow loading times when a large region is in view, unless the ""LinearPileupDisplay"" display option is selected for that track in the styling options section. The default is LinearBasicDisplay, which shows all details and works well for relatively sparse bed files. A better option is to make a bigwig track using a set of windows based on the lengths of each assembly or reference contig. BAM Pileups ~~~~~~~~~~~ We support BAM files and can automatically generate SNP tracks based on that bam data. BlastXML ~~~~~~~~ JiG now supports both blastn and blastp datasets. JiG internally uses a blastXML to gapped GFF3 tool to convert your blastxml datasets into a format amenable to visualization in JBrowse. This tool is also available separately from the IUC on the toolshed. 
Minimum Gap Size
 reflects how long a gap must be before it becomes a real gap in the processed gff3 file. In the picture above, various sizes of gaps can be seen. If the minimum gap size was set much higher, say 100nt, many of the smaller gaps would disappear, and the features on both sides would be merged into one, longer feature. This setting is inversely proportional to runtime and output file size. 
Do not set this to a low value for large datasets
. By setting this number lower, you will have extremely large outputs and extremely long runtimes. The default was configured based off of the author's experience, but the author only works on small viruses. It is 
strongly
 recommended that you filter your blast results before display, e.g. picking out the top 10 hits or so. 
Protein blast search
 option merely informs underlying tools that they should adjust feature locations by 3x. Local viewing ============= The same browser data and setup can also be downloaded as a compressed zip archive by clicking the download (""floppy disk"") icon in the history. This can be shared and viewed without Galaxy. A replacement application to serve the browser is required without Galaxy. A local python web server can be started using a script included in each archive, assuming that Python3 is already working on your desktop - if not you will have to install it first. Unzip the archive (
unzip [filename].zip
) and change directory to the first level in that zip archive. It contains a file named 
jb2_webserver.py
 With python3 installed, 
python3 jb2_webserver.py
 will serve the unarchived JBrowse2 configuration from the same directory as the python script automatically. If a new browser window does not open, but the script appears to be running, try pointing your web browser to the default of 
localhost:8080"
toolshed.g2.bx.psu.edu/repos/crs4/taxonomy_krona_chart/taxonomy_krona_chart/2.7.1+galaxy0	"What it does
 This tool renders results of a metagenomic profiling as a zoomable pie chart using Krona_. ------ 
Krona options
 The Galaxy version supports the following options:: -n Name of the highest level. -c Combine data from each file, rather than creating separate datasets within the chart. -d Maximum depth of wedges to include in the chart. ----- 
Input format
 
Tabular
 input format should be a tab-delimited file with the first column containing a count and the remaining columns describing the hierarchy. For example:: 2 Fats Saturated fat 3 Fats Unsaturated fat Monounsaturated fat 3 Fats Unsaturated fat Polyunsaturated fat 13 Carbohydrates Sugars 4 Carbohydrates Dietary fiber 21 Carbohydrates 5 Protein 4 which would yield this 
Krona plot
. .. _Krona plot: https://marbl.github.io/Krona/examples/xml.krona.html ----- 
License and citation
 This Galaxy tool is Copyright © 2013-2014 
CRS4 Srl.
 and is released under the 
MIT license
. .. _CRS4 Srl.: http://www.crs4.it/ .. _MIT license: https://opensource.org/licenses/MIT You can use this tool only if you agree to the license terms of: 
Krona
. .. _Krona: https://github.com/marbl/Krona/wiki"
toolshed.g2.bx.psu.edu/repos/peterjc/mummer/mummerplot_wrapper/0.0.7	"What it does
 Takes two FASTA files (
species A
 and 
species B
), compares them using one of the MUMmer 3 tools (
mummer
, 
nucmer
, or 
promer
), checking both strands, and then draws a dotplot using 
mummerplot
. The full MUMmer suite is more flexible and capable than this limited wrapper. 
References
 MUMmer manual: v3.22 http://mummer.sourceforge.net/manual/ MUMmer tutorials: http://mummer.sourceforge.net/examples/ If you use MUMmer 3, please cite: S. Kurtz et al. (2004). Versatile and open software for comparing large genomes. Genome Biology (2004), 5:R12. http://dx.doi.org/10.1186/gb-2004-5-2-r12 This wrapper is available to install into other Galaxy Instances via the Galaxy Tool Shed at http://toolshed.g2.bx.psu.edu/view/peterjc/mummer"
toolshed.g2.bx.psu.edu/repos/bgruening/ml_visualization_ex/ml_visualization_ex/1.0.11.0	"What it does
 This tool ouputs serveral machine learning visualization plots using Plotly, including 'feature_importances', 'learning curve', 'precison recall curve', 'roc_curve', and 'number of featues vs. rfecv gridscores'. This tool also ouputs configuration diagram for a deep learning model using the Keras model JSON file as input. 
Feature importances
 .. image:: https://raw.githubusercontent.com/goeckslab/Galaxy-ML/master/galaxy_ml/tools/images/feature_importances.png :width: 400 :alt: feature importances 
Learning curve
 .. image:: https://raw.githubusercontent.com/goeckslab/Galaxy-ML/master/galaxy_ml/tools/images/learning_curve.png :width: 400 :alt: learning curve 
Precison recall curve
 .. image:: https://raw.githubusercontent.com/goeckslab/Galaxy-ML/master/galaxy_ml/tools/images/pr_curve.png :width: 400 :alt: precison recall curve 
Receiver operating characteristic curve
 .. image:: https://raw.githubusercontent.com/goeckslab/Galaxy-ML/master/galaxy_ml/tools/images/roc_curve.png :width: 400 :alt: Receiver operating characteristic curve 
Number of featues vs. rfecv gridscores
 .. image:: https://raw.githubusercontent.com/goeckslab/Galaxy-ML/master/galaxy_ml/tools/images/rfecv_gridscore.png :width: 400 :alt: Number of featues vs. rfecv gridscores 
Deep learning model configuration
 .. image:: https://raw.githubusercontent.com/goeckslab/Galaxy-ML/master/galaxy_ml/tools/images/deepsea.png :width: 400 :alt: deapsea model configuration"
toolshed.g2.bx.psu.edu/repos/iuc/newick_utils/newick_display/1.6+galaxy1	"What it does
 This tool outputs a graph representing the tree, either as text or as a SVG or PNG image. Underscores in labels are replaced with spaces. Trees with no branch lengths are taken to be cladograms and are drawn with leaves aligned. Otherwise, the tree is assumed to be a phylogram: branch lengths are honored and a scale bar is drawn. .. image:: $PATH_TO_IMAGES/example.png :alt: example newick tree graph"
toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_pca/ggplot2_pca/3.4.0+galaxy0	"What it does
 This tool generates a Principal component analysis (PCA) for a given table using a combination of ggplot2 and ggfortify. ----- 
Example
 
WARNING:
 Be carefull when selecting row names in the second option because the grouping elements do not update automaticly before executing the script. This means that columns have to be chosen as if the row name column was already be removed. 
Example for row names in table
 +--------+-----------+-----------+---------------+---------------+ | name | control 1 | control 2 | treatment 1 | treatment 2 | +========+===========+===========+===============+===============+ | gene 1 | 10 | 12 | 3455 | 232 | +--------+-----------+-----------+---------------+---------------+ | gene 2 | 20 | 2 | 345 | 334 | +--------+-----------+-----------+---------------+---------------+ | gene 3 | 200 | 210 | 20 | 2 | +--------+-----------+-----------+---------------+---------------+ | | | | | | +--------+-----------+-----------+---------------+---------------+ | 1 | 2 | 3 | 4 | 5 | +--------+-----------+-----------+---------------+---------------+ The new index after reading the table will be: +--------+-----------+-----------+--------------+---------------+ | name | control 1 | control 2 | treatment 1 | treatment 2 | +========+===========+===========+==============+===============+ |* | 1 | 2 | 3 | 4 | +--------+-----------+-----------+--------------+---------------+ ----- Pictures coming soon."
toolshed.g2.bx.psu.edu/repos/iuc/intervene/intervene_pairwise/0.6.5+galaxy2	"What it does
 Plots the pairwise intersections of multiple interval files. .. image:: $PATH_TO_IMAGES/pairwise.png"
toolshed.g2.bx.psu.edu/repos/bgruening/plotly_parallel_coordinates_plot/plotly_parallel_coordinates_plot/0.2	"What it does
 Produce a 
parallel coordinates plot &lt;https://plot.ly/python/parallel-coordinates-plot/&gt;
_ from a tabular file. Multiple columns are chosen for dimensions and a single column for coloring. The plot is buried in a html file which provides rich interactive features. Image can be saved in various format, such as 'png', 'svg', 'jpeg' and so on."
toolshed.g2.bx.psu.edu/repos/iuc/pathview/pathview/1.34.0+galaxy0	".. class:: infomark 
What it does
 Pathview is a stand-alone software package for pathway based data integration and visualization. This package can be divided into four functional modules: the Downloader, Parser, Mapper andViewer. Mostly importantly, pathview maps and renders user data on relevant pathway graphs. Notice that KEGG requires subscription for FTP access since May 2011. However,Pathview downloads individual pathway graphs and data files through API or HTTP access, which is freely available (for academic and non-commerical uses). Pathview uses KEGGgraph (Zhang and Wiemann, 2009) when parsing KEGG xml data files. Options map closely to the excellent pathview manual_. ----- 
Inputs
 Pathview provides strong support for data integration. It works with: - essentially all types of biological data mappable to pathways (gene expression, protein expression, genetic association, metabolite, genomic data, literature, etc) - over 10 types of gene or protein IDs, and 20 types of compound or metabolite IDs - pathways for about 4800 species as well as KEGG orthology - various data attributes and formats, i.e. continuous/discrete data, matrices/vectors, single/multiple samples etc. Pathview can be directly used for metagenomic, microbiome or unknown species data when the data are mapped to KEGG ortholog pathways. 
Pathway ids
 Either just the name or a table with one column with the different pathway ids to represent. 
Gene data
 The first input of Pathview is a gene data table. Here gene data is a broad concept including genes, transcripts, protein, enzymes and their expression, modifications and any measurable attributes. It should be a table with first column being the gene ids and other being information (p-value, fold change, levels, etc) from one or several samples. The first line can include the sample names. Here gene id is a generic concepts, including multiple types of gene, transcript and protein uniquely mappable to KEGG gene IDs. KEGG ortholog IDs are also treated as gene IDs as to handle metagenomic data. Example: =========== ================= FBgn0039155 -4.14844993705661 FBgn0003360 -2.99977727873544 FBgn0026562 -2.38016404989418 FBgn0025111 2.69993883050214 FBgn0029167 -2.10506155636758 =========== ================= 
Compound data
 We also frequently want to look at metabolic pathways too. Besides gene nodes, these pathways also have compound nodes. Therefore, we may integrate or visualize both gene data and compound data with metabolic pathways. Here compound data is a broad concept including metabolites, drugs, their measurements and attributes. The format is similar to the gene data table, except named with IDs mappable to KEGG compound IDs. Over 20 types of IDs included in CHEMBL database can be used here. ----- 
Outputs
 Pathview generates both native KEGG view and Graphviz view for pathways. KEGG view keeps all the meta-data on pathways, spacial and temporal information, tissue/cell types, inputs, outputs and connections. This is important for human reading and interpretation of pathway biology. .. image:: $PATH_TO_IMAGES/dme00010_native.png :width: 60 % Graphviz view provides better control of node and edge attributes, better view of pathway topology, better understanding of the pathway analysis statistics. .. image:: $PATH_TO_IMAGES/dme00010_graphviz.png :width: 60 % .. _manual: https://bioconductor.org/packages/release/bioc/vignettes/pathview/inst/doc/pathview.pdf .. _KEGG: http://www.genome.jp/kegg"
toolshed.g2.bx.psu.edu/repos/bgruening/plotly_regression_performance_plots/plotly_regression_performance_plots/0.1	"What it does
 Produce a 
line and scatter curves &lt;https://plot.ly/python/line-and-scatter/&gt;
_ from tabular files. The input data contains the true values (last column) and the predicted data contains the predicted values (last column). The true and predicted values are plotted against each other. The plot is buried in a html file which provides rich interactive features. Image can be saved in various format, such as 'png', 'svg', 'jpeg' and so on."
toolshed.g2.bx.psu.edu/repos/bgruening/plotly_ml_performance_plots/plotly_ml_performance_plots/0.4	"What it does
 Produce a 
heatmap plot (confusion matrix) &lt;https://plot.ly/python/heatmaps/&gt;
_ from tabular files. The input data contains the original/true class labels (last column) and the predicted data contains the predicted class labels (last column). The true and predicted class labels are plotted against each other. The diagonal of this heatmap shows the correctly predicted data. The plot is buried in a html file which provides rich interactive features. Image can be saved in various format, such as 'png', 'svg', 'jpeg' and so on."
toolshed.g2.bx.psu.edu/repos/devteam/xy_plot/XY_Plot_1/1.0.2	".. class:: infomark This tool allows you to plot values contained in columns of a dataset against each other and also allows you to have different series corresponding to the same or different datasets in one plot. ----- .. class:: warningmark This tool throws an error if the columns selected for plotting are absent or are not numeric and also if the lengths of these columns differ. ----- 
Example
 Input file:: 1 68 4.1 2 71 4.6 3 62 3.8 4 75 4.4 5 58 3.2 6 60 3.1 7 67 3.8 8 68 4.1 9 71 4.3 10 69 3.7 Create a two series XY plot on the above data: - Series 1: Red Dashed-Line plot between columns 1 and 2 - Series 2: Blue Circular-Point plot between columns 3 and 2 .. image:: xy_example.jpg"
toolshed.g2.bx.psu.edu/repos/devteam/scatterplot/scatterplot_rpy/1.0.3	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation-&gt;Convert
 ----- 
Syntax
 This tool creates a simple scatter plot between two variables containing numeric values of a selected dataset. - All invalid, blank and comment lines in the dataset are skipped. The number of skipped lines is displayed in the resulting history item. - 
Plot title
 The scatterplot title - 
Label for x axis
 and 
Label for y axis
 The labels for x and y axis of the scatterplot. ----- 
Example
 - Input file:: 1 68 4.1 2 71 4.6 3 62 3.8 4 75 4.4 5 58 3.2 6 60 3.1 7 67 3.8 8 68 4.1 9 71 4.3 10 69 3.7 - Create a simple scatterplot between the variables in column 2 and column 3 of the above dataset. .. image:: scatterplot.png"
toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_point/ggplot2_point/3.4.0+galaxy1	"This tool will generate a scatterplot representing data from two groups/conditions. The input data should be in tabular format and the user can determine which groups (columns) to plot. Multiple groups can be plotted on the same or multiple plots by providing a column with a group identifier under ""advanced - plotting multiple groups"". Feel free to explore the (many) advanced options to customize your plot. Galaxy makes this type optimization easy for the user! The ouput is a pdf file with your scatterplot. The dimensions of this file can be modified under ""advanced - output dimensions"""
toolshed.g2.bx.psu.edu/repos/iuc/intervene/intervene_upset/0.6.5+galaxy2	"What it does
 This tool produces an upset plot for the intersection of sets from different datasets. These can be regular lists or BED/GFF/interval files which will be processed using bedtools intersect. .. image:: $PATH_TO_IMAGES/upset.png"
vcf_to_maf_customtrack1	"What it does
 This tool converts a Variant Call Format (VCF) file into a Multiple Alignment Format (MAF) custom track file suitable for display at genome browsers. This file should be used for display purposes only (e.g as a UCSC Custom Track). Performing an analysis using the output created by this tool as input is not recommended; the source VCF file should be used when performing an analysis. 
Unknown nucleotides
 are represented as '
' as required to allow the display to draw properly; these include e.g. reference bases which appear before a deletion and are not available without querying the original reference sequence. 
Example
 Starting with a VCF:: ##fileformat=VCFv3.3 ##fileDate=20090805 ##source=myImputationProgramV3.1 ##reference=1000GenomesPilot-NCBI36 ##phasing=partial ##INFO=NS,1,Integer,""Number of Samples With Data"" ##INFO=DP,1,Integer,""Total Depth"" ##INFO=AF,-1,Float,""Allele Frequency"" ##INFO=AA,1,String,""Ancestral Allele"" ##INFO=DB,0,Flag,""dbSNP membership, build 129"" ##INFO=H2,0,Flag,""HapMap2 membership"" ##FILTER=q10,""Quality below 10"" ##FILTER=s50,""Less than 50% of samples have data"" ##FORMAT=GT,1,String,""Genotype"" ##FORMAT=GQ,1,Integer,""Genotype Quality"" ##FORMAT=DP,1,Integer,""Read Depth"" ##FORMAT=HQ,2,Integer,""Haplotype Quality"" #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA00001 NA00002 NA00003 20 14370 rs6054257 G A 29 0 NS=3;DP=14;AF=0.5;DB;H2 GT:GQ:DP:HQ 0|0:48:1:51,51 1|0:48:8:51,51 1/1:43:5:-1,-1 20 17330 . T A 3 q10 NS=3;DP=11;AF=0.017 GT:GQ:DP:HQ 0|0:49:3:58,50 0|1:3:5:65,3 0/0:41:3:-1,-1 20 1110696 rs6040355 A G,T 67 0 NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:4:-1,-1 20 1230237 . T . 47 0 NS=3;DP=13;AA=T GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:51,51 0/0:61:2:-1,-1 20 1234567 microsat1 G D4,IGA 50 0 NS=3;DP=9;AA=G GT:GQ:DP 0/1:35:4 0/2:17:2 1/1:40:3 Under the following conditions: 
VCF Source type:
 
Per Population (file)
, 
Name for this population:
 
CHB+JPT
 Results in the following MAF custom track:: track name=""Galaxy Custom Track"" visibility=pack ##maf version=1 a score=0 s hg18.chr20 14369 1 + 14370 G s CHB+JPT_1.1 0 1 + 1 A a score=0 s hg18.chr20 17329 1 + 17330 T s CHB+JPT_1.2 0 1 + 1 A a score=0 s hg18.chr20 1110695 1 + 1110696 A s CHB+JPT_1.3 0 1 + 1 G s CHB+JPT_2.3 0 1 + 1 T a score=0 s hg18.chr20 1230236 1 + 1230237 T s CHB+JPT_1.4 0 1 + 1 . a score=0 s hg18.chr20 1234565 5 + 1234572 
G--
 s CHB+JPT_1.5 0 1 + 1 
------ s CHB+JPT_2.5 0 7 + 7 *GGA
*"
toolshed.g2.bx.psu.edu/repos/peterjc/venn_list/venn_list/0.1.1	".. class:: infomark 
TIP:
 If your data is in tabular files, the identifier is assumed to be in column one. 
What it does
 Draws Venn Diagram for one, two or three sets (as a PDF file). You must supply one, two or three sets of identifiers -- corresponding to one, two or three circles on the Venn Diagram. In general you should also give the full list of all the identifiers explicitly. This is used to calculate the number of identifers outside the circles (and check the identifiers in the other files match up). The full list can be omitted by implicitly taking the union of the category sets. In this case, the count outside the categories (circles) will always be zero. The identifiers can be taken from the first column of a tabular file (e.g. query names in BLAST tabular output, or signal peptide predictions after filtering, etc), or from a sequence file (FASTA, FASTQ, SFF). For example, you may have a set of NGS reads (as a FASTA, FASTQ or SFF file), and the results of several different read mappings (e.g. to different references) as tabular files (filtered to have just the mapped reads). You could then show the different mappings (and their overlaps) as a Venn Diagram, and the outside count would be the unmapped reads. 
Citations
 The Venn Diagrams are drawn using Konstantin Tretyakov's matplotlib-venn package, https://pypi.org/project/matplotlib-venn/ If you use this Galaxy tool in work leading to a scientific publication please cite: Peter J.A. Cock, Björn A. Grüning, Konrad Paszkiewicz and Leighton Pritchard (2013). Galaxy tools and workflows for sequence analysis with applications in molecular plant pathology. PeerJ 1:e167 https://doi.org/10.7717/peerj.167 This tool uses Biopython to read and write SFF files, so you may also wish to cite the Biopython application note (and Galaxy too of course): Cock et al 2009. Biopython: freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics 25(11) 1422-3. https://doi.org/10.1093/bioinformatics/btp163 pmid:19304878."
toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_violin/ggplot2_violin/3.4.0+galaxy0	Supply this tool with a text file with headers indicating the various groups to be plotted. This tool will sniff out each column with values that can be plotted and display the distribution of that data group. Note that columns may be excluded from this plot if they contain questionable characters.
toolshed.g2.bx.psu.edu/repos/saskia-hiltemann/krona_text/krona-text/1	Input file must be a tab-delimited file with first column being a count, rest the hierarchy, for example:: 2 Fats Saturated fat 3 Fats Unsaturated fat Monounsaturated fat 3 Fats Unsaturated fat Polyunsaturated fat 13 Carbohydrates Sugars 4 Carbohydrates Dietary fiber 21 Carbohydrates 5 Protein 4 Would yield the following graph: http://krona.sourceforge.net/examples/text.krona.html
toolshed.g2.bx.psu.edu/repos/iuc/volcanoplot/volcanoplot/0.0.6	".. class:: infomark 
What it does
 This tool creates a Volcano plot using ggplot2. Points can be labelled via ggrepel. It was inspired by this Getting Genetics Done 
blog post
. In statistics, a 
Volcano plot
 is a type of scatter-plot that is used to quickly identify changes in large data sets composed of replicate data. It plots significance versus fold-change on the y and x axes, respectively. These plots are increasingly common in omic experiments such as genomics, proteomics, and metabolomics where one often has a list of many thousands of replicate data points between two conditions and one wishes to quickly identify the most meaningful changes. A volcano plot combines a measure of statistical significance from a statistical test (e.g., a p value from an ANOVA model) with the magnitude of the change, enabling quick visual identification of those data-points (genes, etc.) that display large magnitude changes that are also statistically significant. A volcano plot is constructed by plotting the negative log of the p value on the y axis (usually base 10). This results in data points with low p values (highly significant) appearing toward the top of the plot. The x axis is the log of the fold change between the two conditions. The log of the fold change is used so that changes in both directions appear equidistant from the center. Plotting points in this way results in two regions of interest in the plot: those points that are found toward the top of the plot that are far to either the left- or right-hand sides. These represent values that display large magnitude fold changes (hence being left or right of center) as well as high statistical significance (hence being toward the top). Source: Wikipedia ----- 
Inputs
 A tabular file containing the columns below (additional columns may be present): * P value * FDR / adjusted P value * Log fold change * Labels (e.g. Gene symbols or IDs) All significant points, those meeting the specified FDR and Log Fold Change thresholds, will be coloured, red for upregulated, blue for downregulated. Users can choose to apply labels to the points (such as gene symbols) from the Labels column. To label all significant points, select ""Significant"" for the 
Points to label
 option, or to only label the top most significant specify a number under ""Only label top most significant"". Users can label any points of interest through selecting 
Points to label
 ""Input from file"" and providing a tabular labels file. The labels file must contain a header row and have the labels in the first column. These labels must match the labels in the main input file. 
Outputs
 A PDF containing a Volcano plot like below. The R code can be output through 
Output Options
 in the tool form. .. image:: $PATH_TO_IMAGES/volcano_plot.png .. 
Volcano plot: https://en.wikipedia.org/wiki/Volcano_plot
(statistics) .. _blog post: https://gettinggeneticsdone.blogspot.com/2016/01/"
toolshed.g2.bx.psu.edu/repos/fubar/jbrowse2/autogenjb2/2.10.2.0	"Autogenerated JBrowse2 ====================== Given a collection of datatypes suited for making JB2 tracks, this tool will create a working JBrowse2 configuration. A paf track requires a nested collection with the paf file and all the genomes used in creating it. Use and local viewing ===================== A JBrowse2 history item can be opened by viewing it (the ""eye"" icon). The same browser data and setup can also be downloaded as a compressed zip archive by clicking the download (""floppy disk"") icon in the history. This can be shared and viewed without Galaxy. A replacement application to serve the browser is required without Galaxy. A local python web server can be started using a script included in each archive, assuming that Python3 is already working on your desktop - if not you will have to install it first. Unzip the archive (
unzip [filename].zip
) and change directory to the first level in that zip archive. It contains a file named 
jb2_webserver.py
 With python3 installed, 
python3 jb2_webserver.py
 will serve the unarchived JBrowse2 configuration from the same directory as the python script automatically. If a new browser window does not open, but the script appears to be running, try pointing your web browser to the default of 
localhost:8080
 Overview -------- JBrowse is a fast, embeddable genome browser built completely with JavaScript and HTML5. The JBrowse-in-Galaxy (JiG) tool was written to help build complex JBrowse installations straight from Galaxy. It allows you to build up a JBrowse instance without worrying about how to run the command line tools to format your data, and which options need to be supplied and where. The JBrowse-in-Galaxy tool has been rejected by 
a Galaxy IUC &lt;https://github.com/galaxyproject/tools-iuc/issues&gt;
__, reviewer. Options ------- 
Reference or Assembly
 Choose either a built-in or select one from your history. Track coordinates and contig names 
must
 match this reference precisely or they will not display. 
Track Groups
 represent a set of tracks in a single category. Annotation Tracks ----------------- GFF3/BED ~~~~~~~~ Standard feature tracks. They usually highlight genes, mRNAs and other features of interest along a genomic region. When these contain tens of millions of features, such as repeat regions from a VGP assembly, displaying one at a time leads to extremely slow loading times when a large region is in view, unless the ""LinearPileupDisplay"" display option is selected for that track in the styling options section. The default is LinearBasicDisplay, which shows all details and works well for relatively sparse bed files. A better option is to make a bigwig track using a set of windows based on the lengths of each assembly or reference contig. BAM Pileups ~~~~~~~~~~~ We support BAM files and can automatically generate SNP tracks based on that bam data. BlastXML ~~~~~~~~ JiG now supports both blastn and blastp datasets. JiG internally uses a blastXML to gapped GFF3 tool to convert your blastxml datasets into a format amenable to visualization in JBrowse. This tool is also available separately from the IUC on the toolshed. 
Minimum Gap Size
 reflects how long a gap must be before it becomes a real gap in the processed gff3 file. In the picture above, various sizes of gaps can be seen. If the minimum gap size was set much higher, say 100nt, many of the smaller gaps would disappear, and the features on both sides would be merged into one, longer feature. This setting is inversely proportional to runtime and output file size. 
Do not set this to a low value for large datasets
. By setting this number lower, you will have extremely large outputs and extremely long runtimes. The default was configured based off of the author's experience, but the author only works on small viruses. It is 
strongly
 recommended that you filter your blast results before display, e.g. picking out the top 10 hits or so. 
Protein blast search
 option merely informs underlying tools that they should adjust feature locations by 3x. 
Attribution
 This Galaxy tool relies on the JBrowse2, maintained by the GMOD Community. The Galaxy wrapper is maintained by Ross Lazarus until the IUC complete their own."
toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_heatmap2/ggplot2_heatmap2/3.1.3.1+galaxy0	This tool employs the heatmap.2 function from the R gplots package and will generate a heatmap of your data. If clustering is enabled, the heatmap uses the Euclidean distance method and the Complete hierarchical clustering method by default. Input data should have row labels in the first column and column labels. For example, the row labels (the first column) should represent gene IDs and the column labels should represent sample IDs.
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicplottads/hicexplorer_hicplottads/2.1.4.0	"Plot Topologic Associated Domains ================================= 
hicPlotTADs
 is a visualization tool to plot the topologically associating domains (TADs) in a given region. Additional tracks can be added to enable the comparisons with other data like gene, ChIP-seq, RNA-seq or other tracks, including TAD seperation scores computed by 
hicFindTADs
. 
_
_
 Usage ----- This tool takes various types of tracks as input: - 
TAD vizualisation:
 corrected Hi-C contact matrix to plot a contact heatmap. It is recommended to follow 
hicPlotMatrix
 instructions. Boundaries file can used, which is the output of 
hicFindTADs
 in bed format. If selected, TADs will be drawn directly on the contact heatmap. - 
Chromatin states:
 display blocks of different colors following a bed file. - 
TAD score:
 display TAD seperation score computed by 
hicFindTADs
. - 
Gene track / Bed Track:
 display genes or bed files. Labels like gene names can be toggled on or off. - 
Bigwig track:
 generic bigwig track plotting. - 
Bedgraph track:
 generic bedgraph track plotting. - 
Bedgraph matrix track
 is used to specifically plot bm files computed by 
hicFindTADs
 (TAD seperation scores). - 
Vlines:
 vertical lines drawn on top of all tracks following a bed file. It is used as a visual support where regions start / end over all tracks, for example to display TAD boundaries computed by 
hicFindTADs
. - 
Spacer:
 Add some space between two tracks. For each track, parameters for the color, the width or the font size can be defined. 
_
_
 Output ------ 
hicPlotTADs
 output is similar to a genome browser screen-shot that besides the usual genes, and score data (like bigwig or bedgraph files) also contains Hi-C data. The plot is composed of tracks that need to be specified. Below is represented the 85 Mb to 110 Mb region from human chromosome 2 visualized using 
hicPlotTADs
. TADs were computed by 
hicFindTADs
. The additional tracks added correspond to: TAD-separation score (as reported by 
hicFindTADs
), chromatin states, principal component 1 (A/B compartment) computed using 
hicPCA
, ChIP-seq coverage for the H3K27ac mark, DNA methylation, and a gene track. Data are from mouse cardiac myocytes, published by 
Nothjunge et al. (2017)
. .. image:: $PATH_TO_IMAGES/hicPlotTADs.png :width: 70 % _________________ | For more information about HiCExplorer please consider our documentation on readthedocs.io
 .. 
readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html .. 
Nothjunge et al. (2017)
: https://www.nature.com/articles/s41467-017-01724-9"
toolshed.g2.bx.psu.edu/repos/iuc/pygenometracks/pygenomeTracks/3.8+galaxy2	"pyGenomeTracks ============== 
pyGenomeTracks
 is a visualization tool which aims to produce high-quality genome browser tracks that are highly customizable. Currently, it is possible to plot: - bigwig - bed/gtf (many options) - bedgraph - epilogos - narrow peaks - links - Hi-C matrices (cool or HiCExplorer h5) - Fasta - MAF (multiple alignment format) 
_
_
 Usage ----- This tool takes various types of tracks as input: - 
Hi-C tracks:
 - 
TAD vizualisation:
 corrected Hi-C contact matrix to plot a contact heatmap. It is recommended to follow HiCExplorer's 
hicPlotMatrix
 instructions. Boundaries file can used, which is the output of HiCExplorer's 
hicFindTADs
 in bed format. If selected, TADs will be drawn directly on the contact heatmap. - 
TAD score:
 display TAD seperation score computed by HiCExplorer's 
hicFindTADs
. - 
Chromatin states:
 display blocks of different colors following a bed file. - 
Gene track / Bed Track:
 display genes or annotations in bed/gtf files. Labels like gene names can be toggled on or off. - 
Link track:
 display links (pair of coordinates) as arcs, triangles or loops. - 
NarrowPeak track:
 display narrowPeak (encode format) as boxes or as curve (reconstructed peak). - 
Bigwig track:
 generic bigwig track plotting. - 
Bedgraph track:
 generic bedgraph track plotting. - 
Bedgraph matrix track
 is used to specifically plot bm files computed by HiCExplorer's 
hicFindTADs
 (TAD seperation scores). - 
Vlines:
 vertical lines drawn on top of all tracks following a bed file. It is used as a visual support where regions start / end over all tracks, for example to display TAD boundaries computed by HiCExplorer's 
hicFindTADs
. - 
Vhighlight:
 vertical rectangles drawn on top of all tracks following a bed file. It is used as a visual support to highlight some regions. - 
Hlines:
 horizontal lines drawn either by themselves or on top of other tracks. - 
Spacer:
 Add some space between two tracks. - 
X-axis:
 Plot x-axis scale wherever you want. - 
Scale bar track:
 Plot scale bar. - 
Fasta track:
 Display sequences from fasta. - 
Maf track:
 Display alignments from maf. For each track, parameters for the color, the width or the font size can be defined. 
_
_
 Output ------ Here are two example plots with the different tracks you can use (on the right, you have the vlines and vhighlight which have been used): .. image:: static/images/demo.png :width: 45 % .. image:: static/images/demo2.png :width: 45 % 
_
_
___ | For more information about pyGenomeTracks please consider our documentation on readthedocs_ or github_ .. 
readthedocs: https://pygenometracks.readthedocs.io .. _github: https://github.com/deeptools/pyGenomeTracks .. 
Nothjunge et al. (2017)
: https://www.nature.com/articles/s41467-017-01724-9"
toolshed.g2.bx.psu.edu/repos/iuc/tsne/tsne/0.0.2	T-distributed Stochastic Neighbor Embedding implementation by Van der Maaten (see <https://github.com/lvdmaaten/bhtsne/> for more information on the original implementation). Your data should be in tabular format. Objects in rows will be clustered according to the observations in columns. Labels for objects can be assigned in the tool form by providing the column number of the identifier you wish to use. Additionaly, the first column containing numeric data should be provided to the tool form.
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_plot/seurat_plot/4.0.4+galaxy0	".. class:: infomark 
What it does
 Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. This tool produces the different plots available on Seurat. ----- 
Inputs
 * Seurat object or compatible (can be specified in different formats) * Plot type (can be specified in different formats) * Potential compulsory options for each plot type All the rest of the parameters are optional. ----- 
Outputs
 * A plot in the specified format * Optionally, an RDS object with the ggplo2 plot. 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_integrate_bbknn/scanpy_integrate_bbknn/1.9.3+galaxy0	".. class:: infomark 
What it does
 Batch balanced kNN alters the kNN procedure to identify each cell’s top neighbours in each batch separately instead of the entire cell pool with no accounting for batch. Aligns batches in a quick and lightweight manner. Use as an alternative to Scanpy ComputeGraph. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_integrate_combat/scanpy_integrate_combat/1.9.3+galaxy0	".. class:: infomark 
What it does
 Corrects for batch effects by fitting linear models, gains statistical power via an EB framework where information is borrowed across genes. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_integrate_mnn/scanpy_integrate_mnn/1.9.3+galaxy0	".. class:: infomark 
What it does
 Corrects for batch effects by fitting linear models, gains statistical power via an EB framework where information is borrowed across genes. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_plot_scrublet/scanpy_plot_scrublet/1.9.3+galaxy0	"Plot histogram of doublet scores for observed transcriptomes and simulated doublets. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_multiplet_scrublet/scanpy_multiplet_scrublet/1.9.3+galaxy0	".. class:: infomark 
What it does
 Predict cell doublets using a nearest-neighbor classifier of observed transcriptomes and simulated doublets. Works best if the input is a raw (unnormalized) counts matrix from a single sample or a collection of similar samples from the same experiment. This function is a wrapper around functions that pre-process using Scanpy and directly call functions of Scrublet(). This is a wrapper around the Scanpy wrapper for Scrublet- see https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pp.scrublet.html. 
Note
 Where a threshold is not provided, Scrublet will try to automatically set one based on simulations, but this does not always work. There will be a warning, and no 'threshold' slot will be populated in .uns['scrublet']. The 'predicted_doublets' slot in .obs will be set to False for all cells such that no filtering will occur if this column is supplied to filtering steps. You can use the 'Scanpy Plot Scrublet' tool to plot the distribution of scores yourself and estimate a threshold to override this behaviour. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_run_umap/seurat_run_umap/4.0.4+galaxy0	".. class:: infomark 
What it does
 Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. For more details on this method, please see the individual in-line documentation or the same method's Seurat 4 documentation. 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_chicaggregatestatistic/hicexplorer_chicaggregatestatistic/3.7.6+galaxy1	Aggregate statistic for differential testing ============================================ chicAggregateStatistic is a preprocessing tool for chicDifferentialTest. It takes two consecutive viewpoint files and one target file and creates one file containing all locations which should be tested for differential interactions. Either one target file for two consecutive viewpoint files or one target file for all viewpoints is accepted. For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_chicdifferentialtest/hicexplorer_chicdifferentialtest/3.7.6+galaxy1	"Differential testing of two viewpoints ====================================== chicDifferentialTest tests if two locations under consideration of the reference point have a different interaction count. Either Fisher's test or chi2 contingency test can be used. Input files for this test can be created with 
chicAggregateStatistic
. H0 is assuming the interactions are not different. Therefore the differential interaction counts are all where H0 was rejected. For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_chicexportdata/hicexplorer_chicexportdata/3.7.6+galaxy1	Exporting the binary data to text files ======================================= chicExportData extracts the data stored in hdf5 files of the capture Hi-C modules to text files. For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_chicplotviewpoint/hicexplorer_chicplotviewpoint/3.7.6+galaxy1	"Plot of viewpoints ================== chicPlotViewpoint plots one or many viewpoints with the average background model and the computed p-value per sample. Moreover it can highlight differential interactions of two samples and/or significant regions. An example usage is: 
$ chicPlotViewpoint --interactionFile interactions.hdf5 --range 500000 500000 --backgroundModelFile background_model.txt --pValue --outFileName viewpoint1_2.tar.gz --dpi 300
 For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_chicqualitycontrol/hicexplorer_chicqualitycontrol/3.7.6+galaxy1	"Compute the quality of viewpoints ================================= Computes the sparsity of each viewpoint to determine their quality. A viewpoint is considered of bad quality if it is too sparse i.e. there are too many locations with no interactions recorded. This script outputs five files: A plot with the sparsity distribution per matrix, a plot with the sparsity distribution as histograms and a filtered reference points file. Additional, the raw filter data and the rejected viewpoints are returned. An example usage is: 
$ chicQualityControl -m matrix1.h5 matrix2.h5 -rp referencePointsFile.txt --range 20000 40000 --sparsity 0.01 -o referencePointFile_QC_passed.txt
 For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_chicsignificantinteractions/hicexplorer_chicsignificantinteractions/3.7.6+galaxy1	"Significant interaction detection ================================= Significant interactions are detected by this tool for each viewpoint based on the background model. chicSignificantInteractions outputs for each viewpoint a file containing all recorded significant interactions and a target file. The target file is especially useful in the batch mode context, as it merges for two consecutive listed control and treatment viewpoint the significant interactions which can then be used to test for a differential interaction scheme. chicSignificantInteractions supports two modes to detect significant interactions, either by an x-fold over the average background or a loose p-value. In both cases neighboring significant peaks are merged together and an additional p-value based on the sum of interactions for this neighborhood is computed. Only interactions with a higher p-value as specified by the threshold 
--pValue
 are accepted as a significant interaction. For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_chicviewpoint/hicexplorer_chicviewpoint/3.7.6+galaxy1	"Compute viewpoints for all given reference points ================================================= Computes per input matrix all viewpoints which are defined in the reference points file. All files are stored in the folder defined by 
--outputFolder
, the files are named by the name of the reference point, the sample name and the location of the reference point: gene_matrix_name_chr_start_end.txt If multiple reference points are used and the processing downstream should be automated via batch processing mode, please activate 
--writeFileNamesToFile
. In this file, all the file names will be written to; in the case of multiple samples two consecutive lines are considered as treatment vs control in the differential analysis. For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_chicviewpointbackgroundmodel/hicexplorer_chicviewpointbackgroundmodel/3.7.6+galaxy1	"Compute a background model for cHi-C data analysis ================================================== chicViewpointBackgroundModel computes for all given samples with all reference points a background model. For all relative distances to a reference point a negative binomial distribution is fitted. Moreover, for each relative distance to a reference point the average value for this location is computed. Both background models are used, the first one for p-value and significance computation, the second one to filter out interactions with a less x-fold over mean. The background distributions are fixed at 
--fixateRange
 i.e. all distances lower / higher than this value use the fixed background distribution. For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicadjustmatrix/hicexplorer_hicadjustmatrix/3.7.6+galaxy1	"Adjust of the Hi-C matrix ========================= This tool is able to remove, keep or mask given regions of a Hi-C interaction matrix, including entire chromosomes. 
_
_
___ | For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicaggregatecontacts/hicexplorer_hicaggregatecontacts/3.7.6+galaxy1	"Aggregation of Hi-C contacts ============================ 
hicAggregateContacts
 allows plotting of aggregated Hi-C sub-matrices of a specified list of positions. Positions of interest can for example be binding sites of a specific protein that were determined by ChIP-seq or genetic elements as transcription start sites of active genes. 
_
_
 Usage ----- This tool must be used on Hi-C matrices corrected by 
hicCorrectMatrix
. One should also consider bigger bins than restriction enzyme resolution bins using 
hicMergeMatrixBins
. 
_
_
 Optional parameters ------------------- Optional data output can be selected: - 
Save values underlying the final matrix:
 if this option is given, then the values underlying the final matrix will be saved to tab-delimited tables (one per chromosome) using the indicated prefix, for example TSS_to_TSS_chrX.tab. If clustering is performed, then the values are saved including the cluster_id a in TSS_to_TSS_chrX_cluster_1.tab - 
Save the position of the contacts:
 if this option is given, then the position of the contacts is saved as (chrom1, start1, end1, chrom2, start2, end2) where chrom_n, start_n, end_n correspond to the pair of positions used to compute the submatrix. The data is saved per chromosome and per cluster separately (one file each). - 
Heatmap file per chromosome:
 if given, a heatmap file (per chromosome) is saved. Each row in the heatmap contains the diagonal of each of the submatrices centered on the bed file. This file is useful to get an idea of the values that are used for the aggregate matrix and to determine the fraction of submatrices that are aggregated that may have an enrichment at the center. 
_
_
 Output ------ 
hicAggregateContacts
 outputs a plot of aggregated contacts. Below, you can find an example of an aggregate Hi-C matrix obtained from 
Drosophila melanogaster
 Hi-C data. The interactions are plotted at binding sites of a protein that were determined by ChIP-seq. We plot sub-matrices of 30 bins (1.5 kb bin size, 45 kb in total). The regions specified in the BED file will be centered between half number of bins and the other half number of bins.The considered range is 300-1000 kb. The range should be adjusted and only contain contacts larger than TAD size to reduce background interactions. .. image:: $PATH
TO_IMAGES/hicAggregateContacts.png :width: 80 % This example was calculated using mean interactions of an observed vs. expected transformed Hi-C matrix. Additional options for the matrix transformation are total-counts or z-score. Aggregate contacts can be plotted in 2D or 3D. 
_
_
 | For more information about HiCExplorer please consider our documentation on readthedocs.io
 .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html .. _Colormaps: https://matplotlib.org/examples/color/colormaps_reference.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicaverageregions/hicexplorer_hicaverageregions/3.7.6+galaxy1	"Average regions =============== This tool sums Hi-C contacts around given reference points and computes their average. This tool is useful to detect differences at certain reference points, TAD boundaries for instance, between samples. WARNING: This tool can only be used with fixed bin size Hi-C matrices. No guarantees how and if it works on restriction site interaction matrices. Use the output to plot the average with hicPlotAverageRegions. 
_
_
___ | For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicbuildmatrix/hicexplorer_hicbuildmatrix/3.7.6+galaxy1	"Creation of the contact matrix =============================== 
hicBuildMatrix
 creates a contact matrix based on Hi-C read pairs. It requires two sam or bam files corresponding to the first and second mates of the paired-end Hi-C reads mapped on the reference genome. The sam and bam files should not be sorted by position. There are two main options to create the Hi-C contact matrix, either by fixed bin size (eg. 10000 bp) or by bins of variable length following restriction enzyme sites location in the genome (restriction enzyme resolution). 
hicBuildMatrix
 generates a quality control output that can be used to analyze the quality of the Hi-C reads to assess if the experiment and sequencing were successful. 
_
_
 Usage ----- This tool must be used on paired sam / bam files produced with a program that supports local alignment (e.g. Bowtie2) where both PE reads are mapped using the --local option. 
_
_
 Output ------ 
hicBuildMatrix
 creates multiple outputs: - The contact matrix used by HiCExplorer for all downstream analyses. - A bam file with the accepted alignments, which can be useful to inspect the distribution of valid Hi-C reads pairs, notably around restriction enzyme sites or for other downstream analyses. This file is not used by any HiCExplorer tools. - A quality control report to assess if the Hi-C protocol and library contrusction were successful. Example plot ++++++++++++ .. image:: hicPlotMatrix.png :width: 50% Contact matrix of 
Drosophila melanogaster
 embryos built with 
hicBuildMatrix
 and visualized using 
hicPlotMatrix
. Hi-C matrix bins were merged to a 25 kb bin size before plotting using 
hicMergeMatrixBins
. Quality report ++++++++++++++ A quality report is produced alongside the contact matrix. .. image:: $PATH_TO_IMAGES/hicQC.png :width: 40% Several plots, that are described in details below, are comprised inside this report. .. image:: $PATH_TO_IMAGES/hicQC_pairs_sequenced.png :width: 40% On the plot above, we can see how many reads were sequenced per sample (pairs considered), how many reads were mappable, unique and of high quality and how many reads passed all quality controls and are thus useful for further analysis (pairs used). All quality controls used for read filtering are explained below. .. image:: $PATH_TO_IMAGES/hicQC_unmappable_and_non_unique.png :width: 40% The figure above contains the fraction of reads with respect to the total number of reads that did not map, that have a low quality score or that didn't map uniquely to the genome. In our example we can see that Sample 3 has the highest fraction of pairs used. We explain the differences between the three samples on the plot below. .. image:: $PATH_TO_IMAGES/hicQC_pairs_discarded.png :width: 40% This figure contains the fraction of read pairs (with respect to mappable and unique reads) that were discarded when building the Hi-C matrix. You can find the description of each category below: - 
Dangling ends:
 reads that start with the restriction site and constitute reads that were digested but not ligated. Sample 1 in our example has a high fraction of dangling ends (and thus a low proportion of pairs used). Reasons for this can be inefficient ligation or insufficient removal of danging ends during samples preparation. - 
Duplicated pairs:
 reads that have the same sequence due to PCR amplification. For example, Sample 2 was amplified too much and thus has a very high fraction of duplicated pairs. - 
Same fragment:
 read mates facing inward, separated by up to 800bp that do not have a restriction enzyme site in between. These read pairs are not valid Hi-C pairs and are thus discarded from further analyses. - 
Self circle:
 read pairs within 25kb with 'outward' read orientation. - 
Self ligation:
 read pairs with a restriction site in between that are within 800bp. .. image:: $PATH_TO_IMAGES/hicQC_distance.png :width: 40% The figure above contains the fraction of read pairs (with respect to mappable reads) that compose inter chromosomal, short range (< 20kb) or long range contacts. Inter chromosomal reads of a wild-type sample are expected to be low. Trans-chromosomal contacts can be primarily considered as random ligation events. These would be expected to contribute to technical noise that may obscure some of the finer features in the Hi-C datasets (Nagano 
et al.
 2015, Comparison of Hi-C results using in-solution versus in-nucleus ligation, doi: https://doi.org/10.1186/s13059-015-0753-7). As such, a high fraction of inter chromosomal reads is an indicator of low sample quality, but it can also be associated to cell cycle changes (Nagano 
et al.
 2018, Cell-cycle dynamics of chromosomal organisation at single-cell resolution, doi: https://doi.org/10.1038/nature23001). Short range and long range contacts proportions can be associated to how the fixation is performed during Hi-C sample preparation. These two proportions also directly impact the Hi-C corrected counts versus genomic distance plots generated by hicPlotDistVsCounts. .. image:: $PATH_TO_IMAGES/hicQC_read_orientation.png :width: 40% The last figure shows the fractions of inward, outward, left or right read pairs (with respect to mappable reads). Deviations from an equal distribution indicates problems during sample preparation. 
_
_
___ | For more information about HiCExplorer please consider our documentation on readthedocs.io_. .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hiccomparematrices/hicexplorer_hiccomparematrices/3.7.6+galaxy1	"Comparison of Hi-C matrices =========================== This tool is useful to compare two matrices by applying operations like difference, ratio or log2ratio after normalization. To normalize the matrices, each element is divided by the sum of the matrix. 
hicCompareMatrices
 can be used for example to determine the effect of a mutation compared to wild-type samples on contact enrichment, or to see TAD structure modifications near differentially expressed genes between two conditions when followed by 
hicPlotMatrix
. It can also be used to compare two biological replicates. 
_
_
 Usage ----- 
hicCompareMatrices
 is usually perfomed on corrected matrices (
hicCorrectMatrix
) with bins merged (
hicMergeMatrixBins
) depending on the downstream analyses to perform (visualisation of a whole chromosome, or of a small region, etc). 
_
_
 Output ------ Here is an example of a log2ratio comparison between M1BP Knockdown and GST cells in 
Drosophila melanogaster
 on corrected matrices with 50 bins merged (about 30kb bins) plotted using 
hicPlotMatrix
. .. image:: $PATH_TO_IMAGES/hicCompareMatrices_m1bp_over_gst_log2_m50_matrix_plot.png :width: 50% In this plot we see that the cells with a M1BP Knockdown display a negative log2ratio compared to the wild-type. Depletion of M1BP thus show a dramatic effect on the distribution of Hi-C contacts in which short range contacts decrease (Ramirez 
et al.
 2017, High-resolution TADs reveal DNA sequences underlying genome organization in flies, https://doi.org/10.1038/s41467-017-02525-w). Below you can find an example of a log2ratio plot between Hi-C matrices of two biological replicates, no differences are observable which means that the replicates are well correlated. .. image:: $PATH_TO_IMAGES/hicCompareMatrices_QC_log2_m50_matrix_plot.png :width: 50% 
_
_
___ | For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hiccompartmentspolarization/hicexplorer_hiccompartmentspolarization/3.7.6+galaxy1	"Comparmentalization =================== 
hicCompartmentalization
 rearranges the average interaction frequencies using the first PC values to represent the global compartmentalization signal. To our knowledge this has been first introduced and implemented by Wibke Schwarzer et al. 2017 (Nature. 2017 Nov 2; 551(7678): 51–56) For more information about HiCExplorer please consider our documentation on readthedocs.io_. .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicconvertformat/hicexplorer_hicconvertformat/3.7.6+galaxy1	Converting between different Hi-C interaction formats ===================================================== Conversion of Hi-C matrices of different file formats. We support the conversion of hic to cool format via hic2cool, and homer, HicPro, h5 and cool format to h5, cool, homer or ginteractions format. Moreover, hicConvertFormat accepts multiple input files from one format with different resolutions and creates a mcool file. Each original file is stored under a path, e.g. matrix.mcool::/resolutions/10000. A batch computation is possible, the number of input files and output files needs to match, all input files need to be of the same format type and all output files too. For input and output of cooler files, special options are available, for all other formats they will be ignored. HiCPro file format needs an additional bed file as input. | For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hiccorrectmatrix/hicexplorer_hiccorrectmatrix/3.7.6+galaxy1	"Hi-C contact matrix correction ============================== 
hicCorrectMatrix
 runs Imakaev's iterative correction, described in 
Imakaev et al. (2012)
, or Knight-Ruiz correction over a Hi-C matrix. For the matrix correction to be efficient, it is important to remove the unassembled scaffolds (e.g. 
NT_
), mitochondrial DNA and Y chromosome and keep only full length chromosomes, as scaffolds create problems with matrix correction. Therefore we use the chromosome names (1-19, X, Y) here. 
Important
: Use ‘chr1 chr2 chr3 etc.’ if your genome index uses chromosome names with the ‘chr’ prefix. Also, for the method to work correctly, bins with zero reads assigned to them should be removed as they can not be corrected. Also, bins with low number of reads should be removed, otherwise, during the correction step, the counts associated with those bins will be amplified (usually, zero and low coverage bins tend contain repetitive regions). Bins with extremely high number of reads can also be removed from the correction as they may represent copy number variations. To aid in the identification of bins with low and high read coverage, the 
diagnostic plot
 function of 
hicCorrectMatrix
 must be used. Indeed, 
hicCorrectMatrix
 works in two steps: - 
Diagnostic plot
: First a histogram containing the sum of contact per bin (row sum) is produced. This plot needs to be inspected to decide the best threshold for removing bins with lower number of reads. - 
Correct
: The second step removes the bins outside of the defined thresholds and perfroms the iterative correction. 
_
_
___ Usage ----- This tool must be used on uncorrected matrices at restriction enzyme resolution or with merged bins (
hicMergeMatrixBins
). 
_
_
 Output ------ Diagnostic plot 
_
_
 The goal of the diagnostic plot is to help the user decide on a cutoff threshold that will ignore Hi-C matrix bins with few reads assigned to them. The plot is a histogram of the total number of Hi-C reads per matrix bin. A secondary scale based on the mean absolute deviation score, is shown on top of the figure. This secondary scale aims to offer 'normalized' values that are comparable across samples independently of the sequencing depth and the fraction of usable Hi-C reads. In all samples that we have studied, the histogram follows a bimodal distribution where the first peak is for bins with zero reads which usually occur at repetitive regions. Other low scoring bins tend to be close to repetitive regions. Also, low scoring bins can be caused by absence of a restriction site in the bin or because the restriction site is present but the restriction enzyme did not cut. The valley between the two peaks in the histogram is set by default as cutoff threshold. However, it is important to revise this as in some cases the selected value could not be correct. .. image:: $PATH_TO_IMAGES/diagnostic_plot.png :width: 50% On the example plot above, a user can then use the lower threshold defined by the Median Absolute Deviation (MAD) method (black bold bar), or define its own threshold based on the contacts distribution. Correct 
_____ Run the iterative correction and outputs the corrected matrix. This matrix can then be used with all downstream analysis tools such as 
hicPlotMatrix
, 
pyGenomeTracks
, 
hicPlotViewpoint
, 
hicAggregateContacts
 for 
visualization of Hi-C data
, 
hicCorrelate
, 
hicPlotDistVsCounts
, 
hicTransform
, 
hicFindTADs
, 
hicPCA
 
for data and scores computation on Hi-C data
. It is noteworthy that 
hicSumMatrices
 and 
hicMergeMatrixBins
 
must be performed on uncorrected matrices
. 
_
_
___ | For more information about HiCExplorer please consider our documentation on readthedocs.io
 .. 
readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html .. 
Imakaev et al. (2012)
: https://doi.org/10.1038/nmeth.2148"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hiccorrelate/hicexplorer_hiccorrelate/3.7.6+galaxy1	"Matrix correlation ================== 
hicCorrelate
 is a dedicated Quality Control tool that allows the correlation of multiple Hi-C matrices at once with either a heatmap or scatterplots output. Computes pairwise correlations between Hi-C matrices data. The correlation is computed taking the values from each pair of matrices and discarding values that are zero in both matrices. Parameters that strongly affect correlations are bin size of the Hi-C matrices (can be changed using 
hicMergeMatrixBins
) and the considered range. The smaller the bin size of the matrices, the finer the differences you score. The 
Range
 parameter should be selected at a meaningful genomic scale according to, for example, the mean size of the TADs in the organism you work with or to specific ranges found using 
hicPlotDistVsCounts
. 
_
_
 Usage ----- It is recommended to use this tool on corrected matrices (
hicCorrectMatrix
) at restriction enzyme resolution (unmerged bins). 
_
_
 Output ------ 
hicCorrelate
 outputs correlation plots of multiple Hi-C matrices. Below, you can find a correlation example of uncorrected Hi-C matrices obtained from 
Drosophila melanogaster
 embryos, either wild-type or having one gene knocked-down by RNAi. Heatmap 
_
 .. image:: $PATH_TO_IMAGES/hicCorrelate_Dmel_heatmap.png :width: 45% This example is showing a heatmap that was calculated using the Pearson correlation of corrected Hi-C matrices with a bin size of 6000 bp at a range of 5000 to 200000. The dendrogram indicates which samples are most similar to each other. You can see that the wild-type samples are seperated from the knock-down samples. In that case, Spearman correlation gives very similar results (not shown). Scatterplot 
_
_ .. image:: $PATH
TO_IMAGES/hicCorrelate_Dmel_scatterplot.png :width: 45% Additionally, pairwise scatterplots comparing interactions between each sample can be plotted. 
_
_
 For more information about HiCExplorer please consider our documentation on readthedocs.io
. .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html .. _Colormap: https://matplotlib.org/examples/color/colormaps_reference.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicdetectloops/hicexplorer_hicdetectloops/3.7.6+galaxy1	"Loop detection ============== Computes enriched regions (peaks) or long range contacts on the given contact matrix. hicDetectLoops can detect enriched interaction regions (peaks / loops) based on a strict candidate selection, negative binomial distributions and Wilcoxon rank-sum tests. The algorithm was mainly develop on GM12878 cells from Rao 2014 on 10kb and 5kb fixed bin size resolution. 
_
_
___ Usage ----- A command line example is available below (easily matchable in Galaxy using each field information): ̀
$ hicDetectLoops -m matrix.cool -o loops.bedgraph --maxLoopDistance 2000000 --windowSize 10 --peakWidth 6 --pValuePreselection 0.05 --pValue 0.05 --peakInteractionsThreshold 20
 The candidate selection is based on the restriction of the maximum genomic distance, here 2MB. This distance is given by Rao 2014. For each genomic distance a negative binomial distribution is computed and only interaction pairs with a threshold less than 
--pValuePreselection
 are accepted. Detected candidates need to have at least an interaction count of 
--maximumInteractionPercentageThreshold
 times the maximum value for their genomic distance. Please note that 
--maximumInteractionPercentageThreshold
 was introduced with HiCExplorer release 3.2. Earlier versions did not have this parameter yet and therefore their outputs may differ. In a second step, each candidate is considered compared to its neighborhood. This neighborhood is defined by the 
--windowSize
 parameter in the x and y dimension. Per neighborhood only one candidate is considered, therefore only the candidate with the highest peak values is accepted. As a last step, the neighborhood is split into a peak and background region (parameter 
--peakWidth
). The peakWidth can never be larger than the windowSize. However, we recommend for 10kb matrices a windowSize of 10 and a peakWidth of 6. The output file (´´-o loops.bedgraph
`) contains the x and y position of each loop and its corresponding p-value of the Anderson-Darling test.
1 120000000 122500000 1 145000000 147500000 0.001
The results can visualized via hicPlotMatrix:
$ hicPlotMatrix -m matrix.cool -o plot.png --log1p --region 1:18000000-22000000 --loops loops.bedgraph` .. image:: $PATH_TO_IMAGES/hicDetectLoops.png :width: 50% For more information about HiCExplorer please consider our documentation on readthedocs.io_. .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicdifferentialtad/hicexplorer_hicdifferentialtad/3.7.6+galaxy1	Differential TAD detection ========================== Computes if precomputed TADs are differentially expressed between two samples. For more information about HiCExplorer please consider our documentation on readthedocs.io_. .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicfindrestrictionsites/hicexplorer_hicfindrestrictionsites/3.7.6+galaxy1	Find restriction sites ====================== This scripts find the locations of a given restriction enzyme sequence. This file can be used to build an Hi-C interaction matrix with restriction enzyme resolution by hicBuildMatrix. For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicfindtads/hicexplorer_hicfindtads/3.7.6+galaxy1	"Calculate Topologic Associated Domains ====================================== Toplogical domains (TADs) are large mainly self-interacting domains. Chromatin interactions occur with higher frequency within a TAD as between TADs. More information_. 
_
_
 Usage ----- This tool must be used on unmerged matrices (restiction enzyme resolution) produced by 
hicBuildMatrix
 and corrected by 
hicCorrectMatrix
. 
_
_
 Computation details ------------------- 
hicFindTADs
 computes the TAD regions in two steps: in a first step it computes a TAD-separation score based on a z-score matrix for all bins. The z-score is defined as: “The absolute value of z represents the distance between the raw score and the population mean in units of the standard deviation. z is negative when the raw score is below the mean, positive when above.” [Source_]. .. image:: $PATH_TO_IMAGES/z-score.svg :width: 100 
Source of image &lt;https://wikimedia.org/api/rest_v1/media/math/render/svg/5ceed701c4042bb34618535c9a902ca1a937a351&gt;
 In our case the distribution describes the counts per bin of a genomic distance. In a second step the local minima of the TAD-separation score is evaluated with respect to the surrounding bins to assign a p-value. Two multiple testing corrections can be applied to filter the results: 
Bonferroni &lt;https://en.wikipedia.org/wiki/Bonferroni_correction&gt;
 or the 
false discovery rate &lt;https://en.wikipedia.org/wiki/False_discovery_rate&gt;
. _________________ Output ------ 
hicFindTADs
 produces multiple outputs: - TAD boundaries positions as a BED file and TAD separation score. - TAD boundaries positions with delta, p-value and TAD separation score as GFF. - TAD domains as a BED file. - TAD seperation score as bigwig (bw), bedgraph and numpy array (npz) format. These files can be used to plot the so-called TAD insulation score or TAD separation score along the genome or at specific regions. This score is much more reliable across samples than the number of TADs or the TADs width that can vary depending on the sequencing depth because of the lack of information at certain bins, and depending on the parameters used with this tool. - Matrix with multi-scale TAD scores as a bed-matrix (bm) file that can be plotted inside 
pyGenomeTracks
 to nicely display TAD insulation score alongside Hi-C heatmap and other datasets. - Z-score matrix in h5 format that is useful to quickly test the --thresholdComparisons, --delta and --correctForMultipleTesting parameters by using the --TAD_sep_score_prefix option pointing to this zscore_matrix.h5 file (will be added in a future update). 
_
_
___ Usage hints ----------- It is mandatory to test multiple parameters of TAD calling with 
hicFindTADs
 before making conclusions about the number of TADs in a given sample or before comparing TAD calling between multiple conditions. In order to compare numerous TAD calling parameters at once, it is recommended to use 
pyGenomeTracks
, below you can find a plot where multiple TAD calling parameters are displayed for 
Drosophila melanogaster
 embryos: .. image:: $PATH_TO_IMAGES/hicFindTADs_TAD_calling_comparison.png :width: 65 % We can see that the fourth set of 
hicFindTADs
 parameters with a threshold of 0.001 gives the best results in terms of TAD calling compared to the corrected Hi-C counts distribution and compared to the enrichment of H3K36me3, which is known to be enriched at TAD boundaries in 
Drosophila melanogaster
. 
_
_
___ For more information about HiCExplorer please consider our documentation on readthedocs.io
 .. 
readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html .. _Source: https://en.wikipedia.org/wiki/Standard_score#Calculation_from_raw_score .. _information: https://en.wikipedia.org/wiki/Topologically_associating_domain"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hichyperoptdetectloops/hicexplorer_hichyperoptDetectLoops/3.7.6+galaxy1	"Hyperopt parameter optimization loop detection ============================================== Compute with a protein peak data file (bed format) optimal parameters for the loop calling. Useful proteins are e.g. CTCF or cohesin. 
_
_
___ Usage ----- A command line example is available below (easily matchable in Galaxy using each field information): ̀
$ hicHyperoptDetectLoops -m matrix.cool -o hyperopt_result.txt --maximumNumberOfLoops 10000 --resolution 10000 --runs 100
 For more information about HiCExplorer please consider our documentation on readthedocs.io_. .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicinfo/hicexplorer_hicinfo/3.7.6+galaxy1	Get information about you Hi-C matrix ===================================== This scripts returns useful information about your Hi-C interaction matrix. However, the following information can be included if it is provided by the Hi-C interaction matrix meta data. In case the meta data is missing, the information can be less. .. code-block:: text # Matrix information file. Created with HiCExplorer's hicInfo version 3.2-dev File: /tmp/tmpyyll1kwi.cool Date: 2019-08-11T19:46:15.959678 Genome assembly: dm3 Size: 33,754 Bin_length: 5000 Number of chromosomes: 15 Non-zero elements: 35,857 The following columns are available: ['chrom' 'start' 'end'] Generated by: HiCMatrix-11-dev Cooler library version: cooler-0.8.5 HiCMatrix url: https://github.com/deeptools/HiCMatrix Interaction matrix created with: HiCExplorer-3.2-dev URL: https://github.com/deeptools/HiCExplorer Build statistics: File /tmp/tmpyyll1kwi.cool Sequenced reads 99983 Min rest. site distance 300 Max library insert size 1000 # count (percentage w.r.t. total sequenced reads) Pairs mappable, unique and high quality 52726 (52.73) Hi-C contacts 37321 (37.33) One mate unmapped 8777 (8.78) One mate not unique 3603 (3.60) Low mapping quality 34877 (34.88) # count (percentage w.r.t. mappable, unique and high quality pairs) dangling end 0 (0.00) self ligation (removed) 0 (0.00) One mate not close to rest site 0 (0.00) same fragment 15393 (29.19) self circle 0 (0.00) duplicated pairs 12 (0.02) # count (percentage w.r.t. total valid pairs used) inter chromosomal 5955 (15.96) Intra short range (< 20kb) 8853 (23.72) Intra long range (>= 20kb) 22513 (60.32) Read pair type: inward pairs 7145 (19.14) Read pair type: outward pairs 9731 (26.07) Read pair type: left pairs 7156 (19.17) Read pair type: right pairs 7334 (19.65) Please have in mind that the provided information depends on the tool and its version with which the matrix was created. | For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicinterintratad/hicexplorer_hicinterintratad/3.7.6+galaxy1	Calculate the ratio of inter-TAD contacts vs intra-TADs ======================================================= Toplogical domains (TADs) are large mainly self-interacting domains. Chromatin interactions occur with higher frequency within a TAD as between TADs. This tool supports to compute the ratio of contacts of a TAD (intra-TAD) with the contacts outside of it (inter-TAD). For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicmergedomains/hicexplorer_hicmergedomains/3.7.6+galaxy1	Merge of TAD domains ==================== Merges and computes the hierachy of TADs from different resolutions. For more information about HiCExplorer please consider our documentation on readthedocs.io_. .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicmergeloops/hicexplorer_hicmergeloops/3.7.6+galaxy1	"Merge detected loops ==================== This script merges the loop locations of different different resolutions. Loops need to have the following format: chr start end chr start end A merge happens if x and y position of a loop overlaps with x and y position of another loop; all loops are considered as an overlap within +/- the bin size of the lowest resolution. I.e. for a loop with coordinates x and y, the overlap to all other loops is searched for (x - lowest resolution) and (y + lowest resolution). If two or more locations should be merged, the one with the lowest resolution is taken as the merged loop. Example usage: 
$ hicMergeLoops -i gm12878_10kb.bedgraph gm12878_5kb.bedgraph gm12878_25kb.bedgraph -o merged_result.bedgraph -r 25000
 Please recall: We work with binned data i.e. the lowest resolution is therefore the one where we merge the most bases into one bin. In the above example the lowest resultion is 25kb, the highest resolution is 5kb. For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicmergematrixbins/hicexplorer_hicmergematrixbins/3.7.6+galaxy1	"Change matrix resolution ======================== 
hicMergeMatrixBins
 is used to decrease the resolution of a matrix. With this tool, you can for example create out of a 100 kb contact matrix a 1000 kb one: Number of bins to merge = 10 100 kb * 10 = 1000 kb = 1 Mb Depending on the downstream analyses to perform on a Hi-C matrix generated with HiCExplorer, one might need different bin resolutions. For example using 
hicPlotMatrix
 to display chromatin interactions of a whole chromosome will not produce any meaningful vizualisation if it is performed on a matrix at restriction sites resolution (unmerged). Furthermore, the higher the resolution of a matrix, the more detailed it is, which can make it difficult to interpret, especially if the read depth of the Hi-C data is not high enough. 
hicMergeMatrixBins
 address these issues by merging a given number of adjacent bins to reduce Hi-C matrices resolution. 
_
_
 Usage ----- To limit the loss of information, it is mandatory to perform 
hicMergeMatrixBins
 on matrices prior to any correction and any other bin merging (direct output from 
hicBuildMatrix
). After bin merging, 
hicCorrectMatrix
 must be used for downstream analyses requiring corrected matrices. 
_
_
 Output ------ 
hicMergeMatrixBins
 outputs a Hi-C matrix with reduced resolution. Below, we will develop the example of a Hi-C matrix in 
Drosophila melanogaster
 that we want to display at the whole X-chromosome scale and at the scale of a 1Mb region of the X chromosome. To do this, we performed two different bin merging using 
hicMergeMatrixBins
 on an uncorrected matrix built at the restiction sites resolution using 
hicBuildMatrix
. Starting from a matrix with bins of a median length of 529bp (restriction enzyme resolution, here DpnII), running 
hicMergeMatrixBins
 with a number of bins to merge of 3 produced a matrix with bins of a median length of 1661bp, while 
hicMergeMatrixBins
 with a number of bins to merge of 50 produced a matrix with bins of a median length of 29798bp. After the correction of these three matrices using 
hicCorrectMatrix
, we plotted them using 
hicPlotMatrix
 at the scale of the whole X-chromosome and at the scale of the X:2000000-3000000 region to see the effect of bin merging on the interactions visualization. - 
Effect of bins merging at the scale of a chromosome:
 .. image:: $PATH_TO_IMAGES/hicMergeMatrixBins_Xchr.png :width: 60 % When observed altogether, the plots above show that the merging of bins by 50 is the most adequate way to plot interactions for a whole chromosome in 
Drosophila melanogaster
 when starting from a matrix with bins of a median length of 529bp. - 
Effect of bins merging at the scale of a specific region:
 .. image:: $PATH_TO_IMAGES/hicMergeMatrixBins_Xregion.png :width: 60 % When observed altogether, the plots above show that the merging of bins by 3 is the most adequate way to plot interactions for a region of 1Mb in Drosophila melanogaster when starting from a matrix with bins of a median length of 529bp. 
_
_
___ | For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicnormalize/hicexplorer_hicnormalize/3.7.6+galaxy1	"Normalization of matrices ========================= 
hicNormalize
 normalizes either each matrix to a 0 - 1 value range, to the smallest read coverage or with a user given factor. 
_
_
 Output ------ The normalized matrices. 
_
_
 | For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. 
readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html .. 
Lieberman-Aiden et al. (2009)
: https://pubmed.ncbi.nlm.nih.gov/19815776/"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicpca/hicexplorer_hicpca/3.7.6+galaxy1	"Principal component analysis ============================ 
Lieberman-Aiden et al. (2009)
 demonstrated that open and closed chromatin domains throughout the genome occupy different spatial compartments in the nucleus, defined as A (activate) and B (inactive) compartments. 
hicPCA
 computes two eigenvector files based on the input matrix for an A / B compartment analysis following the computation steps detailed by 
Lieberman-Aiden et al. (2009)
: the transformation of the contact matrix into an observed vs. expected matrix and consecutively a Pearson correlation matrix shows a plaid pattern. These plaid pattern are called A and B. Applying a PCA on the obs/exp matrix gives the eigenvectors and Lieberman-Aiden shows that the values of the eigenvectors correspond to the distribution of genes and with features of open and closed chromatin. In some cases the first principal component corresponds to the two chromosomes arms and the second eigenvector to the plaid pattern. Therefore always the first two principal components needs to be returned and investigated. 
_
_
 Usage ----- This tool must be used on Hi-C contact matrices with large bins (over 20kb) using 
hicMergeMatrixBins
 and corrected with 
hicCorrectMatrix
. Using matrices with a too high resolution (small bins or at restriction enzyme resolution) might take several days to run (even with over 100 CPU) or will fail due to memory limitations. 
_
_
 Output ------ Two files are outputed by 
hicPCA
, one with the first (pca1) and one with the second (pca2) eigenvector as bigwig or bedgraph. These files can be plotted alongside Hi-C heatmaps, gene density or external datasets such as open chromatin or histone marks enrichment using 
pyGenomeTracks
 or 
hicPlotMatrix
. For example, below you can find a 
hicPlotMatrix
 of the Pearson correlation matrix derived from a contact matrix for chromosome 6 in mouse computed with 
hicTransform
 (which is part of A/B compartments computation). The optional data track at the bottom shows the first eigenvector for A/B compartment obtained using 
hicPCA
. .. image:: $PATH_TO_IMAGES/hicPCA.png :scale: 35 % 
_
_
___ | For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. 
readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html .. 
Lieberman-Aiden et al. (2009)
: https://pubmed.ncbi.nlm.nih.gov/19815776/"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicplotaverageregions/hicexplorer_hicplotaverageregions/3.7.6+galaxy1	Plot of hicAverageRegions matrix ================================ hicPlotAverage regions plots the data computed by hicAverageRegions. It shows the summed up and averaged regions around all given reference points. This is useful to determine the difference between samples if the TAD configuration is equal or changed. | For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicplotdistvscounts/hicexplorer_hicplotdistvscounts/3.7.6+galaxy1	"Relation of genomic distance and number of contacts =================================================== 
hicPlotDistVsCounts
 allows a quick comparison between multiple Hi-C matrices of the Hi-C counts enrichment at different genomic ranges / distances up to whole chromosome. Biological replicates should display the exact same distribution while samples coming from different cell-lines, treated versus untreated samples or mutant versus wild-type samples should display a different distribution at long and/or close range. The results of this tool usually reflect the proportion of long-range and short-range contacts calculated in each sample by 
hicQC
 (which is part of 
hicBuildMatrix
). Local TAD or contact enrichments will not impact the results computed by this tool, 
hicPCA
 is better suited for that purpose. When plotting multiple matrices, the denser ones (more coverage) are scaled down to match the sum of the smaller matrix of the comparison. 
_
_
 Usage ----- 
hicPlotDistVsCounts
 should be used on corrected matrices with large bins (e.g. at least 30 to 50kb bins), otherwise the curves will be spiky at longer ranges because of the sparness of the contacts, thus the likelyness of the samples will become hard to assess after a certain distance. 
hicPlotDistVsCounts
 is thus often ran after 
hicMergeMatrixBins
 and 
hicCorrectMatrix
. 
_
_
 Output ------ This program makes distance vs. Hi-C counts plots. It can use several matrix files to compare them. If the 
--perchr
 option is given, each chromosome is plotted independently. Below can be found an example output: .. image:: $PATH_TO_IMAGES/hicPlotDistVsCounts.png :scale: 50 % Here, we see that the samples from the first condition are not so well correlated, but they follow the same tendancies and are distinct from the two samples of the second condition. The later are well correlated and display enriched long-range contacts compared to the first condition samples. On the second graph below, the distance vs. Hi-C contact counts is computed and plotted per chromosome: .. image:: $PATH_TO_IMAGES/hicPlotDistVsCounts_result2.png :scale: 50 % 
_
_
___ | For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicplotmatrix/hicexplorer_hicplotmatrix/3.7.6+galaxy1	"Contact matrix plot ======================= 
hicPlotMatrix
 is a visualization tool for Hi-C contact matrices. It supports to plot genome-wide contact matrices, one or multiple chromosomes, a region or two regions against each other. Additionally it can plot the result of a principal component analysis obtained by 
hicPCA
 to have a better understanding of A / B compartments. 
_
_
 Usage ----- This tool can be used on any h5 or cool Hi-C contact matrix. It is noteworthy that for comparisons of 2 matrices or more, they must all have the same or similar number of contacts. 
_
_
 Output ------ 
hicPlotMatrix
 outputs a heatmap of a contact matrix in either png or svg format, below is an example of such a plot: .. image:: $PATH_TO_IMAGES/hicPlotMatrix.png :width: 50% Contact matrix of 
Drosophila melanogaster
 embryos visualized using 
hicPlotMatrix
. Hi-C matrix bins were merged to a 25 kb bin size using 
hicMergeMatrixBins
 and the matrix has then been corrected using 
hicCorrectMatrix
 before plotting. Another example is available below using Hi-C data published by 
Lieberman-Aiden &lt;https://pubmed.ncbi.nlm.nih.gov/19815776/&gt;
 in 2009, (
GSE18199 &lt;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE18199&gt;
). A Hi-C contact matrix has been plotted with the 
--perChr
 option and the first eigenvector (pca1) computed by 
hicPCA
. For this plot a pearson correlated matrix was used, which is computed by first creating an observed / expected matrix and then a pearson correlation matrix with 
hicTransform
. .. image:: $PATH_TO_IMAGES/SRR0279XX_perChr_eigenvector1.png :width: 70 % 
_
_
___ | For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html .. _Colormaps: https://matplotlib.org/examples/color/colormaps_reference.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicplotsvl/hicexplorer_hicplotsvl/3.7.6+galaxy1	Short vs long range contacts ============================ hicPlotSVL computes the ratio between short range and long range contacts per chromosome independently. Per sample one box plot is created and, if more than one sample is given, the computed ratios are assumed to be one distribution and a Wilcoxon rank-sum test under H0 'distributions are equal' is computed. All used data is written to a third raw data file. The distance to distinct short and long range contacts can be set via the –distance parameter; for short range the sum for all contacts smaller or equal this distance are computed, for long range contacts all contacts greater this distance. Usage ----- .. code-block:: text $ hicPlotSVL -m hmec_10kb.cool nhek_10kb.cool hmec_10kb.cool --distance 2000000 --threads 4 --plotFileName plot.png --outFileName pvalues.txt --outFileNameData rawData.txt This results in three files: The raw data containing the sums for short range, long range and ratio per sample and chromosome. .. code-block:: text # Created with HiCExplorer's hicPlotSVL 3.3 # Short range vs long range contacts per chromosome: raw data # Short range contacts: <= 2000000 # hmec_10kb.cool nhek_10kb.cool hmec_10kb.cool # Chromosome Ratio Sum <= 2000000 Sum > 2000000 Ratio Sum <= 2000000 Sum > 2000000 Ratio Sum <= 2000000 Sum > 2000000 1 3.0399769346724543 33476834 11012200 2.79740105237572 44262902 15822866 3.0399769346724543 33476834 11012200 2 2.7532203542810625 31723954 11522490 2.5007877714355438 47468438 18981394 2.7532203542810625 31723954 11522490 3 2.922650759458664 26251027 8981924 2.6235211241878442 39640848 15109788 2.922650759458664 26251027 8981924 4 2.7235598858451637 22474680 8251950 2.5572455199457864 37486882 14659086 2.7235598858451637 22474680 8251950 5 2.9585962905193712 22716268 7678056 2.752922527526723 35445722 12875670 2.9585962905193712 22716268 7678056 6 3.168274165465025 22872690 7219290 2.8602111006131703 33990211 11883812 3.168274165465025 22872690 7219290 7 3.1093346580597516 19603416 6304698 2.8021236966788887 29712823 10603680 3.1093346580597516 19603416 6304698 8 3.135391026076832 18355087 5854162 2.7964394470859024 28660624 10248970 3.135391026076832 18355087 5854162 9 4.1147978383348125 15395763 3741560 3.819940066283481 21994046 5757694 4.1147978383348125 15395763 3741560 10 3.448063050802953 17964043 5209894 3.1116673856502253 26270171 8442474 3.448063050802953 17964043 5209894 11 3.5924666993070407 18651850 5191934 3.1364875011923035 26240350 8366158 3.5924666993070407 18651850 5191934 12 3.6817551043464416 18640866 5063038 3.306662109403207 26101554 7893626 3.6817551043464416 18640866 5063038 13 3.476204237522881 11018462 3169682 3.0976674036654805 18922281 6108558 3.476204237522881 11018462 3169682 14 3.70550850832778 11164875 3013048 3.6226817463785164 17245704 4760480 3.70550850832778 11164875 3013048 15 4.607631079612186 11165313 2423222 4.567998349104569 15273742 3343640 4.607631079612186 11165313 2423222 16 4.397874357146307 10745775 2443402 3.890983210350018 14666462 3769346 4.397874357146307 10745775 2443402 17 5.809374740402161 12168235 2094586 5.3360710927739285 14154110 2652534 5.809374740402161 12168235 2094586 18 3.7647349280938895 9339833 2480874 3.485487446356812 15019063 4309028 3.7647349280938895 9339833 2480874 19 6.492239632778196 8466283 1304062 5.774337450385819 9368978 1622520 6.492239632778196 8466283 1304062 20 5.542933774973686 8962935 1617002 4.977679877778358 12009479 2412666 5.542933774973686 8962935 1617002 21 6.665622315255486 3910374 586648 6.1843701763589225 6554715 1059884 6.665622315255486 3910374 586648 22 8.063663557923096 4992327 619114 7.433759425439728 5932928 798106 8.063663557923096 4992327 619114 X 2.208752982178897 14424173 6530460 2.3130534357407995 27628734 11944702 2.208752982178897 14424173 6530460 Y 4.165021803993573 36294 8714 3.8063291139240505 45105 11850 4.165021803993573 36294 8714 MT The p-values between the samples: .. code-block:: text # Created with HiCExplorer's hicPlotSVL 3.3 # Short range vs long range contacts per chromosome, p-values of each distribution against each other distribution with Wilcoxon rank-sum # Short range contacts: <= 2000000 hmec_10kb.cool nhek_10kb.cool 0.28362036331636575 hmec_10kb.cool hmec_10kb.cool 1.0 nhek_10kb.cool hmec_10kb.cool 0.28362036331636575 The box plot: .. image:: $PATH_TO_IMAGES/plot_svl.png :width: 50% For more information about HiCExplorer please consider our documentation on readthedocs.io_. .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicplotviewpoint/hicexplorer_hicplotviewpoint/3.7.6+galaxy1	"Plot Viewpoint ============== 
hicPlotViewpoint
 plots the number of interactions around a specific reference point or region in the genome like a 4C analysis using Hi-C data. This plotting method allows to make long-range interactions more visible. 
_
_
 Usage ----- This tool must be used on Hi-C contact matrices corrected using 
hicCorrectMatrix
. 
_
_
 Output ------ 
hicPlotViewpoint
 outputs an image with the plotted interactions around the reference point in png or svg format, an example plot can be found bellow. An option allows to also output the interactions as a bedgraph file. .. image:: $PATH_TO_IMAGES/pulication_plots_viewpoint.png :width: 50 % 
_
_
___ | For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicquickqc/hicexplorer_hicquickqc/3.7.6+galaxy1	Quick QC ======== Get a quick impression on the quality of your Hi-C data. hicQuickQC considers the first n lines of two bam/sam files to get a first estimate of the quality of the data. It is highly recommended to set the restriction enzyme and dangling end parameter to get a good quality report. The default is to read the first 1,000,000 reads of the mapped bam files to get a quality estimate of the Hi-C data. For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicsummatrices/hicexplorer_hicsummatrices/3.7.6+galaxy1	"Summation of matrices ===================== 
hicSumMatrix
 is combining two (or more) contact matrices of the same size to one. This is useful if replicates of an Hi-C experiment should be merged into one contact matrix to increase the coverage of the data. With Hi-C, real contacts are sometimes difficult to be distinguished from noise, especially with a low contact count. The more contacts are given, the more likely it is that a high number of contacts are biologically releveant. It is therefore common to merge biological and technical replicates of Hi-C experiments and 
hicSumMatrix
 address this concern. 
_
_
 Usage ----- This tool takes two or more Hi-C contact matrices and sum the contacts into one matrix. It is recommended to use uncorrected and unmerged matrices as input (use the direct output from 
hicBuildMatrix
). Bin merging (
hicMergeMatrixBins
) and Hi-C contact matrix correction (
hicCorrectMatrix
) must be performed afterwards. 
_
_
 Output ------ 
hicSumMatrix
 outputs a Hi-C contact matrix comprising the sum of all the contacts of the Hi-C matrices used as input. 
_
_
___ | For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hictadclassifier/hicexplorer_hictadclassifier/3.7.2+galaxy0	Predict TADs based on a ML model ================================ Uses Supervised Learning to call TAD boundaries. One Hi-C matrix can be passed, from which a BED file will be produced containing the predicted boundary positions. By default, a EasyEnsembleClassifier as described in Liu et al.: “Exploratory Undersampling for Class-Imbalance Learning” will be used to call TADs. Internally this classifier relies on Resampling, Boosting and Bagging. Passed matrices will be observed/expected normalized by default. Alternatively, a 0 -1 range normalization can be used. Currently, only classifiers for 10, 25, 50 and 100 kb resolution are provided. For building own classifiers or tune existing ones, hicTrainClassifier can be used and passed with the saved_classifer argument. A simple usage example can be seen here: Usage ----- .. code-block:: text $ hicTADClassifier -m my_matrix.cool -o predictions --normalization_method obs_exp For more information about HiCExplorer please consider our documentation on readthedocs.io_. .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hictraintadclassifier/hicexplorer_hictraintadclassifier/3.7.2+galaxy0	"Train TAD predictor ==================== This program can be used to train new classifiers for hicTADClassifier. These classifiers can later be run to call boundaries for TADs. By default, an EasyEnsembleClassifier as described in Liu et al.: “Exploratory Undersampling for Class-Imbalance Learning” will be trained, but you can pass any sklearn classifier that allows for a warm start. You may also vary the resampling method and a range of hyperparameters to fine tune the model. Do mind to set the correct normalization method and resolution for the classifier. The program will check and raise warnings, when resolutions and normalization methods are mixed up. Also, a protein track file in the narrowPeak format with a threshold value may be passed to filter out low quality boundaries. The resulting classifier will be pickled at the specified out_file. A quick example can be seen here, where we varied the feature distance: ## <!-- <param name=""proteinFile"" value='hicTrainTADClassifier/ctcf_chr2.csv' /> --> Usage ----- .. code-block:: text $ hicTrainTADClassifier -m 'train_new' -f 'my_test_matrix.cool' -d 'domains.bed' -o 'new_classifier.data' -n 'range' -r 10000 --distance 18 For more information about HiCExplorer please consider our documentation on readthedocs.io_. .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hictransform/hicexplorer_hictransform/3.7.6+galaxy1	"Transformation of matrix for plotting ===================================== 
hicTransform
 computes a matrix based on one Hi-C contact matrix as input: - An 
observed/expected matrix
 obtained ""by dividing each entry in the contact matrix by the genome-wide average contact probability for loci at that genomic distance"" (
Lieberman-Aiden et al. (2009)
). This transformation allows to better assess long range interactions. - An 
observed/expected norm matrix
 which computes the expected matrix as EXP_i,j = sum(diagonal(i-j)) * sum(row(j)) * sum(row(i)) / sum(matrix) - An 
observed/expected non-zero values matrix
 which computes the expected matrix as the sum per genomic distance j divided by sum of non-zero contacts: sum(diagonal(j) / number of non-zero elements in diagonal(j) - A 
Pearson correlation matrix
 obtained by computing the Pearson correlation between each bin based on observed/expected values. This matrix transformation allows to better identify the bins that are entering in contact together, or not, at long ranges, and thus helps defining compartments in the nucleus (
hicPCA
). - A 
covariance matrix
, which is used as a basis for the Principal Component Analysis (PCA) to compute the eigenvectors outputed by 
hicTransform
. These matrices can be used with 
hicPlotMatrix
 or 
pyGenomeTracks
 for a visualization of the A / B compartment analysis. 
_
_
___ Output ------ From one Hi-C contact matrix, 
hicTransform
 outputs a matrix with the selected method applied. 
_
_
___ | For more information about HiCExplorer please consider our documentation on readthedocs.io
 .. 
readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html .. 
Lieberman-Aiden et al. (2009)
: https://pubmed.ncbi.nlm.nih.gov/19815776/"
toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicvalidatelocations/hicexplorer_hicvalidatelocations/3.7.6+galaxy1	"Validate locations ================== This script overlaps the loop locations with protein locations to determine the accuracy of the loop detection. Loops need to have format as follows: 
chr start end chr start end
 The protein peaks need to be in narrowPeaks or broadPeak format. A protein match is successfull if at the bin of the x and y location a protein peak is overlapped. A bin is assumed to have a protein if one or more protein peaks falling within the bin region. The value of the protein is not considered, only match or non-match. For more information about HiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://hicexplorer.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schicadjustmatrix/schicexplorer_schicadjustmatrix/4.1	Adjust all matrices =================== scHicAdjustMatrix is a tool to keep or remove a list of chromosomes of all Hi-C matrices stored in the scool file. For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schiccluster/schicexplorer_schiccluster/4.1	"Clustering on raw data ====================== scHicCluster uses kmeans or spectral clustering to associate each cell to a cluster and therefore to its cell cycle. The clustering can be run on the raw data, on a kNN computed via the exact euclidean distance or via PCA. Please consider also the other clustering and dimension reduction approaches of the scHicExplorer suite such as 
scHicCluster
, 
scHicClusterMinHash
 and 
scHicClusterSVL
. They can give you better results, can be faster or less memory demanding. For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/"
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schicclustercompartments/schicexplorer_schicclustercompartments/4.1	"Clustering with dimension reduction via A/B compartments ======================================================== scHicClusterCompartments uses kmeans or spectral clustering to associate each cell to a cluster and therefore to its cell cycle. The clustering is applied on dimension reduced data based on the A/B compartments track. This approach reduces the number of dimensions from samples * (number of bins)^2 to samples * (number of bins). Please consider also the other clustering and dimension reduction approaches of the scHicExplorer suite such as 
scHicCluster
, 
scHicClusterMinHash
 and 
scHicClusterSVL
. They can give you better results, can be faster or less memory demanding. For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/"
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schicclusterminhash/schicexplorer_schicclusterminhash/4.1	"Clustering with dimension reduction via MinHash =============================================== scHicClusterMinHash uses kmeans or spectral clustering to associate each cell to a cluster and therefore to its cell cycle. The clustering is applied on dimension reduced data based on an approximate kNN search with the local sensitive hashing technique MinHash. This approach reduces the number of dimensions from samples * (number of bins)^2 to samples * samples. The clustering is applied on dimension reduced data based on an approximate kNN search with the local sensitive hashing technique MinHash. This approach reduces the number of dimensions from samples * (number of bins)^2 to samples * samples. Please consider also the other clustering and dimension reduction approaches of the scHicExplorer suite such as 
scHicCluster
, 
scHicClusterMinHash
 and 
scHicClusterSVL
. They can give you better results, Please consider also the other clustering and dimension reduction approaches of the scHicExplorer suite such as 
scHicCluster
, 
scHicClusterCompartments
 and 
scHicClusterSVL
. They can give you better results, can be faster or less memory demanding. can be faster or less memory demanding. For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/"
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schicclustersvl/schicexplorer_schicclustersvl/4.1	"Clustering with dimension reduction via short vs long range ratio ================================================================= scHicClusterSVL uses kmeans or spectral clustering to associate each cell to a cluster and therefore to its cell cycle. The clustering is applied on dimension reduced data based on the ratio of short vs long range contacts per chromosome. This approach reduces the number of dimensions from samples * (number of bins)^2 to samples * (number of chromosomes). Please consider also the other clustering and dimension reduction approaches of the scHicExplorer suite such as 
scHicCluster
, 
scHicClusterMinHash
 and 
scHicClusterSVL
. They can give you better results, can be faster or less memory demanding. For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/"
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schicconsensusmatrices/schicexplorer_schicconsensusmatrices/4.1	Consensus matrices ================== scHicConsensusMatrices creates one consensus matrix for each cluster based on the clustered samples. The consensus matrices are normalized to an equal read coverage level and are all stored in one s cool file. For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schiccorrectmatrices/schicexplorer_schiccorrectmatrices/4.1	Correct all matrices ==================== scHicCorrectmatrices is a tool to correct all Hi-C matrices stored in the provided scool file using HiCExplorer's KR algorithm, which is detailed here: https://hicexplorer.readthedocs.io/en/latest/content/tools/hicCorrectMatrix.html For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schiccreatebulkmatrix/schicexplorer_schiccreatebulkmatrix/4.1	Create bulk matrix ================== scHicCreateBulkMatrix adds all available single-cell matrices of a scool file together and writes it as a single matrix to a cool file. For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schicdemultiplex/schicexplorer_schicdemultiplex/4.1	"Demultiplexing of raw fastq files ================================= scHicDemultiplex demultiplexes fastq files from Nagano 2017: ""Cell-cycle dynamics of chromosomal organization at single-cell resolution"" according their barcodes to a seperated forward and reverse strand fastq files per cell. For other datasets, a third-party demultiplexing strategy must be used. Afterwards, the demultiplexed mapped data can be used with HiCExplorer 
hicBuildMatrix
 to create single cell .cool matrices that must be stored in a .scool file using 
scHicMergeToSCool
, in order to be used for downstream analyses in the scHiCExplorer suite. For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/"
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schicinfo/schicexplorer_schicinfo/4.1	Consensus matrices ================== scHicInfo gives information about the provided single-cell Hi-C scool file. For example: .. code-block:: Filename: nagano2017_raw.scool Contains 3882 single-cell matrices The information stored via cooler.info of the first cell is: bin-size 1000000 bin-type fixed creation-date 2019-05-16T11:46:31.826214 format HDF5::Cooler format-url https://github.com/mirnylab/cooler format-version 3 generated-by cooler-0.8.3 genome-assembly unknown metadata {} nbins 2744 nchroms 35 nnz 55498 storage-mode symmetric-upper sum 486056 For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schicmergematrixbins/schicexplorer_schicmergematrixbins/4.1	"Change the resolution of the scHi-C matrices ============================================ 
scHicMergeMatrixBins
 is used to decrease the resolution of single cell matrices stored in a scool file by merging their adjacent bins. With this tool, you can for example create out of a 5kb contact matrix a 50kb one: Number of bins to merge = 10 5kb * 10 = 50k Depending on the downstream analyses to perform on single cell Hi-C matrices generated with HiCExplorer, one might need different bin resolutions, for instance during clustering or for plotting small or large regions of consensus matrices. The best practice is to merge bins in uncorrected matrices, and correct the lower resolution matrices using 
scHicCorrectMatrices
 subsequently. For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/"
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schicmergetoscool/schicexplorer_schicmergetoscool/4.1	Merge cool matrices to one scool matrix ======================================= Creates out of n cool files created by HiCExplorer's hicBuildMatrix (each file representing a single cell) one scool file containing the n matrices. For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schicnormalize/schicexplorer_schicnormalize/4.1	Normalize matrices ================== scHicNormalize scales the read coverage of all matrices stored in the provided scool file to the matrix displaying the lowest read coverage in the said file. This is a highly parallelized version of HiCExplorer's ̀ hicNormalize`, for more information, you can refer to this page: https://hicexplorer.readthedocs.io/en/latest/content/tools/hicNormalize.html For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schicplotclusterprofiles/schicexplorer_schicplotclusterprofiles/4.1	Plot cluster profiles ===================== scHicPlotClusterProfiles plots the profile of each scHi-C interaction matrix associated to a cluster, within the cluster the matrices can be ordered either via their short vs long range ratio or by the order of the file. This tool is useful as a quality control of a cluster to check if all matrices assigned to that cluster are displaying similar contacts distribution. .. image:: $PATH_TO_IMAGES/clusters_svl_spectral.png :width: 50% For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schicplotconsensusmatrices/schicexplorer_schicplotconsensusmatrices/4.1	Plot consensus matrices ======================= scHicPlotConsensusMatrices plots the consensus (average) matrix of a cluster. This tool is useful to compare the single cell clusters. .. image:: $PATH_TO_IMAGES/consensus_svl_spectral.png :width: 50% For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/
toolshed.g2.bx.psu.edu/repos/iuc/schicexplorer_schicqualitycontrol/schicexplorer_schicqualitycontrol/4.1	Quality control =============== scHicQualityControl removes scHi-C interaction matrices with a too low read coverage or density. It creates four output files: 1. Read coverage plot .. image:: $PATH_TO_IMAGES/read_coverage.png 2. Density plot .. image:: $PATH_TO_IMAGES/density.png 3. Quality report .. code-block:: scHi-C sample contained 3882 cells: Number of removed matrices containing bad chromosomes 0 Number of removed matrices due to low read coverage (< 100000): 1374 Number of removed matrices due to too many zero bins (< 0.02 density, within 30000000 relative genomic distance): 610 2508 samples passed the quality control. Please consider matrices with a low read coverage may be the matrices with a low density and overlap therefore. 4. The scHi-C scool matrix with the filtered matrices. For more information about scHiCExplorer please consider our documentation on readthedocs.io_ .. _readthedocs.io: http://schicexplorer.readthedocs.io/
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_annotate/hyphy_annotate/2.5.83+galaxy0	"HyPhy Annotate (label-tree) =========================== Brief description ----------------- This tool uses the 
label-tree
 function from HyPhy to annotate a phylogenetic tree. It allows users to select a subset of leaves using either a regular expression or a list of sequence names, and then apply a specified label to these selected branches. The tool also provides options for rerooting the tree, inverting the selection, and defining strategies for labeling internal and leaf nodes. This functionality is crucial for customizing tree visualizations and focusing on specific evolutionary events or clades. Input ----- 1. A phylogenetic tree in Newick format. 2. A regular expression or a list of sequence names to define the subset of leaves for annotation. Output ------ 1. Labeled tree: A Newick file containing the annotated phylogenetic tree. 2. Annotate Report: A Markdown file with a summary of the analysis. Tool options ------------ :: --tree The tree to annotate (Newick format). --regexp Use the following regular expression to select a subset of leaves. --list Line list of sequences to include in the set (required if --regexp is not supplied). --label Use the following label for annotation. --reroot Reroot the tree on this node ('None' to skip rerooting). --invert Invert selection. --internal-nodes Strategy for labeling internal nodes. None: Do not label internal nodes. All descendants: Label all descendants of the selected internal node. All descendants, no MRCA: Label all descendants of the selected internal node, excluding the most recent common ancestor. Some descendants: Label some descendants of the selected internal node. Parsimony: Label internal nodes based on parsimony. --leaf-nodes Strategy for labeling selected leaves. Label: Apply the specified label to selected leaf nodes. Skip: Do not label leaf nodes."
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_bgm/hyphy_bgm/2.5.83+galaxy0	"BGM : Bayesian Graphical Models for Co-evolving Sites ===================================================== 
What does this do?
 This tool identifies groups of sites in a sequence alignment that appear to be co-evolving. Co-evolving sites are those that experience substitutions along the same branches of a phylogenetic tree more often than expected by chance. This pattern of correlated substitutions can imply a functional or structural relationship between the sites. For example, a destabilizing mutation at one site might be compensated for by a mutation at another site to preserve the protein's structure or function. 
How does it work?
 BGM employs a Bayesian Graphical Model to uncover these dependencies. The core idea is to represent each site in the alignment as a node in a graph. The algorithm then seeks to find the edges (links) between these nodes that represent statistically significant correlations in substitution patterns. The process involves several steps: 1. 
Ancestral State Reconstruction:
 First, the method reconstructs the evolutionary history of the sequences using maximum likelihood. This allows the tool to map substitution events to specific branches of the phylogenetic tree. For coding data, only non-synonymous substitutions (those that change the amino acid) are considered. 2. 
Bayesian Graphical Model:
 The joint distribution of these substitution maps is then analyzed using a Bayesian graphical model. This model represents the probability of the observed substitution patterns given a particular network of dependencies between sites. 3. 
MCMC Sampling:
 To explore the vast space of possible network structures, BGM uses a Markov Chain Monte Carlo (MCMC) analysis. This method generates a random sample of network structures from the posterior distribution, meaning it finds the networks that are most likely given the data. 4. 
Identifying Co-evolving Sites:
 The links (edges) that appear most frequently in the sampled networks are the ones with the highest posterior support. These links connect the sites that are most likely to be co-evolving. 
Input
 * A multiple sequence alignment in FASTA or NEXUS format. * A phylogenetic tree in Newick format. The names of the sequences in the alignment must match the names of the tips in the tree. 
Output
 * 
JSON file:
 A JSON file containing the detailed results of the analysis, including the posterior probabilities of the links between sites. (See http://hyphy.org/resources/json-fields.pdf for a description of the fields). * 
Markdown report:
 A summary report in Markdown format. A custom visualization module for viewing BGM results is available at http://vision.hyphy.org/BGM. 
Tool Options
 * 
Type of data:
 The type of sequence data in the alignment file. * 
nucleotide
: For DNA or RNA sequences. * 
amino-acid
: For protein sequences. * 
codon
: For coding DNA sequences. This is the default. * 
Genetic code:
 If using codon data, the genetic code to use for translation. * 
Substitution model:
 If using amino-acid data, the substitution model to use. * 
Set of branches to test:
 The set of branches in the phylogeny to consider for the analysis. * 
All branches
: (Default) Use all branches in the tree. * 
Internal branches
: Use only the internal branches. This can be useful for studying pathogen evolution within a host, for example, where terminal branches might represent polymorphism rather than fixed differences. * 
Leaf branches
: Use only the terminal (leaf) branches. * 
Unlabeled branches
: If the Newick tree is annotated with labels, use only the branches that do not have a label. * 
Custom
: Specify a custom set of branches by providing a label. * 
Length of MCMC chain:
 The total number of steps in the MCMC simulation. A longer chain will explore the space of possible networks more thoroughly but will take longer to run. * 
Number of samples to discard for burn-in:
 The initial portion of the MCMC chain is often discarded to allow the simulation to converge to the posterior distribution. This parameter specifies the number of initial samples to discard. * 
Number of steps to extract from chain sample:
 The number of samples to draw from the MCMC chain after the burn-in period. These samples are used to estimate the posterior probabilities of the links. * 
Maximum number of parents allowed per node:
 This parameter controls the complexity of the graphical model. It sets the maximum number of other sites that can directly influence a given site. Increasing this number can reveal more complex dependency networks but also significantly increases the computational complexity. * 
Minimum number of substitutions per site:
 Sites with very few substitutions provide little information for detecting co-evolution. This parameter allows you to filter out such low-complexity sites from the analysis. 
Further Reading
 For more information, please see the HyPhy documentation: http://hyphy.org/methods/selection-methods/#BGM"
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_busted/hyphy_busted/2.5.83+galaxy0	"BUSTED : Bayesian UnresTricted Test of Episodic Diversification =============================================================== What question does this method answer? -------------------------------------- Is there evidence that some sites in the alignment have been subject to positive diversifying selection, either pervasive (throughout the evolutionary tree) or episodic (only on some lineages)? In other words, BUSTED asks whether a given gene has been subject to positive, diversifying selection at any site, at any time. If a priori information about lineages of interest is available (e.g., due to migration, change in the environment, etc.), then BUSTED can be restricted to test for selection only on a subset of tree lineages, potentially boosting power. Recommended Applications ------------------------ 1. Annotating a collection of alignments with a binary attribute: has this alignment been subject to positive diversifying selection (yes/no)? 2. Testing small or low-divergence alignments (i.e. ~30 sequences) for evidence of positive diversifying selection, where neither branch nor site level methods have sufficient power. Brief description ----------------- BUSTED (Branch-site Unrestricted Statistical Test for Episodic Diversification) is a powerful tool for detecting gene-wide evidence of episodic positive selection. It works by fitting a codon model to the data and comparing a null model, which does not allow for positive selection, to an alternative model that does. If the alternative model provides a statistically significant better fit to the data, then we can conclude that there is evidence for positive selection. The core of BUSTED is a random effects branch-site model. This model allows the selection pressure (represented by the omega ratio, dN/dS) to vary both among sites in the alignment and across branches in the phylogenetic tree. The model includes three rate classes for omega: one for negative/purifying selection (omega < 1), one for neutral evolution (omega = 1), and one for positive/diversifying selection (omega > 1). BUSTED tests for positive selection by comparing a constrained model (where omega is not allowed to be greater than 1) to an unconstrained model (where omega can be greater than 1). A likelihood ratio test is used to determine if the unconstrained model is a significantly better fit to the data. If it is, then there is evidence for positive selection acting on the gene. MSS Methodology --------------- BUSTED can also incorporate models of selection on synonymous substitutions (MSS models). This is a new comparative framework for estimating selection on synonymous substitutions. These models account for selection by partitioning synonymous substitutions into multiple classes and estimating relative substitution rates for each, while also considering confounders like mutation bias. This framework allows for the study of selection on synonymous substitutions in diverse taxa without prior assumptions about the driving forces. For more information, please see the source publication: http://pubmed.ncbi.nlm.nih.gov/40129111/ Input ----- 1. A 
FASTA
 sequence alignment. 2. A phylogenetic tree in the 
Newick
 format Note: the names of sequences in the alignment must match the names of the sequences in the tree. Output ------ A JSON file with analysis results (http://hyphy.org/resources/json-fields.pdf). For each tested branch the analysis will infer the appropriate number of selective regimes, and whether or not there is statistical evidence of positive selection on that branch. A custom visualization module for viewing these results is available (see http://vision.hyphy.org/BUSTED for an example) Further reading --------------- http://hyphy.org/methods/selection-methods/#busted Tool options ------------ :: --code Which genetic code to use --alignment An in-frame codon alignment in one of the formats supported by HyPhy. --tree A phylogenetic tree (optionally annotated with {}). --branches Which branches should be tested for selection? All [default] : test all branches Internal : test only internal branches (suitable for intra-host pathogen evolution for example, where terminal branches may contain polymorphism data) Leaves: test only terminal (leaf) branches Unlabeled: if the Newick string is labeled using the {} notation, test only branches without explicit labels (see http://hyphy.org/tutorials/phylotree/) --kill-zero-lengths Automatically delete internal zero-length branches for computational efficiency. Advanced parameters ................... --srv Include synonymous rate variation in the model. --grid-size The number of points in the initial distributional guess for likelihood fitting. --starting-points The number of initial random guesses to seed rate values optimization. --syn-rates The number of synonymous rate classes to include in the model [1-10, default 3]. --rates The number of non-synonymous rate classes to include in the model [1-10, default 3]. --multiple-hits Include support for multiple nucleotide substitutions. None: No correction. Double: Allow double substitutions. Double+Triple: Allow double and triple substitutions. --error-sink [Advanced experimental setting] Include a rate class to capture misalignment artifacts. --mss Include support for multiple synonymous rate class substitutions. --mss-type How to partition synonymous codons into classes. Full: Each set of codons mapping to the same amino-acid class have a separate substitution rate (Valine == neutral) SynREV: Each set of codons mapping to the same amino-acid class have a separate substitution rate (mean = 1) SynREV2: Each pair of synonymous codons mapping to the same amino-acid class and separated by a transition have a separate substitution rate (no rate scaling)) SynREV2g: Each pair of synonymous codons mapping to the same amino-acid class and separated by a transition have a separate substitution rate (Valine == neutral). All between-class synonymous substitutions share a rate. SynREVCodon: Each codon pair that is exchangeable gets its own substitution rate (fully estimated, mean = 1) Random: Random partition (specify how many classes; largest class = neutral) Empirical: Load a TSV file with an empirical rate estimate for each codon pair File: Load a TSV partition from file (prompted for neutral class) Codon-file: Load a TSV partition for pairs of codons from a file (prompted for neutral class) --mss-file File defining the model partition. --mss-reference-rate Normalize relative to these rates. --mss-classes How many codon rate classes. --mss-neutral Designation for the neutral substitution rate."
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_cfel/hyphy_cfel/2.5.83+galaxy0	"Contrast-FEL : A Test for Differences in Selective Pressures at Individual Sites among Clades and Sets of Branches ================================================================================================================== Brief description ----------------- Contrast-FEL (Fixed Effects Likelihood) is a statistical method designed to identify individual sites within genes that experience different selective pressures among various clades or sets of branches in a phylogenetic tree. It extends the traditional Fixed Effects Likelihood (FEL) method to detect differences in ω ratios (the ratio of nonsynonymous to synonymous substitution rates) using a likelihood-ratio test. The intuition behind Contrast-FEL is that if different evolutionary pressures are acting on different parts of a phylogenetic tree, then the ω ratios at specific sites might vary significantly between these groups of branches. For example, a site might be under strong purifying selection in one clade but under positive selection in another. Contrast-FEL allows for the direct comparison of these selective regimes at a site-by-site level. This method is particularly useful for testing evolutionary hypotheses that involve comparing selective pressures among predefined sets of branches. It provides site-level resolution for comparing selective pressures, which is often lacking in other approaches. Simulations have shown that Contrast-FEL offers good power and maintains control over false positive rates when the model is correctly specified. Methodology and Intuition ------------------------- Contrast-FEL operates by comparing evolutionary rates at individual sites across different predefined branch sets in a phylogenetic tree. The core idea is to detect shifts in selective pressure (quantified by the ω ratio, dN/dS) that are specific to certain lineages or clades. 1. 
Site-wise Likelihood Calculation:
 For each site in the alignment, Contrast-FEL estimates the synonymous (α) and nonsynonymous (β) substitution rates. Crucially, it estimates a separate nonsynonymous rate (β) for each specified branch set, while the synonymous rate (α) is shared across all branches. This allows for direct comparison of selective pressures. 2. 
Hypothesis Testing:
 The method then performs a likelihood-ratio test (LRT) for each site. The null hypothesis is that the ω ratios are the same across all specified branch sets for that site. The alternative hypothesis is that at least one branch set has a significantly different ω ratio. 3. 
Permutation Testing (Optional):
 To account for potential biases and improve the robustness of significance calls, Contrast-FEL can perform permutation tests. In this approach, branch labels are permuted across the tree, and the analysis is re-run multiple times. This generates an empirical null distribution of LRT statistics, which can then be used to calculate more accurate p-values. 4. 
False Discovery Rate (FDR) Control:
 To address the multiple testing problem inherent in site-wise analyses, Contrast-FEL applies a False Discovery Rate (FDR) correction (e.g., Benjamini-Hochberg procedure) to the p-values. This helps to control the proportion of false positives among the significant sites. 5. 
Interpretation of ω Ratios:
 * 
ω < 1 (Purifying Selection):
 Nonsynonymous mutations are deleterious and are removed by selection. * 
ω = 1 (Neutral Evolution):
 Nonsynonymous mutations are neither advantageous nor deleterious. * 
ω > 1 (Positive Selection):
 Nonsynonymous mutations are advantageous and are favored by selection. By comparing the site-specific ω ratios among different branch sets, Contrast-FEL can pinpoint sites that have undergone adaptive evolution (positive selection) or strong functional constraint (purifying selection) in specific lineages, providing insights into the evolutionary history and functional divergence of genes. Input ----- 1. A 
FASTA
 sequence alignment. 2. A phylogenetic tree in the 
Newick
 format Note: the names of sequences in the alignment must match the names of the sequences in the tree. Output ------ A JSON file with analysis results. A Markdown file with a summary of the analysis. Tool options ------------ :: --code Which genetic code to use. --branch-set The set of branches to use for testing. --srv Include synonymous rate variation in the model. Yes (recommended): Allow synonymous rates to vary from site to site. No: Do not allow synonymous rates to vary from site to site. --permutations Perform permutation significance tests. --p-value Significance value for site-tests. --q-value Significance value for FDR reporting. Advanced parameters ................... --limit-to-sites Only analyze sites whose 1-based indices match the following list (null to skip). --save-lf-for-sites For sites whose 1-based indices match the following list, write out likelihood function snapshots (null to skip). --intermediate-fits Use/save parameter estimates from 'initial-guess' model fits to a JSON file. --kill-zero-lengths Automatically delete internal zero-length branches for computational efficiency."
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_conv/hyphy_conv/2.5.83+galaxy0	HyPhy-CONV: Translate an in-frame codon alignment to proteins ============================================================= This tool takes a codon-aligned fasta file and outputs the amino acid sequence it represents, with the option to keep or skip deletions in the input file.
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_fade/hyphy_fade/2.5.83+galaxy0	"FADE : FUBAR Approach to Directional Evolution ============================================== What question does this method answer? -------------------------------------- Which site(s) in an alignment evolve towards to or away from a particular residue. Recommended Applications ------------------------ Screen protein sequence alignments where the direction of evolution can be resolved (via tree rooting, e.g. using an outgroup) to find sites which evolve differently from a standard protein model (selected by the user), or a gene-average model (GTR) to find evidence of directional selection. Brief description ----------------- FADE (FUBAR Approach to Directional Evolution) is a fast method to test whether or not a subset of sites in a protein alignment evolve towards a particular residue along a subset of branches at accelerated rates compared to a reference model. FADE uses a random effects model and latent Dirichlet allocation (LDA)-inspired approximation methods to allocate sites to rate classes. The intuition behind FADE is to detect directional selection, where amino acid substitutions are consistently biased towards a particular residue type. This can be indicative of adaptation to new functional constraints or environments. By comparing the observed substitution patterns to a null model (e.g., a standard protein substitution model or a gene-average model), FADE identifies sites that exhibit significant directional bias in their evolutionary trajectory. Input ----- 1. A 
FASTA
 sequence alignment of protein sequences. 2. A 
rooted
 phylogenetic tree in the 
Newick
 format Note: the names of sequences in the alignment must match the names of the sequences in the tree. Output ------ A JSON file with analysis results (http://hyphy.org/resources/json-fields.pdf). A custom visualization module for viewing these results is available (see http://vision.hyphy.org/FADE for an example) Further reading --------------- http://hyphy.org/methods/selection-methods/#FADE Tool options ------------ :: --model The baseline substitution model to use [default] use GTR --branches Which branches should be tested for selection? All [default] : test all branches Internal : test only internal branches (suitable for intra-host pathogen evolution for example, where terminal branches may contain polymorphism data) Leaves: test only terminal (leaf) branches Unlabeled: if the Newick string is labeled using the {} notation, test only branches without explicit labels (see http://hyphy.org/tutorials/phylotree/) --grid The number of grid points Smaller : faster Larger : more precise posterior estimation but slower default value: 20 --method Inference method to use Variational-Bayes : 0-th order Variational Bayes approximation; fastest [default] Metropolis-Hastings : Full Metropolis-Hastings MCMC algorithm; orignal method [slowest] Collapsed-Gibbs : Collapsed Gibbs sampler [intermediate speed] --chains How many MCMC chains to run (does not apply to Variational-Bayes) default value: 5 --chain-length MCMC chain length (does not apply to Variational-Bayes) default value: 2,000,000 --burn-in MCMC chain burn in (does not apply to Variational-Bayes) default value: 1,000,000 --samples MCMC samples to draw (does not apply to Variational-Bayes) default value: 1,000 --concentration_parameter The concentration parameter of the Dirichlet prior default value: 0.5 Output ------ A JSON file with analysis results (http://hyphy.org/resources/json-fields.pdf). A custom visualization module for viewing these results is available (see http://vision.hyphy.org/FADE for an example) A Markdown file with a summary of the analysis."
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_fel/hyphy_fel/2.5.83+galaxy0	"FEL : Fixed effects likelihood ============================== What question does this method answer? -------------------------------------- FEL (Fixed Effects Likelihood) is a statistical method used to identify individual sites in a gene that are subject to pervasive diversifying selection. It addresses the question: Which specific sites in a gene show evidence of positive selection that has been consistently maintained across the entire evolutionary phylogeny of the analyzed sequences? Recommended Applications ------------------------ The phenomenon of pervasive selection is generally most prevalent in pathogen evolution and any biological system influenced by evolutionary arms race dynamics (or balancing selection), including adaptive immune escape by viruses. As such, FEL is ideally suited to identify sites under positive selection which represent candidate sites subject to strong selective pressures across the entire phylogeny. FEL is our recommended method for analyzing small-to-medium size datasets when one wishes only to study pervasive selection at individual sites. Brief description ----------------- FEL (Fixed Effects Likelihood) is a powerful method for detecting pervasive positive or negative selection at individual sites in a coding sequence. It operates by estimating site-wise synonymous (alpha, dS) and non-synonymous (beta, dN) substitution rates using a maximum likelihood approach. For each site, FEL then performs a likelihood ratio test (LRT) to compare a null model (where dN = dS) against an alternative model (where dN != dS). A significant p-value from this test indicates that the site is under selection. The method aggregates information across all branches of the phylogenetic tree, making it suitable for identifying sites under pervasive diversifying selection (dN > dS) or pervasive purifying selection (dN < dS). While primarily designed for pervasive selection, FEL can also infer an additional nuisance parameter for the non-synonymous rate on branches not selected for testing, allowing for analysis of a subset of branches. 
Intuition:
 Imagine you're looking at a gene's evolution across different species. Some parts of the gene might change a lot (diversifying selection), while others stay the same (purifying selection). FEL helps pinpoint the exact ""letters"" (sites) in the gene that are consistently under pressure to change or stay the same throughout its evolutionary history. It does this by comparing how often synonymous (silent) changes happen versus non-synonymous (amino acid altering) changes at each site. If non-synonymous changes happen significantly more often, it suggests positive selection. Input ----- 1. A 
FASTA
 sequence alignment. 2. A phylogenetic tree in the 
Newick
 format Note: the names of sequences in the alignment must match the names of the sequences in the tree. Output ------ A JSON file with analysis results (http://hyphy.org/resources/json-fields.pdf). A custom visualization module for viewing these results is available (see http://vision.hyphy.org/FEL for an example) Further reading --------------- http://hyphy.org/methods/selection-methods/#FEL Tool options ------------ :: --alignment [required] An in-frame codon alignment in one of the formats supported by HyPhy. --tree [conditionally required] A phylogenetic tree (optionally annotated with {}). --code Which genetic code to use (see tool form for available options). --multiple-hits Include support for multiple nucleotide substitutions. Double : Include branch-specific rates for double nucleotide substitutions. Double+Triple : Include branch-specific rates for double and triple nucleotide substitutions. None [default] : Use standard models which permit only single nucleotide changes to occur instantly. --site-multihit Estimate multiple hit rates for each site. This option is available only if 'Include support for multiple nucleotide substitutions' is set to 'Double' or 'Double+Triple'. Estimate [default] : Estimate multiple hit rates. No : Do not estimate multiple hit rates. --branches Which branches should be tested for selection? All [default] : test all branches. Internal : test only internal branches (suitable for intra-host pathogen evolution for example, where terminal branches may contain polymorphism data). Leaves: test only terminal (leaf) branches. Unlabeled: if the Newick string is labeled using the {} notation, test only branches without explicit labels (see http://hyphy.org/tutorials/phylotree/). Custom : Enter a branch label. --pvalue The significance level used to determine significance (default: 0.1, range: 0 to 1). --srv Include site-to-site synonymous rate variation? Yes [default] : Allow synonymous rates to vary from site to site. No : Do not allow synonymous rates to vary. --ci Compute profile likelihood confidence intervals for each variable site (default: No). Advanced Attributes ------------------- :: --resample Perform parametric bootstrap resampling to derive site-level null LRT distributions. Warning: This will result in a significantly slower analysis. A value of 0 means no resampling is performed. This parameter specifies the maximum number of replicates per site (default: 0, range: 0 to 1000). --restrict-sites Restrict FEL analysis to a subset of sites. If Yes, allows specifying a subset of sites for analysis. Yes : Restrict analysis to a subset of sites. No [default] : Do not restrict analysis to a subset of sites. --limit-to-sites Only analyze sites whose 1-based indices match the following list (null to skip). This option is available only if 'Restrict FEL analysis to a subset of sites' is set to 'Yes'. Comma-separated list of site indices. --save-lf-for-sites For sites whose 1-based indices match the following list, write out likelihood function snapshots (empty string to skip). This option is available only if 'Restrict FEL analysis to a subset of sites' is set to 'Yes'. Comma-separated list of site indices. --precision Optimization precision settings for preliminary fits. Standard [default] Reduced for faster fitting --kill-zero-lengths Automatically delete internal zero-length branches for computational efficiency. Yes [default] : Automatically delete internal zero-length branches for computational efficiency (will not affect results otherwise). Constrain : Keep zero-length branches, but constrain their values to 0. No : Keep all branches. --full-model Perform branch length re-optimization under the full codon model (default: Yes). If true, re-optimizes branch lengths under the full codon model. ;"
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_fubar/hyphy_fubar/2.5.83+galaxy0	"FUBAR : Faste Unbiased Bayesian AppRoximation ============================================= What question does this method answer? -------------------------------------- Which site(s) in a gene are subject to pervasive, i.e. consistently across the entire phylogeny, diversifying selection? Recommended Applications ------------------------ The phenomenon of pervasive selection is generally most prevalent in pathogen evolution and any biological system influenced by evolutionary arms race dynamics (or balancing selection), including adaptive immune escape by viruses. As such, FUBAR is ideally suited to identify sites under positive selection which represent candidate sites subject to strong selective pressures across the entire phylogeny. FUBAR is our recommended method for detecting pervasive selection at individual sites on large (> 500 sequences) datasets for which other methods have prohibitive runtimes, unless you have access to a computer cluster. Brief description ----------------- FUBAR (Fast, Unconstrained Bayesian AppRoximation) is a Bayesian method for detecting site-specific positive and negative selection. It is designed to be fast and efficient, making it suitable for large datasets. The core idea behind FUBAR is to model the non-synonymous (dN) and synonymous (dS) substitution rates at each site in a codon alignment. The ratio of these rates (dN/dS, or omega) is a measure of the selective pressure acting on a site. An omega value greater than 1 indicates positive (diversifying) selection, a value less than 1 indicates negative (purifying) selection, and a value of 1 indicates neutral evolution. FUBAR uses a Bayesian approach to infer the posterior distribution of dN and dS at each site. It does this by discretizing the dN and dS rates into a grid of points and then using a Bayesian graphical model to infer the posterior probability of each grid point for each site. This approach is much faster than traditional MCMC-based methods, which require long run times to converge. FUBAR offers three different methods for estimating the posterior distribution: * 
Variational-Bayes:
 A fast approximation method that is the recommended default. * 
Collapsed-Gibbs:
 A faster MCMC method. * 
Metropolis-Hastings:
 The original, slowest MCMC method. Input ----- 1. A 
FASTA
 sequence alignment. 2. A phylogenetic tree in the 
Newick
 format Note: the names of sequences in the alignment must match the names of the sequences in the tree. Output ------ A JSON file with analysis results (http://hyphy.org/resources/json-fields.pdf). A custom visualization module for viewing these results is available (see http://vision.hyphy.org/FUBAR for an example) Further reading --------------- http://hyphy.org/methods/selection-methods/#FUBAR Tool options ------------ :: --code Which genetic code to use --grid The number of grid points used to approximate the posterior distribution of dN and dS. A larger grid will provide a more accurate approximation but will also be slower. The default value of 20 is a good compromise between speed and accuracy. --method The inference method to use for estimating the posterior distribution. Variational-Bayes : 0-th order Variational Bayes approximation; fastest [default] Metropolis-Hastings : Full Metropolis-Hastings MCMC algorithm; orignal method [slowest] Collapsed-Gibbs : Collapsed Gibbs sampler [intermediate speed] --chains The number of MCMC chains to run. This is only applicable to the Metropolis-Hastings and Collapsed-Gibbs methods. A larger number of chains will provide a better exploration of the posterior distribution but will also be slower. default value: 5 --chain-length The length of each MCMC chain. This is only applicable to the Metropolis-Hastings and Collapsed-Gibbs methods. A longer chain will provide a better exploration of the posterior distribution but will also be slower. default value: 2,000,000 --burn-in The number of samples to discard from the beginning of each MCMC chain. This is done to ensure that the chain has converged to the posterior distribution. This is only applicable to the Metropolis-Hastings and Collapsed-Gibbs methods. default value: 1,000,000 --samples The number of samples to draw from each MCMC chain after the burn-in period. These samples are used to estimate the posterior distribution. This is only applicable to the Metropolis-Hastings and Collapsed-Gibbs methods. default value: 1,000 --concentration_parameter The concentration parameter of the Dirichlet prior on the grid weights. default value: 0.5 --non-zero Enforce non-zero synonymous rates on the grid. This is useful for calculating dN/dS ratios, as it prevents division by zero. --kill-zero-lengths Automatically delete internal zero-length branches for computational efficiency. This will not affect the results."
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_gard/hyphy_gard/2.5.83+galaxy0	"GARD : Genetic Algorithms for Recombination Detection. ====================================================== 
What does this do?
 This tool screens an alignment of sequences for evidence of recombination in one or more sequences. The main idea is that if sufficient recombination has occurred, then no single phylogenetic tree will properly fit the entire length of the alignment and instead a separate tree will be preferred for each 
nonrecombinant
 segment. 
Methodology
 GARD (Genetic Algorithm for Recombination Detection) implements a heuristic approach to screening alignments of sequences for recombination. It uses the CHC genetic algorithm to search for phylogenetic incongruence among different partitions of the data. The number of partitions is determined using a step-up procedure, while the placement of breakpoints is searched for with the GA. The best fitting model (based on c-AIC) is returned; and additional post-hoc tests run to distinguish topological incongruence from rate-variation. 
The Intuition
 Imagine you have a long DNA sequence, and you suspect that different parts of this sequence might have evolved under different evolutionary histories due to recombination events. If you try to build a single phylogenetic tree for the entire sequence, it might not accurately represent the relationships between the organisms. GARD addresses this by looking for ""breakpoints"" in the sequence where the evolutionary history changes. It uses a genetic algorithm to efficiently search for these breakpoints and then infers separate phylogenetic trees for each segment between the breakpoints. This allows for a more accurate understanding of the evolutionary history of recombinant sequences. 
Input
 A 
FASTA
 sequence alignment 
Output
 A JSON file with analysis results (http://hyphy.org/resources/json-fields.pdf). A custom visualization module for viewing these results is available (see http://vision.hyphy.org/GARD for an example) A Markdown file with a summary of the analysis. 
Further reading
 
Tool options
 :: --type type of alignment to screen Nucleotide [default]. Assumes aligned nucleotide data and screens the alignment using the general time reversible model of sequence evolution. This is the fastest option Protein Assumes aligned aminoacid sequences. One of several protein substitution models may be used to screen the alignment. Codon Assumes an in-frame coding sequence alignment. The Muse-Gaut 94 (GTR) model will be used to screen the alignment. Selecting this option will dramatically increase run times. --code Genetic code/translation table to use (for codon alignments). Default value: Universal --model The substitution model to use (for protein alignments). default value: JTT --rv Site to site rate variation. None: Constant rates. Gamma: Unit mean gamma distribution discretized into N rates. GDD: General discrete distribution on N rates. --rate-classes How many site rate classes to use (if GDD or Beta-Gamma are selected) default value: 4 --max-breakpoints Maximum number of breakpoints to consider. --mode Run mode. Normal: Default optimization and convergence settings. Faster: Reduce individual optimization precision and relax convergence settings."
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_meme/hyphy_meme/2.5.83+galaxy0	"MEME: Mixed Effects Model of Evolution ====================================== 
What question does this method answer?
 Which site(s) in a gene are subject to pervasive or 
episodic
, i.e. only on a single lineage or subset of lineages, diversifying selection? 
Recommended Applications
 The phenomenon of pervasive selection is generally most prevalent in pathogen evolution and any biological system influenced by evolutionary arms race dynamics (or balancing selection), including adaptive immune escape by viruses. MEME is ideally suited to identify sites under positive selection which represent candidate sites subject to strong selective pressures across the entire phylogeny or only on parts of the phylogeny. MEME is the sole method in HyPhy for detecting selection at individual sites that considers both pervasive and episodic selection. MEME is therefore our recommended method if maximum power is desired. 
Methodology
 MEME (Mixed Effects Model of Evolution) is a powerful statistical method for detecting sites in a coding alignment that have been subject to positive selection. It extends classical fixed-effects likelihood (FEL) models by allowing the non-synonymous substitution rate (dN) to vary from branch to branch at a given site. This ""mixed-effects"" approach provides increased power to detect episodic selection, where a site may be under positive selection in some lineages but under neutral or purifying selection in others. 
The Intuition
 Imagine you are studying the evolution of a gene across a group of species. Some sites in that gene might be under constant pressure to change (pervasive selection), while others might only experience this pressure for a short period of time in a specific lineage (episodic selection). For example, a virus might evolve a new protein to escape the host's immune system, but once the host population adapts, the pressure on that protein might disappear. Standard methods that assume a single dN/dS rate across the entire phylogeny might miss this kind of episodic selection. MEME addresses this by modeling the dN/dS ratio at each site as a mixture of two or more rate classes. For each site, MEME infers the probability that it evolves under each rate class on a given branch. This allows the model to identify sites that show evidence of positive selection (dN/dS > 1) even if that selection is confined to a small number of lineages. 
The Test
 For each site, MEME fits a baseline model where dN/dS is constrained to be less than or equal to 1, and an alternative model where a proportion of branches are allowed to have a dN/dS ratio greater than 1. A likelihood ratio test (LRT) is then used to determine if the alternative model provides a significantly better fit to the data. A significant p-value indicates that the site has experienced episodic diversifying selection. 
Input
 1. A 
FASTA
 sequence alignment. 2. A phylogenetic tree in the 
Newick
 format Note: the names of sequences in the alignment must match the names of the sequences in the tree. 
Output
 A JSON file with analysis results (http://hyphy.org/resources/json-fields.pdf). A custom visualization module for viewing these results is available (see http://vision.hyphy.org/MEME for an example) 
Further reading
 http://hyphy.org/methods/selection-methods/#MEME 
Tool options
 :: --alignment [required] An in-frame codon alignment in one of the formats supported by HyPhy. --tree [conditionally required] A phylogenetic tree (optionally annotated with {}). --code Which genetic code to use (see tool form for available options). --branches Which branches should be tested for selection? All [default] : test all branches. Internal : test only internal branches (suitable for intra-host pathogen evolution for example, where terminal branches may contain polymorphism data). Leaves: test only terminal (leaf) branches. Unlabeled: if the Newick string is labeled using the {} notation, test only branches without explicit labels (see http://hyphy.org/tutorials/phylotree/). Custom : Enter a branch label. --pvalue The significance level used to determine significance (default: 0.1, range: 0 to 1). --resample Perform parametric bootstrap resampling to derive site-level null LRT distributions. Warning: This will result in a significantly slower analysis. A value of 0 means no resampling is performed. This parameter specifies the maximum number of replicates per site (default: 0, range: 0 to 1000). --rates The number omega rate classes to include in the model (default: 2, range: 2 to 4). --multiple-hits Include support for multiple nucleotide substitutions. Double : Include branch-specific rates for double nucleotide substitutions. Double+Triple : Include branch-specific rates for double and triple nucleotide substitutions. None [default] : Use standard models which permit only single nucleotide changes to occur instantly. --site-multihit Estimate multiple hit rates for each site. This option is available only if 'Include support for multiple nucleotide substitutions' is set to 'Double' or 'Double+Triple'. Estimate [default] : Estimate multiple hit rates. No : Do not estimate multiple hit rates. --impute-states Use site-level model fits to impute likely character states for each sequence (default: No). --precision Optimization precision settings for preliminary fits. Standard [default] Reduced for faster fitting --kill-zero-lengths Automatically delete internal zero-length branches for computational efficiency. Yes [default] : Automatically delete internal zero-length branches for computational efficiency (will not affect results otherwise). Constrain : Keep zero-length branches, but constrain their values to 0. No : Keep all branches. --restrict-sites Restrict MEME analysis to a subset of sites. If Yes, allows specifying a subset of sites for analysis. Yes : Restrict analysis to a subset of sites. No [default] : Do not restrict analysis to a subset of sites. --limit-to-sites Only analyze sites whose 1-based indices match the following list (null to skip). This option is available only if 'Restrict MEME analysis to a subset of sites' is set to 'Yes'. Comma-separated list of site indices. --save-lf-for-sites For sites whose 1-based indices match the following list, write out likelihood function snapshots (empty string to skip). This option is available only if 'Restrict MEME analysis to a subset of sites' is set to 'Yes'. Comma-separated list of site indices. --full-model Perform branch length re-optimization under the full codon model (default: Yes)."
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_prime/hyphy_prime/2.5.83+galaxy0	"What question does this method answer?
 Does evolution at specific sites in a coding alignment preserve or alter a set of pre-defined biochemical properties? 
Recommended Applications
 - Identify biochemical evolutionary constraints or changes with site-level resolution (e.g. site 23 is evolving to conserve residue polarity, but alter its volume). Method ------ 
Background: Limitations of standard dN/dS models
 Standard models for detecting natural selection in coding sequences use the dN/dS ratio (ω) to quantify selection pressure. These models typically assume that all non-synonymous substitutions have the same rate, regardless of the specific amino acid change. This is a simplification, as substitutions between biochemically similar amino acids (e.g., Leucine to Isoleucine) are expected to occur more frequently than substitutions between dissimilar ones (e.g., Arginine to Cysteine). 
PRIME: A Property-Informed Model
 PRIME (PRoperty-Informed Models of Evolution) extends the standard dN/dS framework by incorporating the biochemical properties of amino acids directly into the substitution model. Instead of a single rate for all non-synonymous changes, PRIME models these rates as a function of the changes in specific biochemical properties between the original and the new amino acid. 
The Intuition
 The core idea is that the fitness cost or benefit of a mutation is often related to how it alters the biochemical characteristics of the resulting protein. PRIME formalizes this by modeling the non-synonymous substitution rate between amino acid 
i
 and 
j
 as a function of a baseline dN/dS ratio (ω) and a set of property-specific parameters (λ). For each biochemical property 
p
 (e.g., volume, polarity), the model includes a parameter 
λ_p
. This parameter quantifies the extent to which evolution at a given site favors or disfavors changes in that property. - If 
λ_p > 0
, changes in property 
p
 are penalized. This indicates 
conservative selection
 with respect to that property. For example, a large positive λ for ""volume"" means that substitutions that significantly alter the amino acid's volume are selected against. - If 
λ_p < 0
, changes in property 
p
 are favored. This indicates 
radical selection
 with respect to that property. For example, a large negative λ for ""charge"" means that substitutions that change the amino acid's charge are selected for. 
The Test
 For each site in the alignment, PRIME performs a likelihood ratio test (LRT) to determine if this more complex, property-informed model is a significantly better fit to the data than a standard dN/dS model. A significant p-value for a specific property suggests that the evolution at that site has been shaped by selection to conserve or radically alter that biochemical property. An omnibus test is also performed to assess the overall significance of all properties combined. 
Input
 - 
Sequence Alignment:
 An in-frame codon alignment in FASTA or NEXUS format. - 
Phylogenetic Tree:
 A phylogenetic tree in Newick format. The names of the sequences in the alignment must match the names of the tips in the tree. 
Tool Options
 - 
Genetic code:
 The genetic code to use for translation. - 
Branches to test:
 Select which branches of the tree to include in the analysis (All, Internal, or Leaves). - 
Source of amino-acid properties:
 - 
Use a built-in property set:
 Choose from a list of pre-defined sets of amino-acid properties. - 
Atchley
: Five properties derived from a factor analysis of 500 amino-acid properties. - 
LCAP
: Four properties from the LCAP model of Conant and Stadler. - 
Random-*
: Sets of 2, 3, 4, or 5 random properties for null hypothesis testing. - 
Provide a custom property file:
 Supply your own set of properties in a JSON file. - 
P-value threshold:
 The significance level for the likelihood ratio test. - 
Impute states:
 Use the fitted model to infer the most likely character states at each internal node of the tree. - 
Save intermediate model fits:
 Save the parameter estimates from the initial model fits to a separate JSON file. - 
Kill zero-length branches:
 Automatically remove internal branches of length zero for computational efficiency. 
Output
 - A JSON file with the detailed analysis results, including site-level p-values and parameter estimates. - A Markdown report summarizing the analysis and results. 
Further reading
 - http://hyphy.org/methods/selection-methods/#PRIME - http://hyphy.org/resources/json-fields.pdf"
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_relax/hyphy_relax/2.5.83+galaxy0	"Method ------ RELAX is a hypothesis testing framework that asks whether the strength of natural selection has been relaxed or intensified in a specific set of branches (the ""test"" set) on a phylogenetic tree, relative to another set of branches (the ""reference"" set). 
The Intuition
 The core idea behind RELAX is to model the distribution of selection intensity (ω, the dN/dS ratio) across sites in the alignment. Natural selection can be: - 
Purifying (negative) selection
 (ω < 1): Non-synonymous mutations are deleterious and are removed from the population. - 
Neutral evolution
 (ω = 1): Non-synonymous mutations have no effect on fitness. - 
Diversifying (positive) selection
 (ω > 1): Non-synonymous mutations are advantageous and are fixed in the population. RELAX fits a model to the data that describes the distribution of ω values as a mixture of several rate classes. It then introduces a 
relaxation parameter (K)
. This parameter scales the ω distribution for the test branches. - If 
K > 1
, the ω distribution is shifted away from neutrality, which indicates that selection has been 
intensified
. This means that both purifying and diversifying selection are stronger on the test branches. - If 
K < 1
, the ω distribution is shifted towards neutrality, which indicates that selection has been 
relaxed
. This means that both purifying and diversifying selection are weaker on the test branches. - If 
K = 1
, there is no difference in selection strength between the test and reference branches. 
The Test
 RELAX performs a likelihood ratio test (LRT) to determine if the model with the relaxation parameter 
K
 is a significantly better fit to the data than a null model where 
K
 is fixed to 1. A significant p-value suggests that the strength of selection is indeed different between the test and reference branches. 
Recommended Applications
 - Testing for a systematic shift (relaxation / intensification) in the distribution of selection pressure associated with major biological transitions such as host switching in viruses, or lifestyle evolution in bacteria (e.g., transition from free-living to endosymbiotic lifestyle). - Comparing selective regimes between two subsets of branches in the tree, e.g., to investigate selective differences due to an environmental or phenotypic change. 
Input
 This tool accepts either a single alignment file or multiple alignment files. For each alignment, a corresponding phylogenetic tree is required. - 
Sequence Alignment:
 An in-frame codon alignment in FASTA or NEXUS format. - 
Phylogenetic Tree:
 A phylogenetic tree in Newick format. The tree's branches must be annotated to define the sets of branches to be tested. 
Multiple Alignment Files
 This mode allows you to run a joint analysis on multiple sequence alignments, each with its own phylogenetic tree. The primary goal of this feature is to boost statistical power by fitting a single, shared dN/dS rate distribution across all provided gene/partition data, while allowing tree topologies, branch lengths, and other model parameters to vary for each dataset. This is particularly useful for analyses where you want to combine data from multiple genes from the same set of species, for example, when analyzing different genes from viral genomes. By combining data, you can obtain more robust estimates of the selection parameters. When using this mode, ensure that the branch labels used for 
TEST
 and 
REFERENCE
 sets are consistent across all your input trees. 
Branch Annotation:
 Branch labels are used to assign branches to different sets for analysis. Labels are added to the Newick string, for example: 
(speciesA:0.1, (speciesB:0.2, speciesC:0.3){TEST}:0.4);
. - In 
Classic mode
, you must define a 
TEST
 set of branches. You can optionally define a 
REFERENCE
 set. If no 
REFERENCE
 set is defined, all branches not in the 
TEST
 set are considered reference branches. - In 
Group mode
, you can define multiple sets of test branches (e.g., 
{TEST1}
, 
{TEST2}
, 
{TEST3}
) and specify one of them as the reference group. 
Tool Options
 - 
Input type:
 Choose between a single alignment file or multiple alignment files. - 
Analysis Type:
 - 
All
: Fit all 4 models, including descriptive models. - 
Minimal
: Perform a 2-model test of relaxation/intensification only (faster). - 
Branches to use as the test set:
 The label used in the Newick tree to define the test set (e.g., 
TEST
). - 
Branches to use as the reference set:
 (Optional) The label for the reference set. - 
Run mode:
 - 
Classic mode
: Test for selection relaxation/intensification on a 
TEST
 set of branches relative to a 
REFERENCE
 set. - 
Group mode
: Test for differences in selective pressure among multiple groups of branches. - 
Branches to use as the reference group:
 In 
Group mode
, specify which of the defined branch sets should be used as the reference for comparison. - 
Advanced settings:
 - 
Synonymous rate variation (SRV):
 Model synonymous rate variation across sites using different methods (
Yes
, 
Branch-site
, 
HMM
). - 
Multiple hits correction:
 Account for multiple nucleotide substitutions at the same site. - 
Kill zero-length branches:
 Automatically remove internal branches of length zero for computational efficiency. 
Output
 - A JSON file with the detailed analysis results. - A Markdown report summarizing the analysis and results. 
Further reading
 - http://hyphy.org/methods/selection-methods/#RELAX - http://hyphy.org/resources/json-fields.pdf"
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_slac/hyphy_slac/2.5.83+galaxy0	"SLAC : Single Likelihood Ancestor Counting ========================================== What question does this method answer? -------------------------------------- SLAC (Single Likelihood Ancestor Counting) is designed to identify individual sites within a gene that are subject to pervasive diversifying selection, meaning selection that acts consistently across the entire evolutionary phylogeny. It helps answer: Which specific sites in a gene show evidence of positive selection that has been maintained throughout the evolutionary history of the analyzed sequences? Recommended Applications ------------------------ The phenomenon of pervasive selection is generally most prevalent in pathogen evolution and any biological system influenced by evolutionary arms race dynamics (or balancing selection), including adaptive immune escape by viruses. As such, SLAC is ideally suited to identify sites under positive selection which represent candidate sites subject to strong selective pressures across the entire phylogeny. SLAC provides legacy functionality as a counting-based method adapted for phylogenetic applications. In general, this method will be the least statistically robust (compared to FEL or FUBAR), but it is the most directly interpretable. Brief description ----------------- SLAC (Single Likelihood Ancestor Counting) is a counting-based method designed to detect pervasive positive or negative selection at individual sites within a gene. It operates by first inferring ancestral sequences at each node of the provided phylogenetic tree using a maximum likelihood approach. This reconstruction allows for the estimation of synonymous (dS) and non-synonymous (dN) substitution rates at each site across the entire phylogeny. Finally, a binomial test is applied to determine if the observed number of non-synonymous substitutions significantly deviates from the expected number under neutrality (dN = dS). The method aggregates information across all branches of the phylogeny, making it suitable for detecting pervasive diversifying selection (dN > dS) or purifying selection (dN < dS) that acts consistently throughout the evolutionary history of the analyzed sequences. While generally less statistically robust than likelihood-based methods like FEL or FUBAR, SLAC offers direct interpretability of its results. How it works ------------ 1. Ancestral Sequence Reconstruction: SLAC begins by reconstructing the most likely ancestral sequences at each internal node of the provided phylogenetic tree. 2. Counting Substitutions: Once ancestral sequences are inferred, the method counts the number of synonymous (dS) and non-synonymous (dN) substitutions that have occurred along each branch of the phylogeny. 3. Binomial Test: For each site, SLAC applies a binomial test. The null hypothesis is that non-synonymous and synonymous mutations occur at equal rates (i.e., no selection). 4. Pervasive Selection: By aggregating evidence across all branches of the tree, SLAC identifies sites that have been under consistent selective pressure throughout the evolutionary history of the gene. Input ----- 1. A coding multiple sequence alignment. 2. A phylogenetic tree in the 
Newick
 format Note: the names of sequences in the alignment must match the names of the sequences in the tree. Output ------ A JSON file with analysis results (http://hyphy.org/resources/json-fields.pdf). This JSON output can be visualized using the HyPhy Vision platform at http://vision.hyphy.org/SLAC/. Further reading --------------- http://hyphy.org/methods/selection-methods/#SLAC Tool options ------------ :: --alignment [required] An in-frame codon alignment in one of the formats supported by HyPhy. --tree [conditionally required] A phylogenetic tree (optionally annotated with {}). --code Which genetic code to use (see tool form for available options). --branches Which branches should be tested for selection? All [default] : test all branches Internal : test only internal branches (suitable for intra-host pathogen evolution for example, where terminal branches may contain polymorphism data) Leaves: test only terminal (leaf) branches Unlabeled: if the Newick string is labeled using the {} notation, test only branches without explicit labels (see http://hyphy.org/tutorials/phylotree/) Custom : Enter a branch label. --pvalue The significance level used to determine significance (default: 0.1, range: 0 to 1). --samples Draw this many alternative ancestral state reconstructions to evaluate uncertainty (default: 100, range: 0 to 10000). --kill-zero-lengths Automatically delete internal zero-length branches for computational efficiency. Yes [default] : Automatically delete internal zero-length branches for computational efficiency (will not affect results otherwise). Constrain : Keep zero-length branches, but constrain their values to 0. No : Keep all branches."
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_sm19/hyphy_sm19/2.5.83+galaxy0	"SM2019 : Structured Slatkin-Maddison ==================================== 
What does this do?
 Test for evidence of genetically segregated populations, using a ""detuned"" version of the Slatkin-Maddison test. Method ------ The SM2019 tool implements canonical and modified versions of the Slatkin-Maddison phylogeny-based test for population segregation. This method is used to detect evidence of genetically segregated populations within a phylogenetic tree. 
The Slatkin-Maddison Test: Intuition
 The core idea of the Slatkin-Maddison test is to quantify the amount of ""mixing"" or ""segregation"" of different populations (or compartments) within a phylogenetic tree. It does this by mapping discrete character states (e.g., population labels like ""Blood"" or ""Semen"") onto the tips of the tree. Then, using a parsimony approach, it infers the minimum number of ""migration events"" (changes in character state) that must have occurred along the branches of the tree to explain the observed distribution of states at the tips. - 
Minimum Migration Events:
 A lower number of inferred migration events suggests stronger population segregation, as fewer changes are needed to explain the observed pattern. A higher number suggests more mixing. 
Hypothesis Testing
 The tool then performs permutation tests to evaluate the significance of the observed number of migration events: 1. 
Panmictic Permutation (Unstructured Population):
 Leaf labels are randomly permuted across the tips of the tree. The minimum number of migration events is calculated for each permuted tree. This generates a null distribution representing what would be expected if there were no population structure (i.e., a panmictic population where individuals mix freely). If the observed number of migration events is significantly lower than this null distribution, it suggests evidence for population segregation. 2. 
Structured Permutation (Detuned Test):
 This is a modified version of the test. Instead of completely random permutations, leaf labels are permuted partially, respecting the underlying subtree structure (block permutations). This ""detuned"" approach can be more powerful in detecting subtle population structure by accounting for phylogenetic relationships. The 
weight
 parameter controls the degree of structure in these permutations. 
Interpretation of Results
 - A significant p-value (typically < 0.05) from the panmictic permutation test indicates that the observed population structure (segregation) is unlikely to have arisen by chance in an unstructured population. - The structured permutation test provides a more nuanced view, accounting for some phylogenetic signal. 
Output:
 The tool outputs the inferred number of migration events and p-values for both panmictic and structured permutations, allowing for a comprehensive assessment of population segregation. 
Input
 - 
Input tree:
 A phylogenetic tree in Newick format. Leaf names should be partitionable into sets using regular expressions. 
Tool Options
 - 
Input tree:
 The Newick tree string defining the topology to use for testing. - 
Partitions:
 Define the compartments (groups of leaves) for the analysis. - 
Partition label:
 A descriptive label for the compartment. - 
Regular expression:
 A regular expression to select the branches belonging to this compartment. - 
Number of bootstrap replicates:
 The number of bootstrap replicates to perform for the permutation tests. - 
Probability of branch selection for structured permutation:
 This parameter controls the type of permutation. A value of 0 corresponds to the classical Slatkin-Maddison test (full panmixia), while a value of 1 corresponds to a fully structured permutation. - 
Use bootstrap weights to respect well supported clades:
 If set to Yes, bootstrap weights are used to respect well-supported clades during permutations. 
Output
 - A JSON file with detailed analysis results, including migration events, p-values for panmictic and structured permutations, and partition counts. - A Markdown report summarizing the analysis and results. 
Further reading
 - https://www.ncbi.nlm.nih.gov/pubmed/2599370 - https://github.com/veg/hyphy-analyses/tree/master/SlatkinMaddison"
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_summary/hyphy_summary/2.5.47+galaxy0	HyPhy-Summary ============= This tool has two operation modes, summary and merge. Summary ------- Given a combined and filtered alignment from TN93-Filter, along with the appropriate HyPhy analysis outputs, this mode will return two JSON files, one with segment annotations and the other with site annotations. Analyses required for this mode: - FADE - PRIME - RELAX - MEME - MEME-Full - Annotate, with the labels in JSON format - BUSTED - SLAC - FEL - CFEL - BGM Merge ----- This mode takes a set of segment and site annotations, one each per gene that was analyzed, and returns a merged set of site and segment annotations.
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_absrel/hyphy_absrel/2.5.83+galaxy0	"aBSREL: Adaptive Branch-Site Random Effects Likelihood ====================================================== 
What question does this method answer?
 aBSREL (adaptive Branch-Site Random Effects Likelihood) is a powerful method for detecting episodic positive selection. It identifies instances where a proportion of sites along specific branches or lineages of a phylogeny have undergone positive selection. 
Recommended Applications
 1. 
Detecting Episodic Diversifying Selection:
 Ideal for exploratory testing to find evidence of lineage-specific positive diversifying selection in alignments of various sizes. 2. 
Targeted Branch Testing:
 Suitable for targeted testing of branches hypothesized to be under positive selection, even in alignments that would be computationally prohibitive for older branch-site models. 
Methodology
 aBSREL is an adaptive branch-site random effects likelihood model that allows the dN/dS ratio to vary across sites and branches. The key innovation of aBSREL is its adaptive nature: it infers the optimal number of dN/dS rate classes for each branch, providing a more nuanced and powerful test for positive selection compared to traditional fixed-rate models. 
The Intuition
 Imagine a gene evolving across a phylogeny. On some branches, the gene might be under strong purifying selection, with most non-synonymous mutations being deleterious. On other branches, the gene might be evolving neutrally. And on a few key branches, the gene might be under positive selection, with non-synonymous mutations providing a fitness advantage. Traditional branch-site models test for positive selection by fitting a model with a fixed number of dN/dS rate classes to each branch. This can be problematic because the evolutionary process is not always so uniform. Some branches might have a simple evolutionary history that can be described by one or two dN/dS rates, while others might have a more complex history that requires more rate classes. aBSREL addresses this by starting with a simple model for each branch and incrementally adding more rate classes until the model fit no longer improves. This ""adaptive"" approach allows the model to tailor itself to the complexity of the evolutionary process on each branch, leading to a more accurate and powerful test for positive selection. 
The Test
 For each branch, aBSREL performs a likelihood ratio test to determine if a model that allows for positive selection (i.e., a dN/dS ratio > 1) is a significantly better fit than a model that does not. The p-values from these tests are then corrected for multiple testing to identify branches that show statistically significant evidence of positive selection. 
Input
 1. A 
FASTA
 sequence alignment. 2. A phylogenetic tree in the 
Newick
 format Note: the names of sequences in the alignment must match the names of the sequences in the tree. 
Output
 A JSON file with analysis results (http://hyphy.org/resources/json-fields.pdf). For each tested branch the analysis will infer the appropriate number of selective regimes, and whether or not there is statistical evidence of positive selection on that branch. A custom visualization module for viewing these results is available (see http://vision.hyphy.org/aBSREL for an example) 
Further reading
 http://hyphy.org/methods/selection-methods/#absrel 
Tool options
 :: --alignment [required] An in-frame codon alignment in one of the formats supported by HyPhy --tree [conditionally required] A phylogenetic tree (optionally annotated with {}) --code Which genetic code to use (see tool form for available options) --branches Which branches should be tested for selection? All [default] Internal Leaves Unlabeled branches Custom : Enter a branch label --multiple-hits Include support for multiple nucleotide substitutions Double : Include branch-specific rates for double nucleotide substitutions Double+Triple : Include branch-specific rates for double and triple nucleotide substitutions None [default] : Use standard models which permit only single nucleotide changes to occur instantly --srv Include synonymous rate variation (default: No) If Yes, then: --syn-rates The number alpha rate classes to include in the model [1-10, default 3] --blb [Advanced option] Bag of Little Bootstraps (BLB) alignment resampling rate (default: 1.0). This parameter controls the fraction of sites to resample for each bootstrap replicate. BLB uses down/upsampling approaches to speed up inference for very long alignments by analyzing subsets of the data. For more details, see https://www.nature.com/articles/s43588-021-00129-5. --output Write the resulting JSON to this file (default is to save to the same path as the alignment file + 'ABSREL.json') --kill-zero-lengths Automatically delete internal zero-length branches for computational efficiency Yes [default] : Automatically delete internal zero-length branches for computational efficiency (will not affect results otherwise) Constrain : Keep zero-length branches, but constrain their values to 0 No : Keep all branches --save-fit Save full adaptive aBSREL model fit to this file (default is not to save)"
toolshed.g2.bx.psu.edu/repos/iuc/hyphy_strike_ambigs/hyphy_strike_ambigs/2.5.83+galaxy0	"HyPhy Strike-Ambigs =================== 
What does this tool do?
 This tool reads an alignment of coding sequences and replaces any ambiguous codons with '---' (three hyphens). Ambiguous codons are those that cannot be unambiguously translated into a single amino acid, often due to sequencing errors or unresolved polymorphisms. 
Input
 - 
Input alignment:
 An in-frame codon alignment in FASTA format. It is crucial that the input alignment is correctly aligned along codon boundaries for accurate processing. 
Tool Options
 - 
Genetic code:
 Specifies which genetic code should be used for translation and codon ambiguity resolution. 
Output
 - 
Ambiguous codons replaced alignment:
 A FASTA format alignment where ambiguous codons have been replaced with '---'. - 
Strike Ambigs Report:
 A Markdown report summarizing the analysis, including details on how many ambiguous codons were replaced. 
Further reading
 - https://www.ncbi.nlm.nih.gov/pubmed/15604363"
toolshed.g2.bx.psu.edu/repos/iuc/sarscov2formatter/sarscov2formatter/1.0+galaxy0	================= sarscov2formatter ================= Custom sript that performs necessary formatting operations for the SARS-CoV2 Selection Analysis workflow. If using NCBI as data source the file can be obtained from https://www.ncbi.nlm.nih.gov/projects/genome/sars-cov-2-seqs/ncov-sequences.yaml. If not given the tool will download it automatically. If using non-NCBI data, the metadata input file must be tabular with the following columns: ID, collection_date, country, state (optional), and locality (optional). Optional columns should still be created even if they are not used. Dates should be of the format: YYMMDD (example: May 1 2020 = 20200501).
toolshed.g2.bx.psu.edu/repos/iuc/sarscov2summary/sarscov2summary/0.1	=============== sarscov2summary =============== Custom summary script for the SARS-CoV2 Selection Analysis workflow. Takes HyPhy outputs as well as metadata to create final summary output.
toolshed.g2.bx.psu.edu/repos/iuc/iwtomics_loadandplot/iwtomics_loadandplot/1.0.0.0	"This tool imports a collection of genomic region datasets, and associates to each region multiple genomic feature measurements. It allows to align the regions in multiple ways (center, left, right or scale alignment), to smooth the feature curves (possibly filling gaps in the measurements) and to create a graphical representation of the feature measurements in each region datasets (aligned curves or pointwise quantile curves). ----- 
Region datasets
 Each region dataset can be provided as a BED or Tabular file with tab delimited columns chr start end (extra columns present in the input file are ignored). Regions can be of different length:: chr2 49960150 50060150 chr2 55912445 56012445 ... ----- 
Feature measurements
 Feature measurements corresponding to all the regions can be provided as a BED or Tabular file with tab delimited columns chr start end value:: chr2 49960150 49962150 0.9426 chr2 49962150 49964150 0.7816 ... Each feature must be measured in windows of a fixed size inside all the regions (missing values must be indicated as NA). Another way to import feature measurements is from a Tabular file with the first three columns chr start end corresponding to the different genomic regions, followed on the same row by all the measurements in fixed-size windows:: chr2 49960150 50060150 0.9426 0.7816 0.8921 ... ... 1.2063 chr2 55912445 56012445 0.8719 0.9975 1.1619 ... ... 0.9601 ... ----- 
Output
 The tool returns: 1. RData with the IWTomicsData object, that stores the aligned genomic region datasets, and their associated feature measurements; 2. Region dataset identifiers; 3. Feature identifiers; 4. PDF file with the plotted data. 1-3 can be used as input of the tool 
IWTomics Test and Plot
 ----- .. class:: infomark 
Notes
 This Galaxy tool has been developed by Fabio Cumbo (Third University of Rome, Italy - fabio.cumbo@iasi.cnr.it) and Marzia A. Cremona (The Pennsylvania State University, USA - mac78@psu.edu). It implements a simplified version of the methods 
smooth
 and 
plot
 for 
IWTomicsData
 objects. The complete version can be found in the 
R/Bioconductor
 package 
IWTomics
 (see vignette_). .. 
vignette: https://bioconductor.org/packages/release/bioc/vignettes/IWTomics/inst/doc/IWTomics.pdf Example data can be found at: 1. Simulated_data
 2. ETn_data_ .. _Simulated_data: https://usegalaxy.org/u/fabio-cumbo/h/iwtomics-example .. _ETn_data: https://usegalaxy.org/u/fabio-cumbo/h/iwtomics-etn-example"
toolshed.g2.bx.psu.edu/repos/iuc/iwtomics_plotwithscale/iwtomics_plotwithscale/1.0.0.0	"This tool allows to select the scale for the Interval-Wise Testing results. In particular, it returns the p-value curves for the different tests performed at the selected scale, and it creates a graphical representation of the Interval-Wise Testing results and a summary plot (optional) at the selected scale. ----- 
Input files
 RData file with the IWTomicsData object with test results, tabular files with test IDs and feature IDs. These files are created by the tool 
IWTomics Test and Plot
. ----- 
Output
 The tool returns: 1. TXT file with an adjusted p-value curve for every test performed at the selected scale; 2. PDF file with the plotted test results; 3. PDF file with the summary plot. ----- .. class:: infomark 
Notes
 This Galaxy tool has been developed by Fabio Cumbo (Third University of Rome, Italy) and Marzia A. Cremona (The Pennsylvania State University, USA). It implements a simplified version of the function 
IWTomicsTest
, 
plotTest
 and 
plotSummary
 for 
IWTomicsData
 objects. The complete version can be found in the 
R/Bioconductor
 package 
IWTomics
 (see vignette_). .. _vignette: https://bioconductor.org/packages/release/bioc/vignettes/IWTomics/inst/doc/IWTomics.pdf"
toolshed.g2.bx.psu.edu/repos/iuc/iwtomics_testandplot/iwtomics_testandplot/1.0.0.0	"This tool statistically evaluates differences in genomic features between groups of regions along the genome. In particular, it implements the Interval-Wise Testing for omics data, an extended version of the Interval-Wise Testing for functional data presented in Pini and Vantini (2017). It allows to perform multiple two sample permutation tests between pairs of region datasets, on several features. It returns the adjusted p-value curves for every test and all possible scales. Moreover, it creates a graphical representation of the Interval-Wise Testing results and a summary plot (optional) with p-values at the maximum scale. The tool 
IWTomics Plot with Threshold on Test Scale
 permits to select the scale to be used in the plots. ----- 
Input files
 RData file with the IWTomicsData object, tabular files with region dataset IDs and feature IDs. These files are created by the tool 
IWTomics Load Smooth and Plot
. ----- 
Output
 The tool returns: 1. TXT file with an adjusted p-value matrix for every test performed. Each matrix contains a p-value curve (row) for every scale considered in the test; 2. PDF file with the plotted test results; 3. PDF file with the summary plot; 4. RData with the IWTomicsData object with the test results; 5. Test identifiers; 6. Feature identifiers. 4-6 can be used as input of the tool 
IWTomics Plot with Threshold on Test Scale
 ----- .. class:: infomark 
Notes
 This Galaxy tool has been developed by Fabio Cumbo (Third University of Rome, Italy) and Marzia A. Cremona (The Pennsylvania State University, USA). It implements a simplified version of the function 
IWTomicsTest
, 
plotTest
 and 
plotSummary
 for 
IWTomicsData
 objects. The complete version can be found in the 
R/Bioconductor
 package 
IWTomics
 (see vignette_). .. _vignette: https://bioconductor.org/packages/release/bioc/vignettes/IWTomics/inst/doc/IWTomics.pdf"
toolshed.g2.bx.psu.edu/repos/bgruening/10x_bamtofastq/10x_bamtofastq/1.4.1	10x Genomics BAM to FASTQ converter
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scater_read_10x_results/scater_read_10x_results/1.8.4+galaxy0	"More information can be found at https://bioconductor.org/packages/release/bioc/html/scater.html 
Version history
 1.8.4+galaxy0: Initial contribution. Suhaib Mohammed, Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/artbio/gsc_scran_normalize/scran_normalize/1.28.1+galaxy1	"What it does
 Takes a raw count expression matrix and returns a table of log transformed scran-normalized expression values. This computes size factors that are used to scale the counts in each cell. The assumption is that most genes are not differentially expressed (DE) between cells, such that any differences in expression across the majority of genes represents some technical bias that should be removed. Cell-specific biases are normalized using the computeSumFactors method, which implements the deconvolution strategy for scaling normalization (A. T. Lun, Bach, and Marioni 2016). It creates a reference : - if no clustering step : the average count of all transcriptomes - if you choose to cluster your cells : the average count of each cluster. Then it pools cells and then sum their expression profiles. The size factor is described as the median ration between the count sums and the average across all genes. Finally it constructs a linear distribution (deconvolution method) of size factors by taking multiple pools of cells. You can apply this method on cell cluster instead of your all set of cells by using quickCluster. It defines cluster using distances based on Spearman correlation on counts between cells, there is two available methods : - 
hclust
 : hierarchical clustering on the distance matrix and dynamic tree cut. - 
igraph
 : constructs a Shared Nearest Neighbor graph (SNN) on the distance matrix and identifies highly connected communities. Note: First header row must NOT start with a '#' comment character"
comp1	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
Syntax
 This tool finds lines in one dataset that HAVE or DO NOT HAVE a common field with another dataset. ----- 
Example
 If this is 
First dataset
:: chr1 10 20 geneA chr1 50 80 geneB chr5 10 40 geneL and this is 
Second dataset
:: geneA tumor-suppressor geneB Foxp2 geneC Gnas1 geneE INK4a Finding lines of the 
First dataset
 whose 4th column matches the 1st column of the 
Second dataset
 yields:: chr1 10 20 geneA chr1 50 80 geneB Conversely, using option 
Non Matching rows of First dataset
 on the same fields will yield:: chr5 10 40 geneL"
Grouping1	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
Syntax
 This tool allows you to group the input dataset by a particular column and perform aggregate functions: Mean, Median, Mode, Sum, Max, Min, Count, Concatenate, and Randomly pick on any column(s). The Concatenate function will take, for each group, each item in the specified column and build a comma delimited list. Concatenate Unique will do the same but will build a list of unique items with no repetition. Count and Count Unique are equivalent to Concatenate and Concatenate Unique, but will only count the number of items and will return an integer. - If multiple modes are present, all are reported. ----- 
Example
 - For the following input:: chr22 1000 1003 TTT chr22 2000 2003 aaa chr10 2200 2203 TTT chr10 1200 1203 ttt chr22 1600 1603 AAA - 
Grouping on column 4
 while ignoring case, and performing operation 
Count on column 1
 will return:: AAA 2 TTT 3 - 
Grouping on column 4
 while not ignoring case, and performing operation 
Count on column 1
 will return:: aaa 1 AAA 1 ttt 1 TTT 2"
join1	".. class:: warningmark 
This tool will force the ouput datatype to tabular.
 To change metadata assignments click on the ""edit attributes"" link of the history item generated by this tool. .. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
Syntax
 This tool joins lines of two datasets on a common field. An empty string ("""") is not a valid identifier. You may choose to include lines of your first input that do not join with your second input. - Columns are referenced with a 
number
. For example, 
3
 refers to the 3rd column of a tab-delimited file. ----- 
Example
 Dataset1:: chr1 10 20 geneA chr1 50 80 geneB chr5 10 40 geneL Dataset2:: geneA tumor-supressor geneB Foxp2 geneC Gnas1 geneE INK4a Joining the 4th column of Dataset1 with the 1st column of Dataset2 will yield:: chr1 10 20 geneA geneA tumor-suppressor chr1 50 80 geneB geneB Foxp2 Joining the 4th column of Dataset1 with the 1st column of Dataset2, while keeping all lines from Dataset1, will yield:: chr1 10 20 geneA geneA tumor-suppressor chr1 50 80 geneB geneB Foxp2 chr5 10 40 geneL"
toolshed.g2.bx.psu.edu/repos/devteam/subtract_query/subtract_query1/0.1	".. class:: infomark 
TIP:
 This tool complements the tool in the 
Operate on Genomic Intervals
 tool set which subtracts the intervals of two datasets. ----- 
Syntax
 This tool subtracts an entire dataset from another dataset. - Any text format is valid. - If both dataset formats are tabular, you may restrict the subtraction to specific columns 
contained in both datasets
 and the resulting dataset will include only the columns specified. - The begin column must be less than or equal to the end column. If it is not, begin column is switched with end column. - If begin column is specified but end column is not, end column will default to begin_column (and vice versa). - All blank and comment lines are skipped and not included in the resulting dataset (comment lines are lines beginning with a # character). - Duplicate lines are eliminated from both dataset prior to subtraction. If any duplicate lines were eliminated from the first dataset, the number is displayed in the resulting history item. ----- 
Example
 If this is the 
First dataset
:: chr1 4225 19670 chr10 6 8 chr1 24417 24420 chr6_hla_hap2 0 150 chr2 1 5 chr10 2 10 chr1 30 55 chrY 1 20 chr1 1225979 42287290 chr10 7 8 and this is the 
Second dataset
:: chr1 4225 19670 chr10 6 8 chr1 24417 24420 chr6_hla_hap2 0 150 chr2 1 5 chr1 30 55 chrY 1 20 chr1 1225979 42287290 Subtracting the 
Second dataset
 from the 
First dataset
 (including all columns) will yield:: chr10 7 8 chr10 2 10 Conversely, subtracting the 
First dataset
 from the 
Second dataset
 (including all columns) will result in an empty dataset. Subtracting the 
Second dataset
 from the 
First dataset
 (restricting to columns c1 and c2) will yield:: chr10 7 chr10 2"
liftOver1	".. class:: warningmark Make sure that the genome build of the input dataset is specified (click the pencil icon in the history item to set it if necessary). .. class:: warningmark This tool can work with interval, GFF, and GTF datasets. It requires the interval datasets to have chromosome in column 1, start co-ordinate in column 2 and end co-ordinate in column 3. BED comments and track and browser lines will be ignored, but if other non-interval lines are present the tool will return empty output datasets. ----- .. class:: infomark 
What it does
 This tool is based on the LiftOver utility and Chain track from 
the UC Santa Cruz Genome Browser
. It converts coordinates and annotations between assemblies and genomes. It produces 2 files, one containing all the mapped coordinates and the other containing the unmapped coordinates, if any. .. 
: http://genome.ucsc.edu/ ----- 
Example
 Converting the following hg16 intervals to hg18 intervals:: chrX 85170 112199 AK002185 0 + chrX 110458 112199 AK097346 0 + chrX 112203 121212 AK074528 0 - will produce the following hg18 intervals:: chrX 132991 160020 AK002185 0 + chrX 158279 160020 AK097346 0 + chrX 160024 169033 AK074528 0 -"
toolshed.g2.bx.psu.edu/repos/bgruening/keras_batch_models/keras_batch_models/1.0.11.0	"What does this tool do?
 This tool builds deep learning training models using API 
galaxy_ml.keras_galaxy_model.KerasGBatchClassifier
, which takes parameters in FIVE categories. - a JSON file that contains layer information for a deep learning model. - a data batch generator that converts raw data, such as images and genomic sequences, into numerical data to be able to fit the deep learning model. That the cycle of 
batch conversion - fitting
 occur in stream mode, also called on-line transformation, guarantees the training to be CPU and memory efficient. Reference: 
galaxy_ml.preprocessors.FastaDNABatchGenerator
, 
galaxy_ml.preprocessors.FastaRNABatchGenerator
, 
galaxy_ml.preprocessors.FastaProteinBatchGenerator
, 
galaxy_ml.preprocessors.GenomicIntervalBatchGenerator
. - compile parameters, are mainly composed of loss function and optimizer. - fit parameters, a group of variables that control the training process, referring to 
galaxy_ml.keras_galaxy_model.KerasGBatchClassifier
 and Keras. - other parameters, including 
class_positive_factor
, 
prediction_steps
, 
seed
 (random seed) and so on. 
Output
 A model file that could be used in 
model_validation
 tool or 
hyperparameter search
 tool. .. 
galaxy_ml.keras_galaxy_model.KerasGBatchClassifier
: https://goeckslab.github.io/Galaxy-ML/APIs/keras-galaxy-models/#kerasgbatchclassifier .. 
galaxy_ml.preprocessors.FastaDNABatchGenerator
: https://goeckslab.github.io/Galaxy-ML/APIs/keras-galaxy-models/#FastaDNABatchGenerator .. 
galaxy_ml.preprocessors.FastaRNABatchGenerator
: https://goeckslab.github.io/Galaxy-ML/APIs/keras-galaxy-models/#FastaRNABatchGenerator .. 
galaxy_ml.preprocessors.FastaProteinBatchGenerator
: https://goeckslab.github.io/Galaxy-ML/APIs/keras-galaxy-models/#FastaProteinBatchGenerator .. _
galaxy_ml.preprocessors.GenomicIntervalBatchGenerator
: https://goeckslab.github.io/Galaxy-ML/APIs/keras-galaxy-models/#GenomicIntervalBatchGenerator"
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_clf_metrics/sklearn_clf_metrics/1.0.11.0	"What it does
 This tool provides several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. This tool is based on sklearn.metrics package. For information about classification metric functions and their parameter settings please refer to 
Scikit-learn classification metrics
. .. 
Scikit-learn classification metrics
: http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_regression_metrics/sklearn_regression_metrics/1.0.11.0	"What it does
 This tool provides several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. This tool is based on sklearn.metrics package. For information about classification metric functions and their parameter settings please refer to 
Scikit-learn classification metrics
. .. 
Scikit-learn classification metrics
: http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"
toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_config/keras_model_config/1.0.11.0	"Help
 
What it does
 Creates a deep learning architecture using Keras deep learning library by adding different types of layers in a sequential manner. Different types of layers include Dense, Dropout, Convolutional, Activation and so on. These layers are added one after another which generates a deep architecture. Moreover, it also offers functional API (where layers are callable and can be chained) for advanced users to create complex models. 
Return
 A JSON file containing the information of all the layers and their respective attributes. 
How to create an architecture using this tool?
 1. Choose the model type. For example - ""Sequential"". It means that all the layers will be linearly stacked. 2. Add ""Input shape"" value. This is the dimensionality (number of columns) of the dataset excluding the 'target' or 'label' column. 3. Add layers using ""Insert layer"" button. Choose the layers and attributes (using advanced options) suited to the dataset. For example - Choose 'Dense' for adding a dense layer to the architecture and insert an integer under ""Units"" which are the number of neurons for this layer. Higher the number of units, the stronger is the architecture. Please be noted that the higher values of units may lead to overfitting and lower values may lead to underfitting. This is one of the hyperparameters of the architecture which needs to be tuned for a dataset. 4. Execute the tool to get a JSON string of the architecture."
toolshed.g2.bx.psu.edu/repos/bgruening/create_tool_recommendation_model/create_tool_recommendation_model/0.0.5	"What it does
 
Description
 It creates a model to recommend tools in Galaxy by learning the connections of tools in workflows. The model is an HDF5 file containing the tool dictionary, weights and configuration of the neural network. The recurrent neural network (Gated Recurrent Units) is used as a deep learner to learn the higher-order dependencies in tool connections of workflows. It takes two tabular files as input - one for the workflows and another for tools' usage frequencies. There are multiple other parameters to be set to find the best configuration of parameters of the neural network. This is achieved using bayesian optimisation hyperparameter search approach. Once the best configuration is found, a model is created which can be used to recommend tools in Galaxy. Further details about the input data and network parameters are explained below. ----- 
Input files
 There are two input files: 1. The first file (""dataset containing workflows"") contains tool connections for workflows in a tabular format. The workflows are arranged as pairs of tool connections. Each row is a pair of tool connections in a workflow as shown below: ========== ================ ========== ============= ============= =========== ============= ============== ============== =========== ============= 
wf_id
 
wf_updated
 
in_id
 
in_tool
 
in_tool_v
 
out_id
 
out_tool
 
out_tool_v
 
published
 
deleted
 
has_error
 ---------- ---------------- ---------- ------------- ------------- ----------- ------------- -------------- -------------- ----------- ------------- 3 2013-02-07 7 Cut1 1.0.0 5 Grep1 1.0.1 f t f ========== ================ ========== ============= ============= =========== ============= ============== ============== =========== ============= The first column (wf_id) is the workflow id, second (wf_updated) is the last updated date timestamp, third (in_id) is the id of the tool which is the input to the tool connection, fourth (in_tool) is the name of the input tool, fifth (in_tool_v) is the version of the input tool, sixth (out_id) is the id of the output tool in the tool connection, seventh (out_tool) is the name of the output tool and the last one (out_tool_v) is the version of the output tool. The tools connections (rows) for each workflow are used to recreate the workflow (directed acyclic graph) and unique tool sequences for each workflow are extracted. These tool sequences are then used to learn higher-order dependencies using a recurrent neural network to recommend tools. The last 3 columns give more information about workflows if they are published, non-deleted and has any errors. Collectively, they are useful to determine if the workflows are of good quality. 2. The second file (""dataset containing usage frequencies of tools"") is also a tabular file containing the usage frequencies of tools for a period of time. It has 3 columns: ============================================================================================ ========== === upload1 2019-03-01 176 toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.72 2019-03-01 97 toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.0.2.0 2019-03-01 67 ============================================================================================ ========== === The first column is the name of the tool, second is the date and the last one is the number of times the tool has been used in a month. For example, if this data is collected for 1 year, then each tool will appear in this list at most 12 times (months in which the usage is > 0). This data helps to know the usage pattern of a tool i.e. if a tool is being used often (high frequency in recent months) in the last one year or not being used at all (low frequency in recent months). This frequency is then used as weights for these tools in the neural network learning. The tools with high frequency in recent months is more important than tools with low frequency in recent months. This constraint allows to phase-out those tools from predictions which are not being used recently. ----- 
Parameters
 There are multiple parameters which can be set. They are divided into 3 categories: 1. Data parameters - ""input_cutoff_date"": It is used to set the earliest date from which the usage frequencies of tools should be considered. The format of the date is YYYY-MM-DD. This date should be in the past. - ""input_maximum_path_length"": This takes an integer and specifies the maximum size of a tool sequence extracted from any workflow. Any tool sequence of length larger than this number is not included in the dataset for training. 2. Training parameters - ""max_evals"": The hyperparameters of the neural network are tuned using a Bayesian optimisation approach and multiple configurations are sampled from different ranges of parameters. The number specified in this parameter is the number of configurations of hyperparameters evaluated to optimise them. Higher the number, the longer is the running time of the tool. - ""optimize_n_epochs"": This number specifies how many iterations would the neural network executes to evaluate each sampled configuration. - ""n_epochs"": Once the best configuration of hyperparameters has been found, the neural network takes this configuration and runs for ""n_epochs"" number of times minimising the error to produce a model at the end. - ""te_share"": It specifies the size of the test set. For example, if it is 0.5, then the test set is half of the entire data available. It should not be set to more than 0.5. This set is used for evaluating the precision on an unseen set. 3. Neural network parameters: - ""batch_size"": The training of the neural network is done using batch learning in this work. The training data is divided into equal batches and for each epoch (a training iteration), all batches of data are trained one after another. A higher or lower value can unsettle the training. Therefore, this parameter should be optimised. - ""units"": This number is the number of hidden recurrent units. A higher number means stronger learning (may lead to overfitting) and a lower number means weaker learning (may lead to underfitting). Therefore, this number should be optimised. - ""embedding_size"": For each tool, a fixed-size vector is learned and this fixed-size is known as the embedding size. This size remains same for all the tools. A lower number may underfit and a higher number may overfit. This parameter should be optimised as well. - ""dropout"": A neural network tends to overfit (especially when it is stronger). Therefore, to avoid or minimize overfitting, dropout is used. The fraction specified by dropout is the fraction of units ""deleted"" randomly from the network to impose randomness which helps in avoiding overfitting. This parameter should be optimised as well. - ""spatial_dropout"": Similar to dropout, this is used to reduce overfitting in the embedding layer. This parameter should be optimised as well. - ""recurrent_dropout"": Similar to dropout and spatial dropout, this is used to reduce overfitting in the recurrent layers (hidden). This parameter should be optimised as well. - ""learning_rate"": The learning rate specifies the speed of learning. A higher value ensures fast learning (the optimiser may diverge) and a lower value causes slow learning (may not reach the optimum). This parameter should be optimised as well. ----- 
Output file
 The output file (model) is an HDF5 file (http://docs.h5py.org/en/latest/high/file.html) containing multiple attributes like a dictionary of tools, neural network configuration and weights for each layer, weights of all tools and so on. After the tool has finished executing, it can be downloaded and placed at ""/galaxy/database/"" inside a Galaxy instance codebase. To see the recommended tools (enable the UI integrations) in Galaxy, the following changes should be made to ""galaxy.yml"" file: - Enable and then set the property ""enable_tool_recommendation"" to ""true""."
toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_builder/keras_model_builder/1.0.11.0	"Help
 
What it does
 Creates an estimator object (classifier or regressor) by using the architecture JSON from 'Create architecture' tool and adding an optimizer, loss function and other fit parameters. The fit parameters include the number of training epochs and batch size. Multiple attributes of an optimizer can also be set. A pre-trained deep learning model can also be used with this tool. 
Return
 An estimator object which can be used to train on a dataset. 
How to compile the architecture using this tool?
 1. Choose the architecture building mode. For example - choose ""Build a training model"". 2. Attach an architecture JSON file (obtained after executing ""Create architecture"" tool) which contains information about multiple layers. 3. Select a loss function. For example - for classification tasks, choose 'cross entropy' losses and for regression tasks, choose 'mean squared' or 'mean absolute' losses. 4. Choose an optimizer which minimizes the loss computed by the loss function. Multiple attributes of the chosen optimizer can be modified. 'RMSProp' and 'Adam' are some of the popular optimizers. 5. Insert the number of iterations (epochs) and the size of training batches (batch_size). 6. Execute the tool to get a compiled estimator object."
toolshed.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/1.0.11.0	"What it does
 Given a pre-built keras deep learning model and labeled training dataset, this tool works in two modes. - Train and Validate: the intput dataset is split into training and validation portions. The model is fitted on the training portion, in the meantime performances are evaluated on the validation portion multiple times while the training is progressing. Finally, a fitted model and its validation performance scores are outputted. - Train, Validate and and Evaluate: the input dataset is split into three portions, training, validation and testing. The same 
Train and Validate
 described above is performed on the training and validation portions. The testing portion is used exclusively for testing (evaluation). As a result, a fitted model and test performance scores are outputted. In both modes, besides the performance scores, the true labels and predicted values are outputted, which could be used in generating plots in other tools, machine learning visualization extensions, for example. Note that since all training and model parameters are accessible and changeable in the 
Hyperparameter Swapping
 section, the training and evaluation processes are flexible and transparent. For metrics, there are two sets of metrics for deep learning training and evaluation, one from the keras model builder and the other from scikit-learn. Keras metrics, if selected, are always evaluated, while the sklearn metrics could be ignored when 
default
 is the selection. Please be aware that not every sklearn metric works with deep learning model at current moment. Feel free to file a ticket if an issue is found and contibuting with PRs is always welcomed. 
Input
 - tabular - sparse - 
sequences in a fasta file
 to work with DNA, RNA and proteins with corresponding fasta data generator - 
reference genome and intervals
 exclusively work with 
GenomicIntervalBatchGenerator
. 
Output
 - performance scores from evaluation - fitted estimator - true labels or values and predicted values from the evaluation"
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_discriminant_classifier/sklearn_discriminant_classifier/1.0.11.0	"What it does
 Linear and Quadratic Discriminant Analysis are two classic classifiers with a linear and a quadratic decision surface respectively. These classifiers are fast and easy to interprete. 
1 - Training input
 When you choose to train a model, discriminant analysis tool expects a tabular file with numeric values, the order of the columns being as follows: :: ""feature_1"" ""feature_2"" ""..."" ""feature_n"" ""class_label"" 
Example for training data
 The following training dataset contains 3 feature columns and a column containing class labels: :: 4.01163365529 -6.10797684314 8.29829894763 1 10.0788438916 1.59539821454 10.0684278289 0 -5.17607775503 -0.878286135332 6.92941850665 2 4.00975406235 -7.11847496542 9.3802423585 1 4.61204065139 -5.71217537352 9.12509610964 1 
2 - Trainig output
 Based on your choice, this tool fits a sklearn discriminant_analysis.LinearDiscriminantAnalysis or discriminant_analysis.QuadraticDiscriminantAnalysis on the traning data and outputs the trained model in the form of pickled object in a text file. 
3 - Prediction input
 When you choose to load a model and do prediction, the tool expects an already trained Discriminant Analysis estimator and a tabular dataset as input. The dataset is a tabular file with new samples which you want to classify. It just contains feature columns. 
Example for prediction data
 :: 8.26530668997 2.96705005011 8.88881190248 2.96366327113 -3.76295851562 11.7113372463 8.13319631944 -0.223645298585 10.5820605308 .. class:: warningmark The number of feature columns must be the same in training and prediction datasets! 
3 - Prediction output
 The tool predicts the class labels for new samples and adds them as the last column to the prediction dataset. The new dataset then is output as a tabular file. The prediction output format should look like the training dataset. Discriminant Analysis is based on sklearn.discriminant_analysis library from Scikit-learn. For more information please refer to 
Scikit-learn site
. .. 
Scikit-learn site
: http://scikit-learn.org/stable/modules/lda_qda.html"
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_ensemble/sklearn_ensemble/1.0.11.0	"What it does
 The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator. This tool offers two sets of ensemble algorithms for classification and regression: random forests and ADA boosting which are based on sklearn.ensemble library from Scikit-learn. Here you can find out about the input, output and methods presented in the tools. For information about ensemble methods and parameters settings please refer to 
Scikit-learn ensemble
. .. 
Scikit-learn ensemble
: http://scikit-learn.org/stable/modules/ensemble.html 
1 - Methods
 There are two groups of operations available: 1 - Train a model : A training set containing samples and their respective labels (or predicted values) are input. Based on the selected algorithm and options, an estimator object is fit to the data and is returned. 2 - Load a model and predict : An existing model predicts the class labels (or regression values) for a new dataset. 
2 - Trainig input
 When you choose to train a model, you need a features dataset X and a labels set y. This tool expects tabular or sparse data for X and a single column for y (tabular). You can select a subset of columns in a tabular dataset as your features dataset or labels column. Below you find some examples: 
Sample tabular features dataset
 The following training dataset contains 3 feature columns and a column containing class labels. You can simply select the first 3 columns as features and the last column as labels: :: 4.01163365529 -6.10797684314 8.29829894763 1 10.0788438916 1.59539821454 10.0684278289 0 -5.17607775503 -0.878286135332 6.92941850665 2 4.00975406235 -7.11847496542 9.3802423585 1 4.61204065139 -5.71217537352 9.12509610964 1 
Sample sparse features dataset
 In this case you cannot specifiy a column range. :: 4 1048577 8738 1 271 0.02083333333333341 1 1038 0.02461995616119806 2 829017 0.01629088031127686 2 829437 0.01209127083516686 2 830752 0.02535100632816968 3 1047487 0.01485722929945572 3 1047980 0.02640566620767753 3 1048475 0.01665869913262564 4 608 0.01662975263094352 4 1651 0.02519674277562741 4 4053 0.04223659971350601 
2 - Trainig output
 The trained model is generated and output in the form of a binary file. 
3 - Prediction input
 When you choose to load a model and do prediction, the tool expects an already trained estimator and a tabular dataset as input. The dataset contains new samples which you want to classify or predict regression values for. .. class:: warningmark The number of feature columns must be the same in training and prediction datasets! 
3 - Prediction output
 The tool predicts the class labels for new samples and adds them as the last column to the prediction dataset. The new dataset then is output as a tabular file. The prediction output format should look like the training dataset."
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_estimator_attributes/sklearn_estimator_attributes/1.0.11.0	"What it does
 Output attribute from an estimator or any scikit object. Common attributes are : - 
estimator.
 
feature_importances_
 - 
RFE
. 
ranking_
 - 
RFECV
. 
grid_scores_
 - 
GridSearchCV
. 
best_estimator_"
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_fitted_model_eval/sklearn_fitted_model_eval/1.0.11.0	"What it does
 Given a fitted estimator and a labeled dataset, this tool outputs the performances of the fitted estimator on the labeled dataset with selected scorers. For the estimator, this tool supports fitted sklearn estimators and trained deep learning models. For input datasets, it supports the following: - tabular - sparse 
Output
 A tabular file containing performance scores, e.g.: ======== ======== ========= accuracy f1_macro precision ======== ======== ========= 0.8613 0.6759 0.7928 ======== ======== ========="
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_pairwise_metrics/sklearn_pairwise_metrics/1.0.11.0	"What it does
 This tool consists of utilities to evaluate pairwise distances or affinity of sets of samples. The base utilities are contained in Scikit-learn python library in sklearn.metrics package. This module contains both distance metrics and kernels. For a brief summary, please refer to: http://scikit-learn.org/stable/modules/metrics.html#metrics"
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_feature_selection/sklearn_feature_selection/1.0.11.0	"What it does
 This tool provides several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. This tool is based on sklearn.metrics package. For information about classification metric functions and their parameter settings please refer to 
Scikit-learn classification metrics
. .. 
Scikit-learn classification metrics
: http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_model_fit/sklearn_model_fit/1.0.11.0	"What it does
 This tools takes a pre-built model to simply fit on the provided labeled dataset by calling scikit-learn API 
fit
. The model could be built in 
build_pipeline
 tool, 
stacking_ensemble
 tool or other places. Limited deep learning model is also supported. The supported labeled dataset are the following: - tabular - sparse 
Output
 - fitted model, h5mlm model. - optional hdf5 file containing weights for deep learning models."
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_generalized_linear/sklearn_generalized_linear/1.0.11.0	"What it does
 This module implements a set of linear models for classification and regression such as: SGD classification and regression, Linear and Ridge regression and classification. This wrapper is using sklearn.linear_model module at its core. For information about linear models and their parameter settings please refer to 
Scikit-learn generalized linear models
. .. 
Scikit-learn generalized linear models
: http://scikit-learn.org/stable/modules/linear_model.html 
1 - Methods
 There are two groups of operations available: 1 - Train a model : A training set containing samples and their respective labels (or predicted values) are input. Based on the selected algorithm and options, an estimator object is fit to the data and is returned. 2 - Load a model and predict : An existing model predicts the class labels (or regression values) for a new dataset. 
2 - Trainig input
 When you choose to train a model, you need a features dataset X and a labels set y. This tool expects tabular or sparse data for X and a single column for y (tabular). You can select a subset of columns in a tabular dataset as your features dataset or labels column. Below you find some examples: 
Sample tabular features dataset
 The following training dataset contains 3 feature columns and a column containing class labels. You can simply select the first 3 columns as features and the last column as labels: :: 4.01163365529 -6.10797684314 8.29829894763 1 10.0788438916 1.59539821454 10.0684278289 0 -5.17607775503 -0.878286135332 6.92941850665 2 4.00975406235 -7.11847496542 9.3802423585 1 4.61204065139 -5.71217537352 9.12509610964 1 
Sample sparse features dataset
 In this case you cannot specifiy a column range. :: 4 1048577 8738 1 271 0.020833 1 1038 0.02461 2 829017 0.016 2 829437 0.012 2 830752 0.025 3 1047487 0.01 3 1047980 0.02 3 1048475 0.01 4 608 0.016629 4 1651 0.02519 4 4053 0.04223 
2 - Trainig output
 The trained model is generated and output in the form of a binary file. 
3 - Prediction input
 When you choose to load a model and do prediction, the tool expects an already trained estimator and a tabular dataset as input. The dataset contains new samples which you want to classify or predict regression values for. .. class:: warningmark The number of feature columns must be the same in training and prediction datasets! 
3 - Prediction output
 The tool predicts the class labels for new samples and adds them as the last column to the prediction dataset. The new dataset then is output as a tabular file. The prediction output format should look like the training dataset."
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_sample_generator/sklearn_sample_generator/1.0.11.0	"What it does
 This tool generates artificial data samples with specified size and controlled complexity. It provides sample generators for the following machine learning problems: 
1 - Single_label classification and clustering
 These generators produce a file containing the data samples. It is a tabular representation with samples in rows having features in columns. (In machine learning, each numerical property of a sample is called a feature.) The corresponding discrete targets are generated in a separate column. This column is added as the last coulmn of the data. 
Example
 Sample data with 4 features and a single target (n_samples=8 , n_features=4) : features columns :: 4.01163365529 -6.10797684314 8.29829894763 -9.10139563721 10.0788438916 1.59539821454 10.0684278289 4.16975127881 -5.17607775503 -0.878286135332 6.92941850665 -5.27083063186 4.00975406235 -7.11847496542 9.3802423585 -9.36732159584 4.61204065139 -5.71217537352 9.12509610964 -9.2260804162 8.26530668997 2.96705005011 8.88881190248 2.75339082289 2.96366327113 -3.76295851562 11.7113372463 -9.79136150321 8.13319631944 -0.223645298585 10.5820605308 4.47715318678 target column :: 1 0 2 1 1 0 1 0 The following generators are included in this section: * 
Isotropic Gaussian blobs for clustering
 creates multiclass datasets by allocating each class one or more normally-distributed clusters of points (isotropic = equally distributed in all directions). It provides control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. * 
Random n-class classification problem
 does the same specialising in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space. * 
Isotropic Gaussian and label samples by quantile
 divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres. * 
Data for binary classification (Hastie)
 generates a binary problem similar to the above with 10 features. * 
Circles
 and 
moons
 generate 2-dimensional binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualisation. 
2 - Generators for regression
 These generators produce output with same same format as in section 1, thoguh aimed for regression problems. The following generators are included in this section: * 
Random regression problem
 produces regression targets as an optionally-sparse random linear combination of random features, with noise. Its informative features may be uncorrelated, or low rank (few features account for most of the variance). It can produce multiple targets for each point. * 
Random regression problem with sparse uncorrelated design
 produces a target as a linear combination of four features with fixed coefficients. * 
Nonlinear generators
 encode explicitly non-linear relations: 
“Friedman #1”
 is related by polynomial and sine transforms; 
“Friedman #2”
 includes feature multiplication and reciprocation; and 
“Friedman #3”
 is similar with an arctan transformation on the target. 
3 - Generators for manifold learning
 Generators belonging to this group produce datasets suitable for non-linear dimensionality reduction problems. The idea behind this type of problem is that the dimensionality of many data sets is only artificially high. 
S curve dataset
 and 
Swiss roll dataset
 produce the same points-targets output format, sample points are 3-dimensional and the target column indicates the univariate position of the sample according to the main dimension of the points in the manifold."
toolshed.g2.bx.psu.edu/repos/goeckslab/ludwig_config_generator/ludwig_config_generator/0.10.1+3	Auto-generate a config file from a dataset for Ludwig training.
toolshed.g2.bx.psu.edu/repos/goeckslab/ludwig_evaluate/ludwig_evaluate/0.10.1+3	"What it does
 This tool conducts 
ludwig evaluate
. 
Input
 - a trained ludwig model. - dataset to be evaluate. 
Output
 - report in html. - a collection of prediction results."
toolshed.g2.bx.psu.edu/repos/goeckslab/ludwig_experiment/ludwig_experiment/0.10.1+3	"What it does
 Generic Learner Experiment: train on one (portion of) dataset and evalue the model performance on another (portion of) dataset. 
Output
 An HTML containing the evaluation report of the trained model. A trained Ludwig model composite dataset. Predictions results (csv and jsons) from the model evaluations."
toolshed.g2.bx.psu.edu/repos/goeckslab/ludwig_hyperopt/ludwig_hyperopt/0.10.1+3	"What it does
 Hyperparameter tuning. 
Input
 
Output
 - Hyperopt report - The best Ludwig model"
toolshed.g2.bx.psu.edu/repos/goeckslab/ludwig_predict/ludwig_predict/0.10.1+3	"What it does
 This tool conducts 
ludwig predict
. 
Input
 - a trained ludwig model. - dataset to be evaluate. 
Output
 - report in html. - a collection of prediction results."
toolshed.g2.bx.psu.edu/repos/goeckslab/ludwig_render_config/ludwig_render_config/0.10.1+3	"What it does
 Render a Ludwig config template. 
Output
 A yaml file."
toolshed.g2.bx.psu.edu/repos/goeckslab/ludwig_train/ludwig_train/0.10.1+3	"What it does
 Train a model. 
Output
 One trained ludwig_model type composite dataset. One html containing the training report."
toolshed.g2.bx.psu.edu/repos/goeckslab/ludwig_visualize/ludwig_visualize/0.10.1+3	"What it does
 This tool supports various of visualizations from Ludwig. 
Input
 Report output from ludwig train/experiment/evaluate/predict tool. 
Output
 PNG or PDF."
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_searchcv/sklearn_searchcv/1.0.11.0	"What it does
 Searches optimized parameter settings for an estimator or pipeline through either exhaustive grid cross validation search or Randomized cross validation search. please refer to 
Scikit-learn model_selection GridSearchCV
, 
Scikit-learn model_selection RandomizedSearchCV
 and 
Tuning hyper-parameters
. 
Return
 Outputs 
cv_results_
 from SearchCV in a tabular dataset if no train_test_split, otherwise the test score(s). Besides, Output of the SearchCV object is optional. 
How to choose search patameters grid?
 Please refer to 
svm
, 
linear_model
, 
ensemble
, 
naive_bayes
, 
tree
, 
neighbors
 and 
xgboost
 for estimator parameters. Refer to 
sklearn.preprocessing
, 
feature_selection
, 
decomposition
, 
kernel_approximation
, 
cluster.FeatureAgglomeration
 and 
skrebate
 for parameter in the pre-processing steps. 
Search parameter list
 can be list, numpy array, or distribution. The evaluation of settings supports operations in Math, list comprehension, numpy.arange(np_arange), most numpy.random(e.g., np_random_uniform) and some scipy.stats(e.g., scipy_stats_zipf) classes or functions, and others. Examples: - [3, 5, 7, 9] - list(range(50, 1001, 50)) - np_arange(0.01, 1, 0.1) - np_random_choice(list(range(1, 51)) + [None], size=20) - scipy_stats_randint(1, 11) 
Estimator / Preprocessor search (additional 
:
 in the front)
:: : [sklearn_tree.DecisionTreeRegressor(), sklearn_tree.ExtraTreeRegressor()] : [sklearn_feature_selection.SelectKBest(), sklearn_feature_selection.VarianceThreshold(), skrebate_ReliefF(), sklearn_preprocessing.RobustScaler()] 
Hot number/keyword for preprocessors
:: 0 sklearn_preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True) 1 sklearn_preprocessing.Binarizer(copy=True, threshold=0.0) 2 sklearn_preprocessing.MaxAbsScaler(copy=True) 3 sklearn_preprocessing.Normalizer(copy=True, norm='l2') 4 sklearn_preprocessing.MinMaxScaler(copy=True, feature_range=(0, 1)) 5 sklearn_preprocessing.PolynomialFeatures(degree=2, include_bias=True, interaction_only=False) 6 sklearn_preprocessing.RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True, with_scaling=True) 7 sklearn_feature_selection.SelectKBest(k=10, score_func=<function f_classif at 0x113806d90>) 8 sklearn_feature_selection.GenericUnivariateSelect(mode='percentile', param=1e-05, score_func=<function f_classif at 0x113806d90>) 9 sklearn_feature_selection.SelectPercentile(percentile=10, score_func=<function f_classif at 0x113806d90>) 10 sklearn_feature_selection.SelectFpr(alpha=0.05, score_func=<function f_classif at 0x113806d90>) 11 sklearn_feature_selection.SelectFdr(alpha=0.05, score_func=<function f_classif at 0x113806d90>) 12 sklearn_feature_selection.SelectFwe(alpha=0.05, score_func=<function f_classif at 0x113806d90>) 13 sklearn_feature_selection.VarianceThreshold(threshold=0.0) 14 sklearn_decomposition.FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=None, noise_variance_init=None, random_state=0, svd_method='randomized', tol=0.01) 15 sklearn_decomposition.FastICA(algorithm='parallel', fun='logcosh', fun_args=None, max_iter=200, n_components=None, random_state=0, tol=0.0001, w_init=None, whiten=True) 16 sklearn_decomposition.IncrementalPCA(batch_size=None, copy=True, n_components=None, whiten=False) 17 sklearn_decomposition.KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=False, gamma=None, kernel='linear', kernel_params=None, max_iter=None, n_components=None, random_state=0, remove_zero_eig=False, tol=0) 18 sklearn_decomposition.LatentDirichletAllocation(batch_size=128, doc_topic_prior=None, evaluate_every=-1, learning_decay=0.7, learning_method=None, learning_offset=10.0, max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001, n_components=10, n_topics=None, perp_tol=0.1, random_state=0, topic_word_prior=None, total_samples=1000000.0, verbose=0) 19 sklearn_decomposition.MiniBatchDictionaryLearning(alpha=1, batch_size=3, dict_init=None, fit_algorithm='lars', n_components=None, n_iter=1000, random_state=0, shuffle=True, split_sign=False, transform_algorithm='omp', transform_alpha=None, transform_n_nonzero_coefs=None, verbose=False) 20 sklearn_decomposition.MiniBatchSparsePCA(alpha=1, batch_size=3, callback=None, method='lars', n_components=None, n_iter=100, random_state=0, ridge_alpha=0.01, shuffle=True, verbose=False) 21 sklearn_decomposition.NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200, n_components=None, random_state=0, shuffle=False, solver='cd', tol=0.0001, verbose=0) 22 sklearn_decomposition.PCA(copy=True, iterated_power='auto', n_components=None, random_state=0, svd_solver='auto', tol=0.0, whiten=False) 23 sklearn_decomposition.SparsePCA(U_init=None, V_init=None, alpha=1, max_iter=1000, method='lars', n_components=None, random_state=0, ridge_alpha=0.01, tol=1e-08, verbose=False) 24 sklearn_decomposition.TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5, random_state=0, tol=0.0) 25 sklearn_kernel_approximation.Nystroem(coef0=None, degree=None, gamma=None, kernel='rbf', kernel_params=None, n_components=100, random_state=0) 26 sklearn_kernel_approximation.RBFSampler(gamma=1.0, n_components=100, random_state=0) 27 sklearn_kernel_approximation.AdditiveChi2Sampler(sample_interval=None, sample_steps=2) 28 sklearn_kernel_approximation.SkewedChi2Sampler(n_components=100, random_state=0, skewedness=1.0) 29 sklearn_cluster.FeatureAgglomeration(affinity='euclidean', compute_full_tree='auto', connectivity=None, linkage='ward', memory=None, n_clusters=2, pooling_func=<function mean at 0x113078ae8>) 30 skrebate_ReliefF(discrete_threshold=10, n_features_to_select=10, n_neighbors=100, verbose=False) 31 skrebate_SURF(discrete_threshold=10, n_features_to_select=10, verbose=False) 32 skrebate_SURFstar(discrete_threshold=10, n_features_to_select=10, verbose=False) 33 skrebate_MultiSURF(discrete_threshold=10, n_features_to_select=10, verbose=False) 34 skrebate_MultiSURFstar(discrete_threshold=10, n_features_to_select=10, verbose=False) 'sk_prep_all': All sklearn preprocessing estimators, i.e., 0-6 'fs_all': All feature_selection estimators, i.e., 7-13 'decomp_all': All decomposition estimators, i.e., 14-24 'k_appr_all': All kernel_approximation estimators, i.e., 25-28 'reb_all': All skrebate estimators, i.e., 30-34 'all_0': All except the imbalanced-learn samplers, i.e., 0-34 'imb_all': All imbalanced-learn sampling methods, i.e., 35-53. 
CAUTION
: Mix of imblearn and other preprocessors may not work. None: opt out of preprocessor Support mix (CAUTION: Mix of imblearn and other preprocessors may not work), e.g.:: : [None, 'sk_prep_all', 21, 'k_appr_all', sklearn_feature_selection.SelectKBest(k=50)] 
Whether to do train_test_split?
 Please refer to 
https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation
 .. image:: https://scikit-learn.org/stable/_images/grid_search_cross_validation.png :height: 300 :width: 400 .. 
Scikit-learn model_selection GridSearchCV
: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html .. 
Scikit-learn model_selection RandomizedSearchCV
: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html .. 
Tuning hyper-parameters
: http://scikit-learn.org/stable/modules/grid_search.html .. 
svm
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm .. 
linear_model
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model .. 
ensemble
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble .. 
naive_bayes
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes .. 
tree
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree .. 
neighbors
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors .. 
xgboost
: https://xgboost.readthedocs.io/en/latest/python/python_api.html .. 
sklearn.preprocessing
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing .. 
feature_selection
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection .. 
decomposition
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition .. 
kernel_approximation
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.kernel_approximation .. 
cluster.FeatureAgglomeration
: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html .. 
skrebate
: https://epistasislab.github.io/scikit-rebate/using/ .. 
https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation
: https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation"
toolshed.g2.bx.psu.edu/repos/goeckslab/image_learner/image_learner/0.1.5	"What it does
 Image Learner for Classification/regression: trains and evaluates a image classification/regression model. It uses the metadata csv to find the image paths and labels. The metadata csv should contain a column with the name 'image_path' and a column with the name 'label'. Optionally, you can also add a column with the name 'split' to specify which split each row belongs to (train, val, test). If you do not provide a split column, the tool will automatically split the data into train, val, and test sets based on the proportions you specify or [0.7, 0.1, 0.2] by default. You can optionally specify a sample ID column to keep related samples in the same split and prevent data leakage. 
Models Available
 This tool supports a wide range of state-of-the-art image classification models including: - Traditional CNNs (ResNet, EfficientNet, VGG, etc.) - Vision Transformers (ViT, Swin Transformer) - Modern architectures (ConvNeXt, MaxViT) - MetaFormer family models (IdentityFormer, RandFormer, PoolFormerV2, ConvFormer, CAFormer) 
MetaFormer Models
 The MetaFormer family represents a unified perspective on transformer-like architectures. These models demonstrate that the success of transformers is largely due to their general architecture rather than specific components like attention mechanisms. All MetaFormer models use pretrained weights from Hugging Face and provide explicit confirmation of weight loading. 
If the selected label column has more than 10 unique values, the tool will automatically treat the task as a regression problem and apply appropriate metrics (e.g., MSE, RMSE, R²).
 
Outputs
 The tool will output a trained model in the form of a ludwig_model file, a report in the form of an HTML file, and a collection of CSV/json/png files containing the predictions, experiment stats and visualizations. The html report will contain metrics&experiment setup parameters, train&val plots and test plots."
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_lightgbm/sklearn_lightgbm/1.0.11.0	"What it does
 LightGBM is a gradient boosting framework that uses tree based learning algorithms. For information about the algorithm and parameter settings please refer to the 
LightGBM website
. .. 
LightGBM website
: https://lightgbm.readthedocs.io/en/latest/index.html 
1 - Methods
 There are two operations available: 1 - Train a model: A training set containing samples and their respective labels (or predicted values) are used as input. Based on the options selected, an estimator object is fitted to the data and is returned. 2 - Load a model and predict: An existing model is used to predict the class labels (or regression values) for a new dataset. 
2 - Training input
 When you choose to train a model, you need a features dataset X and a labels set y. This tool expects tabular or sparse data for X and a single column for y (tabular). You can select a subset of columns in a tabular dataset as your features dataset or labels column. Below some examples are shown: 
Sample tabular features dataset
 The following training dataset contains 3 feature columns and a column containing class labels. You can simply select the first 3 columns as features and the last column as labels: :: 4.01163365529 -6.10797684314 8.29829894763 1 10.0788438916 1.59539821454 10.0684278289 0 -5.17607775503 -0.878286135332 6.92941850665 2 4.00975406235 -7.11847496542 9.3802423585 1 4.61204065139 -5.71217537352 9.12509610964 1 
Sample sparse features dataset
 In this case you cannot specify a column range. :: 4 1048577 8738 1 271 0.02083333333333341 1 1038 0.02461995616119806 2 829017 0.01629088031127686 2 829437 0.01209127083516686 2 830752 0.02535100632816968 3 1047487 0.01485722929945572 3 1047980 0.02640566620767753 3 1048475 0.01665869913262564 4 608 0.01662975263094352 4 1651 0.02519674277562741 4 4053 0.04223659971350601 
2 - Training output
 The trained model is generated and output in the form of a text file. 
3 - Prediction input
 When you choose to load a model and do prediction, the tool expects an already trained estimator and a tabular dataset as input. The dataset contains new samples for which you want to classify or predict values. .. class:: warningmark The number of feature columns must be the same in training and prediction datasets! 
3 - Prediction output
 The tool predicts the class labels for new samples and adds them as the last column to the prediction dataset. The new dataset (i.e. tabular input plus an additional column containing predicted values) is then returned as a tabular file. The prediction output format should look like the training dataset."
toolshed.g2.bx.psu.edu/repos/bgruening/model_prediction/model_prediction/1.0.11.0	"What it does
 Given a fitted estimator and new data sets, this tool outpus the prediction results on the data sets via invoking the estimator's 
predict
 or 
predict_proba
 method. For estimator, this tool supports fitted sklearn estimators and trained deep learning models. It predicts on three different dataset inputs, - tabular - sparse - bio-sequences in a fasta file - reference genome and variant call file"
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_model_validation/sklearn_model_validation/1.0.11.0	"What it does
 This tool includes model validation functions to evaluate estimator performance in the cross-validation approach. This tool is based on sklearn.model_selection package. For information about model validation functions and their parameter settings please refer to 
Scikit-learn model_selection
. .. 
Scikit-learn model_selection
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection"
toolshed.g2.bx.psu.edu/repos/goeckslab/multimodal_learner/multimodal_learner/0.1.4	"AutoGluon Multimodal Learner
 Trains a powerful multimodal model combining tabular features, images, and text using AutoGluon-Multimodal. - Handles missing images intelligently - Supports cross-validation - Produces detailed HTML reports and transparent metrics - Fully reproducible Ideal for medical imaging + clinical data, product images + descriptions, etc."
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_nn_classifier/sklearn_nn_classifier/1.0.11.0	"What it does
 This module implements the k-nearest neighbors classification algorithms. For more information check http://scikit-learn.org/stable/modules/neighbors.html"
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_numeric_clustering/sklearn_numeric_clustering/1.0.11.0	"What it does
 This tool offers different clustering algorithms which are provided by scikit-learn to find similarities among samples and cluster the samples based on these similarities."
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_build_pipeline/sklearn_build_pipeline/1.0.11.0	"What it does
 This tool not only builds sklearn pipeline object, but also builds single main estimator or single preprocessing component. The output object type is based on the length of pipeline steps. When there is only one step (choose 
None
 for others), either a main estimator or preprocessor, the component is output directly instead of wrapping in a pipeline object. A typical pipeline chains one or more preprocessing steps plus a final main estimator, for example, [VarianceThreshold, StandardScaler, SGDClassifier] which is composed of a feature selctor, a preprocessing scaler and a main estimator together. For more information, please refer to 
Scikit-learn pipeline Pipeline
. 
Pre-processing components
 come from 
sklearn.preprocessing
, 
feature_selection
, 
decomposition
, 
kernel_approximation
, 
cluster.FeatureAgglomeration
, 
skrebate
 and more. 
Final Estimator
 supports estimators from 
xgboost
 and many scikit-learn modules, including 
svm
, 
linear_model
, 
ensemble
, 
naive_bayes
, 
tree
, 
neighbors
 and so on. 
Custom estimators
 - 
GenomeOneHotEncoder
 - 
ProteinOnehotEncoder
 - 
IRAPSClassifier
 - 
BinarizeTargetClassifier
 - 
BinarizeTargetRegressor
 
Output
 - Pipeline/estimator object - Hyperparameter of the object (optional) .. 
Scikit-learn pipeline Pipeline
: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html .. 
svm
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm .. 
linear_model
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model .. 
ensemble
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble .. 
naive_bayes
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes .. 
tree
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree .. 
neighbors
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors .. 
xgboost
: https://xgboost.readthedocs.io/en/latest/python/python_api.html .. 
sklearn.preprocessing
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing .. 
feature_selection
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection .. 
decomposition
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition .. 
kernel_approximation
: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.kernel_approximation .. 
cluster.FeatureAgglomeration
: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html .. 
skrebate
: https://epistasislab.github.io/scikit-rebate/using/ .. 
GenomeOneHotEncoder
: https://goeckslab.github.io/Galaxy-ML/APIs/preprocessors/#genomeonehotencoder .. 
ProteinOnehotEncoder
: https://goeckslab.github.io/Galaxy-ML/APIs/preprocessors/#proteinonehotencoder .. 
IRAPSClassifier
: https://goeckslab.github.io/Galaxy-ML/APIs/iraps-classifier/#irapsclassifier .. 
BinarizeTargetClassifier
: https://goeckslab.github.io/Galaxy-ML/APIs/binarize-target/#binarizetargetclassifier .. 
BinarizeTargetRegressor
: https://goeckslab.github.io/Galaxy-ML/APIs/binarize-target/#binarizetargetregressor"
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_data_preprocess/sklearn_data_preprocess/1.0.11.0	"What it does
 This tool provides several transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators. The library is provided by sklearn.preprocessing package. For information about preprocessing classes and parameter settings please refer to 
Scikit-learn preprocessing
. .. 
Scikit-learn preprocessing
: http://scikit-learn.org/stable/modules/preprocessing.html"
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_pca/sklearn_pca/1.0.11.0	".. class:: infomark 
What it does
 This tool takes a tabular input file (one data point per row, each column a variable) and performs principal component analysis (PCA) using Singular Value Decomposition, returning an equally sized tabular file with the first PC in the first column, second PC in the second column, etc."
toolshed.g2.bx.psu.edu/repos/goeckslab/pycaret_predict/pycaret_predict/3.3.2+2	"This tool uses PyCaret to evaluate a machine learning model or do prediction. 
Outputs
: - 
prediction
: The prediction results on the dataset in a csv format. - 
report
: The evaluation report is generated in HTML format. if you upload a dataset with a target column and select the target column in the target_feature input field."
toolshed.g2.bx.psu.edu/repos/bgruening/scipy_sparse/scipy_sparse/1.0.11.0	"What it does
 This tool stacks sparse matrices horizontally (column wise) or vertically (row wise). It can handle two different formats: * Compressed Sparse Column matrix (csc_matrix) * Compressed Sparse Row matrix (csr_matrix) Sparse matrices in column format should be stacked horizontally (hstack) , while matrices in row format are stacked vertically (vstack). This tool outputs a single resulting sparse matrix which is compatible with the inputs in format. 
Parameters:
 blocks sequence of sparse matrices with compatible shapes format. For more information please refer to DOI:10.1109/MCSE.2011.37."
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_train_test_split/sklearn_train_test_split/1.0.11.0	"What it does
 This tool implements splitter function and classes from 
sklearn.model_selection
 module to split contents (rows) of a table into two subsets for training and test, respectively . The simple train test split mode not only supports shuffle split and stratified shuffle split natively carried by the 
train_test_split
 function, but also gets extended to do group shuffle. The cross-validation splitter mode supports more diverse splitting strategies. Each tool run outputs one split, train and test. To get different splitting sets, for example, nested CV, multiple tool runs are needed with different 
nth_split
. - Train Test Split mode - direct split, no shuffle - shuffle split - stratified shuffle split - group shuffle split - Cross-Validation Splitter mode - KFold - StratifiedKFold - LeaveOneOut - LeavePOut - ... Input: a tabular dataset. Output: two tabular datasets containing training and test subsets, respectively."
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_stacking_ensemble_models/sklearn_stacking_ensemble_models/1.0.8.4	"This tool wrapps Stacking Regression, also called Super Learning, in which different base algorithms train on the original dataset and predict results respectively, a second level of 
metalearner
 fits on the previous prediction results to ensemble a strong learner. Refer to 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html#introduction
. .. 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html#introduction
: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html#introduction"
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_svm_classifier/sklearn_svm_classifier/1.0.11.0	"What it does
 This module implements the Support Vector Machine (SVM) classification algorithms. Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. 
The advantages of support vector machines are:
 1- Effective in high dimensional spaces. 2- Still effective in cases where number of dimensions is greater than the number of samples. 3- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient. 4- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels. 
The disadvantages of support vector machines include:
 1- If the number of features is much greater than the number of samples, the method is likely to give poor performances. 2- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation For more information check http://scikit-learn.org/stable/modules/neighbors.html"
toolshed.g2.bx.psu.edu/repos/goeckslab/tabular_learner/tabular_learner/0.1.4	"This tool uses PyCaret to train and evaluate machine learning models. It compares different models on a dataset and provides the best model based on the performance metrics. You can optionally select a sample ID column to keep related records in the same split and reduce data leakage when the tool creates splits internally. 
Outputs
 - 
Model
: The best model trained on the dataset in h5 format. - 
Comparison Result
: The comparison result of different models in html format. It contains the performance metrics of different models, plots of the best model on the testing set (or part of the training set if a separate test set is not uploaded), and feature analysis plots."
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_to_categorical/sklearn_to_categorical/1.0.11.0	"What it does
 Converts a class vector (integers) to binary class matrix. tf.keras.utils.to_categorical( y, num_classes=None, dtype='float32' ) E.g. for use with categorical_crossentropy. Arguments y: a vector of numbers to be converted into a matrix of one-hot encoded values. num_classes: total number of classes. If None, this would be inferred as the (largest number in y) + 1. dtype: The data type expected by the input. Default: 'float32'. Returns A binary matrix representation of the input. The classes axis is placed last. Raises Value Error: If input contains string value"
toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_train_test_eval/sklearn_train_test_eval/1.0.11.0	"What it does
 Given an estimator and dataset, this tool fits the estimator with part of the datasets and evalue the performance of the fitted estimator on the rest of the datasets. It consists of two modes: train-test and train-val-test. - train-test: data sets will be split into train and test portions. Estimator is training on the train portion, and performance is evaluated on the test portion. - train-val-test: data sets are split into three portions, train, val and test. Validations happen along with the training process, which is often useful in 
deep learnings
. 
Output
 Performance scores."
toolshed.g2.bx.psu.edu/repos/iuc/chopin2/chopin2/1.0.9.post1+galaxy0	"chopin2 is a domain-agnostic supervised learning classifier built according to the Hyperdimensional Computing paradigm. It also implements a feature selection method based on the backward variable elimination strategy. ----- 
Input
 The input is a CSV file representing a matrix with the observations on the rows and features on columns. The first column must contain the observation IDs, while the last column contains the classes. Also, the first line must contain the header with the column names. The tool doesn't support datasets with missing values. It also supports numerical datasets only. Please note that categorical values are allowed under the first and last columns. ----- 
Output
 The output is a summary table with information about the accuracy of the hyperdimensional model and the number of retraining iterations that were required to achieve that level of accuracy. In case the feature selection is enabled, it also returns a file with the list of selected features that come out from the hyperdimensional classification model with the best accuracy. ----- .. class:: infomark 
Notes
 Please visit the official GitHub repository_ for other information about 
chopin2
. .. _repository: https://github.com/cumbof/chopin2"
toolshed.g2.bx.psu.edu/repos/iuc/bioext_bealign/bioext_bealign/0.21.10+galaxy0	bealign ------- Align sequences to a reference using a codon alignment algorithm. NOTES ----- Reference can be one of the presets or a custom history reference.
toolshed.g2.bx.psu.edu/repos/iuc/bbtools_bbmap/bbtools_bbmap/39.01+galaxy0	"What it does
 BBMap is a splice-aware global aligner for DNA and RNA sequencing reads. It is fast and extremely accurate, particularly with highly mutated genomes or reads with long indels, even whole-gene deletions over 100kbp long. It has no upper limit to genome size or number of contigs and has been successfully used for mapping to an 85 gigabase soil metagenome with over 200 million contigs. the indexing phase is very fast compared to other aligners. BBMap can output many different statistics files; an empirical read quality histogram, insert-size distribution, and genome coverage with or without generating a sam file. It is useful in quality control of libraries and sequencing runs or evaluating new sequencing platforms. 
Options
 
Bam sorting mode
 - the generated bam files can be sorted according to three criteria: coordinates, names and input order. * Sort by chromosomal coordinates - the file is sorted by coordinates (i.e., the reads from the beginning of the first chromosome are first in the file. * Sort by read names - the file is sorted by the reference ID (i.e., the QNAME field). * Not sorted (sorted as input) - the file is sorted in the order of the reads in the input file."
toolshed.g2.bx.psu.edu/repos/iuc/bwa_mem2/bwa_mem2/2.3+galaxy0	"What is does
 BWA-MEM2 is the new version of the bwa-mem algorithm in bwa. It produces alignment identical to bwa and is ~1.3-3.1x faster depending on the use-case, dataset and the running machine. The algorithm is robust to sequencing errors and applicable to a wide range of sequence lengths from 70bp to a few megabases. The Galaxy implementation takes fastq files as input and produces output in BAM format, which can be further processed using various BAM utilities exiting in Galaxy (BAMTools, SAMTools, Picard). ----- 
Indices: Selecting reference genomes for BWA
 Galaxy wrapper for BWA allows you select between precomputed and user-defined indices for reference genomes using 
Will you select a reference genome from your history or use a built-in index?
 flag. This flag has two options: 1. 
Use a built-in genome index
 - when selected (this is default), Galaxy provides the user with 
Select reference genome index
 dropdown. Genomes listed in this dropdown have been pre-indexed with bwa index utility and are ready to be mapped against. 2. 
Use a genome from the history and build index
 - when selected, Galaxy provides the user with 
Select reference genome sequence
 dropdown. This dropdown is populated by all FASTA formatted files listed in your current history. If your genome of interest is uploaded into history it will be shown there. Selecting a genome from this dropdown will cause Galaxy to first transparently index it using 
bwa index
 command, and then run mapping with 
bwa mem
. If your genome of interest is not listed here you have two choices: 1. Contact galaxy team using 
Help->Support
 link at the top of the interface and let us know that an index needs to be added 2. Upload your genome of interest as a FASTA file to Galaxy history and selected 
Use a genome from the history and build index
 option. ----- 
Galaxy-specific option
 Galaxy allows four levels of control over bwa-mem options provided by 
Select analysis mode
 menu option. These are: 1. 
Simple Illumina mode
: The simplest possible bwa mem application in which it alignes single or paired-end data to reference using default parameters. It is equivalent to the following command: bwa mem <reference index> <fastq dataset1> [fastq dataset2] 2. 
PacBio mode
: The mode adjusted specifically for mapping of long PacBio subreads. Equivalent to the following command: bwa mem -k17 -W40 -r10 -A1 -B1 -O1 -E1 -L0 <reference index> <PacBio dataset in fastq format> 3. 
Full list of options
: Allows access to all options through Galaxy interface. ----- 
Bam sorting mode
 The generated bam files can be sorted according to three criteria: coordinates, names and input order. In coordinate sorted mode the reads are sorted by coordinates. It means that the reads from the beginning of the first chromosome are first in the file. When sorted by read name, the file is sorted by the reference ID (i.e., the QNAME field). Finally, the 
No sorted (sorted as input)
 option yield a BAM file in which the records are sorted in an order corresponding to the order of the reads in the original input file. This option requires using a single thread to perform the conversion from SAM to BAM format, so the runtime is extended. ----- .. class:: warningmark 
Read Groups are Important!
 One of the recommended best practices in NGS analysis is adding read group information to BAM files. You can do this directly in BWA interface using the 
Specify read group information?
 widget. If you are not familiar with read groups you shold know that this is effectively a way to tag reads with an additional ID. This allows you to combine BAM files from, for example, multiple BWA runs into a single dataset. This significantly simplifies downstream processing as instead of dealing with multiple datasets you only have to handle only one. This is possible because the read group information allows you to identify data from different experiments even if they are combined in one file. Many downstream analysis tools such as variant callers (e.g., FreeBayes or Naive Variant Caller present in Galaxy) are aware of read groups and will automatically generate calls for each individual sample even if they are combined within a single file. 
Description of read groups fields
 (from GATK FAQ webpage): .. csv-table:: :header-rows: 1 Tag,Importance,Definition,Meaning ""ID"",""Required"",""Read group identifier. Each @RG line must have a unique ID. The value of ID is used in the RG tags of alignment records. Must be unique among all read groups in header section. Read group IDs may be modified when merging SAM files in order to handle collisions."",""Ideally, this should be a globally unique identify across all sequencing data in the world, such as the Illumina flowcell + lane name and number. Will be referenced by each read with the RG:Z field, allowing tools to determine the read group information associated with each read, including the sample from which the read came. Also, a read group is effectively treated as a separate run of the NGS instrument in tools like base quality score recalibration (a GATK component) -- all reads within a read group are assumed to come from the same instrument run and to therefore share the same error model."" ""SM"",""Sample. Use pool name where a pool is being sequenced."",""Required. As important as ID."",""The name of the sample sequenced in this read group. GATK tools treat all read groups with the same SM value as containing sequencing data for the same sample. Therefore it's critical that the SM field be correctly specified, especially when using multi-sample tools like the Unified Genotyper (a GATK component)."" ""PL"",""Platform/technology used to produce the read. Valid values: ILLUMINA, SOLID, LS454, HELICOS and PACBIO."",""Important. Not currently used in the GATK, but was in the past, and may return. The only way to known the sequencing technology used to generate the sequencing data"",""It's a good idea to use this field."" ""LB"",""DNA preparation library identify"",""Essential for MarkDuplicates"",""MarkDuplicates uses the LB field to determine which read groups might contain molecular duplicates, in case the same DNA library was sequenced on multiple lanes."" 
Example of Read Group usage
 Suppose we have a trio of samples: MOM, DAD, and KID. Each has two DNA libraries prepared, one with 400 bp inserts and another with 200 bp inserts. Each of these libraries is run on two lanes of an illumina hiseq, requiring 3 x 2 x 2 = 12 lanes of data. When the data come off the sequencer, we would create 12 BAM files, with the following @RG fields in the header:: Dad's data: @RG ID:FLOWCELL1.LANE1 PL:illumina LB:LIB-DAD-1 SM:DAD PI:200 @RG ID:FLOWCELL1.LANE2 PL:illumina LB:LIB-DAD-1 SM:DAD PI:200 @RG ID:FLOWCELL1.LANE3 PL:illumina LB:LIB-DAD-2 SM:DAD PI:400 @RG ID:FLOWCELL1.LANE4 PL:illumina LB:LIB-DAD-2 SM:DAD PI:400 Mom's data: @RG ID:FLOWCELL1.LANE5 PL:illumina LB:LIB-MOM-1 SM:MOM PI:200 @RG ID:FLOWCELL1.LANE6 PL:illumina LB:LIB-MOM-1 SM:MOM PI:200 @RG ID:FLOWCELL1.LANE7 PL:illumina LB:LIB-MOM-2 SM:MOM PI:400 @RG ID:FLOWCELL1.LANE8 PL:illumina LB:LIB-MOM-2 SM:MOM PI:400 Kid's data: @RG ID:FLOWCELL2.LANE1 PL:illumina LB:LIB-KID-1 SM:KID PI:200 @RG ID:FLOWCELL2.LANE2 PL:illumina LB:LIB-KID-1 SM:KID PI:200 @RG ID:FLOWCELL2.LANE3 PL:illumina LB:LIB-KID-2 SM:KID PI:400 @RG ID:FLOWCELL2.LANE4 PL:illumina LB:LIB-KID-2 SM:KID PI:400 Note the hierarchical relationship between read groups (unique for each lane) to libraries (sequenced on two lanes) and samples (across four lanes, two lanes for each library). ----- .. class:: infomark 
More info
 To obtain more information about BWA and ask questions use these resources: 1. https://biostar.usegalaxy.org/ 2. https://www.biostars.org/ 3. https://github.com/bwa-mem2/bwa-mem2 4. http://bio-bwa.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/iuc/bwa_mem2/bwa_mem2_idx/2.3+galaxy0	"What is does
 BWA-MEM2 is the new version of the bwa-mem algorithm in bwa. It produces alignment identical to bwa and is ~1.3-3.1x faster depending on the use-case, dataset and the running machine. The algorithm is robust to sequencing errors and applicable to a wide range of sequence lengths from 70bp to a few megabases. This tools build a reference index for the bwa-mem2 galaxy tool. ----- .. class:: infomark 
More info
 To obtain more information about BWA and ask questions use these resources: 1. https://biostar.usegalaxy.org/ 2. https://www.biostars.org/ 3. https://github.com/bwa-mem2/bwa-mem2 4. http://bio-bwa.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/richard-burhans/batched_lastz/batched_lastz/1.04.22+galaxy6	"Batched LASTZ is a special version of LASTZ designed to process output of Galaxy's KegAlign tool. It outputs pairiwse alignments in a verietry of formats. 
Using this tool
 .. class:: warningmark This tool is the second part of a two-step process for generation of paiwrise alignments. The output of this tool is alignments in a format that was specified in 
Output Options
 section of KegAlign tool. 
What it does
 KegAlign processes 
Target
 and 
Query
 sequences to identify highly similar regions where gapped extension will be performed to create actual alignments. The actual alignments are generated by 
Batched LASTZ
 that should be run on the output of this tool. The default output is a MAF alignment file. Other formats can be configured in 
Output Options
 section. See LASTZ manual <https://lastz.github.io/lastz/#formats>`_ for description of possible formats."
toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.4+galaxy0	"Bowtie2 Overview
 Bowtie2_ is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters to relatively long (e.g. mammalian) genomes. Bowtie 2 supports gapped, local, and paired-end alignment modes. Galaxy wrapper for Bowtie 2 outputs alignments in 
BAM format
, enabling interoperation with a large number of other tools available at this site. Majority of information in this page is derived from an excellent 
Bowtie2 manual
 written by Ben Langmead. .. 
Bowtie2: http://bowtie-bio.sourceforge.net/bowtie2/ .. 
Bowtie2 manual
: http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml .. _
BAM format
: http://samtools.github.io/hts-specs/SAMv1.pdf ----- 
Selecting reference genomes for Bowtie2
 Galaxy wrapper for Bowtie2 allows you select between precomputed and user-defined indices for reference genomes using 
Will you select a reference genome from your history or use a built-in index?
 flag. This flag has two options: 1. 
Use a built-in genome index
 - when selected (this is default), Galaxy provides the user with 
Select reference genome index
 dropdown. Genomes listed in this dropdown have been pre-indexed with bowtie2-build utility and are ready to be mapped against. 2. 
Use a genome from the history and build index
 - when selected, Galaxy provides the user with 
Select reference genome sequence
 dropdown. This dropdown is populated by all FASTA formatted files listed in your current history. If your genome of interest is uploaded into history it will be shown there. Selecting a genome from this dropdown will cause Galaxy to first transparently index it using bowtie2-build command, and then run mapping with bowtie2. If your genome of interest is not listed here you have two choices: 1. Contact galaxy team using 
Help->Support
 link at the top of the interface and let us know that an index needs to be added 2. Upload your genome of interest as a FASTA file to Galaxy history and selected 
Use a genome from the history and build index
 option. ------ .. class:: infomark 
Bowtie2 options
 Galaxy wrapper for Bowtie2 implements most but not all options available through the command line. Supported options are described below. ----- 
Inputs
 Bowtie 2 accepts files in Sanger FASTQ format (single or paired-end). Paired-end data can represented as two individual (forward and reverse) datasets, as well as a single interleaved dataset (see an example at the end of the help section). ------ 
Input options
:: --interleaved Reads interleaved FASTQ files where the first two records (8 lines) represent a mate pair. -s/--skip <int> Skip (i.e. do not align) the first 
&lt;int&gt;
 reads or pairs in the input. -u/--qupto <int> Align the first 
&lt;int&gt;
 reads or read pairs from the input (after the 
-s
/
--skip
 reads or pairs have been skipped), then stop. Default: no limit. -5/--trim5 <int> Trim 
&lt;int&gt;
 bases from 5' (left) end of each read before alignment (default: 0). -3/--trim3 <int> Trim 
&lt;int&gt;
 bases from 3' (right) end of each read before alignment (default: 0). --phred33 Input qualities are ASCII chars equal to the Phred quality plus 33. This is also called the ""Phred+33"" encoding, which is used by the very latest Illumina pipelines. --phred64 Input qualities are ASCII chars equal to the Phred quality plus 64. This is also called the ""Phred+64"" encoding. --solexa-quals Convert input qualities from Solexa Phred quality (which can be negative) to Phred Phred quality (which can't). This scheme was used in older Illumina GA Pipeline versions (prior to 1.3). Default: off. --int-quals Quality values are represented in the read input file as space-separated ASCII integers, e.g., 
40 40 30 40
..., rather than ASCII characters, e.g., 
II?I
.... Integers are treated as being on the Phred quality scale unless 
--solexa-quals
 is also specified. Default: off. ------ 
Presets in 
--end-to-end
 mode
:: --very-fast Same as: 
-D 5 -R 1 -N 0 -L 22 -i S,0,2.50
 --fast Same as: 
-D 10 -R 2 -N 0 -L 22 -i S,0,2.50
 --sensitive Same as: 
-D 15 -R 2 -L 22 -i S,1,1.15
 (default in 
--end-to-end
 mode) --very-sensitive Same as: 
-D 20 -R 3 -N 0 -L 20 -i S,1,0.50
 ------ 
Presets options in 
--local
 mode
:: --very-fast-local Same as: 
-D 5 -R 1 -N 0 -L 25 -i S,1,2.00
 --fast-local Same as: 
-D 10 -R 2 -N 0 -L 22 -i S,1,1.75
 --sensitive-local Same as: 
-D 15 -R 2 -N 0 -L 20 -i S,1,0.75
 (default in 
--local
 mode) --very-sensitive-local Same as: 
-D 20 -R 3 -N 0 -L 20 -i S,1,0.50
 ------ 
Alignment options
:: -N <int> Sets the number of mismatches to allowed in a seed alignment during multiseed alignment. Can be set to 0 or 1. Setting this higher makes alignment slower (often much slower) but increases sensitivity. Default: 0. -L <int> Sets the length of the seed substrings to align during multiseed alignment. Smaller values make alignment slower but more sensitive. Default: the 
--sensitive
 preset is used by default, which sets 
-L
 to 22 in 
--end-to-end
 mode and to 20 in 
--local
 mode. -i <func> Sets a function governing the interval between seed substrings to use during multiseed alignment. For instance, if the read has 30 characers, and seed length is 10, and the seed interval is 6, the seeds extracted will be: Read: TAGCTACGCTCTACGCTATCATGCATAAAC Seed 1 fw: TAGCTACGCT Seed 1 rc: AGCGTAGCTA Seed 2 fw: CGCTCTACGC Seed 2 rc: GCGTAGAGCG Seed 3 fw: ACGCTATCAT Seed 3 rc: ATGATAGCGT Seed 4 fw: TCATGCATAA Seed 4 rc: TTATGCATGA Since it's best to use longer intervals for longer reads, this parameter sets the interval as a function of the read length, rather than a single one-size-fits-all number. For instance, specifying 
-i S,1,2.5
 sets the interval function 
f
 to 
f(x) = 1 + 2.5 * sqrt(x)
, where x is the read length. If the function returns a result less than 1, it is rounded up to 1. Default: the 
--sensitive
 preset is used by default, which sets 
-i
 to 
S,1,1.15
 in 
--end-to-end
 mode to 
-i S,1,0.75
 in 
--local
 mode. --n-ceil <func> Sets a function governing the maximum number of ambiguous characters (usually 
N
s and/or 
.
s) allowed in a read as a function of read length. For instance, specifying 
-L,0,0.15
 sets the N-ceiling function 
f
 to 
f(x) = 0 + 0.15 * x
, where x is the read length. Reads exceeding this ceiling are filtered out. Default: 
L,0,0.15
. --dpad <int> ""Pads"" dynamic programming problems by 
&lt;int&gt;
 columns on either side to allow gaps. Default: 15. --gbar <int> Disallow gaps within 
&lt;int&gt;
 positions of the beginning or end of the read. Default: 4. --ignore-quals When calculating a mismatch penalty, always consider the quality value at the mismatched position to be the highest possible, regardless of the actual value. I.e. input is treated as though all quality values are high. This is also the default behavior when the input doesn't specify quality values (e.g. in 
-f
, 
-r
, or 
-c
 modes). --nofw/--norc If 
--nofw
 is specified, 
bowtie2
 will not attempt to align unpaired reads to the forward (Watson) reference strand. If 
--norc
 is specified, 
bowtie2
 will not attempt to align unpaired reads against the reverse-complement (Crick) reference strand. In paired-end mode, 
--nofw
 and 
--norc
 pertain to the fragments; i.e. specifying 
--nofw
 causes 
bowtie2
 to explore only those paired-end configurations corresponding to fragments from the reverse-complement (Crick) strand. Default: both strands enabled. --no-1mm-upfront By default, Bowtie 2 will attempt to find either an exact or a 1-mismatch end-to-end alignment for the read 
before
 trying the multiseed heuristic. Such alignments can be found very quickly, and many short read alignments have exact or near-exact end-to-end alignments. However, this can lead to unexpected alignments when the user also sets options governing the multiseed heuristic, like 
-L
 and 
-N
. For instance, if the user specifies 
-N 0
 and 
-L
 equal to the length of the read, the user will be surprised to find 1-mismatch alignments reported. This option prevents Bowtie 2 from searching for 1-mismatch end-to-end alignments before using the multiseed heuristic, which leads to the expected behavior when combined with options such as 
-L
 and 
-N
. This comes at the expense of speed. --end-to-end In this mode, Bowtie 2 requires that the entire read align from one end to the other, without any trimming (or ""soft clipping"") of characters from either end. The match bonus 
--ma
 always equals 0 in this mode, so all alignment scores are less than or equal to 0, and the greatest possible alignment score is 0. This is mutually exclusive with 
--local
. 
--end-to-end
 is the default mode. --local In this mode, Bowtie 2 does not require that the entire read align from one end to the other. Rather, some characters may be omitted (""soft clipped"") from the ends in order to achieve the greatest possible alignment score. The match bonus 
--ma
 is used in this mode, and the best possible alignment score is equal to the match bonus (
--ma
) times the length of the read. Specifying 
--local
 and one of the presets (e.g. 
--local --very-fast
) is equivalent to specifying the local version of the preset (
--very-fast-local
). This is mutually exclusive with 
--end-to-end
. 
--end-to-end
 is the default mode. ----- 
Scoring options
:: --ma <int> Sets the match bonus. In 
--local
 mode 
&lt;int&gt;
 is added to the alignment score for each position where a read character aligns to a reference character and the characters match. Not used in 
--end-to-end
 mode. Default: 2. --mp MX,MN Sets the maximum (
MX
) and minimum (
MN
) mismatch penalties, both integers. A number less than or equal to 
MX
 and greater than or equal to 
MN
 is subtracted from the alignment score for each position where a read character aligns to a reference character, the characters do not match, and neither is an 
N
. If 
--ignore-quals
 is specified, the number subtracted quals 
MX
. Otherwise, the number subtracted is 
MN + floor( (MX-MN)(MIN(Q, 40.0)/40.0) )
 where Q is the Phred quality value. Default: 
MX
 = 6, 
MN
 = 2. --np <int> Sets penalty for positions where the read, reference, or both, contain an ambiguous character such as 
N
. Default: 1. --rdg <int1>,<int2> Sets the read gap open (
&lt;int1&gt;
) and extend (
&lt;int2&gt;
) penalties. A read gap of length N gets a penalty of 
&lt;int1&gt;
 + N * 
&lt;int2&gt;
. Default: 5, 3. --rfg <int1>,<int2> Sets the reference gap open (
&lt;int1&gt;
) and extend (
&lt;int2&gt;
) penalties. A reference gap of length N gets a penalty of 
&lt;int1&gt;
 + N * 
&lt;int2&gt;
. Default: 5, 3. --score-min <func> Sets a function governing the minimum alignment score needed for an alignment to be considered ""valid"" (i.e. good enough to report). This is a function of read length. For instance, specifying 
L,0,-0.6
 sets the minimum-score function 
f
 to 
f(x) = 0 + -0.6 * x
, where 
x
 is the read length. The default in 
--end-to-end
 mode is 
L,-0.6,-0.6
 and the default in 
--local
 mode is 
G,20,8
. ----- 
Reporting options
:: -k <int> By default, 
bowtie2
 searches for distinct, valid alignments for each read. When it finds a valid alignment, it continues looking for alignments that are nearly as good or better. The best alignment found is reported (randomly selected from among best if tied). Information about the best alignments is used to estimate mapping quality and to set SAM optional fields, such as 
AS:i
 and 
XS:i
. When 
-k
 is specified, however, 
bowtie2
 behaves differently. Instead, it searches for at most 
&lt;int&gt;
 distinct, valid alignments for each read. The search terminates when it can't find more distinct valid alignments, or when it finds 
&lt;int&gt;
, whichever happens first. All alignments found are reported in descending order by alignment score. The alignment score for a paired-end alignment equals the sum of the alignment scores of the individual mates. Each reported read or pair alignment beyond the first has the SAM 'secondary' bit (which equals 256) set in its FLAGS field. For reads that have more than 
&lt;int&gt;
 distinct, valid alignments, 
bowtie2
 does not guarantee that the 
&lt;int&gt;
 alignments reported are the best possible in terms of alignment score. 
-k
 is mutually exclusive with 
-a
. Note: Bowtie 2 is not designed with large values for 
-k
 in mind, and when aligning reads to long, repetitive genomes large 
-k
 can be very, very slow. -a Like 
-k
 but with no upper limit on number of alignments to search for. 
-a
 is mutually exclusive with 
-k
. Note: Bowtie 2 is not designed with 
-a
 mode in mind, and when aligning reads to long, repetitive genomes this mode can be very, very slow. ----- 
Effort options
:: -D <int> Up to 
&lt;int&gt;
 consecutive seed extension attempts can ""fail"" before Bowtie 2 moves on, using the alignments found so far. A seed extension ""fails"" if it does not yield a new best or a new second-best alignment. This limit is automatically adjusted up when -k or -a are specified. Default: 15. -R <int> 
&lt;int&gt;
 is the maximum number of times Bowtie 2 will ""re-seed"" reads with repetitive seeds. When ""re-seeding,"" Bowtie 2 simply chooses a new set of reads (same length, same number of mismatches allowed) at different offsets and searches for more alignments. A read is considered to have repetitive seeds if the total number of seed hits divided by the number of seeds that aligned at least once is greater than 300. Default: 2. ----- 
Paired-end options
:: -I/--minins <int> The minimum fragment length for valid paired-end alignments. E.g. if 
-I 60
 is specified and a paired-end alignment consists of two 20-bp alignments in the appropriate orientation with a 20-bp gap between them, that alignment is considered valid (as long as 
-X
 is also satisfied). A 19-bp gap would not be valid in that case. If trimming options 
-3
 or 
-5
 are also used, the 
-I
 constraint is applied with respect to the untrimmed mates. The larger the difference between 
-I
 and 
-X
, the slower Bowtie 2 will run. This is because larger differences bewteen 
-I
 and 
-X
 require that Bowtie 2 scan a larger window to determine if a concordant alignment exists. For typical fragment length ranges (200 to 400 nucleotides), Bowtie 2 is very efficient. Default: 0 (essentially imposing no minimum) -X/--maxins <int> The maximum fragment length for valid paired-end alignments. E.g. if 
-X 100
 is specified and a paired-end alignment consists of two 20-bp alignments in the proper orientation with a 60-bp gap between them, that alignment is considered valid (as long as 
-I
 is also satisfied). A 61-bp gap would not be valid in that case. If trimming options 
-3
 or 
-5
 are also used, the 
-X
 constraint is applied with respect to the untrimmed mates, not the trimmed mates. The larger the difference between 
-I
 and 
-X
, the slower Bowtie 2 will run. This is because larger differences bewteen 
-I
 and 
-X
 require that Bowtie 2 scan a larger window to determine if a concordant alignment exists. For typical fragment length ranges (200 to 400 nucleotides), Bowtie 2 is very efficient. Default: 500. --fr/--rf/--ff The upstream/downstream mate orientations for a valid paired-end alignment against the forward reference strand. E.g., if 
--fr
 is specified and there is a candidate paired-end alignment where mate 1 appears upstream of the reverse complement of mate 2 and the fragment length constraints (
-I
 and 
-X
) are met, that alignment is valid. Also, if mate 2 appears upstream of the reverse complement of mate 1 and all other constraints are met, that too is valid. 
--rf
 likewise requires that an upstream mate1 be reverse-complemented and a downstream mate2 be forward-oriented. 
--ff
 requires both an upstream mate 1 and a downstream mate 2 to be forward-oriented. Default: 
--fr
 (appropriate for Illumina's Paired-end Sequencing Assay). --no-mixed By default, when 
bowtie2
 cannot find a concordant or discordant alignment for a pair, it then tries to find alignments for the individual mates. This option disables that behavior. --no-discordant By default, 
bowtie2
 looks for discordant alignments if it cannot find any concordant alignments. A discordant alignment is an alignment where both mates align uniquely, but that does not satisfy the paired-end constraints (
--fr
/
--rf
/
--ff
, 
-I
, 
-X
). This option disables that behavior. --dovetail If the mates ""dovetail"", that is if one mate alignment extends past the beginning of the other such that the wrong mate begins upstream, consider that to be concordant. Default: mates cannot dovetail in a concordant alignment. --no-contain If one mate alignment contains the other, consider that to be non-concordant. Default: a mate can contain the other in a concordant alignment. --no-overlap If one mate alignment overlaps the other at all, consider that to be non-concordant. Default: mates can overlap in a concordant alignment. ------ 
SAM options
:: --rg-id <text> Set the read group ID to 
&lt;text&gt;
. This causes the SAM 
@RG
 header line to be printed, with 
&lt;text&gt;
 as the value associated with the 
ID:
 tag. It also causes the 
RG:Z:
 extra field to be attached to each SAM output record, with value set to 
&lt;text&gt;
. --rg <text> Add 
&lt;text&gt;
 (usually of the form 
TAG:VAL
, e.g. 
SM:Pool1
) as a field on the 
@RG
 header line. Note: in order for the 
@RG
 line to appear, 
--rg-id
 must also be specified. This is because the 
ID
 tag is required by the SAM Specification. Specify 
--rg
 multiple times to set multiple fields. See the SAM Specification for details about what fields are legal. --omit-sec-seq When printing secondary alignments, Bowtie 2 by default will write out the 
SEQ
 and 
QUAL
 strings. Specifying this option causes Bowtie 2 to print an asterix in those fields instead. ----- 
Other options
:: --reorder Guarantees that output SAM records are printed in an order corresponding to the order of the reads in the original input file, even when 
-p
 is set greater than 1. Specifying 
--reorder
 and setting 
-p
 greater than 1 causes Bowtie 2 to run somewhat slower and use somewhat more memory then if 
--reorder
 were not specified. Has no effect if 
-p
 is set to 1, since output order will naturally correspond to input order in that case. --seed <int> Use 
&lt;int&gt;
 as the seed for pseudo-random number generator. Default: 0. --non-deterministic Normally, Bowtie 2 re-initializes its pseudo-random generator for each read. It seeds the generator with a number derived from (a) the read name, (b) the nucleotide sequence, (c) the quality sequence, (d) the value of the 
--seed
 option. This means that if two reads are identical (same name, same nucleotides, same qualities) Bowtie 2 will find and report the same alignment(s) for both, even if there was ambiguity. When 
--non-deterministic
 is specified, Bowtie 2 re-initializes its pseudo-random generator for each read using the current time. This means that Bowtie 2 will not necessarily report the same alignment for two identical reads. This is counter-intuitive for some users, but might be more appropriate in situations where the input consists of many identical reads. ----- 
Paired-end (and mate-pair) data in fastq format
 Paired end datasets can be represented as two individual datasets: First dataset:: @1/1 AGGGATGTGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTA + EGGEGGGDFGEEEAEECGDEGGFEEGEFGBEEDDECFEFDD@CDD<ED @2/1 AGGGATGTGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTA + HHHHHHEGFHEEFEEHEEHHGGEGGGGEFGFGGGGHHHHFBEEEEEFG Second dataset:: @1/2 CCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC + GHHHDFDFGFGEGFBGEGGEGEGGGHGFGHFHFHHHHHHHEF?EFEFF @2/2 CCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC + HHHHHHHHHHHHHGHHHHHHGHHHHHHHHHHHFHHHFHHHHHHHHHHH Or a single 
interleaved
 dataset:: @1/1 AGGGATGTGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTA + EGGEGGGDFGEEEAEECGDEGGFEEGEFGBEEDDECFEFDD@CDD<ED @1/2 CCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC + GHHHDFDFGFGEGFBGEGGEGEGGGHGFGHFHFHHHHHHHEF?EFEFF @2/1 AGGGATGTGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTA + HHHHHHEGFHEEFEEHEEHHGGEGGGGEFGFGGGGHHHHFBEEEEEFG @2/2 CCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC + HHHHHHHHHHHHHGHHHHHHGHHHHHHHHHHHFHHHFHHHHHHHHHHH"
toolshed.g2.bx.psu.edu/repos/richard-burhans/kegalign/kegalign/0.1.2.8+galaxy0	"KegAlign is a scalable, GPU-accelerated system for computing pairwise WGA. KegAlign is based on the standard seed-filter-extend heuristic, in which the filtering stage dominates the runtime (e.g. 98% for human-mouse WGA), and is accelerated using GPU(s). KegAlign was designed as a faster replacement for lastz pairwise aligner. 
Using this tool
 .. class:: warningmark This tool is the first part of a two-step process for generation of paiwrise alignments. The output of this tool is used as an input to 
Batched LASTZ
 tool. The 
Batched LASTZ
 can be found in the list of tool of this Galaxy instance. 
What it does
 KegAlign processes 
Target
 and 
Query
 sequences to identify highly similar regions where gapped extension will be performed to create actual alignments. The actual alignments are generated by 
Batched LASTZ
 that should be run on the output of this tool. .. class:: infomark Although this tool is only the first part of the alignment process all parameters related to generation of alignments are set 
during this stage
. 
Scoring Options
 By default the HOXD70 substitution scores are used (from 
Chiaromonte et al. 2002 &lt;https://www.ncbi.nlm.nih.gov/pubmed/11928468&gt;
_):: bad_score = X:-1000 # used for sub['X'][
] and sub[
]['X'] fill_score = -100 # used when sub[
][
] is not defined gap_open_penalty = 400 gap_extend_penalty = 30 A C G T A 91 -114 -31 -123 C -114 100 -125 -31 G -31 -125 100 -114 T -123 -31 -114 91 Matrix can be supplied as an input to 
Read the substitution scores
 parameter in 
Scoring
 section. Substitution matrix can be inferred from your data using another LASTZ-based tool (LASTZ_D: Infer substitution scores). 
Output Options
 .. class:: infomark The final format in which alignmnets will be generated by 
Batched LASTZ
 are set here. The default output is a MAF alignment file. Other formats can be configured in 
Output Options* section. See LASTZ manual <https://lastz.github.io/lastz/#formats>`_ for description of possible formats."
toolshed.g2.bx.psu.edu/repos/devteam/lastz/lastz_wrapper_2/1.04.22+galaxy0	"What is does
 LASTZ is designed to preprocess one sequence or set of sequences (which we collectively call the 
TARGET
) and then align several 
QUERY
 sequences to it. It was developed by 
Bob Harris &lt;http://www.bx.psu.edu/~rsharris/&gt;
 in the lab of Webb Miller at Penn State. .. class:: infomark 
Read documentation
 before proceeding. LASTZ is a complex tool with many parameter options. Fortunately, there is a 
great manual &lt;https://lastz.github.io/lastz/&gt;
 maintained by its author. Default parameters may be sufficient to obtain the initial idea about how similar your sequences are, but to produce reliable alignments you may need to tweak the parameters. Read the manual. .. class:: warningmark Galaxy version of LASTZ sets 
--ambiguous=iupac
 as default (see 
Scoring
 section). This prevents LASTZ from erroring out if one of the DNA inputrs contains ""non-standard"" nucleotides. 
About LASTZ parameters
 Galaxy's version of LASTZ has nine parameter sections (
Where to look
, 
Scoring
, 
Seeding
, 
HSPs
, 
Chaining
, 
Gapped extension
, 
Filtering
, 
Interpolation
, and 
Output
). These sections closely follow parameter description in the 
manual &lt;https://lastz.github.io/lastz/#syntax&gt;
. 
Defaults
 here are defaults for some of the most important parameters:: --seed=<pattern> set seed pattern (12of19, 14of22, or general pattern) (default is 1110100110010101111) SEE ""Seeding"" SECTION -> ""Select seed type"" --[no]transition allow (or don't) one transition in a seed hit (by default a transition is allowed) SEE ""Seeding"" SECTION -> ""Allow transitions"" --[no]chain perform chaining (by default no chaining is performed) SEE ""Chaining"" SECTION --[no]gapped perform gapped alignment (instead of gap-free) (by default gapped alignment is performed) SEE ""Gapped extension"" SECTION --strand=both search both strands --strand=plus search + strand only (matching strand of query spec) (by default both strands are searched) SEE ""Where to look"" SECTION --scores=<file> read substitution and gap scores from a file SEE ""Scoring"" SECTION --xdrop=<score> set x-drop threshold (default is 10sub[A][A]) SEE ""HSPs"" SECTION --ydrop=<score> set y-drop threshold (default is open+300extend) SEE ""Gapped extension"" SECTION --hspthresh=<score> set threshold for high scoring pairs (default is 3000) ungapped extensions scoring lower are discarded <score> can also be a percentage or base count SEE ""HSPs"" SECTION --gappedthresh=<score> set threshold for gapped alignments gapped extensions scoring lower are discarded <score> can also be a percentage or base count (default is to use same value as --hspthresh) SEE ""Gapped extension"" SECTION 
Substitution matrix
 By default the HOXD70 substitution scores are used (from 
Chiaromonte et al. 2002 &lt;https://www.ncbi.nlm.nih.gov/pubmed/11928468&gt;
):: bad_score = X:-1000 # used for sub['X'][
] and sub[
]['X'] fill_score = -100 # used when sub[
][
] is not defined gap_open_penalty = 400 gap_extend_penalty = 30 A C G T A 91 -114 -31 -123 C -114 100 -125 -31 G -31 -125 100 -114 T -123 -31 -114 91 Matrix can be supplied as an input to 
Read the substitution scores
 parameter in 
Scoring
 section. Substitution matrix can be inferred from your data using another LASTZ-based tool (LASTZ_D: Infer substitution scores). 
Output
 This version of LASTZ produces one output by default: a BAM alignment file. Other formats as well as a Dot Plot can be configured in 
Output
 section. This incarnation of LASTZ produces outputs without comment line starting with '#'. To learn identity of each column, consult 
formats section of LASTZ manual &lt;https://lastz.github.io/lastz/#formats&gt;
_."
toolshed.g2.bx.psu.edu/repos/devteam/lastz/lastz_d_wrapper/1.04.22+galaxy0	"What is does
 LASTZ_D is a non-integer (
D
 stands for Double) version of LASTZ that can be used to estimate substitution matrix that will be used to score alignments. It was developed by 
Bob Harris &lt;http://www.bx.psu.edu/~rsharris/&gt;
 in the lab of Webb Miller at Penn State as a part of LASTZ. Matrix computed by this tool is to be used by LASTZ (see below). .. class:: warningmark 
Read documentation
 before proceeding. LASTZ is a complex tool with many parameter options. Fortunately, there is a 
great manual &lt;https://lastz.github.io/lastz/&gt;
 maintained by its author. The two sections that are particularly relevant to the inference of substitution matrix are 
Inferring Score Sets &lt;https://www.bx.psu.edu/~rsharris/lastz/README.lastz-1.04.03.html#adv_inference&gt;
 and 
Inference Control File &lt;https://www.bx.psu.edu/~rsharris/lastz/README.lastz-1.04.03.html#fmt_inference&gt;
. 
Notes on the inference
 Inference is achieved by computing the probability of each of the 18 different alignment events (gap open, gap extend, and 16 substitutions). These probabilities are estimated from alignments of the sequences. Of course, at first we don't have alignments, so the process begins by using a generic scoring set to create alignments, infer scores from those, then realign, and so on, until the scores stabilize or ""converge"". Ungapped alignments are performed until the substitution scores converge, then gapped alignments are performed (holding the substitution scores constant) until the gap penalties converge. In the end you get a matrix like this:: # (a LASTZ scoring set, created by ""LASTZ --infer"") bad_score = X:-1781 # used for sub[X][
] and sub[
][X] fill_score = -178 # used when sub[
][
] not otherwise defined gap_open_penalty = 400 gap_extend_penalty = 30 A C G T A 72 -79 -49 -97 C -79 100 -178 -49 G -49 -178 100 -79 T -97 -49 -79 72 This dataset can then be used as an input to the 
Read the substitution scores
 parameter of LASTZ (Parameter section 
Scoring
). The iterative process can fail if there's not a lot of sequence to align. E.g. if after the 4th iteration there's nothing in the central 50% denominators go to zero and the process fails. If the sequences you are aligning have GC content different than the usual ACGT 30-20-20-30 split, scoring inference should discover this and give you better alignments."
toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/0.7.19	"What it does
 BWA_ is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. The bwa-aln algorithm is designed for Illumina sequence reads up to 100bp. For longer reads use the separate BWA-MEM Galaxy tool. This Galaxy tool wraps the bwa-aln, bwa-samse and -sampe modules of the BWA read mapping tool: - 
bwa aln
 - actual mapper placing reads onto the reference sequence - 
bwa samse
 - post-processor converting suffix array coordinates into genome coordinates in SAM format for single reads - 
bam sampe
 - post-processor for paired reads For more details about the different modules of the BWA package see the 
BWA manual
. The Galaxy implementation takes fastq or BAM (unaligned BAM) datasets as input and produces output in BAM format, which can be further processed using various BAM utilities existing in Galaxy (BAMTools, SAMTools, Picard). ----- 
Indices: Selecting reference genomes for BWA
 The Galaxy wrapper for BWA allows you to select between precomputed and user-defined indices for reference genomes using the 
Will you select a reference genome from your history or use a built-in index?
 select box. This select box has two options: 1. 
Use a built-in genome index
 With this option (which is the default), Galaxy provides you with a dropdown select menu populated with genomes that have been pre-indexed with the bwa index utility and are ready to map sequenced reads against. The collection of pre-indexed genomes is managed by the administrators of your Galaxy instance. If your genome of interest is missing and its impractical to use the second option below to work with it, consider contacting the support team for the Galaxy server you are working on to let them know that you would like to have an additional genome indexed. 2. 
Use a genome from history and build index
 With this option, Galaxy provides you with a dropdown select menu populated with all FASTA formatted files listed in your current history. If you have uploaded your genome of interest into your history it will be shown there. Selecting a genome from this dropdown will cause Galaxy to index it transparently first using the 
bwa index
 command, and then map against it with 
bwa aln
. ----- .. class:: warningmark 
Read Groups are Important!
 One of the recommended best practices in NGS analysis is adding read group information to BAM files. You can do this directly in BWA interface using the 
Specify read group information?
 widget. If you are not familiar with read groups you shold know that this is effectively a way to tag reads with an additional ID. This allows you to combine BAM files from, for example, multiple BWA runs into a single dataset. This significantly simplifies downstream processing as instead of dealing with multiple datasets you only have to handle only one. This is possible because the read group information allows you to identify data from different experiments even if they are combined in one file. Many downstream analysis tools such as variant callers (e.g., FreeBayes or Naive Variant Caller present in Galaxy) are aware of read groups and will automatically generate calls for each individual sample even if they are combined within a single file. 
Description of read groups fields
 (from GATK FAQ webpage): .. csv-table:: :header-rows: 1 Tag,Importance,Definition,Meaning ""ID"",""Required"",""Read group identifier. Each @RG line must have a unique ID. The value of ID is used in the RG tags of alignment records. Must be unique among all read groups in header section. Read group IDs may be modified when merging SAM files in order to handle collisions."",""Ideally, this should be a globally unique identify across all sequencing data in the world, such as the Illumina flowcell + lane name and number. Will be referenced by each read with the RG:Z field, allowing tools to determine the read group information associated with each read, including the sample from which the read came. Also, a read group is effectively treated as a separate run of the NGS instrument in tools like base quality score recalibration (a GATK component) -- all reads within a read group are assumed to come from the same instrument run and to therefore share the same error model."" ""SM"",""Sample. Use pool name where a pool is being sequenced."",""Required. As important as ID."",""The name of the sample sequenced in this read group. GATK tools treat all read groups with the same SM value as containing sequencing data for the same sample. Therefore it's critical that the SM field be correctly specified, especially when using multi-sample tools like the Unified Genotyper (a GATK component)."" ""PL"",""Platform/technology used to produce the read. Valid values: ILLUMINA, SOLID, LS454, HELICOS and PACBIO."",""Important. Not currently used in the GATK, but was in the past, and may return. The only way to known the sequencing technology used to generate the sequencing data"",""It's a good idea to use this field."" ""LB"",""DNA preparation library identify"",""Essential for MarkDuplicates"",""MarkDuplicates uses the LB field to determine which read groups might contain molecular duplicates, in case the same DNA library was sequenced on multiple lanes."" 
Example of Read Group usage
 Suppose we have a trio of samples: MOM, DAD, and KID. Each has two DNA libraries prepared, one with 400 bp inserts and another with 200 bp inserts. Each of these libraries is run on two lanes of an illumina hiseq, requiring 3 x 2 x 2 = 12 lanes of data. When the data come off the sequencer, we would create 12 BAM files, with the following @RG fields in the header:: Dad's data: @RG ID:FLOWCELL1.LANE1 PL:illumina LB:LIB-DAD-1 SM:DAD PI:200 @RG ID:FLOWCELL1.LANE2 PL:illumina LB:LIB-DAD-1 SM:DAD PI:200 @RG ID:FLOWCELL1.LANE3 PL:illumina LB:LIB-DAD-2 SM:DAD PI:400 @RG ID:FLOWCELL1.LANE4 PL:illumina LB:LIB-DAD-2 SM:DAD PI:400 Mom's data: @RG ID:FLOWCELL1.LANE5 PL:illumina LB:LIB-MOM-1 SM:MOM PI:200 @RG ID:FLOWCELL1.LANE6 PL:illumina LB:LIB-MOM-1 SM:MOM PI:200 @RG ID:FLOWCELL1.LANE7 PL:illumina LB:LIB-MOM-2 SM:MOM PI:400 @RG ID:FLOWCELL1.LANE8 PL:illumina LB:LIB-MOM-2 SM:MOM PI:400 Kid's data: @RG ID:FLOWCELL2.LANE1 PL:illumina LB:LIB-KID-1 SM:KID PI:200 @RG ID:FLOWCELL2.LANE2 PL:illumina LB:LIB-KID-1 SM:KID PI:200 @RG ID:FLOWCELL2.LANE3 PL:illumina LB:LIB-KID-2 SM:KID PI:400 @RG ID:FLOWCELL2.LANE4 PL:illumina LB:LIB-KID-2 SM:KID PI:400 Note the hierarchical relationship between read groups (unique for each lane) to libraries (sequenced on two lanes) and samples (across four lanes, two lanes for each library). .. _BWA: https://github.com/lh3/bwa .. 
BWA manual
: https://bio-bwa.sourceforge.net/bwa.shtml"
toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.19	"What it does
 This Galaxy tool wraps the bwa-mem module of the BWA_ read mapping tool. For more details about the different modules of the BWA package see the 
BWA manual
. The Galaxy implementation takes fastq files as input and produces output in BAM format, which can be further processed using various BAM utilities existing in Galaxy (BAMTools, SAMTools, Picard). From http://arxiv.org/abs/1303.3997: BWA-MEM is an alignment algorithm for aligning sequence reads or long query sequences against a large reference genome such as human. It automatically chooses between local and end-to-end alignments, supports paired-end reads and performs chimeric alignment. The algorithm is robust to sequencing errors and applicable to a wide range of sequence lengths from 70bp to a few megabases. ----- 
Indices: Selecting reference genomes for BWA
 The Galaxy wrapper for BWA allows you to select between precomputed and user-defined indices for reference genomes using the 
Will you select a reference genome from your history or use a built-in index?
 select box. This select box has two options: 1. 
Use a built-in genome index
 With this option (which is the default), Galaxy provides you with a dropdown select menu populated with genomes that have been pre-indexed with the bwa index utility and are ready to map sequenced reads against. The collection of pre-indexed genomes is managed by the administrators of your Galaxy instance. If your genome of interest is missing and its impractical to use the second option below to work with it, consider contacting the support team for the Galaxy server you are working on to let them know that you would like to have an additional genome indexed. 2. 
Use a genome from history and build index
 With this option, Galaxy provides you with a dropdown select menu populated with all FASTA formatted files listed in your current history. If you have uploaded your genome of interest into your history it will be shown there. Selecting a genome from this dropdown will cause Galaxy to index it transparently first using the 
bwa index
 command, and then map against it with 
bwa aln
. ----- 
Analysis modes
 The tool supports different preconfigured analysis modes optimized for different types of input data. Alternatively, it allows you to take full control over all available options. The preconfigured modes are: 1. 
Simple Illumina mode
 This corresponds to the simplest possible and standard bwa mem application in which it aligns single or paired-end data to a reference using default parameters. It is equivalent to the following command: bwa mem <reference index> <fastq dataset1> [fastq dataset2] 2. 
PacBio mode
 This mode is adjusted specifically for mapping of long PacBio subreads. It is running bwa mame with the 
-x pacbio
 option. 3. 
Nanopore 2D-reads mode
 This mode is running bwa mem with the 
-x ont2d
 option. 4. 
Intra-sepcies contigs mode
 This mode is running bwa mem with the 
-x intractg
 option. .. class:: infomark Please note: minimap2
 is recommended over and outperforms BWA-MEM for most types of input data except for Illumina short reads. For Illumina short-read mapping you may also consider using 
BWA-MEM2
, which is about twice as fast as BWA-MEM. ----- 
Bam sorting mode
 The generated bam files can be sorted according to three criteria: coordinates, names and input order. In coordinate sorted mode the reads are sorted by coordinates. It means that the reads from the beginning of the first chromosome are first in the file. When sorted by read name, the file is sorted by the reference ID (i.e., the QNAME field). Finally, the 
No sorted (sorted as input)
 option yield a BAM file in which the records are sorted in an order corresponding to the order of the reads in the original input file. This option requires using a single thread to perform the conversion from SAM to BAM format, so the runtime is extended. ----- .. class:: warningmark 
Read Groups are Important!
 One of the recommended best practices in NGS analysis is adding read group information to BAM files. You can do this directly in BWA interface using the 
Specify read group information?
 widget. If you are not familiar with read groups you shold know that this is effectively a way to tag reads with an additional ID. This allows you to combine BAM files from, for example, multiple BWA runs into a single dataset. This significantly simplifies downstream processing as instead of dealing with multiple datasets you only have to handle only one. This is possible because the read group information allows you to identify data from different experiments even if they are combined in one file. Many downstream analysis tools such as variant callers (e.g., FreeBayes or Naive Variant Caller present in Galaxy) are aware of read groups and will automatically generate calls for each individual sample even if they are combined within a single file. 
Description of read groups fields
 (from GATK FAQ webpage): .. csv-table:: :header-rows: 1 Tag,Importance,Definition,Meaning ""ID"",""Required"",""Read group identifier. Each @RG line must have a unique ID. The value of ID is used in the RG tags of alignment records. Must be unique among all read groups in header section. Read group IDs may be modified when merging SAM files in order to handle collisions."",""Ideally, this should be a globally unique identify across all sequencing data in the world, such as the Illumina flowcell + lane name and number. Will be referenced by each read with the RG:Z field, allowing tools to determine the read group information associated with each read, including the sample from which the read came. Also, a read group is effectively treated as a separate run of the NGS instrument in tools like base quality score recalibration (a GATK component) -- all reads within a read group are assumed to come from the same instrument run and to therefore share the same error model."" ""SM"",""Sample. Use pool name where a pool is being sequenced."",""Required. As important as ID."",""The name of the sample sequenced in this read group. GATK tools treat all read groups with the same SM value as containing sequencing data for the same sample. Therefore it's critical that the SM field be correctly specified, especially when using multi-sample tools like the Unified Genotyper (a GATK component)."" ""PL"",""Platform/technology used to produce the read. Valid values: ILLUMINA, SOLID, LS454, HELICOS and PACBIO."",""Important. Not currently used in the GATK, but was in the past, and may return. The only way to known the sequencing technology used to generate the sequencing data"",""It's a good idea to use this field."" ""LB"",""DNA preparation library identify"",""Essential for MarkDuplicates"",""MarkDuplicates uses the LB field to determine which read groups might contain molecular duplicates, in case the same DNA library was sequenced on multiple lanes."" 
Example of Read Group usage
 Suppose we have a trio of samples: MOM, DAD, and KID. Each has two DNA libraries prepared, one with 400 bp inserts and another with 200 bp inserts. Each of these libraries is run on two lanes of an illumina hiseq, requiring 3 x 2 x 2 = 12 lanes of data. When the data come off the sequencer, we would create 12 BAM files, with the following @RG fields in the header:: Dad's data: @RG ID:FLOWCELL1.LANE1 PL:illumina LB:LIB-DAD-1 SM:DAD PI:200 @RG ID:FLOWCELL1.LANE2 PL:illumina LB:LIB-DAD-1 SM:DAD PI:200 @RG ID:FLOWCELL1.LANE3 PL:illumina LB:LIB-DAD-2 SM:DAD PI:400 @RG ID:FLOWCELL1.LANE4 PL:illumina LB:LIB-DAD-2 SM:DAD PI:400 Mom's data: @RG ID:FLOWCELL1.LANE5 PL:illumina LB:LIB-MOM-1 SM:MOM PI:200 @RG ID:FLOWCELL1.LANE6 PL:illumina LB:LIB-MOM-1 SM:MOM PI:200 @RG ID:FLOWCELL1.LANE7 PL:illumina LB:LIB-MOM-2 SM:MOM PI:400 @RG ID:FLOWCELL1.LANE8 PL:illumina LB:LIB-MOM-2 SM:MOM PI:400 Kid's data: @RG ID:FLOWCELL2.LANE1 PL:illumina LB:LIB-KID-1 SM:KID PI:200 @RG ID:FLOWCELL2.LANE2 PL:illumina LB:LIB-KID-1 SM:KID PI:200 @RG ID:FLOWCELL2.LANE3 PL:illumina LB:LIB-KID-2 SM:KID PI:400 @RG ID:FLOWCELL2.LANE4 PL:illumina LB:LIB-KID-2 SM:KID PI:400 Note the hierarchical relationship between read groups (unique for each lane) to libraries (sequenced on two lanes) and samples (across four lanes, two lanes for each library). .. _BWA: https://github.com/lh3/bwa .. 
BWA manual
: https://bio-bwa.sourceforge.net/bwa.shtml .. 
minimap2: https://github.com/lh3/minimap2 .. 
BWA-MEM2
: https://github.com/bwa-mem2/bwa-mem2"
toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy2	"Users' Guide ------------ Minimap2 is a versatile sequence alignment program that aligns DNA or mRNA sequences against a large reference database. Typical use cases include: (1) mapping PacBio or Oxford Nanopore genomic reads to the human genome; (2) finding overlaps between long reads with error rate up to ~15%; (3) splice-aware alignment of PacBio Iso-Seq or Nanopore cDNA or Direct RNA reads against a reference genome; (4) aligning Illumina single- or paired-end reads; (5) assembly-to-assembly alignment; (6) full-genome alignment between two closely related species with divergence below ~15%. For ~10kb noisy reads sequences, minimap2 is tens of times faster than mainstream long-read mappers such as BLASR, BWA-MEM, NGMLR and GMAP. It is more accurate on simulated long reads and produces biologically meaningful alignment ready for downstream analyses. For >100bp Illumina short reads, minimap2 is three times as fast as BWA-MEM and Bowtie2, and as accurate on simulated data. Detailed evaluations are available from the 
minimap2 preprint
. General usage ~~~~~~~~~~~~~ Minimap2 seamlessly works with gzip’d FASTA and FASTQ formats as input. You don’t need to convert between FASTA and FASTQ or decompress gzip’d files first. For the human reference genome, minimap2 takes a few minutes to generate a minimizer index for the reference before mapping. To reduce indexing time, you can optionally save the index with option 
-d
 and replace the reference sequence file with the index file on the minimap2 command line: 
Importantly
, it should be noted that once you build the index, indexing parameters such as 
-k
, 
-w
, 
-H
 and 
-I
 can’t be changed during mapping. If you are running minimap2 for different data types, you will probably need to keep multiple indexes generated with different parameters. This makes minimap2 different from BWA which always uses the same index regardless of query data types. Use cases ~~~~~~~~~ Minimap2 uses the same base algorithm for all applications. However, due to the different data types it supports (e.g. short vs long reads; DNA vs mRNA reads), minimap2 needs to be tuned for optimal performance and accuracy. It is usually recommended to choose a preset with option 
-x
, which sets multiple parameters at the same time. The default setting is the same as 
map-ont
. Map long noisy genomic reads ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The difference between 
map-pb
 and 
map-ont
 is that 
map-pb
 uses homopolymer-compressed (HPC) minimizers as seeds, while 
map-ont
 uses ordinary minimizers as seeds. Emperical evaluation suggests HPC minimizers improve performance and sensitivity when aligning PacBio reads, but hurt when aligning Nanopore reads. Map long mRNA/cDNA reads ^^^^^^^^^^^^^^^^^^^^^^^^ There are different long-read RNA-seq technologies, including tranditional full-length cDNA, EST, PacBio Iso-seq, Nanopore 2D cDNA-seq and Direct RNA-seq. They produce data of varying quality and properties. By default, 
-x splice
 assumes the read orientation relative to the transcript strand is unknown. It tries two rounds of alignment to infer the orientation and write the strand to the 
ts
 SAM/PAF tag if possible. For Iso-seq, Direct RNA-seq and tranditional full-length cDNAs, it would be desired to apply 
-u f
 to force minimap2 to consider the forward transcript strand only. This speeds up alignment with slight improvement to accuracy. For noisy Nanopore Direct RNA-seq reads, it is recommended to use a smaller k-mer size for increased sensitivity to the first or the last exons. It is worth noting that by default 
-x splice
 prefers GT[A/G]..[C/T]AG over GT[C/T]..[A/G]AG, and then over other splicing signals. Considering one additional base improves the junction accuracy for noisy reads, but reduces the accuracy when aligning against the widely used SIRV control data. This is because SIRV does not honor the evolutionarily conservative splicing signal. If you are studying SIRV, you may apply 
--splice-flank=no
 to let minimap2 only model GT..AG, ignoring the additional base. Find overlaps between long reads ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Similarly, 
ava-pb
 uses HPC minimizers while 
ava-ont
 uses ordinary minimizers. It is usually not recommended to perform base-level alignment in the overlapping mode because it is slow and may produce false positive overlaps. However, if performance is not a concern, you may try to add 
-a
 or 
-c
 anyway. Map short accurate genomic reads ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ When two read files are specified, minimap2 reads from each file in turn and merge them into an interleaved stream internally. Two reads are considered to be paired if they are adjacent in the input stream and have the same name (with the 
/[0-9]
 suffix trimmed if present). Single- and paired-end reads can be mixed. Minimap2 does not work well with short spliced reads. There are many capable RNA-seq mappers for short reads. Full genome/assembly alignment ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ For cross-species full-genome alignment, the scoring system needs to be tuned according to the sequence divergence. Self-homology map creation ^^^^^^^^^^^^^^^^^^^^^^^^^^ A self-homology map is created by mapping a genome (e.g. that of E. coli) against itself. When this option is used the same FASTA file should be used for reference and for the (single ended mode) query. Advanced features ~~~~~~~~~~~~~~~~~ Working with >65535 CIGAR operations ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Due to a design flaw, BAM does not work with CIGAR strings with >65535 operations (SAM and CRAM work). However, for ultra-long nanopore reads minimap2 may align ~1% of read bases with long CIGARs beyond the capability of BAM. If you convert such SAM/CRAM to BAM, Picard and recent samtools will throw an error and abort. Older samtools and other tools may create corrupted BAM. To avoid this issue, you can add option 
-L
 at the minimap2 command line. This option moves a long CIGAR to the 
CG
 tag and leaves a fully clipped CIGAR at the SAM CIGAR column. Current tools that don’t read CIGAR (e.g. merging and sorting) still work with such BAM records; tools that read CIGAR will effectively ignore these records. It has been decided that future tools will seamlessly recognize long-cigar records generated by option 
-L
. 
TD;DR
: if you work with ultra-long reads and use tools that only process BAM files, please add option 
-L
. The cs optional tag ^^^^^^^^^^^^^^^^^^^ The 
cs
 SAM/PAF tag encodes bases at mismatches and INDELs. It matches regular expression 
/(:[0-9]+|\*[a-z][a-z]|[=\+\-][A-Za-z]+)+/
. Like CIGAR, 
cs
 consists of series of operations. Each leading character specifies the operation; the following sequence is the one involved in the operation. The 
cs
 tag is enabled by command line option 
--cs
. The following alignment, for example: .. code:: CGATCGATAAATAGAGTAG---GAATAGCA |||||| |||||||||| |||| ||| CGATCG---AATAGAGTAGGTCGAATtGCA is represented as 
:6-ata:10+gtc:4*at:3
, where 
:[0-9]+
 represents an identical block, 
-ata
 represents a deltion, 
+gtc
 an insertion and 
*at
 indicates reference base 
a
 is substituted with a query base 
t
. It is similar to the 
MD
 SAM tag but is standalone and easier to parse. If 
--cs=long
 is used, the 
cs
 string also contains identical sequences in the alignment. The above example will become 
=CGATCG-ata=AATAGAGTAG+gtc=GAAT*at=GCA
. The long form of 
cs
 encodes both reference and query sequences in one string. Algorithm overview ~~~~~~~~~~~~~~~~~~ In the following, minimap2 command line options have a dash ahead and are highlighted in bold. The description may help to tune minimap2 parameters. 1. Read 
-I
 [=
4G
] reference bases, extract (
-k
,\ 
-w
)-minimizers and index them in a hash table. 2. Read 
-K
 [=
200M
] query bases. For each query sequence, do step 3 through 7: 3. For each (
-k
,\ 
-w
)-minimizer on the query, check against the reference index. If a reference minimizer is not among the top 
-f
 [=
2e-4
] most frequent, collect its the occurrences in the reference, which are called 
seeds
. 4. Sort seeds by position in the reference. Chain them with dynamic programming. Each chain represents a potential mapping. For read overlapping, report all chains and then go to step 8. For reference mapping, do step 5 through 7: 5. Let 
P
 be the set of primary mappings, which is an empty set initially. For each chain from the best to the worst according to their chaining scores: if on the query, the chain overlaps with a chain in 
P
 by 
–mask-level
 [=
0.5
] or higher fraction of the shorter chain, mark the chain as 
secondary
 to the chain in 
P
; otherwise, add the chain to 
P
. 6. Retain all primary mappings. Also retain up to 
-N
 [=
5
] top secondary mappings if their chaining scores are higher than 
-p
 [=
0.8
] of their corresponding primary mappings. 7. If alignment is requested, filter out an internal seed if it potentially leads to both a long insertion and a long deletion. Extend from the left-most seed. Perform global alignments between internal seeds. Split the chain if the accumulative score along the global alignment drops by 
-z
 [=
400
], disregarding long gaps. Extend from the right-most seed. Output chains and their alignments. 8. If there are more query sequences in the input, go to step 2 until no more queries are left. 9. If there are more reference sequences, reopen the query file from the start and go to step 1; otherwise stop. Limitations ----------- - Minimap2 may produce suboptimal alignments through long low-complexity regions where seed positions may be suboptimal. This should not be a big concern because even the optimal alignment may be wrong in such regions."
toolshed.g2.bx.psu.edu/repos/bgruening/pileometh/pileometh/0.5.2+galaxy0	"What it does
 MethylDackel (formerly named PileOMeth, which was a temporary name derived due to it using a PILEup to extract METHylation metrics) will process a coordinate-sorted and indexed BAM or CRAM file containing some form of BS-seq alignments and extract per-base methylation metrics from them. MethylDackel requires an indexed fasta file containing the reference genome as well. By default, MethylDackel will only calculate metrics for Cytosines in a CpG context, but metrics for those in CHG and CHH contexts are supported as well. 
Methylation context
 MethylDackel groups all Cytosines into one of three sequence contexts: CpG, CHG, and CHH. Here, H is the IUPAC ambiguity code for any nucleotide other than G. If an N is encountered in the reference sequence, then the context will be assigned to CHG or CHH, as appropriate (e.g., CNG would be categorized as in a CHG context and CNC as in a CHH context). If a Cytosine is close enough to the end of a chromosome/contig such that its context can't be inferred, then it is categorized as CHH (e.g., a Cytosine as the last base of a chromosome is considered as being in a CHH context). 
Output information
 If no methylation can be found, the output will be empty. Otherwise a variant of bedGraph that's similar to the ""coverage"" file is produced. In short, each line consists of 6 tab separated columns: 1. The chromosome/contig/scaffold name 2. The start coordinate 3. The end coordinate 4. The methylation percentage rounded to an integer 5. The number of alignments/pairs reporting methylated bases 6. The number of alignments/pairs reporting unmethylated bases All coordinates are 0-based half open, which conforms to the bedGraph definition. When paired-end reads are aligned, it can often occur that their alignments overlap. In such cases, MethylDackel will not count both reads of the pair in its output, as doing so would lead to incorrect downstream statistical results. An example of the output is below:: #track type=""bedGraph"" description=""SRR1182519.sorted CpG methylation levels"" #1 25115 25116 100 3 0 #1 29336 29337 50 1 1 Note the header line, which starts with ""track"". The ""description"" field is used as a label in programs such as IGV. Each of the subsequent lines describe single Cytosines, the 25116th and 29337th base on chromosome 1, respectively. The first position has 3 alignments (or pairs of alignments) indicating methylation and 0 indicating unmethylation (100% methylation) and the second position has 1 alignment each supporting methylation and unmethylation (50% methylation). 
Per-CpG/CHG metrics
 In many circumstances, it's desireable for metrics from individual Cytosines in a CpG to be merged, producing per-CpG metrics rather than per-Cytosine metrics. This can be accomplished with the 
Merge per-Cytosine
 parameter. If this is used, then this output:: #track type=""bedGraph"" description=""SRR1182519.sorted CpG methylation levels"" #1 25114 25115 100 2 1 #1 25115 25116 100 3 0 is changed to this:: #track type=""bedGraph"" description=""SRR1182519.sorted merged CpG methylation levels"" #1 25114 25116 100 5 1 This also works for CHG-level metrics. If bedGraph files containing per-Cytosine metrics already exist, they can be converted to instead contain per-CpG/CHG metrics with MethylDackel mergeContext. 
Methylation bias plotting and correction
 In an ideal experiment, we expect that the probability of observing a methylated C is constant across the length of any given read. In practice, however, there are often increases/decreases in observed methylation rate at the ends of reads and/or more global changes. These are termed methylation bias and including such regions in the extracted methylation metrics will result in noisier and less accurate data. For this reason, users are strongly encouraged to make a methylation bias plot. That command will create a methylation bias (mbias for short) plot for each of the strands for which there are valid alignments. The resulting mbias graphs are in SVG format and can be viewed in most modern web browsers. If you have paired-end data, both reads in the pair will be shown separately, as is the case above. The program will suggest regions for inclusion (""--OT 2,0,0,98"" above) and mark them on the plot, if applicable. The format of this output is described in MethylDackel extract -h. These suggestions should not be accepted blindly; users are strongly encouraged to have a look for themselves and tweak the actual bounds as appropriate. The lines indicate the average methylation percentage at a given position and the shaded regions the 99.9% confidence interval around it. This is useful in gauging how many methylation calls a given position has relative to its neighbors. Note the spike in methylation at the end of read #2 and the corresponding dip at the beginning of read #1. This is common and these regions can be ignored with the suggested trimming bounds. Note also that the numbers refer to the first and last base that should be included during methylation extraction, not the last and first base to ignore!. 
Excluding low-coverage regions
 If your downstream analysis requires an absolute minimum coverage (here, defined as the number of methylation calls kept after filtering for MAPQ, phred score, etc.), you can use the 
--minDepth
 option to achieve this. By default, 
MethylDackel extract
 will output all methylation metrics as long as the coverage is at least 1. If you use 
--minDepth 10
, then only sites covered at least 10x will be output. This works in conjunction with the 
--mergeContext
 option, above. So if you request per-CpG context output (i.e., with 
--mergeContext
) and 
--minDepth 10
 then only CpGs with a minimum coverage of 10 will be output. 
Logit, fraction, and counts only output
 The standard output described above can be modified if you supply the 
--fraction
, 
--counts
, or 
--logit
 options to 
MethylDackel extract
. The 
--fraction
 option essentially produces the first 4 columns of the standard output described above. The only other difference is that the range of the 4th column is now between 0 and 1, instead of 0 and 100. Instead of producing a file ending simply in 
.bedGraph
, one ending in 
.meth.bedGraph
 will instead be produced. The 
--counts
 option produces the first three columns of the standard output followed by a column of total coverage counts. This last column is equivalent to the sum of the 5th and 6th columns of the standard output. The resulting file ends in 
.counts.bedGraph
 rather than simply 
.bedGraph
. The 
--logit
 option produces the first three columns of the standard output followed by the logit transformed methylation fraction. The logit transformation is log(Methylation fraction/(1-Methylation fraction)). Note that log uses base e. Logit transformed methylation values range between +/- infinity, rather than [0,1]. The resulting file ends in 
.logit.bedGraph
 rather than simply 
.bedGraph
. Note that these options may be combined with 
--mergeContext
. However, 
MethylDackel mergeContext
 can not be used after the fact to combine these. 
methylKit-compatible output
 methylKit has its own format, which can be produced with the 
--methylKit
 option. Merging Cs into CpGs or CHGs is forbidden in this format. Likewise, this option is mutually exclusive with 
--logit
 et al. 
Excluding likely variant sites
 If your samples are not genetically homogenous, it can sometimes be advantageous to exclude likely variant sites from methylation extraction. As an example, since unmethylated Cs are read as Ts, extracting methylation from a position with a C->T mutation will cause incorrect results. In such a case, the opposite strand will have an A rather than a G (in the non-variant case, there would be a G regardless of methylation status). MethylDackel tracks the number of non-Gs on the strand opposite of Cs in the reference sequence. If the fraction of these exceeds the 
--maxVariantFrac
 option, then that position will be excluded from output. To exclude cases where the 
--maxVariantFrac
 value is exceeded only due to low coverage, the opposite strand must have a depth of coverage of at least 
--minOppositeDepth
. Note that the default value for 
--minOppositeDepth
 is 0, indicating that the variant site exclusion process is skipped. Note that if one additionally specifies 
--mergeContext
, that a given CpG or CHG will be excluded from output if either of its individual Cs would be excluded given the specified 
--minOppositeDepth
 and 
--maxVariantFrac
. ----- 
MethylDackel
 is a Free and Open Source Software, see more details on the MethylDackel_ Website. .. _MethylDackel: https://github.com/dpryan79/MethylDackel"
toolshed.g2.bx.psu.edu/repos/devteam/megablast_xml_parser/megablast_xml_parser/1.0.1	"What it does
 This tool processes the XML output of any NCBI blast tool (if you run your own blast jobs, the XML output can be generated with 
-m 7
 option). ----- 
Output fields
 This tools returns tab-delimited output with the following fields:: Description Example ----------------------------------------- ----------------- 1. Name of the query sequence Seq1 2. Length of the query sequence 30 3. Name of target sequence gnl|BL_ORD_ID|0 4. Length of target sequence 5528445 5. Alignment bit score 59.96 6. E-value 8.38112e-11 7. Start of alignment within query 1 8. End of alignment within query 30 9. Start of alignment within target 5436010 10. End of alignment within target 5436039 11. Query frame 1 12. Target frame 1 13. Number of identical bases within 29 the alignment 14. Alignment length 30 15. Aligned portion (sequence) of query CGGACAGCGCCGCCACCAACAAAGCCACCA 16. Aligned portion (sequence) of target CGGACAGCGCCGCCACCAACAAAGCCATCA 17. Midline indicating positions of ||||||||||||||||||||||||||| || matches within the alignment ------ .. class:: infomark Note that this form of output does not contain alignment identify value. However, it can be computed by dividing the number of identical bases within the alignment (Field 13) by the alignment length (Field 14) using 
Text Manipulation->Compute
 tool"
toolshed.g2.bx.psu.edu/repos/iuc/rna_starsolo/rna_starsolo/2.7.11b+galaxy0	This is a reimplementation of the original WASP algorithm by Bryce van de Geijn, Graham McVicker, Yoav Gilad and Jonathan K Pritchard. https://doi.org/10.1038/nmeth.3582. This option will add the vW tag to the SAM output. vW:i:1 means alignment passed WASP filtering, and all other values mean it did not:<br/> - vW:i:2 = multi-mapping read<br/> - vW:i:3 = variant base in the read is N (non-ACGT)<br/> - vW:i:4 = remapped read did not map <br/> - vW:i:5 = remapped read multi-maps <br/> - vW:i:6 = remapped read maps to a different locus <br/> - vW:i:7 = read overlaps too many variants <br/>
toolshed.g2.bx.psu.edu/repos/iuc/star_fusion/star_fusion/0.5.4-3+galaxy1	"What it does
 STAR-Fusion is a component of the Trinity Cancer Transcriptome Analysis Toolkit (CTAT). STAR-Fusion uses the STAR aligner to identify candidate fusion transcripts supported by Illumina reads. STAR-Fusion further processes the output generated by the STAR aligner to map junction reads and spanning reads to a reference annotation set. 
Input: files required to run STAR-Fusion
 - A genome reference sequence (FASTA-format) - A corresponding protein-coding gene annotation set (GTF/GFF Format) - A last-matching gene pairs file - in Galaxy you can create such files with the 
ncbi_blast_plus
 tool suite containing 
blastn
: https://toolshed.g2.bx.psu.edu/view/devteam/ncbi_blast_plus - A STAR chimeric/junction output file - this is optional as STAR Fusion can control running STAR as well. The authors of STAR Fusion have made some of these files avaialble at: https://data.broadinstitute.org/Trinity/CTAT_RESOURCE_LIB/. The gene annotations in each case are restricted to the protein-coding and lincRNA transcripts. More info: https://github.com/STAR-Fusion/STAR-Fusion/wiki"
toolshed.g2.bx.psu.edu/repos/richard-burhans/segalign/segalign/0.1.2.7+galaxy1	"SegAlign is a scalable, GPU-accelerated system for computing pairwise WGA. SegAlign is based on the standard seed-filter-extend heuristic, in which the filtering stage dominates the runtime (e.g. 98% for human-mouse WGA), and is accelerated using GPU(s). SegAlign was designed as a faster replacement for lastz pairwise aligner. 
Using this tool
 .. class:: warningmark This tool is the first part of a two-step process for generation of paiwrise alignments. The output of this tool is used as an input to 
Batched LASTZ
 tool. The 
Batched LASTZ
 can be found in the list of tool of this Galaxy instance. 
What it does
 SegAlign processes 
Target
 and 
Query
 sequences to identify highly similar regions where gapped extension will be performed to create actual alignments. The actual alignments are generated by 
Batched LASTZ
 that should be run on the output of this tool. .. class:: infomark Although this tool is only the first part of the alignment process all parameters related to generation of alignments are set 
during this stage
. 
Scoring Options
 By default the HOXD70 substitution scores are used (from 
Chiaromonte et al. 2002 &lt;https://www.ncbi.nlm.nih.gov/pubmed/11928468&gt;
_):: bad_score = X:-1000 # used for sub['X'][
] and sub[
]['X'] fill_score = -100 # used when sub[
][
] is not defined gap_open_penalty = 400 gap_extend_penalty = 30 A C G T A 91 -114 -31 -123 C -114 100 -125 -31 G -31 -125 100 -114 T -123 -31 -114 91 Matrix can be supplied as an input to 
Read the substitution scores
 parameter in 
Scoring
 section. Substitution matrix can be inferred from your data using another LASTZ-based tool (LASTZ_D: Infer substitution scores). 
Output Options
 .. class:: infomark The final format in which alignmnets will be generated by 
Batched LASTZ
 are set here. The default output is a MAF alignment file. Other formats can be configured in 
Output Options* section. See LASTZ manual <https://lastz.github.io/lastz/#formats>`_ for description of possible formats."
toolshed.g2.bx.psu.edu/repos/iuc/winnowmap/winnowmap/2.03+galaxy1	"Users’ Guide ------------ Winnowmap is a long-read mapping algorithm optimized for mapping ONT and PacBio reads to repetitive reference sequences. Winnowmap development began on top of minimap2 codebase, and since then we have incorporated the following two ideas to improve mapping accuracy within repeats. Winnowmap implements a novel weighted minimizer sampling algorithm (>=v1.0). This optimization was motivated by the need to avoid masking of frequently occurring k-mers during the seeding stage in an efficient manner, and achieve better mapping accuracy in complex repeats (e.g., long tandem repeats) of the human genome. Using weighted minimizers, Winnowmap down-weights frequently occurring k-mers, thus reducing their chance of getting selected as minimizers. Users can refer to this paper for more details. This idea is helpful to preserve the theoretical guarantee of minimizer sampling technique, i.e., if two sequences share a substring of a specified length, then they must be guaranteed to have a matching minimizer. We noticed that the highest scoring alignment doesn't necessarily correspond to correct placement of reads in repetitive regions of T2T human chromosomes. In the presence of a non-reference allele within a repeat, a read sampled from that region could be mapped to an incorrect repeat copy because the standard pairwise sequence alignment scoring system penalizes true variants. This is also sometimes referred to as allelic bias. To address this bias, we introduced and implemented an idea of using minimal confidently alignable substrings (>=v2.0). These are minimal-length substrings in a read that align end-to-end to a reference with mapping quality score above a user-specified threshold. This approach treats each read mapping as a collection of confident sub-alignments, which is more tolerant of structural variation and more sensitive to paralog-specific variants (PSVs). Our most recent paper desribes this concept and benchmarking results. General usage ~~~~~~~~~~~~~ For either mapping long reads or computing whole-genome alignments, Winnowmap requires pre-computing high frequency k-mers (e.g., top 0.02% most frequent) in a reference. Winnowmap uses meryl k-mer counting tool for this purpose. Mapping ONT or PacBio-hifi WGS reads .. code:: meryl count k=15 output merylDB ref.fa meryl print greater-than distinct=0.9998 merylDB > repetitive_k15.txt .. code:: winnowmap -W repetitive_k15.txt -ax map-ont ref.fa ont.fq.gz > output.sam [OR] winnowmap -W repetitive_k15.txt -ax map-pb ref.fa hifi.fq.gz > output.sam Mapping genome assemblies .. code:: meryl count k=19 output merylDB asm1.fa meryl print greater-than distinct=0.9998 merylDB > repetitive_k19.txt .. code:: winnowmap -W repetitive_k19.txt -ax asm20 asm1.fa asm2.fa > output.sam For the genome-to-genome use case, it may be useful to visualize the dot plot. This perl script can be used to generate a dot plot from paf-formatted output. In both usage cases, pre-computing repetitive k-mers using meryl is quite fast, e.g., it typically takes 2-3 minutes for the human genome reference. Use cases ~~~~~~~~~ Winnowmap uses the same base algorithm for all applications. However, due to the different data types it supports, Winnowmap needs to be tuned for optimal performance and accuracy. It is usually recommended to choose a preset with option 
-x
, which sets multiple parameters at the same time. The default setting is the same as 
map-ont
. Map long noisy genomic reads ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The difference between 
map-pb
 and 
map-ont
 is that 
map-pb
 uses homopolymer-compressed (HPC) minimizers as seeds, while 
map-ont
 uses ordinary minimizers as seeds. Emperical evaluation suggests HPC minimizers improve performance and sensitivity when aligning PacBio reads, but hurt when aligning Nanopore reads. Map long mRNA/cDNA reads ^^^^^^^^^^^^^^^^^^^^^^^^ There are different long-read RNA-seq technologies, including tranditional full-length cDNA, EST, PacBio Iso-seq, Nanopore 2D cDNA-seq and Direct RNA-seq. They produce data of varying quality and properties. By default, 
-x splice
 assumes the read orientation relative to the transcript strand is unknown. It tries two rounds of alignment to infer the orientation and write the strand to the 
ts
 SAM/PAF tag if possible. For Iso-seq, Direct RNA-seq and tranditional full-length cDNAs, it would be desired to apply 
-u f
 to force Winnowmap to consider the forward transcript strand only. This speeds up alignment with slight improvement to accuracy. For noisy Nanopore Direct RNA-seq reads, it is recommended to use a smaller k-mer size for increased sensitivity to the first or the last exons. It is worth noting that by default 
-x splice
 prefers GT[A/G]..[C/T]AG over GT[C/T]..[A/G]AG, and then over other splicing signals. Considering one additional base improves the junction accuracy for noisy reads, but reduces the accuracy when aligning against the widely used SIRV control data. This is because SIRV does not honor the evolutionarily conservative splicing signal. If you are studying SIRV, you may apply 
--splice-flank=no
 to let Winnowmap only model GT..AG, ignoring the additional base. Map short accurate genomic reads ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ When two read files are specified, Winnowmap reads from each file in turn and merge them into an interleaved stream internally. Two reads are considered to be paired if they are adjacent in the input stream and have the same name (with the 
/[0-9]
 suffix trimmed if present). Single- and paired-end reads can be mixed. Winnowmap does not work well with short spliced reads. There are many capable RNA-seq mappers for short reads. Full genome/assembly alignment ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ For cross-species full-genome alignment, the scoring system needs to be tuned according to the sequence divergence. Self-homology map creation ^^^^^^^^^^^^^^^^^^^^^^^^^^ A self-homology map is created by mapping a genome (e.g. that of E. coli) against itself. When this option is used the same FASTA file should be used for reference and for the (single ended mode) query. Advanced features ~~~~~~~~~~~~~~~~~ Working with >65535 CIGAR operations ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Due to a design flaw, BAM does not work with CIGAR strings with >65535 operations (SAM and CRAM work). However, for ultra-long nanopore reads Winnowmap may align ~1% of read bases with long CIGARs beyond the capability of BAM. If you convert such SAM/CRAM to BAM, Picard and recent samtools will throw an error and abort. Older samtools and other tools may create corrupted BAM. To avoid this issue, you can add option 
-L
 at the Winnowmap command line. This option moves a long CIGAR to the 
CG
 tag and leaves a fully clipped CIGAR at the SAM CIGAR column. Current tools that don’t read CIGAR (e.g. merging and sorting) still work with such BAM records; tools that read CIGAR will effectively ignore these records. It has been decided that future tools will seamlessly recognize long-cigar records generated by option 
-L
. 
TD;DR
: if you work with ultra-long reads and use tools that only process BAM files, please add option 
-L
. The cs optional tag ^^^^^^^^^^^^^^^^^^^ The 
cs
 SAM/PAF tag encodes bases at mismatches and INDELs. It matches regular expression 
/(:[0-9]+|\*[a-z][a-z]|[=\+\-][A-Za-z]+)+/
. Like CIGAR, 
cs
 consists of series of operations. Each leading character specifies the operation; the following sequence is the one involved in the operation. The 
cs
 tag is enabled by command line option 
--cs
. The following alignment, for example: .. code:: CGATCGATAAATAGAGTAG---GAATAGCA |||||| |||||||||| |||| ||| CGATCG---AATAGAGTAGGTCGAATtGCA is represented as 
:6-ata:10+gtc:4*at:3
, where 
:[0-9]+
 represents an identical block, 
-ata
 represents a deltion, 
+gtc
 an insertion and 
*at
 indicates reference base 
a
 is substituted with a query base 
t
. It is similar to the 
MD
 SAM tag but is standalone and easier to parse. If 
--cs=long
 is used, the 
cs
 string also contains identical sequences in the alignment. The above example will become 
=CGATCG-ata=AATAGAGTAG+gtc=GAAT*at=GCA
. The long form of 
cs
 encodes both reference and query sequences in one string. Benchmarking ~~~~~~~~~~~~ When comparing Winnowmap (v1.0) to minimap2 (v2.17-r954), we observed a reduction in the mapping error-rate from 0.14% to 0.06% in the recently finished human X chromosome, and from 3.6% to 0% within the highly repetitive X centromere (3.1 Mbp). Winnowmap improves mapping accuracy within repeats and achieves these results with sparser sampling, leading to better index compression and competitive runtimes. By avoiding masking, we show that Winnowmap maintains uniform minimizer density."
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_axtchain/ucsc_axtchain/482+galaxy1	"What it does
 
axtChain
 is a tool to chain together alignments in 
axt
 format. The 
chain
 format describes a pairwise alignment that allow gaps in both sequences simultaneously. For implementation details see axtChain's 
source code
. linearGap defaults: loose:: tablesize 11 smallSize 111 position 1 2 3 11 111 2111 12111 32111 72111 152111 252111 qGap 325 360 400 450 600 1100 3600 7600 15600 31600 56600 tGap 325 360 400 450 600 1100 3600 7600 15600 31600 56600 bothGap 625 660 700 750 900 1400 4000 8000 16000 32000 57000 medium:: tableSize 11 smallSize 111 position 1 2 3 11 111 2111 12111 32111 72111 152111 252111 qGap 350 425 450 600 900 2900 22900 57900 117900 217900 317900 tGap 350 425 450 600 900 2900 22900 57900 117900 217900 317900 bothGap 750 825 850 1000 1300 3300 23300 58300 118300 218300 318300 .. _axtChain: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _chain: https://genome.ucsc.edu/goldenPath/help/chain.html .. _axt: https://genome.ucsc.edu/goldenPath/help/axt.html .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/hg/mouseStuff/axtChain/axtChain.c"
toolshed.g2.bx.psu.edu/repos/iuc/bwameth/bwameth/0.2.9+galaxy0	"What it does
 BWA-meth performs the alignment of reads in a bisulfite-sequencing experiment (e.g., RRBS or WGBS) to a genome. This methodology is similar to bismark, where both the reads and the reference genome are 
in silico
 converted prior to alignment. Methylation extraction on the resulting BAM file can be done with the PileOMeth tool."
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_chainantirepeat/ucsc_chainantirepeat/482+galaxy0	"What it does
 
chainAntiRepeat
 is a tool that gets rid chains that are primarily the results of repeats and degenerate DNA. The 
chain
 format describes a pairwise alignment that allow gaps in both sequences simultaneously. For implementation details see chainAntiRepeat's 
source code
_. .. _chainAntiRepeat: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _chain: https://genome.ucsc.edu/goldenPath/help/chain.html .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/hg/mouseStuff/chainAntiRepeat/chainAntiRepeat.c"
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_chainnet/ucsc_chainnet/482+galaxy0	"What it does
 
chainNet
 is a tool that makes alignment nets out of 
alignment chains
. It outputs two files in the 
ucsc.net
 format -- one for the target sequence and one for the query sequence. For implementation details see ChainNet's 
source code
. .. _chainNet: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _ucsc.net: https://genome.ucsc.edu/goldenPath/help/net.html .. _alignment chains: https://genome.ucsc.edu/goldenPath/help/chain.html .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/hg/mouseStuff/chainNet/chainNet.c"
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_chainprenet/ucsc_chainprenet/482+galaxy0	"What it does
 
chainPreNet
 is a tool to remove 
chains
 that don't have a chance of being 
netted
. For implementation details see chainPreNet's 
source code
. .. _chainPreNet: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _chains: https://genome.ucsc.edu/goldenPath/help/chain.html .. _netted: https://genome.ucsc.edu/goldenPath/help/net.html .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/hg/mouseStuff/chainPreNet/chainPreNet.c"
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_chainsort/ucsc_chainsort/482+galaxy0	"What it does
 
chainSort
 is a tool to sort 
chains
. Note this tool loads all chains into memory, so it is not suitable for large datasets. Instead, run chainSort on multiple small dataset, followed by chainMergeSort. For implementation details see chainSort's 
source code
_. .. _chainSort: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _chains: https://genome.ucsc.edu/goldenPath/help/chain.html .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/hg/mouseStuff/chainSort/chainSort.c"
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_chainswap/ucsc_chainswap/482+galaxy0	"What it does
 
chainSwap
 is a tool to swap target with query in a 
chain
 file. For implementation details see chainSwap's 
source code
_. .. _chainSwap: https://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER .. _chain: https://genome.ucsc.edu/goldenPath/help/chain.html .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/0d2aae0eda9aad8255bced9aa142e457f1a25c5d/src/hg/mouseStuff/chainSwap/chainSwap.c"
toolshed.g2.bx.psu.edu/repos/rnateam/metilene/metilene/0.2.6.1	"Fast and sensitive calling of differentially methylated regions from bisulfite sequencing data
 Detailed documentation of the stand-alone tool metilene is available at .. 
: http://www.bioinf.uni-leipzig.de/Software/metilene/manual.pdf The software metilene is available for download at .. 
: http://www.bioinf.uni-leipzig.de/Software/metilene/"
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_netchainsubset/ucsc_netchainsubset/482+galaxy0	"What it does
 
netChainSubset
 is a tool to create a 
chain
 file with subset of chains that appear in the net. This process creates so called 
liftOver
 file used to convert coordinates between asesemblies. For implementation details see netChainSubset's 
source code
. .. _netChainSubset: https://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER .. _net: https://genome.ucsc.edu/goldenPath/help/net.html .. _chain: https://genome.ucsc.edu/goldenPath/help/chain.html .. _liftOver: https://genome.ucsc.edu/goldenPath/help/hgTracksHelp.html#Liftover .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/151a59e4e7526124ff096019c6c4052f0a900711/src/hg/mouseStuff/netChainSubset/netChainSubset.c"
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_netfilter/ucsc_netfilter/482+galaxy0	"What it does
 
netFilter
 is a tool to filter out parts of 
net
. What passes filter goes to output. Note a net is a recursive data structure. If a parent fails to pass the filter, the children are not even considered. For implementation details see netFilter's 
source code
_. .. _netFilter: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _net: https://genome.ucsc.edu/goldenPath/help/net.html .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/hg/mouseStuff/netFilter/netFilter.c"
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_netsyntenic/ucsc_netsyntenic/482+galaxy0	"What it does
 
netSyntenic
 is a tool to add synteny info to a net. See details of 
net format
. For implementation details see netSyntenic's 
source code
_. .. _netSyntenic: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _net format: https://genome.ucsc.edu/goldenPath/help/net.html .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/hg/mouseStuff/netSyntenic/netSyntenic.c"
toolshed.g2.bx.psu.edu/repos/lecorguille/camera_annotate/abims_CAMERA_annotateDiffreport/2.2.7+camera1.48.0-galaxy1	".. class:: infomark 
Authors
 Colin A. Smith csmith@scripps.edu, Ralf Tautenhahn rtautenh@gmail.com, Steffen Neumann sneumann@ipb-halle.de, Paul Benton hpaul.benton08@imperial.ac.uk and Christopher Conley cjconley@ucdavis.edu .. class:: infomark 
Galaxy integration
 ABiMS TEAM - UPMC/CNRS - Station biologique de Roscoff and Yann Guitton yann.guitton@oniris-nantes.fr - part of Workflow4Metabolomics.org [W4M] | Contact support@workflow4metabolomics.org for any questions or concerns about the Galaxy implementation of this tool. --------------------------------------------------- ================ CAMERA.annotate ================ ----------- Description ----------- The R-package CAMERA is a Collection of Algorithms for MEtabolite pRofile Annotation. Its primary purpose is the annotation and evaluation of LC-MS data. It includes algorithms for annotation of isotope peaks, adducts and fragments in peak lists. Additional methods cluster mass signals that originate from a single metabolite, based on rules for mass differences and peak shape comparison. To use the strength of already existing programs, CAMERA is designed to interact directly with processed peak data from the R-package 
xcms
. 
What it does?
 The CAMERA annotation procedure can be split into two parts: We want to answer the questions which peaks occur from the same molecule and secondly compute its exact mass and annotate the ion species. Therefore CAMERA annotation workflow contains following primary functions: 1. peak grouping after retention time (
groupFWHM
) 2. peak group verification with peakshape correlation (
groupCorr
) Both methods separate peaks into different groups, which we define as ”pseu- dospectra”. Those pseudospectra can consists from one up to 100 ions, de- pending on the molecules amount and ionizability. Afterwards the exposure of the ion species can be performed with: 2 1. annotation of possible isotopes (
findIsotopes
) 2. annotation of adducts and calculating hypothetical masses for the group (
findAdducts
) This workflow results in a data-frame similar to a xcms peak table, that can be easily stored in a comma separated table .csv (Excel-readable). If you have two or more conditions, it will return a diffreport result within the annotation results. The diffreport result shows the most significant differences between two sets of samples. Optionally create extracted ion chromatograms for the most significant differences. ----------------- Workflow position ----------------- 
Upstream tools
 ========================= ==================== ====================== ========== Name output file format parameter ========================= ==================== ====================== ========== xcms.fillPeaks xset.fillPeaks.RData rdata.xcms.fillpeaks RData file ========================= ==================== ====================== ========== 
Downstream tools
 +---------------------------+---------------------------------------+------------------------------------------------+ | Name | Output file | Format | +===========================+=======================================+================================================+ |CAMERA_combinexsAnnot |xset.annotate.Rdata | rdata.camera.positive or rdata.camera.negative | +---------------------------+---------------------------------------+------------------------------------------------+ |Determine Vdk or Lowess |xset.annotate.variableMetadata.tsv | Tabular | +---------------------------+---------------------------------------+------------------------------------------------+ |Normalization Vdk/Lowess |xset.annotate.variableMetadata.tsv | Tabular | +---------------------------+---------------------------------------+------------------------------------------------+ |Anova |xset.annotate.variableMetadata.tsv | Tabular | +---------------------------+---------------------------------------+------------------------------------------------+ |PCA |xset.annotate.variableMetadata.tsv | Tabular | +---------------------------+---------------------------------------+------------------------------------------------+ |Hierarchical Clustering |xset.annotate.variableMetadata.tsv | Tabular | +---------------------------+---------------------------------------+------------------------------------------------+ The output file 
xset.annotate.variableMetadata.tsv
 is an tabular file. You can continue your analysis using it in the following tools: | Determine Vdk or Lowess | Normalization Vdk/Lowess | Anova | PCA | Hierarchical Clustering 
General schema of the metabolomic workflow
 .. image:: annotate_workflow.png ----------- Input files ----------- +---------------------------+---------------------------+ | Parameter : num + label | Format | +===========================+===========================+ | 1 : RData file | rdata.xcms.fillpeaks | +---------------------------+---------------------------+ [Optional] User defined ruleset: -------------------------------- Example: | ""name"",""nmol"",""charge"",""massdiff"",""oidscore"",""quasi"",""ips"" | ""[M-H]-"",1,-1,-1.007276,1,1,1 | ""[M-2H]2-"",1,-2,-2.014552,2,0,1 | ""[M-3H]3-"",1,-3,-3.021828,3,0,1 | ""[M-2H+Na]-"",1,-1,20.974666,4,0,0.25 | ""[M-H+Cl]2-"",1,-2,33.962126,5,0,1 | ""[M-2H+K]-"",1,-1,36.948606,6,0,0.25 | ""[M+Cl]-"",1,-1,34.969402,7,1,1 | ""[M+2Cl]2-"",1,-2,69.938804,8,0,1 | ""[2M-H]-"",2,-1,-1.007276,1,0,0.5 ---------- Parameters ---------- diffreport: Intensity values to be used for the diffreport ---------------------------------------------------------- | If 
value=""into""
, integrated peak intensities are used. | If 
value=""maxo""
, maximum peak intensities are used. | If 
value=""intb""
, baseline corrected integrated peak intensities are used (only available if peak detection was done by ‘findPeaks.centWave’). ------------ Output files ------------ xset.annotate.variableMetadata.tsv | For each metabolite (row) : | the value of the intensity in each sample, fold, anova, mzmed, mzmin, mzmax, rtmed, rtmin, rtmax, npeaks, isotopes, adduct and pcgroup xset.annotate.zip | It contains filebase_eic, filebase_box and filebase.tsv for one conditon vs another (Anova analysis). xset.annotate.Rdata rdata.camera.quick or rdata.camera.positive or rdata.camera.negative | Rdata file, that be used outside Galaxy in R. ------ .. class:: infomark The output 
""xset.annotate.variableMetadata.tsv""
 is a tabular file. You can continue your analysis using it in the following tools of the workflow: | Determine Vdk or Lowess (Data correction) | Normalization Vdk/Lowess (Data correction) | Anova (Statistical analysis) | PCA (Statistical analysis) | Hierarchical Clustering (Statistical analysis) --------------------------------------------------- --------------- Working example --------------- Input files ----------- | RData file -> 
xset.fillPeaks.RData
 Parameters ---------- | sortpval -> 
false
 | sigma -> 
6 (default)
 | perfwhm -> 
0.6 (default)
 | maxcharge -> 
3 (default)
 | maxiso -> 
4 (default)
 | minfrac -> 
0.5 (default)
 | ppm -> 
500
 | mzabs -> 
0.015(default)
 | Advanced options -> 
hide
 | Numver of condition -> Two or more conditioons | eicmax -> 
200
 | eicwidth -> 
200 (default)
 Output files ------------ 
Example of a part of xset.annotate.variableMetadata.tsv output
 .. image:: annotatediffreport_variableMetadata.png --------------------------------------------------- Changelog/News -------------- .. 
News: https://bioconductor.org/packages/release/bioc/news/CAMERA/NEWS 
Version 2.2.6+camera1.48.0 - 11/06/2021
 - UPGRADE: upgrade the CAMERA version from 1.46.0 to 1.48.0 (see CAMERA News
) 
Version 2.2.6+camera1.46.0 - 12/04/2021
 - UPGRADE: upgrade the CAMERA version from 1.42.0 to 1.46.0 (see CAMERA News_) 
Version 2.2.6+camera1.42.0-galaxy1 - 09/03/2020
 - BUGFIX: Fix the zip export of the pictures (eic and boxplot) 
Version 2.2.6+camera1.42.0 - 13/02/2020
 - UPGRADE: upgrade the CAMERA version from 1.38.0 to 1.42.0 (see CAMERA News_) 
Version 2.2.5 - 09/04/2019
 - NEW: zip export are back for pictures (eic and boxplot) and diffreport tables - UPGRADE: upgrade the CAMERA version from 1.34.0 to 1.38.1 (see CAMERA News_) - UPGRADE: refactoring of internal code 
Version 2.2.4 - 09/10/2018
 - NEW: CAMERA.annotate no longer export a DataMatrix. fillChromPeaks does the job 
Version 2.2.3 - 30/04/2018
 - NEW: support the new xcms 3.0.0 wrapper 
Version 2.2.2 - 01/03/2018
 - UPGRADE: upgrate the CAMERA version from 1.26.0 to 1.32.0 
Version 2.2.1 - 29/11/2017
 - BUGFIX: To avoid issues with accented letter in the parentFile tag of the mzXML files, we changed a hidden mechanim to LC_ALL=C 
Version 2.2.0 - 28/03/2017
 - BUGFIX: the diffreport ids didn't convert the rt in minute as the other export - UPDATE: the settings (digits, convertion in minutes) of the identifiants will no longer modify the native one. Because we want to be conservative and because it can be dangerous for the data integrity during a futur merge of the table, we decide to put those customization in a new column namecustom within the variableMetadata. - IMPROVEMENT: add some sections within to separate the different parts of the process - IMPROVEMENT: add the possibility to use defined ruleset - IMPROVEMENT: add the possibility to set the MZ digit within the identifiants - IMPROVEMENT: CAMERA.annotate is now compatible with merged individual data from xcms.xcmsSet 
Version 2.1.5 - 21/04/2016
 - UPGRADE: upgrade the CAMERA version from 1.22.0 to 1.26.0 
Version 2.1.4 - 18/04/2016
 - TEST: refactoring to pass planemo test using conda dependencies 
Version 2.1.3 - 10/02/2016
 - BUGFIX: better management of errors. Datasets remained green although the process failed - BUGFIX: the conversion into minutes of the retention time was applied to the diffreport outputs (several conditions) - IMPROVEMENT: when there are several conditions, the tool will generate individual datasets (tsv, pdf) instead of a zip file. The usual png (eic, boxplot) will from now be integrated in two pdf. - UPDATE: refactoring of internal management of inputs/outputs 
VERSION 2.1.0 - 09/10/2015
 - BUGFIX: There was a bug with the CAMERA.annotate (generating a bad dataMatrix (intensities which don't match with the metabolites)) 
VERSION 2.1.0 - 07/06/2015
 - IMPROVEMENT: new datatype/dataset formats (rdata.camera.positive, rdata.camera.negative, rdata.camera.quick ...) will facilitate the sequence of tools and so avoid incompatibility errors. - IMPROVEMENT: parameter labels have changed to facilitate their reading. - UPDATE: merged with annotateDiffreport. Some parameters are dedicated to experiences with several conditions"
toolshed.g2.bx.psu.edu/repos/ethevenot/checkformat/checkFormat/3.0.0	".. class:: infomark 
Author
 Etienne Thevenot (W4M Core Development Team, MetaboHUB Paris, CEA) --------------------------------------------------- .. class:: infomark 
Tool updates
 See the 
NEWS
 section at the bottom of this page --------------------------------------------------- ============ Check Format ============ ----------- Description ----------- | 
Checks the format (row and column names)
 of the dataMatrix, sampleMetadata and variableMetadata tables; in case of difference of orders of the samples and/or variables between (some of) the tables, the 
orders from the dataMatrix are permuted
 to match those of the sampleMetadata and/or the variableMetadata; sample and variables names can also be modified to be 
syntactically valid
 for R by selecting the corresponding argument (e.g. an 'X' is added to names starting with a digit, blanks will be converted to '.', etc.). ----------------- Workflow position ----------------- .. image:: ./static/images/checkFormat_workflowPositionImage.png ----------- Input files ----------- +----------------------------+---------+ | Parameter : num + label | Format | +============================+=========+ | 1 : Data matrix file | tabular | +----------------------------+---------+ | 2 : Sample metadata file | tabular | +----------------------------+---------+ | 3 : Variable metadata file | tabular | +----------------------------+---------+ | The 
required formats
 for the dataMatrix, sampleMetadata, and variableMetadata files are described in the 
HowTo
 entitled 'Format Data For Postprocessing' available on the main page of Workflow4Metabolomics.org (http://web11.sb-roscoff.fr/download/w4m/howto/w4m_HowToFormatDataForPostprocessing_v02.pdf) ---------- Parameters ---------- Data matrix file | variable x sample 
dataMatrix
 tabular separated file of the numeric data matrix, with . as decimal, and NA for missing values; the table must not contain metadata apart from row and column names; the row and column names must be identical to the rownames of the sample and variable metadata, respectively (see below) | Sample metadata file | sample x metadata 
sampleMetadata
 tabular separated file of the numeric and/or character sample metadata, with . as decimal and NA for missing values | Variable metadata file | variable x metadata 
variableMetadata
 tabular separated file of the numeric and/or character variable metadata, with . as decimal and NA for missing values | Make syntactically valid sample and variable names | if set to 'yes', sample and variable names will converted to syntactically valid names with the 'make.names' R function when required (e.g. an 'X' is added to names starting with a digit, blanks will be converted to '.', etc.) | ------------ Output files ------------ dataMatrix_out.tabular | dataMatrix data file; may be identical to the input dataMatrix in case no renaming of sample/variable names nor re-ordering of samples/variables (see the 'information' file for the presence/absence of modifications) | sampleMetadata_out.tabular | sampleMetadata data file; may be identical to the input sampleMetadata in case no renaming of sample names nor re-ordering of samples (see the 'information' file for the presence/absence of modifications) | variableMetadata_out.tabular | variableMetadata data file; may be identical to the input variableMetadata in case no renaming of variable names nor re-ordering of variables (see the 'information' file for the presence/absence of modifications) | information.txt | Text file with all messages when error(s) in formats are detected | --------------------------------------------------- --------------- Working example --------------- .. class:: infomark See the 
W4M00001a_sacurine-subset-statistics
, 
W4M00001b_sacurine-complete
, 
W4M00002_mtbls2
, or 
W4M00003_diaplasma
 shared histories in the 
Shared Data/Published Histories
 menu (https://galaxy.workflow4metabolomics.org/history/list_published) --------------------------------------------------- ---- NEWS ---- CHANGES IN VERSION 3.0.0 ======================== NEW FEATURES Automated re-ordering (if necessary) of sample and/or variable names from dataMatrix based on sampleMetadata and variableMetadata New argument to make sample and variable names syntactically valid Output of dataMatrix, sampleMetadata, and variableMetadata files, whether they have been modified or not CHANGES IN VERSION 2.0.4 ======================== INTERNAL MODIFICATIONS Minor internal modifications CHANGES IN VERSION 2.0.2 ======================== INTERNAL MODIFICATIONS Test for R code Planemo running validation Planemo installing validation Travis automated testing Toolshed export"
toolshed.g2.bx.psu.edu/repos/melpetera/idchoice/idchoice/19.12	".. class:: infomark 
Authors
 | Melanie Petera - PFEM ; INRA ; MetaboHUB --------------------------------------------------- ======================== ID Choice ======================== ----------- Description ----------- Change the current identifiers into new ones defined in your metadata file. Can be used for sample ID as well as variable ID. ----------- Input files ----------- +----------------------------+---------+ | Parameter : num + label | Format | +============================+=========+ | 1 : Data matrix file | tabular | +----------------------------+---------+ | 2 : Metadata file | tabular | +----------------------------+---------+ Data matrix file contains the intensity values of the variables. | Metadata file is meant to be chosen from sample metadata and variable metadata files. | It should include a column containing your wanted IDs. | ----------- Parameters ----------- Which ID do you want to change? | This must be consistant with the metadata file you chose as input | Name of the column to consider as new ID | Must be one of your metadata file's column | Force unicity if not unique? | If no (default value), if your ID column does not contain unique values, it will stop the process and send you back an explicit error. | If yes, non-unique values will be converted by adding ""
x"" at the end of concerned IDs. | ------------- Output files ------------- ID_metadata | tabular output | Corresponds to the input metadata file with new IDs as first column | ID_datamatrix | tabular output | Corresponds to the input data matrix file with new IDs | --------------------------------------------------- ---------------------- Additional information ---------------------- .. class:: warningmark For more information about input files, refer to the corresponding ""W4M HowTo"" page: | 
W4M table format for Galaxy &lt;http://workflow4metabolomics.org/sites/workflow4metabolomics.org/files/files/w4m_TableFormatForGalaxy_150908.pdf&gt;
 |"
toolshed.g2.bx.psu.edu/repos/melpetera/intensity_checks/intens_check/2.0.1	".. class:: infomark 
Author:
 Anthony Fernandes for original code (PFEM - INRA) 
Maintainer:
 Melanie Petera (PFEM - INRAE - MetaboHUB) --------------------------------------------------- ==================== Intensity Check ==================== ----------- Description ----------- This tool performs various metrics: mean fold change calculation, number and proportion of missing values, and mean, sd and decile calculation. You can choose to perform these metrics according to sample groups defined in the sample metadata file given as input. ----------------- Workflow position ----------------- .. image:: int_check.png :width: 800 ----------- Input files ----------- +----------------------------+------------+ | Parameter | Format | +============================+============+ | 1 : Data matrix file | tabular | +----------------------------+------------+ | 2 : Sample metadata file | tabular | +----------------------------+------------+ | 3 : Variable metadata file | tabular | +----------------------------+------------+ ---------- Parameters ---------- 
Computation method
 - 
Without distinction between samples:
 calculates chosen statistic(s) for each variable. - 
For each class of samples:
 separates samples between each class (class column to be specified). Chosen statistic(s) and/or mean fold change are calculated for each of them. - 
Between one class versus all the remaining samples:
 If you want to focus only on one class versus all the remaining samples without class distinction. In the case of two classes: ""each class"" and ""one class"" give the same results for statistical measures. We recommend to choose ""one class"" for mean fold change calculation in order to select the class you want to put as numerator or denominator (see below). 
Statistics
 Select the statistical measures you want to add in the variable metadata table. If the method is ""each class"" or ""one class"", you can choose to leave this section blank if you only want to calculate the mean fold change (see below). 
Class column
 (
only if ""each class"" or ""one class""
) Select the class column in sample metadata table. 
Selected class
 (
only if ""one class""
) If the method is ""one class"", specify it. Remaining samples will be named ""Other"". 
Calculate the mean fold change
 (
only if ""each class"" or ""one class""
) Choose if you want to calculate the mean fold change. If the method is ""each class"": mean fold change will be calculated for all combinations of classes. If the method is ""one class"": it will be calculated between the selected class (see above) and the remaining samples. 
Where should the class be placed for the mean fold change calculation?
 (
only if ""one class""
) If the method is ""one class"", choose ""top"" or ""bottom"" to put the selected class as numerator or denominator (respectively) for the mean fold change calculation. 
Logarithm
 (
only if ""each class"" or ""one class""
) Choose if you want to transform the mean fold change with a log2 or log10. ------------ Output files ------------ 
Variable metadata file
 Contains the previous columns in variable metadata file and the new ones. In the column names for fold, the first class specified is the one used as numerator in the ratio. 
Graphs file
 Contains barplots with the proportion of NA considering classes and boxplots with the fold values. --------------------------------------------------- ---------------------- Additional information ---------------------- .. class:: warningmark For more information about input files, refer to the corresponding ""W4M HowTo"" page: 
W4M table format for Galaxy &lt;https://nextcloud.inrae.fr/s/qLkNZRf84QQ5YLY&gt;
_"
toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_findrecalseries/mfassignr_findRecalSeries/1.1.2+galaxy1	"MFAssignR - FindRecalSeries ============================= This tool is the sixth step of the MFAssignR workflow (RecalList -> FindRecalSeries -> Recal) This function takes on input the CH2 homologous recalibration series, which are provided by the RecalList function and tries to find the most suitable series combination for recalibration based on the following criteria: (1) Series should cover the full mass spectral range, (2) Series should be optimally long and combined have a “Tall Peak” at least every 100 m/z, (3) Abundance score: the higher, the better, (4) Peak score: the closer to 0, the better, (5) Peak Distance: the closer to 1, the better, (6) Series Score: the closer to this value, the better. Combinations of 5 series are assembled, scores are computed for other metrics (in case of Peak proximity and Peak distance, an inverted score is computed) and these are summed. Finally, either a series of the size of combination or top 10 unique series having the highest score are outputted. Output: - Dataframe of n or 10 most suitable recalibrant series. General Information =================== Overview -------- MFAssignR is an R package for the molecular formula (MF) assignment of ultrahigh resolution mass spectrometry measurements. It contains several functions for the noise assessment, isotope filtering, interal mass recalibration, and MF assignment. The MFAssignR package was originally developed by Simeon Schum et al. (2020), the source code can be found on 
GitHub
. Please submit eventual Galaxy-related bug reports as 
issues
 on the repository. .. _GitHub: https://github.com/skschum/MFAssignR .. _issues: https://github.com/RECETOX/galaxytools/issues Workflow -------- .. image:: https://github.com/RECETOX/MFAssignR/raw/master/overview.png :width: 1512 :height: 720 :scale: 60 :alt: A picture of a workflow diagram. The recommended workflow how to run the MFAssignR package is as follows: (1) Run KMDNoise() to determine the noise level for the data. (2) Check effectiveness of S/N threshold using SNplot(). (3) Use IsoFiltR() to identify potential 13C and 34S isotope masses. (4) Using the S/N threshold, and the two data frames output from IsoFiltR(), run MFAssignCHO() to assign MF with C, H, and O to assess the mass accuracy. (5) Use RecalList() to generate a list of the potential recalibrant series. (6) Choose the most suitable recalibrant series using FindRecalSeries(). (7) After choosing recalibrant series, use Recal() to recalibrate the mass lists. (8) Assign MF to the recalibrated mass list using MFAssign(). (9) Check the output plots from MFAssign() to evaluate the quality of the assignments. For detailed documentation on the individual steps please see the individual tool wrappers."
toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_histnoise/mfassignr_histnoise/1.1.2+galaxy0	"MFAssignR - HistNoise ============================= This tool is the first step of the MFAssignR workflow (can be substitued by KMDNoise or run in paralell (-> SNplot)). HistNoise function creates a histogram using natural log of the intensity, which can be then used to determine the noise level for the data analyze, and also the estimated noise level. The noise level can be then multiplied by whatever value in order to reach the value to be used to cut the data. Output: - noise estimate - this noise level can then be multiplied by the user chosen value in order to set the signal to noise cut for formula assignment - Histogram - shows where the cut is being applied General Information =================== Overview -------- MFAssignR is an R package for the molecular formula (MF) assignment of ultrahigh resolution mass spectrometry measurements. It contains several functions for the noise assessment, isotope filtering, interal mass recalibration, and MF assignment. The MFAssignR package was originally developed by Simeon Schum et al. (2020), the source code can be found on 
GitHub
. Please submit eventual Galaxy-related bug reports as 
issues
 on the repository. .. _GitHub: https://github.com/skschum/MFAssignR .. _issues: https://github.com/RECETOX/galaxytools/issues Workflow -------- .. image:: https://github.com/RECETOX/MFAssignR/raw/master/overview.png :width: 1512 :height: 720 :scale: 60 :alt: A picture of a workflow diagram. The recommended workflow how to run the MFAssignR package is as follows: (1) Run KMDNoise() to determine the noise level for the data. (2) Check effectiveness of S/N threshold using SNplot(). (3) Use IsoFiltR() to identify potential 13C and 34S isotope masses. (4) Using the S/N threshold, and the two data frames output from IsoFiltR(), run MFAssignCHO() to assign MF with C, H, and O to assess the mass accuracy. (5) Use RecalList() to generate a list of the potential recalibrant series. (6) Choose the most suitable recalibrant series using FindRecalSeries(). (7) After choosing recalibrant series, use Recal() to recalibrate the mass lists. (8) Assign MF to the recalibrated mass list using MFAssign(). (9) Check the output plots from MFAssign() to evaluate the quality of the assignments. For detailed documentation on the individual steps please see the individual tool wrappers."
toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_isofiltr/mfassignr_isofiltr/1.1.2+galaxy1	IsoFiltR identifies and separates likely isotopic masses from monoisotopic masses in a mass list. This should be done prior to formula assignment to reduce incorrect formula assignments. The input is a table containing abundance and peak mass in the following format: +--------------------+-------------------+------------------------+ | mz | area | rt | +====================+===================+========================+ | 110.03486266079899 | 3410926.862054969 | 190.03735922916195 | +--------------------+-------------------+------------------------+ | 110.05988136843429 | 7658687.858 | 241.17645551084158 | +--------------------+-------------------+------------------------+ | ... | ... | ... | +--------------------+-------------------+------------------------+
toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_kmdnoise/mfassignr_kmdnoise/1.1.2+galaxy0	"MFAssignR - KMDNoise ============================= This tool is the first step of the MFAssignR workflow (can be substitued by HistNoise or run in paralell). KMDnoise is a Kendrick Mass Defect (KMD) approach for the noise estimation. It selects a subset of the data using the linear equation y=0.1132x + b, where y stands for the KMD value, x for the measured ion mass and b is the y-intercept. The default y-intercepts of 0.05 and 0.2 in KMDNoise are used to isolate the largest analyte free region of noise in most mass spectra. The intensity of the peaks within this “slice” are then averaged and that value is defined as the noise level for the mass spectrum. This value is then multiplied with a user-defined signal-to-noise ratio (typically 3-10) to remove low intensity m/z values. Output: - noise estimate - this noise level can then be multiplied by the user chosen value (3, 6, 10) in order to set the signal to noise cut for formula assignment. - KMD plot - bounds of the noise estimation area are highlighted in red. General Information =================== Overview -------- MFAssignR is an R package for the molecular formula (MF) assignment of ultrahigh resolution mass spectrometry measurements. It contains several functions for the noise assessment, isotope filtering, interal mass recalibration, and MF assignment. The MFAssignR package was originally developed by Simeon Schum et al. (2020), the source code can be found on 
GitHub
. Please submit eventual Galaxy-related bug reports as 
issues
 on the repository. .. _GitHub: https://github.com/skschum/MFAssignR .. _issues: https://github.com/RECETOX/galaxytools/issues Workflow -------- .. image:: https://github.com/RECETOX/MFAssignR/raw/master/overview.png :width: 1512 :height: 720 :scale: 60 :alt: A picture of a workflow diagram. The recommended workflow how to run the MFAssignR package is as follows: (1) Run KMDNoise() to determine the noise level for the data. (2) Check effectiveness of S/N threshold using SNplot(). (3) Use IsoFiltR() to identify potential 13C and 34S isotope masses. (4) Using the S/N threshold, and the two data frames output from IsoFiltR(), run MFAssignCHO() to assign MF with C, H, and O to assess the mass accuracy. (5) Use RecalList() to generate a list of the potential recalibrant series. (6) Choose the most suitable recalibrant series using FindRecalSeries(). (7) After choosing recalibrant series, use Recal() to recalibrate the mass lists. (8) Assign MF to the recalibrated mass list using MFAssign(). (9) Check the output plots from MFAssign() to evaluate the quality of the assignments. For detailed documentation on the individual steps please see the individual tool wrappers."
toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_mfassign/mfassignr_mfassign/1.1.2+galaxy1	"MFAssignR - MFAssign ============================= This tool is the last step of the MFAssignR workflow (Recal -> MFAssign) Recal() function recalibrates the 'Mono' and 'Iso' outputs from the IsoFiltR() function and prepares a dataframe containing chose recalibrants. Also it outputs a plot for the qualitative assessment of recalibrants. The input to the function is output from MFAssign() or MFAssignCHO(). It is important for recalibrant masses to cover the entire mass range of interest, and they should be among the most abundant peaks in their region of the spectrum - by default we take first 10 recalibrant series. We recommend to sort the Recalibration Series table based on the Series Score (largest to smallest). In case that error ""Gap in recalibrant coverage, try adding more recalibrant series"" would occur, we recommend to provide more diverse series. Output: - Unambig - data frame containing unambiguous assignments - Ambig - data frame containing ambiguous assignments - None - data frame containing unassigned masses - MSAssign - ggplot of mass spectrum highlighting assigned/unassigned - Error - ggplot of the Error vs. m/z - MSgroups - ggplot of mass spectrum colored by molecular group - VK - ggplot of van Krevelen plot, colored by molecular group General Information =================== Overview -------- MFAssignR is an R package for the molecular formula (MF) assignment of ultrahigh resolution mass spectrometry measurements. It contains several functions for the noise assessment, isotope filtering, interal mass recalibration, and MF assignment. The MFAssignR package was originally developed by Simeon Schum et al. (2020), the source code can be found on 
GitHub
. Please submit eventual Galaxy-related bug reports as 
issues
 on the repository. .. _GitHub: https://github.com/skschum/MFAssignR .. _issues: https://github.com/RECETOX/galaxytools/issues Workflow -------- .. image:: https://github.com/RECETOX/MFAssignR/raw/master/overview.png :width: 1512 :height: 720 :scale: 60 :alt: A picture of a workflow diagram. The recommended workflow how to run the MFAssignR package is as follows: (1) Run KMDNoise() to determine the noise level for the data. (2) Check effectiveness of S/N threshold using SNplot(). (3) Use IsoFiltR() to identify potential 13C and 34S isotope masses. (4) Using the S/N threshold, and the two data frames output from IsoFiltR(), run MFAssignCHO() to assign MF with C, H, and O to assess the mass accuracy. (5) Use RecalList() to generate a list of the potential recalibrant series. (6) Choose the most suitable recalibrant series using FindRecalSeries(). (7) After choosing recalibrant series, use Recal() to recalibrate the mass lists. (8) Assign MF to the recalibrated mass list using MFAssign(). (9) Check the output plots from MFAssign() to evaluate the quality of the assignments. For detailed documentation on the individual steps please see the individual tool wrappers."
toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_mfassigncho/mfassignr_mfassignCHO/1.1.2+galaxy1	"MFAssignR - MFAssignCHO ============================= This tool is the fourth step of the MFAssignR workflow (IsoFiltR -> MFAssignCHO -> RecalList) MFAssignCHO is a simplified version of MSAssign funcion, which only assigns MF with CHO elements. It is useful for the prelimiary MF assignments prior to the selection of internal recalibration ions in conjunction with RecalList and Recal. Output: - Unambig - data frame containing unambiguous assignments - Ambig - data frame containing ambiguous assignments - None - data frame containing unassigned masses - MSAssign - ggplot of mass spectrum highlighting assigned/unassigned - Error - ggplot of the Error vs. m/z - MSgroups - ggplot of mass spectrum colored by molecular group - VK - ggplot of van Krevelen plot, colored by molecular group General Information =================== Overview -------- MFAssignR is an R package for the molecular formula (MF) assignment of ultrahigh resolution mass spectrometry measurements. It contains several functions for the noise assessment, isotope filtering, interal mass recalibration, and MF assignment. The MFAssignR package was originally developed by Simeon Schum et al. (2020), the source code can be found on 
GitHub
. Please submit eventual Galaxy-related bug reports as 
issues
 on the repository. .. _GitHub: https://github.com/skschum/MFAssignR .. _issues: https://github.com/RECETOX/galaxytools/issues Workflow -------- .. image:: https://github.com/RECETOX/MFAssignR/raw/master/overview.png :width: 1512 :height: 720 :scale: 60 :alt: A picture of a workflow diagram. The recommended workflow how to run the MFAssignR package is as follows: (1) Run KMDNoise() to determine the noise level for the data. (2) Check effectiveness of S/N threshold using SNplot(). (3) Use IsoFiltR() to identify potential 13C and 34S isotope masses. (4) Using the S/N threshold, and the two data frames output from IsoFiltR(), run MFAssignCHO() to assign MF with C, H, and O to assess the mass accuracy. (5) Use RecalList() to generate a list of the potential recalibrant series. (6) Choose the most suitable recalibrant series using FindRecalSeries(). (7) After choosing recalibrant series, use Recal() to recalibrate the mass lists. (8) Assign MF to the recalibrated mass list using MFAssign(). (9) Check the output plots from MFAssign() to evaluate the quality of the assignments. For detailed documentation on the individual steps please see the individual tool wrappers."
toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_recal/mfassignr_recal/1.1.2+galaxy0	"MFAssignR - Recal ============================= This tool is the seventh step of the MFAssignR workflow (FindRecalSeries -> Recal -> MFAssign) Recal() function recalibrates the 'Mono' and 'Iso' outputs from the IsoFiltR() function and prepares a dataframe containing chose recalibrants. Also it outputs a plot for the qualitative assessment of recalibrants. The input to the function is output from MFAssign() or MFAssignCHO(). It is important for recalibrant masses to cover the entire mass range of interest, and they should be among the most abundant peaks in their region of the spectrum - by default we take first 10 recalibrant series. We recommend to sort the Recalibration Series table based on the Series Score (largest to smallest). In case that error ""Gap in recalibrant coverage, try adding more recalibrant series"" would occur, we recommend to provide more diverse series. Output: - Mass spectrum - Recalibrated dataframe of monoisotopic masses - Recalibrated dataframe of isotopic masses - Recalibrants list General Information =================== Overview -------- MFAssignR is an R package for the molecular formula (MF) assignment of ultrahigh resolution mass spectrometry measurements. It contains several functions for the noise assessment, isotope filtering, interal mass recalibration, and MF assignment. The MFAssignR package was originally developed by Simeon Schum et al. (2020), the source code can be found on 
GitHub
. Please submit eventual Galaxy-related bug reports as 
issues
 on the repository. .. _GitHub: https://github.com/skschum/MFAssignR .. _issues: https://github.com/RECETOX/galaxytools/issues Workflow -------- .. image:: https://github.com/RECETOX/MFAssignR/raw/master/overview.png :width: 1512 :height: 720 :scale: 60 :alt: A picture of a workflow diagram. The recommended workflow how to run the MFAssignR package is as follows: (1) Run KMDNoise() to determine the noise level for the data. (2) Check effectiveness of S/N threshold using SNplot(). (3) Use IsoFiltR() to identify potential 13C and 34S isotope masses. (4) Using the S/N threshold, and the two data frames output from IsoFiltR(), run MFAssignCHO() to assign MF with C, H, and O to assess the mass accuracy. (5) Use RecalList() to generate a list of the potential recalibrant series. (6) Choose the most suitable recalibrant series using FindRecalSeries(). (7) After choosing recalibrant series, use Recal() to recalibrate the mass lists. (8) Assign MF to the recalibrated mass list using MFAssign(). (9) Check the output plots from MFAssign() to evaluate the quality of the assignments. For detailed documentation on the individual steps please see the individual tool wrappers."
toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_recallist/mfassignr_recallist/1.1.2+galaxy0	"MFAssignR - RecalList ============================= This tool is the fifth step of the MFAssignR workflow (MFAssignCHO -> RecalList -> FindRecalSeries) RecalList() function identifies the homologous series that could be used for recalibration. On the input, there is the output from MFAssign() or MFAssignCHO() functions. It returns a dataframe that contains the CH2 homologous series that contain more than 3 members. Output: - Dataframe that contains the CH2 homologous series that contain more than 3 members. General Information =================== Overview -------- MFAssignR is an R package for the molecular formula (MF) assignment of ultrahigh resolution mass spectrometry measurements. It contains several functions for the noise assessment, isotope filtering, interal mass recalibration, and MF assignment. The MFAssignR package was originally developed by Simeon Schum et al. (2020), the source code can be found on 
GitHub
. Please submit eventual Galaxy-related bug reports as 
issues
 on the repository. .. _GitHub: https://github.com/skschum/MFAssignR .. _issues: https://github.com/RECETOX/galaxytools/issues Workflow -------- .. image:: https://github.com/RECETOX/MFAssignR/raw/master/overview.png :width: 1512 :height: 720 :scale: 60 :alt: A picture of a workflow diagram. The recommended workflow how to run the MFAssignR package is as follows: (1) Run KMDNoise() to determine the noise level for the data. (2) Check effectiveness of S/N threshold using SNplot(). (3) Use IsoFiltR() to identify potential 13C and 34S isotope masses. (4) Using the S/N threshold, and the two data frames output from IsoFiltR(), run MFAssignCHO() to assign MF with C, H, and O to assess the mass accuracy. (5) Use RecalList() to generate a list of the potential recalibrant series. (6) Choose the most suitable recalibrant series using FindRecalSeries(). (7) After choosing recalibrant series, use Recal() to recalibrate the mass lists. (8) Assign MF to the recalibrated mass list using MFAssign(). (9) Check the output plots from MFAssign() to evaluate the quality of the assignments. For detailed documentation on the individual steps please see the individual tool wrappers."
toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_snplot/mfassignr_snplot/1.1.2+galaxy1	"MFAssignR - SNplot ============================= This tool is the second step of the MFAssignR workflow (KMDNoise -> SNplot -> IsoFiltR). SNplot function plots the mass spectrum with the S/N cut denoted by different colors for the mass spectrum peaks (red indicates noise, blue indicates signal). This is useful for a qualitative look at the effectiveness of the S/N cut being used. Output: - SNplot - S/N colored mass spectrum showing where the cut is being applied General Information =================== Overview -------- MFAssignR is an R package for the molecular formula (MF) assignment of ultrahigh resolution mass spectrometry measurements. It contains several functions for the noise assessment, isotope filtering, interal mass recalibration, and MF assignment. The MFAssignR package was originally developed by Simeon Schum et al. (2020), the source code can be found on 
GitHub
. Please submit eventual Galaxy-related bug reports as 
issues
 on the repository. .. _GitHub: https://github.com/skschum/MFAssignR .. _issues: https://github.com/RECETOX/galaxytools/issues Workflow -------- .. image:: https://github.com/RECETOX/MFAssignR/raw/master/overview.png :width: 1512 :height: 720 :scale: 60 :alt: A picture of a workflow diagram. The recommended workflow how to run the MFAssignR package is as follows: (1) Run KMDNoise() to determine the noise level for the data. (2) Check effectiveness of S/N threshold using SNplot(). (3) Use IsoFiltR() to identify potential 13C and 34S isotope masses. (4) Using the S/N threshold, and the two data frames output from IsoFiltR(), run MFAssignCHO() to assign MF with C, H, and O to assess the mass accuracy. (5) Use RecalList() to generate a list of the potential recalibrant series. (6) Choose the most suitable recalibrant series using FindRecalSeries(). (7) After choosing recalibrant series, use Recal() to recalibrate the mass lists. (8) Assign MF to the recalibrated mass list using MFAssign(). (9) Check the output plots from MFAssign() to evaluate the quality of the assignments. For detailed documentation on the individual steps please see the individual tool wrappers."
toolshed.g2.bx.psu.edu/repos/lecorguille/msnbase_readmsdata/msnbase_readmsdata/2.16.1+galaxy3	".. class:: infomark 
Authors
 Laurent Gatto, Johannes Rainer and Sebastian Gibb with contributions from Guangchuang Yu, Samuel Wieczorek, Vasile-Cosmin Lazar, Vladislav Petyuk, Thomas Naake, Richie Cotton and Martina Fisher. .. class:: infomark 
Galaxy integration
 ABiMS TEAM - SU/CNRS - Station biologique de Roscoff and Yann Guitton - LABERCA Part of Workflow4Metabolomics.org [W4M] | Contact support@workflow4metabolomics.org for any questions or concerns about the Galaxy implementation of this tool. --------------------------------------------------- ================== MSnbase readMSData ================== ----------- Description ----------- Reads as set of XML-based mass-spectrometry data files and generates an MSnExp object. This function uses the functionality provided by the ‘mzR’ package to access data and meta data in ‘mzData’, ‘mzXML’ and ‘mzML’. .. 
xcms: https://bioconductor.org/packages/release/bioc/html/xcms.html ----------------- Workflow position ----------------- 
Upstream tools
 ========================= ========================================== Name Format ========================= ========================================== Upload File mzxml,mzml,mzdata,netcdf,zip ========================= ========================================== The easier way to process is to create a Dataset Collection of the type List 
Downstream tools
 =========================== ==================== ==================== Name Output file Format =========================== ==================== ==================== xcms.findChromPeaks 
*
.raw.RData rdata.msnbase.raw =========================== ==================== ==================== 
Example of a metabolomic workflow
 .. image:: msnbase_readmsdata_workflow.png --------------------------------------------------- ----------- Input files ----------- =========================== ================================== Parameter : num + label Format =========================== ================================== OR : Zip file zip --------------------------- ---------------------------------- OR : Single file mzXML, mzML, mzData, netCDF =========================== ================================== 
Choose your inputs
 You have two methods for your inputs: | Single file (recommended): You can put a single file as input. That way, you will be able to launch several readMSData and findChromPeaks in parallel and use ""findChromPeaks Merger"" before groupChromPeaks. | Zip file: You can put a zip file containing your inputs: myinputs.zip (containing all your conditions as sub-directories). Zip file: Steps for creating the zip file ----------------------------------------- 
Step1: Creating your directory and hierarchize the subdirectories
 VERY IMPORTANT: If you zip your files under Windows, you must use the 7Zip
 software, otherwise your zip will not be well unzipped on the W4M platform (corrupted zip bug). .. 
7Zip: http://www.7-zip.org/ Your zip should contain all your conditions as sub-directories. For example, two conditions (mutant and wild): arabidopsis/wild/01.raw arabidopsis/mutant/01.raw 
Step2: Creating a zip file
 Create your zip file (
e.g.
 arabidopsis.zip). 
Step 3 : Uploading it to our Galaxy server
 Advices for converting your files into mzXML format (XCMS input) ---------------------------------------------------------------- We recommend you to convert your raw files into 
mzXML
 in centroid mode (smaller files); this way the files will be compatible with the xmcs centWave algorithm. 
We recommend you the following parameters:
 Use Filtering: 
True
 Use Peak Picking: 
True
 Peak Peaking -Apply to MS Levels: 
All Levels (1-)
 : Centroid Mode Use zlib: 
64
 Binary Encoding: 
64
 m/z Encoding: 
64
 Intensity Encoding: 
64
 ------------ Output files ------------ xset.RData: rdata.msnbase.raw format | Rdata file that is necessary in the second step of the workflow ""xcms.findChromPeaks"". sampleMetadata.tsv (only when a zip is used) | Tabular file that contains for each sample its associated class and polarity (positive,negative and mixed). | This file is necessary in further steps of the workflow, as the Anova and PCA steps for example. | You get a sampleMetadata.tsv only if you use a zip. Otherwise, you have to provide one for the findChromPeaks Merger step. --------------------------------------------------- Changelog/News -------------- .. _News: https://lgatto.github.io/MSnbase/news/index.html 
Version 2.16.1+galaxy0 - 08/04/2019
 - UPGRADE: upgrade the MSnbase version from 2.8.2 to 2.16.1 (see MSnbase News
). Almost all the new features may not concern our usage of MSnbase. 
Version 2.8.2.1 - 30/04/2019
 - BUGFIX: remove the pre-compute of the chromatograms which was memory consuming. Now, only xcms plot chromatogram will generate the Chromatograms. 
Version 2.8.2.0 - 08/04/2019
 - UPGRADE: upgrade the MSnbase version from 2.4.0 to 2.8.2 (see MSnbase News_). Almost all the new features may not concern our usage of MSnbase. 
Version 2.4.0.0 - 29/03/2018
 - NEW: a new dedicated tool to read the raw data. This function was previously included in xcms.findChromPeaks. This way, you will now be able to display TICs and BPCs before xcms.findChromPeaks."
toolshed.g2.bx.psu.edu/repos/recetox/ramclustr/ramclustr/1.3.1+galaxy0	"Documentation For documentation on the tool see https://github.com/cbroeckl/RAMClustR/blob/master/vignettes/RAMClustR.Rmd Upstream Tools +------------------------------+-------------------------------+----------------------+---------------------+ | Name | Output File | Format | Parameter | +==============================+===============================+======================+=====================+ | xcms | xset.fillPeaks.RData | rdata.xcms.fillpeaks | xcmsObj | +------------------------------+-------------------------------+----------------------+---------------------+ | RAMClustR define experiment | Table with experiment details | csv | Experimental design | +------------------------------+-------------------------------+----------------------+---------------------+ The tool takes an 
xcmsSet
 object as input and extracts all relevant information. +-------+------------------------+--------+------------+ | Name | Output File | Format | Parameter | +=======+========================+========+============+ | ??? | Feature Table with MS1 | csv | ms | +-------+------------------------+--------+------------+ | ??? | Feature Table with MS2 | csv | idmsms | +-------+------------------------+--------+------------+ Alternatively, the tool takes a 
csv
 table as input which has to fulfill the following requirements (1) no more than one sample (or file) name column and one feature name row; (2) feature names that contain the mass and retention times, separated by a constant delimiter; and (3) features in columns and samples in rows. +----------------------+-------------------+-------------------+--------------------+--------------------+ | sample | 100.88_262.464 | 100.01_423.699 | 100.003_128.313 | 100.0057_154.686 | +======================+===================+===================+====================+====================+ | 10_qc_16x_dil_milliq | 0 | 195953.6376 | 0 | 0 | +----------------------+-------------------+-------------------+--------------------+--------------------+ | 11_qc_8x_dil_milliq | 0 | 117742.1828 | 4247300.664 | 0 | +----------------------+-------------------+-------------------+--------------------+--------------------+ | 12_qc_32x_dil_milliq | 4470859.38 | 0 | 2206092.112 | 0 | +----------------------+-------------------+-------------------+--------------------+--------------------+ | 15_qc_16x_dil_milliq | 0 | 0 | 2767477.481 | 0 | +----------------------+-------------------+-------------------+--------------------+--------------------+ Downstream Tools The output is a msp file or a collection of msp files, with additional Spec Abundance file. +---------+--------------+----------------------+ | Name | Output File | Format | +=========+==============+======================+ | matchms | Mass Spectra | collection (tgz/msp) | +---------+--------------+----------------------+ Background Metabolomics Metabolomics is frequently performed using chromatographically coupled mass spectrometry, with gas chromatography, liquid chromatography, and capillary electrophoresis being the most frequently utilized methods of separation. The coupling of chromatography to mass spectrometry is enabled with an appropriate ionization source - electron impact (EI) for gas phase separations and electrospray ionization (ESI) for liquid phase separations. XCMS is a commonly used tool to detect all the signals from a metabolomics dataset, generating aligned features, where a feature is represented by a mass and retention time. Each feature is presumed to derive from a single compound. However, each compound is represented by several features. With any ionization method, isotopic peaks will be observed reflective of the elemental composition of the analyte. In EI, fragmentation is a byproduct of ionization, and has driven the generation of large mass spectral libraries. In ESI, in-source fragmentation frequently occurs, the magnitude of which is compound dependent, with more labile compounds being more prone to in-source fragmentation. ESI can also product multiple adduct forms (protonated, potassiated, sodiated, ammoniated...), and can produce multimers (i.e. [2M+H]+, [3M+K]+, etc) and multiple charged species ([M+2H]++). This can become further complicated by considering combinations of these phenomena. For example [2M+3H]+++ (triply charged dimer) or an in-source fragment of a dimer. RAMClustR approach RAMClustR was designed to group features designed from the same compound using an approach which is 
1.
 unsupervised, 
2.
 platform agnostic, and 
3.
 devoid of curated rules, as the depth of understanding of these processes is insufficient to enable accurate curation/prediction of all phenomenon that may occur. We achieve this by making two assumptions. The first is that two features derived from the same compound with have (approximately) the same retention time. The second is that two features derived from the same compound will have (approximately) the same quantitative trend across all samples in the xcms sample set. From these assumptions, we can calculate a retention time similarity score and a correlational similarity score for each feature pair. A high similarity score for both retention time and correlation indicates a strong probability that two features derive from the same compound. Since both conditions must be met, the product of the two similarity scores provides the best approximation of the total similarity score - i.e. a feature pair with retention time similarity of 1 and correlational similarity of 0 is unlikely to derive from one compound - 1 x 0 = 0, the final similarity score is zero, indicating the two features represent two different compounds. Similarly, a feature pair with retention time similarity of 0 and correlational similarity of 1 is unlikely to derive from one compound - 0 x 1 = 0. Alternatively - a feature pair with retention time similarity of 1 and correlational similarity of 1 is likely to derive from one compound - 1 x 1 = 1. The RAMClustR algorithm is built on creating similarity scores for all pairs of features, submitting this score matrix for hierarchical clustering, and then cutting the resulting dendrogram into neat chunks using the dynamicTreeCut package - where each 'chunk' of the dendrogram results in a group of features likely to be derived from a single compound. Importantly, this is achieved without looking for specific phenomenon (i.e. sodiation), meaning that grouping can be performed on any dataset, whether it is positive or negative ionization mode, EI or ESI, LC-MS GC-MS or CE-MS, in-source fragment or complex adduction event, and predictable or unpredictable signals. .. rubric:: 
Footnotes
 .. [1] Correlation, Variance and Covariance - 
stats::cor &lt;https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor&gt;
 .. [2] Hierarchical Clustering - 
stats::hclust &lt;https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/hclust&gt;
 .. [3] Dynamic Dendrogram Pruning Based on Dendrogram Only - 
dynamicTreeCut::cutreeDynamicTree &lt;https://www.rdocumentation.org/packages/dynamicTreeCut/versions/1.63-1/topics/cutreeDynamicTree&gt;
_"
toolshed.g2.bx.psu.edu/repos/recetox/ramclustr_define_experiment/ramclustr_define_experiment/1.0.2+galaxy2	Create an Experimental Design specification for RAMClustR experiment. Downstream Tools +-----------+-----------------------+--------+ | Name | Output File | Format | +===========+=======================+========+ | RAMClustR | Experiment definition | csv | +-----------+-----------------------+--------+
toolshed.g2.bx.psu.edu/repos/recetox/riassigner/riassigner/0.4.1+galaxy1	A list of compounds with retention times for which to compute the retention index. Expected columns for RT (oneof): ['rt', 'retention_time'].
toolshed.g2.bx.psu.edu/repos/recetox/waveica/waveica/0.2.0+galaxy9	"Description
 Removal of batch effects for large-scale untargeted metabolomics data based on wavelet analysis and independent component analysis. The WaveICA method uses the time trend of samples over the injection order, decomposes the original data into new multi-scale features, extracts and removes the batch effect resulting in normalized intensities across samples. The input is an intensity-by-feature table with metadata in the following format: +---------------+--------+------------+----------------+-------+------------+--------------+-------------+-------------+-----+ | sampleName | class | sampleType | injectionOrder | batch | M85T34 | M86T41 | M86T518 | M86T539 | ... | +===============+========+============+================+=======+============+==============+=============+=============+=====+ | VT_160120_002 | sample | sample | 1 | 1 | 228520.064 | 35646729.215 | 2386896.979 | 1026645.836 | ... | +---------------+--------+------------+----------------+-------+------------+--------------+-------------+-------------+-----+ | QC1 | sample | QC | 2 | 1 | 90217.384 | 35735702.457 | 2456290.696 | 1089246.460 | ... | +---------------+--------+------------+----------------+-------+------------+--------------+-------------+-------------+-----+ | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | +---------------+--------+------------+----------------+-------+------------+--------------+-------------+-------------+-----+ + The required columns are 
sampleName
, 
class
, 
sampleType
, 
injectionOrder
, and the 
features
 that you want to normalize. + The 
batch
 column is required if batch correction mode is 
Multiple batches
 and optional otherwise. + The presence of any additional columns (except features) will result in incorrect batch correction or job failure. + the input table must not contain missing values. Missing intensities must be filled with 0. + 
sampleType
 column accepts three possible values: [QC, sample, blank] (case insensitive). + 
class
 column is used to denote a biological group of a sample (e.g., positive/negative species). The column accepts any values. + the 
output
 is the same table with corrected feature intensities. .. rubric:: 
Footnotes
 .. [1] for details on wavelet-filter parameters refer to R 
wavelets::wt.filter &lt;https://www.rdocumentation.org/packages/wavelets/versions/0.3-0.2/topics/wt.filter&gt;
_; .. [2] when using 'Multiple batches', please cite the WaveICA (2019) paper; else, cite WaveICA 2.0 (2021) paper;"
toolshed.g2.bx.psu.edu/repos/recetox/matchms/matchms/0.17.0+galaxy0	"Documentation For documentation on the tool see https://github.com/matchms/matchms/blob/master/README.rst and https://matchms.readthedocs.io/en/latest/. Upstream Tools +-----------+---------------+--------+-----------+ | Name | Output File | Format | Parameter | +===========+===============+========+===========+ | RAMClustR | Mass spectra | msp | references| +-----------+---------------+--------+-----------+ | RAMClustR | Mass spectra | msp | queries | +-----------+---------------+--------+-----------+ Downstream Tools The output is a JSON file containing serialized matchMS Scores object. The file can be processed by 
matchMS output formatter
."
toolshed.g2.bx.psu.edu/repos/recetox/matchms_filtering/matchms_filtering/0.30.2+galaxy0	Documentation The Python library matchms provides functions to convert, filter and compare mass spectrometry data. For an overview of the different galaxy modules, see the figure below. For detailed documentation on the tool, see https://matchms.readthedocs.io/en/latest/ for the Python API. Overview .. image:: https://github.com/RECETOX/galaxytools/raw/29e54e69dab6ab1263f56d35ea19f5d7f284d728/tools/matchms/images/matchms_galaxytools.png :width: 3120 :height: 1686 :scale: 30 :alt: Overview of different matchms galaxytools and how they are connected.
toolshed.g2.bx.psu.edu/repos/recetox/matchms_formatter/matchms_formatter/0.30.2+galaxy0	"Usage This tool creates user friendly tables from the similarity scores produced by 
matchms similarity
. The tool can be operated on two modes based on (i) thresholds or (ii) top k matches. Input Table Format The tool expects a JSON file containing serialized matchms Scores object. Output Table Format +----------+-----------+---------+--------+ | query | reference | matches | scores | +==========+===========+=========+========+ | C001 | Glycine | 6 | 0.5 | +----------+-----------+---------+--------+ | C002 | Glycine | 3 | 0.34 | +----------+-----------+---------+--------+ | ... | ... | ... | ... | +----------+-----------+---------+--------+ Documentation The Python library matchms provides functions to convert, filter and compare mass spectrometry data. For an overview of the different galaxy modules, see the figure below. For detailed documentation on the tool, see https://matchms.readthedocs.io/en/latest/ for the Python API. Overview .. image:: https://github.com/RECETOX/galaxytools/raw/29e54e69dab6ab1263f56d35ea19f5d7f284d728/tools/matchms/images/matchms_galaxytools.png :width: 3120 :height: 1686 :scale: 30 :alt: Overview of different matchms galaxytools and how they are connected."
toolshed.g2.bx.psu.edu/repos/yguitton/metams_rungc/metams_runGC/3.0.0+metaMS1.24.0-galaxy0	".. class:: infomark 
Author(s)
 Ron Wehrens (ron.wehrens@gmail.com), Georg Weingart, Fulvio Mattivi .. class:: infomark 
Galaxy wrapper and scripts developpers
 Guitton Yann LABERCA yann.guitton@oniris-nantes.fr and Saint-Vanne Julien julien.saint-vanne@sb-roscoff.fr .. class:: infomark 
Please cites
 metaMS : Wehrens, R.; Weingart, G.; Mattivi, F. Journal of Chromatography B. xcms : Smith, C. A.; Want, E. J.; O’Maille, G.; Abagyan, R.; Siuzdak, G. Anal. Chem. 2006, 78, 779–787. CAMERA : Kuhl, C.; Tautenhahn, R.; Böttcher, C.; Larson, T. R.; Neumann, S. Analytical Chemistry 2012, 84, 283–289. ==================== metaMS.runGC ==================== ----------- Description ----------- metaMS.runGC is a function dedicated to GCMS data processing from converted files to the generation of pseudospectra (compounds) table. Current version for metaMS R package: 1.18.1 
Process:
 Each of the converted data (cdf, mzML...) is profiled by a combination of xcms and CAMERA functions. Then all the mass spectra of detected peaks are compared and clustered. For more details see metaMS : Wehrens, R.; Weingart, G.; Mattivi, F. Journal of Chromatography B (10.1016/j.jchromb.2014.02.051) link_ .. 
link: http://www.sciencedirect.com/science/article/pii/S1570023214001548 
Main outputs:
 A PeakTable is generated with one line per ""compound"" and one column per sample. A dataMatrix.csv file is generated and can be used for PCA or for further analysis. A peakspectra.mps file is generated that contains all the spectra of the detected compounds in MSP format. That file can be used for database search online (Golm, MassBank) or locally (NIST MSSEARCH) for NIST search a tutorial is available here
. .. 
here: http://web11.sb-roscoff.fr/download/w4m/howto/w4m_HowToUseNIST_V01.pdf ----------------- Workflow position ----------------- 
Upstream tools
 You can start from here (XCMS 1.x) or use result file from (XCMS 3.x) : ========================= ==================== ======= ========== Name output file format parameter ========================= ==================== ======= ========== xcms.xcmsSet xset.RData RData RData file ========================= ==================== ======= ========== 
Downstream tools
 +---------------------------+---------------------------------------+--------+ | Name | Output file | Format | +===========================+=======================================+========+ |Determine Vdk or Lowess | dataMatrix.tsv | Tabular| +---------------------------+---------------------------------------+--------+ |Normalization Vdk/Lowess | dataMatrix.tsv | Tabular| +---------------------------+---------------------------------------+--------+ |Anova | dataMatrix.tsv | Tabular| +---------------------------+---------------------------------------+--------+ |PCA | dataMatrix.tsv | Tabular| +---------------------------+---------------------------------------+--------+ |Hierarchical Clustering | dataMatrix.tsv | Tabular| +---------------------------+---------------------------------------+--------+ |Golm Metabolome Search | peakspectra.msp | Text | +---------------------------+---------------------------------------+--------+ 
General schema of the metabolomic workflow for GCMS
 .. image:: workflow_metaMS_runGC.png ----------- Input files ----------- If you choose to use results from xcms.xcmsSet with XCMS > 3.0 +---------------------------+------------+ | Parameter : num + label | Format | +===========================+============+ | 1 : RData file from XCMS | RData | +---------------------------+------------+ .. |lt| unicode:: U+0003C .. LESS-THAN SIGN Or converted GCMS files (mzML, CDF...) in your local libray with XCMS |lt| 3.0 (from metaMS) ---------- Parameters ---------- Parameters are described in metaMS R package and mainly correspond to those of xcms.xcmsSet ---------- Outputs ---------- - The output file 
dataMatrix.tsv
 is a tabular file. You can continue your analysis using it in the statistical tools. - The output file 
peakspectra.msp
 is a text file. You can continue your analysis using it in the golm search tool. Or you can load it in your personnal NIST MSsearch program (c:/NISTXX/mssearch/nistms.exe) that tool is in general installed by default on GCMS apparatus. Tutorial available here
. .. _here: http://web11.sb-roscoff.fr/download/w4m/howto/w4m_HowToUseNIST_V01.pdf ---------------- Working Example ---------------- .. class:: warningmark 
Reference Data for testing are taken from:
 Dittami,S.M. et al. (2012) 
Towards deciphering dynamic changes and evolutionary mechanisms involved in the adaptation to low salinities in Ectocarpus (brown algae): Adaptation to low salinities in Ectocarpus.
 The Plant Journal Input files ----------- | 
RData file from XCMS
 -> xset_merged.RData Parameters ---------- | Settings -> 
Default
 | DB option -> 
show
 | DB file: -> 
W4M0004_database_small.msp
 | ...all default option values Output files ------------ | 
RData file
 -> rungc.RData --------------------------------------------------- Changelog/News -------------- 
Version 3.0 - 20/05/2019
 - Divided tool into two new. One will be metaMS_plot and this one stay the same without plotting chromatograms and without zip files 
Version 2.1 - 08/06/2017
 - Quality: add sessionInfo logs with packages versions - Processing: add RI usage 
Version 1.1 - 11/07/2016
 - TEST: refactoring to pass planemo test using conda dependencies 
Version 1.0 - 01/06/2015
 - NEW: new tool"
toolshed.g2.bx.psu.edu/repos/recetox/table_pandas_rename_column/table_pandas_rename_column/2.2.3+galaxy1	"This tool renames columns in a table. Inputs ------ - 
Input Dataset
: The input dataset in CSV, TSV, tabular, or Parquet format. - 
Columns to Rename
: Specify the columns to rename and their new names. You can use the ""repeat"" function to rename multiple columns. Outputs ------- - 
Output Dataset
: The output dataset with the renamed columns."
toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_retcor/abims_xcms_retcor/3.12.0+galaxy3	".. class:: infomark 
Authors
 Colin A. Smith csmith@scripps.edu, Ralf Tautenhahn rtautenh@gmail.com, Steffen Neumann sneumann@ipb-halle.de, Paul Benton hpaul.benton08@imperial.ac.uk and Christopher Conley cjconley@ucdavis.edu .. class:: infomark 
Galaxy integration
 ABiMS TEAM - SU/CNRS - Station biologique de Roscoff and Yann Guitton - LABERCA Part of Workflow4Metabolomics.org [W4M] | Contact support@workflow4metabolomics.org for any questions or concerns about the Galaxy implementation of this tool. --------------------------------------------------- ================ xcms adjustRtime ================ ----------- Description ----------- After matching peaks into groups, xcms can use those groups to identify and correct correlated drifts in retention time from run to run. The aligned peaks can then be used for a second pass of peak grouping which will be more accurate than the first. The whole process can be repeated in an iterative fashion. Not all peak groups will be helpful for identifying retention time drifts. Some groups may be missing peaks from a large fraction of samples and thus provide an incomplete picture of the drift at that time point. Still others may contain multiple peaks from the same sample, which is a sign of impropper grouping. .. class:: warningmark 
After an adjustRtime step, it is mandatory to do a groupChromPeaks step, otherwise the rest of the workflow will not work with the RData file. (the initial peak grouping becomes invalid and is discarded)
 ----------------- Workflow position ----------------- 
Upstream tools
 ========================= ============================ ============================== Name Output file Format ========================= ============================ ============================== xcms.findChromPeaks raw.xset.RData rdata.xcms.findchrompeaks ------------------------- ---------------------------- ------------------------------ xcms.groupChromPeaks 
*
.groupChromPeaks.RData rdata.xcms.group ========================= ============================ ============================== 
Downstream tools
 =========================== ============================ ================ Name Output file Format =========================== ============================ ================ xcms.groupChromPeaks 
*
.groupChromPeaks.RData rdata.xcms.group =========================== ============================ ================ 
General schema of the metabolomic workflow
 .. image:: xcms_retcor_workflow.png --------------------------------------------------- ---------- Parameters ---------- Method ------ 
PeakGroups
 | This method performs retention time adjustment based on the alignment of chromatographic peak groups present in all/most samples (hence corresponding to house keeping compounds). First the retention time deviation of these peak groups is described by fitting either a polynomial (‘smooth = ""loess""’) or a linear ( ‘smooth = ""linear""’) model to the data points. These models are subsequently used to adjust the retention time of each spectrum in each sample. | See the PeakGroups_manual_ 
Obiwarp
 | This method performs retention time adjustment using the Obiwarp method [Prince 2006]. It is based on the code at http://obi-warp.sourceforge.net but supports alignment of multiple samples by aligning each against a 
center
 sample. The alignment is performed directly on the ‘profile-matrix’ and can hence be performed independently of the peak detection or peak grouping. | See the Obiwarp_manual_ .. 
PeakGroups_manual: https://rdrr.io/bioc/xcms/man/adjustRtime-peakGroups.html#heading-2 .. _Obiwarp_manual: https://rdrr.io/bioc/xcms/man/adjustRtime-obiwarp.html WARNING: if a retention time ajustment have already been applied to your data. The function applyAdjustedRtime will replace raw retention times with adjusted retention times and so alloww to cumulate the ajustments. For details and explanations concerning all the parameters and workflow of xcms
 package, see its manual_ and this example_ .. 
xcms: https://bioconductor.org/packages/release/bioc/html/xcms.html .. _manual: http://www.bioconductor.org/packages/release/bioc/manuals/xcms/man/xcms.pdf .. _example: https://bioconductor.org/packages/release/bioc/vignettes/xcms/inst/doc/xcms.html .. _news: https://bioconductor.org/packages/release/bioc/news/xcms/NEWS ------------ Output files ------------ xset.groupChromPeaks.adjustRtime.RData: rdata.xcms.retcor format | Rdata file that will be necessary in the 
xcms.groupChromPeaks
 step of the workflow. --------------------------------------------------- Changelog/News -------------- 
Version 3.12.0+galaxy* - 03/03/2020
 - UPGRADE: upgrade the xcms version from 3.6.1 to 3.12.0 (see XCMS news
) 
Version 3.6.1+galaxy1 - 13/02/2020
 - NEW: if a retention time ajustment have already been applied to your data. The function applyAdjustedRtime will replace raw retention times with adjusted retention times and so alloww to cumulate the ajustments. 
Version 3.6.1+galaxy* - 03/09/2019
 - UPGRADE: upgrade the xcms version from 3.4.4 to 3.6.1 (see XCMS news_) 
Version 3.4.4.1 - 30/04/2019
 - BUGFIX: remove the pre-compute of the chromatograms which was memory consuming. Now, only xcms plot chromatogram will generate the Chromatograms. 
Version 3.4.4.0 - 08/02/2019
 - UPGRADE: upgrade the xcms version from 3.0.0 to 3.4.4 (see XCMS news_) 
Version 3.0.0.0 - 08/03/2018
 - UPGRADE: upgrade the xcms version from 1.46.0 to 3.0.0. So refactoring of a lot of underlying codes and methods. Some parameters may have been renamed. - NEW: a bunch of new options: Obiwarp.(centerSample, response, distFun, gapInit, gapExtend, factorDiag, factorGap, localAlignment, initPenalty) - IMPROVEMENT: the advanced options are now in sections. It will allow you to access to all the parameters and to know their default values. - CHANGE: removing of the TIC and BPC plots. You can now use the dedicated tool ""xcms plot chromatogram"" 
Version 2.1.1 - 29/11/2017
 - BUGFIX: To avoid issues with accented letter in the parentFile tag of the mzXML files, we changed a hidden mechanim to LC_ALL=C 
Version 2.1.0 - 03/02/2017
 - IMPROVEMENT: xcms.retcor can deal with merged individual data 
Version 2.0.8 - 22/12/2016
 - BUGFIX: when having only one group (i.e. one folder of raw data) the BPC and TIC pdf files do not contain any graph @HELP_XCMS_NEWVERSION_2090@ 
Version 2.0.6 - 04/04/2016
 - TEST: refactoring to pass planemo test using conda dependencies 
Version 2.0.5 - 10/02/2016
 - BUGFIX: better management of errors. Datasets remained green although the process failed - BUGFIX: some pdf remained empty even when the process succeed - UPDATE: refactoring of internal management of inputs/outputs - UPDATE: refactoring to feed the new report tool 
Version 2.0.2 - 02/06/2015
 - IMPROVEMENT: new datatype/dataset formats (rdata.xcms.raw, rdata.xcms.group, rdata.xcms.retcor ...) will facilitate the sequence of tools and so avoid incompatibility errors. - IMPROVEMENT: parameter labels have changed to facilitate their reading."
toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_fillpeaks/abims_xcms_fillPeaks/3.12.0+galaxy3	".. class:: infomark 
Authors
 Colin A. Smith csmith@scripps.edu, Ralf Tautenhahn rtautenh@gmail.com, Steffen Neumann sneumann@ipb-halle.de, Paul Benton hpaul.benton08@imperial.ac.uk and Christopher Conley cjconley@ucdavis.edu .. class:: infomark 
Galaxy integration
 ABiMS TEAM - SU/CNRS - Station biologique de Roscoff and Yann Guitton - LABERCA Part of Workflow4Metabolomics.org [W4M] | Contact support@workflow4metabolomics.org for any questions or concerns about the Galaxy implementation of this tool. --------------------------------------------------- =================== xcms fillChromPeaks =================== ----------- Description ----------- 
Integrate areas of missing peaks
 For each sample, identify peak groups where that sample is not represented. For each of those peak groups, integrate the signal in the region of that peak group and create a new peak. ----------------- Workflow position ----------------- 
Upstream tools
 ========================= ============================ ================== Name Output file Format ========================= ============================ ================== xcms.groupChromPeaks 
*
.groupChromPeaks.RData rdata.xcms.group ========================= ============================ ================== 
Downstream tools
 =========================== =========================== ======================= Name Output file Format =========================== =========================== ======================= CAMERA.annotate 
*
.fillChromPeaks.RData rdata.xcms.fillpeaks --------------------------- --------------------------- ----------------------- xcms.process_history 
*
.fillChromPeaks.RData rdata.xcms.fillpeaks =========================== =========================== ======================= 
General schema of the metabolomic workflow
 .. image:: xcms_fillpeaks_workflow.png --------------------------------------------------- ---------- Parameters ---------- | See the fillChromPeaks_manual_ .. 
fillChromPeaks_manual: https://rdrr.io/bioc/xcms/man/fillChromPeaks.html For details and explanations concerning all the parameters and workflow of xcms
 package, see its manual_ and this example_ .. 
xcms: https://bioconductor.org/packages/release/bioc/html/xcms.html .. _manual: http://www.bioconductor.org/packages/release/bioc/manuals/xcms/man/xcms.pdf .. _example: https://bioconductor.org/packages/release/bioc/vignettes/xcms/inst/doc/xcms.html .. _news: https://bioconductor.org/packages/release/bioc/news/xcms/NEWS Get a Peak List --------------- If 'true', the module generates two additional files corresponding to the peak list: - the variable metadata file (corresponding to information about extracted ions such as mass or retention time) - the data matrix (corresponding to related intensities) 
decimal places for [mass or retention time] values in identifiers
 | Ions' identifiers are constructed as MxxxTyyy where 'xxx' is the ion median mass and 'yyy' the ion median retention time. | Two parameters are used to adjust the number of decimal places wanted in identifiers for mass and retention time respectively. | Theses parameters do not affect decimal places in columns other than the identifier one. 
Reported intensity values
 | This parameter determines which values should be reported as intensities in the dataMatrix table; it correspond to xcms 'intval' parameter: | - into: integrated area of original (raw) peak | - maxo: maximum intensity of original (raw) peak | - intb: baseline corrected integrated peak area (only available if peak detection was done by ‘findPeaks.centWave’) ------------ Output files ------------ xset.fillPeaks.RData : rdata.xcms.fillpeaks format | Rdata file that will be used in the 
CAMERA.annotate
 or 
xcms.process_history
 step of the workflow. xset.variableMetadata.tsv : tabular format | Table containing information about ions; can be used as one input of 
Quality_Metrics
 or 
Generic_filter
 modules. xset.dataMatrix.tsv : tabular format | Table containing ions' intensities; can be used as one input of 
Quality_Metrics
 or 
Generic_filter
 modules. --------------------------------------------------- Changelog/News -------------- 
Version 3.12.0+galaxy* - 03/03/2020
 - UPGRADE: upgrade the xcms version from 3.6.1 to 3.12.0 (see XCMS news
) 
Version 3.6.1+galaxy* - 03/09/2019
 - UPGRADE: upgrade the xcms version from 3.4.4 to 3.6.1 (see XCMS news_) 
Version 3.4.4.0 - 08/02/2019
 - UPGRADE: upgrade the xcms version from 3.0.0 to 3.4.4 (see XCMS news_) - BUGFIX: issue with Inf values in the exported DataMatrix: https://github.com/sneumann/xcms/issues/323#issuecomment-433044378 
Version 3.0.0.2 - 09/11/2018
 - BUGFIX: issue when the vector at peakidx is too long and is written in a new line during the export of the peaklist 
Version 3.0.0.1 - 09/10/2018
 - IMPROVEMENT: the export of the PeakList is now mandatory 
Version 3.0.0.0 - 08/03/2018
 - UPGRADE: upgrade the xcms version from 1.46.0 to 3.0.0. So refactoring of a lot of underlying codes and methods. Some parameters may have been renamed. - UPDATE: since xcms 3.0.0, the selection of a method is no more needed (chrom or MSW). xcms will detect from the data the peak picking method used in findChromPeaks - UPDATE: since xcms 3.0.0, new parameters are available: expandMz, expandRt and ppm 
Version 2.1.1 - 29/11/2017
 - BUGFIX: To avoid issues with accented letter in the parentFile tag of the mzXML files, we changed a hidden mechanim to LC_ALL=C 
Version 2.1.0 - 07/02/2017
 - IMPROVEMENT: change the management of the peaklist ids. The main ids remain the same as xcms generated. The export setiings now only add custom names in the variableMetadata tab (namecustom) - IMPROVEMENT: xcms.fillpeaks can deal with merged individual data 
Version 2.0.8 - 22/12/2016
 - IMPROVEMENT: Add an option to export the peak list at this step without having to wait for CAMERA.annotate 
Version 2.0.7 - 06/07/2016
 - UPGRADE: upgrate the xcms version from 1.44.0 to 1.46.0 
Version 2.0.6 - 04/04/2016
 - TEST: refactoring to pass planemo test using conda dependencies 
Version 2.0.5 - 10/02/2016
 - BUGFIX: better management of errors. Datasets remained green although the process failed - UPDATE: refactoring of internal management of inputs/outputs - UPDATE: refactoring to feed the new report tool 
Version 2.0.2 - 02/06/2015
 - IMPROVEMENT: new datatype/dataset formats (rdata.xcms.raw, rdata.xcms.group, rdata.xcms.retcor ...) will facilitate the sequence of tools and so avoid incompatibility errors. - IMPROVEMENT: parameter labels have changed to facilitate their reading."
toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_xcmsset/abims_xcms_xcmsSet/3.12.0+galaxy3	".. class:: infomark 
Authors
 Colin A. Smith csmith@scripps.edu, Ralf Tautenhahn rtautenh@gmail.com, Steffen Neumann sneumann@ipb-halle.de, Paul Benton hpaul.benton08@imperial.ac.uk and Christopher Conley cjconley@ucdavis.edu .. class:: infomark 
Galaxy integration
 ABiMS TEAM - SU/CNRS - Station biologique de Roscoff and Yann Guitton - LABERCA Part of Workflow4Metabolomics.org [W4M] | Contact support@workflow4metabolomics.org for any questions or concerns about the Galaxy implementation of this tool. --------------------------------------------------- =================== xcms findChromPeaks =================== ----------- Description ----------- This tool is used for preprocessing data from multiple LC/MS files (NetCDF, mzXML and mzData formats) using the xcms_ R package. It extracts ions from each sample independently, and using a statistical model, peaks are filtered and integrated. A tutorial on how to perform xcms preprocessing is available as GTN_ (Galaxy Training Network). .. 
xcms: https://bioconductor.org/packages/release/bioc/html/xcms.html .. _GTN: https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/lcms/tutorial.html ----------------- Workflow position ----------------- 
Upstream tools
 ========================= ================= ============================== Name Output file Format ========================= ================= ============================== MSnbase.readMSData 
*
.raw.RData rdata.msnbase.raw ========================= ================= ============================== 
Downstream tools
 ==================================== ======================== ========================= Name Output file Format ==================================== ======================== ========================= xcms.findChromPeaks Merger (single) 
*
.raw.xset.RData rdata.xcms.findchrompeaks ------------------------------------ ------------------------ ------------------------- xcms.groupChromPeaks (zip) 
*
.raw.xset.RData rdata.xcms.findchrompeaks ==================================== ======================== ========================= 
Example of a metabolomic workflow
 .. image:: xcms_xcmsset_workflow.png --------------------------------------------------- ---------- Parameters ---------- Extraction method for peaks detection ------------------------------------- 
MatchedFilter
 | The matchedFilter algorithm identifies peaks in the chromatographic time domain as described in [Smith 2006]. The intensity values are binned by cutting The LC/MS data into slices (bins) of a mass unit (‘binSize’ m/z) wide. Within each bin the maximal intensity is selected. The chromatographic peak detection is then performed in each bin by extending it based on the ‘steps’ parameter to generate slices comprising bins ‘current_bin - steps +1’ to ‘current_bin + steps - 1’. Each of these slices is then filtered with matched filtration using a second-derative Gaussian as the model peak shape. After filtration peaks are detected using a signal-to-ratio cut-off. For more details and illustrations see [Smith 2006]. | See the MatchedFilter_manual
 
CentWave
 | The centWave algorithm perform peak density and wavelet based chromatographic peak detection for high resolution LC/MS data in centroid mode [Tautenhahn 2008]. | Due to the fact that peak centroids are used, a binning step is not necessary. | The method is capable of detecting close-by-peaks and also overlapping peaks. Some efforts are made to detect the exact peak boundaries to get precise peak integrals. | See the CentWave_manual_ 
CentWaveWithPredIsoROIs
 | This method performs a two-step centWave-based chromatographic peak detection: in a first centWave run peaks are identified for which then the location of their potential isotopes in the mz-retention time is predicted. A second centWave run is then performed on these regions of interest (ROIs). The final list of chromatographic peaks comprises all non-overlapping peaks from both centWave runs. | See the CentWaveWithPredIsoROIs_manual_ 
MSW
 | Wavelet based, used for direct infusion data. Continuous wavelet transform (CWT) can be used to locate chromatographic peaks on different scales. | See the MSW_manual_ .. 
MatchedFilter_manual: https://rdrr.io/bioc/xcms/man/findChromPeaks-matchedFilter.html#heading-2 .. _CentWave_manual: https://rdrr.io/bioc/xcms/man/findChromPeaks-centWave.html#heading-2 .. _CentWaveWithPredIsoROIs_manual: https://rdrr.io/bioc/xcms/man/findChromPeaks-centWaveWithPredIsoROIs.html#heading-2 .. _MSW_manual: https://rdrr.io/bioc/xcms/man/findPeaks-MSW.html#heading-2 For details and explanations concerning all the parameters and workflow of xcms
 package, see its manual_ and this example_ .. 
xcms: https://bioconductor.org/packages/release/bioc/html/xcms.html .. _manual: http://www.bioconductor.org/packages/release/bioc/manuals/xcms/man/xcms.pdf .. _example: https://bioconductor.org/packages/release/bioc/vignettes/xcms/inst/doc/xcms.html .. _news: https://bioconductor.org/packages/release/bioc/news/xcms/NEWS ------------ Output files ------------ xset.RData: rdata.xcms.findchrompeaks format | (single) RData files that are necessary in the second step of the workflow ""xcms.groupChromPeaks"" - must be merged first using ""xcms.findChromPeaks Merger"" | (zip) RData file that is necessary in the second step of the workflow ""xcms.groupChromPeaks"". --------------------------------------------------- Changelog/News -------------- 
Version 3.12.0+galaxy* - 03/03/2020
 - UPGRADE: upgrade the xcms version from 3.6.1 to 3.12.0 (see XCMS news
) 
Version 3.6.1+galaxy1 - 22/04/2020
 - NEW: possibility to get a tabular file with all the chromatographic peaks obtained with the CentWave and MatchedFilter methods. 
Version 3.6.1+galaxy* - 03/09/2019
 - UPGRADE: upgrade the xcms version from 3.4.4 to 3.6.1 (see XCMS news_) 
Version 3.4.4.1 - 30/04/2019
 - BUGFIX: remove the pre-compute of the chromatograms which was memory consuming. Now, only xcms plot chromatogram will generate the Chromatograms. 
Version 3.4.4.0 - 08/02/2019
 - UPGRADE: upgrade the xcms version from 3.0.0 to 3.4.4 (see XCMS news_) 
Version 3.0.0.0 - 08/03/2018
 - UPGRADE: upgrade the xcms version from 1.46.0 to 3.0.0. So refactoring of a lot of underlying codes and methods. Some parameters may have been renamed. - CHANGE: xcms.findChromPeaks no longer read the raw data. You have to run MSnbase readMSData first. - NEW: a bunch of new options: Spectra Filters (previously scanrange), CentWave.(mzCenterFun, fitgauss, verboseColumns), MatchedFilter.(sigma, impute, baseValue, max), MSW.(verboseColumns), ... - NEW: new Filters for Spectra - NEW: new methods: CentWaveWithPredIsoROIs - UPDATE: since xcms 3.0.0, some options are no more available: scanrange (replace by filters), profmethod, MatchedFilter.step, MatchedFilter.sigma, MSW.winSize.noise, MSW.SNR.method - IMPROVEMENT: the advanced options are now in sections. It will allow you to access to all the parameters and to know their default values. - IMPROVEMENT: the tool ""should"" be now more flexible in term of file naming: it ""should"" accept space and comma. But don't be too imaginative :) - CHANGE: removing of the TIC and BPC plots. You can new use the dedicated tool ""xcms plot chromatogram"" 
Version 2.1.1 - 29/11/2017
 - BUGFIX: To avoid issues with accented letter in the parentFile tag of the mzXML files, we changed a hidden mechanim to LC_ALL=C 
Version 2.1.0 - 22/02/2017
 - NEW: The W4M tools will be able now to take as input a single file. It will allow to submit in parallel several files and merge them afterward using ""xcms.xcmsSet Merger"" before ""xcms.group"". - BUGFIX: the default value of ""matchedFilter"" -> ""Step size to use for profile generation"" which was of 0.01 have been changed to fix with the XMCS default values to 0.1 
Version 2.0.11 - 22/12/2016
 - BUGFIX: propose scanrange for all methods 
Version 2.0.10 - 22/12/2016
 - BUGFIX: when having only one group (i.e. one folder of raw data) the BPC and TIC pdf files do not contain any graph 
Version 2.0.9 - 06/07/2016
 - UPGRADE: upgrade the xcms version from 1.44.0 to 1.46.0 
Version 2.0.8 - 06/04/2016
 - TEST: refactoring to pass planemo test using conda dependencies 
Version 2.0.7 - 10/02/2016
 - BUGFIX: better management of errors. Datasets remained green although the process failed - BUGFIX/IMPROVEMENT: New checking steps around the imported data in order to raise explicte error message before or after launch XCMS: checking of bad characters in the filenames, checking of the XML integrity and checking of duplicates which can appear in the sample names during the XCMS process because of bad characters - BUGFIX/IMPROVEMENT: New step to check and delete bad characters in the XML: accented characters in the storage path of the mass spectrometer - UPDATE: refactoring of internal management of inputs/outputs - TEST: refactoring to feed the new report tool 
Version 2.0.2 - 18/01/2016
 - BUGFIX: Some zip files were tag as ""corrupt"" by R. We have changed the extraction mode to deal with thoses cases. 
Version 2.0.2 - 09/10/2015
 - BUGFIX: Some users reported a bug in xcms.xcmsSet. The preprocessing stops itself and doesn't import the whole dataset contained in the zip file without warning. But meanwhile, please check your samplemetadata dataset and the number of rows. 
Version 2.0.2 - 02/06/2015
 - NEW: The W4M workflows will now take as input a zip file to ease the transfer and to improve dataset exchange between tools and users. (See How_to_upload). The previous ""Library directory name"" is still available but we invite user to switch on the new zip system as soon as possible. - IMPROVEMENT: new datatype/dataset formats (rdata.xcms.raw, rdata.xcms.group, rdata.xcms.retcor ...) will facilitate the sequence of tools and so avoid incompatibility errors. - IMPROVEMENT: parameter labels have changed to facilitate their reading."
toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_merge/xcms_merge/3.12.0+galaxy3	".. class:: infomark 
Authors
 Colin A. Smith csmith@scripps.edu, Ralf Tautenhahn rtautenh@gmail.com, Steffen Neumann sneumann@ipb-halle.de, Paul Benton hpaul.benton08@imperial.ac.uk and Christopher Conley cjconley@ucdavis.edu .. class:: infomark 
Galaxy integration
 ABiMS TEAM - SU/CNRS - Station biologique de Roscoff and Yann Guitton - LABERCA Part of Workflow4Metabolomics.org [W4M] | Contact support@workflow4metabolomics.org for any questions or concerns about the Galaxy implementation of this tool. --------------------------------------------------- ========================== xcms findChromPeaks Merger ========================== ----------- Description ----------- This tool allows you to run one xcms findChromPeaks process per sample in parallel and then to merge all RData images into one. The result is then suitable for xcms groupChromPeaks. You can provide a sampleMetadata table to attribute phenotypic values to your samples. ----------------- Workflow position ----------------- 
Upstream tools
 ========================= ===================== ============================== Name Output file Format ========================= ===================== ============================== xcms.findChromPeaks 
*
.raw.xset.RData rdata.xcms.findchrompeaks ------------------------- --------------------- ------------------------------ xcms.findChromPeaks 
*
.raw.xset.RData rdata.xcms.findchrompeaks ------------------------- --------------------- ------------------------------ xcms.findChromPeaks 
*
.raw.xset.RData rdata.xcms.findchrompeaks ------------------------- --------------------- ------------------------------ ... ... ... ------------------------- --------------------- ------------------------------ Upload file [optional] sampleMetadata tabular ========================= ===================== ============================== 
Downstream tools
 =========================== ==================== ============================ Name Output file Format =========================== ==================== ============================ xcms.groupChromPeaks xset.merged.RData rdata.xcms.findchrompeaks =========================== ==================== ============================ .. image:: xcms_merge_workflow.png ----------- Input files ----------- =========================== ================================== Parameter : num + label Format =========================== ================================== 1 : RData file rdata.xcms.findchrompeaks --------------------------- ---------------------------------- 2 : RData file rdata.xcms.findchrompeaks --------------------------- ---------------------------------- N : RData file rdata.xcms.findchrompeaks --------------------------- ---------------------------------- Optional : sampleMetadata tsv or csv =========================== ================================== Example of a sampleMetadata: =========================== ============ Samples class =========================== ============ HU_neg_028 bio --------------------------- ------------ HU_neg_034 bio --------------------------- ------------ Blanc04 blank --------------------------- ------------ Blanc06 blank --------------------------- ------------ Blanc09 blank =========================== ============ ------------ Output files ------------ xset.merged.RData: rdata.xcms.findchrompeaks format | Rdata file that is necessary in the next step of the workflow ""xcms.groupChromPeaks"". For details and explanations concerning all the parameters and workflow of xcms_ package, see its manual_ and this example_ .. 
xcms: https://bioconductor.org/packages/release/bioc/html/xcms.html .. _manual: http://www.bioconductor.org/packages/release/bioc/manuals/xcms/man/xcms.pdf .. _example: https://bioconductor.org/packages/release/bioc/vignettes/xcms/inst/doc/xcms.html .. _news: https://bioconductor.org/packages/release/bioc/news/xcms/NEWS --------------------------------------------------- Changelog/News -------------- 
Version 3.12.0+galaxy* - 03/03/2020
 - UPGRADE: upgrade the xcms version from 3.6.1 to 3.12.0 (see XCMS news
) 
Version 3.6.1+galaxy* - 03/09/2019
 - UPGRADE: upgrade the xcms version from 3.4.4 to 3.6.1 (see XCMS news_) 
Version 3.4.4.0 - 08/02/2019
 - UPGRADE: upgrade the xcms version from 3.0.0 to 3.4.4 (see XCMS news_) 
Version 3.0.0.0 - 08/03/2018
 - UPGRADE: upgrade the xcms version from 1.46.0 to 3.0.0. So refactoring of a lot of underlining codes and methods - IMPROVMENT: a new dedicated tool ""xcms plot chromatogram"" will allow you to get TIC and BPI of your raw data. - IMPROVMENT: the tool will now generate a sampleMetadata file if any was provided. It will be useful to add some further information for the normalization and statistics steps. 
Version 1.0.1 - 13/02/2017
 - IMPROVMENT: the tool will now raise an error if a sample isn't describe in the sampleMetadata file 
Version 1.0.0 - 03/02/2017
 - NEW: a new tool to merge individual xcmsSet outputs to be used by xcms.group"
toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_export_samplemetadata/xcms_export_samplemetadata/3.12.0+galaxy3	".. class:: infomark 
Authors
 Colin A. Smith csmith@scripps.edu, Ralf Tautenhahn rtautenh@gmail.com, Steffen Neumann sneumann@ipb-halle.de, Paul Benton hpaul.benton08@imperial.ac.uk and Christopher Conley cjconley@ucdavis.edu .. class:: infomark 
Galaxy integration
 ABiMS TEAM - SU/CNRS - Station biologique de Roscoff and Yann Guitton - LABERCA Part of Workflow4Metabolomics.org [W4M] | Contact support@workflow4metabolomics.org for any questions or concerns about the Galaxy implementation of this tool. --------------------------------------------------- ======================= xcms get sampleMetadata ======================= ----------- Description ----------- This tool generates a skeleton of sampleMetadata with perhaps some strange sample names which are definitely compatible with xcms and R This sampleMetadata file have to be filled with extra information as the class, batch information and maybe conditions ----------------- Workflow position ----------------- 
Upstream tools
 ========================= ===================== ============================== Name Output file Format ========================= ===================== ============================== xcms.findChromPeaks 
*
.raw.xset.RData rdata.xcms.findchrompeaks ------------------------- --------------------- ------------------------------ xcms.findChromPeaks 
*
.raw.xset.RData rdata.xcms.findchrompeaks ------------------------- --------------------- ------------------------------ xcms.findChromPeaks 
*
.raw.xset.RData rdata.xcms.findchrompeaks ------------------------- --------------------- ------------------------------ ... ... ... ========================= ===================== ============================== 
Downstream tools
 =========================== ==================== ============================ Name Output file Format =========================== ==================== ============================ xcms.findChromPeaks Merger sampleMetadata.tsv tabular =========================== ==================== ============================ .. image:: xcms_merge_workflow.png ----------- Input files ----------- =========================== ================================== Parameter : num + label Format =========================== ================================== 1 : RData file rdata.xcms.findchrompeaks --------------------------- ---------------------------------- 2 : RData file rdata.xcms.findchrompeaks --------------------------- ---------------------------------- N : RData file rdata.xcms.findchrompeaks =========================== ================================== ------------ Output files ------------ Example of a sampleMetadata: =========================== ============ sample_name class =========================== ============ HU_neg_028 bio --------------------------- ------------ HU_neg_034 bio --------------------------- ------------ Blanc04 blank --------------------------- ------------ Blanc06 blank --------------------------- ------------ Blanc09 blank =========================== ============ For details and explanations concerning all the parameters and workflow of xcms_ package, see its manual_ and this example_ .. 
xcms: https://bioconductor.org/packages/release/bioc/html/xcms.html .. _manual: http://www.bioconductor.org/packages/release/bioc/manuals/xcms/man/xcms.pdf .. _example: https://bioconductor.org/packages/release/bioc/vignettes/xcms/inst/doc/xcms.html .. _news: https://bioconductor.org/packages/release/bioc/news/xcms/NEWS --------------------------------------------------- Changelog/News -------------- 
Version 3.12.0+galaxy* - 03/03/2020
 - UPGRADE: upgrade the xcms version from 3.6.1 to 3.12.0 (see XCMS news
) 
Version 3.6.1+galaxy* - 03/09/2019
 - UPGRADE: upgrade the xcms version from 3.4.4 to 3.6.1 (see XCMS news_) 
Version 3.6.1+galaxy* - 03/09/2019
 - UPGRADE: upgrade the xcms version from 3.4.4 to 3.6.1 (see XCMS news_) 
Version 3.4.4.0 - 08/02/2019
 - UPGRADE: upgrade the xcms version from 3.0.0 to 3.4.4 (see XCMS news_) 
Version 3.0.0.0 - 09/10/2018
 - NEW: a new tool to generate a sampleMetadata file you will be able to complete"
toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_group/abims_xcms_group/3.12.0+galaxy3	".. class:: infomark 
Authors
 Colin A. Smith csmith@scripps.edu, Ralf Tautenhahn rtautenh@gmail.com, Steffen Neumann sneumann@ipb-halle.de, Paul Benton hpaul.benton08@imperial.ac.uk and Christopher Conley cjconley@ucdavis.edu .. class:: infomark 
Galaxy integration
 ABiMS TEAM - SU/CNRS - Station biologique de Roscoff and Yann Guitton - LABERCA Part of Workflow4Metabolomics.org [W4M] | Contact support@workflow4metabolomics.org for any questions or concerns about the Galaxy implementation of this tool. --------------------------------------------------- ==================== xcms groupChromPeaks ==================== ----------- Description ----------- After peak identification with xcmsSet, this tool groups the peaks which represent the same analyte across samples using overlapping m/z bins and calculation of smoothed peak distributions in chromatographic time. Allows rejection of features, which are only partially detected within the replicates of a sample class. ----------------- Workflow position ----------------- 
Upstream tools
 ==================================== ======================== ============================== Name Output file Format ==================================== ======================== ============================== xcms.findChromPeaks Merger (single) xset.merged.RData rdata.xcms.findchrompeaks ------------------------------------ ------------------------ ------------------------------ xcms.findChromPeaks (zip) 
*
.raw.xset.RData rdata.xcms.findchrompeaks ------------------------------------ ------------------------ ------------------------------ xcms.adjustRtime 
*
.adjustRtime.RData rdata.xcms.retcor ==================================== ======================== ============================== 
Downstream tools
 =========================== =========================== ==================== Name Output file Format =========================== =========================== ==================== xcms.adjustRtime 
*
.groupChromPeaks.RData rdata.xcms.group --------------------------- --------------------------- -------------------- xcms.fillChromPeaks 
*
.groupChromPeaks.RData rdata.xcms.group =========================== =========================== ==================== 
General schema of the metabolomic workflow
 .. image:: xcms_group_workflow.png --------------------------------------------------- ---------- Parameters ---------- Method to use for grouping -------------------------- 
MzClust
 | This method performs high resolution correspondence for single spectra samples. | See the MzClust_manual_ 
PeakDensity
 | This method performs performs correspondence (chromatographic peak grouping) based on the density (distribution) of identified peaks along the retention time axis within slices of overlapping mz ranges. All peaks (from the same or from different samples) being close on the retention time axis are grouped into a feature (peak group). | See the PeakDensity_manual_ 
NearestPeaks
 | This method is inspired by the grouping algorithm of mzMine [Katajamaa 2006] and performs correspondence based on proximity of peaks in the space spanned by retention time and mz values. The method creates first a master peak list consisting of all chromatographic peaks from the sample in which most peaks were identified, and starting from that, calculates distances to peaks from the sample with the next most number of peaks. If peaks are closer than the defined threshold they are grouped together. | See the NearestPeaks_manual_ .. 
MzClust_manual: https://rdrr.io/bioc/xcms/man/groupChromPeaks-mzClust.html#heading-2 .. _PeakDensity_manual: https://rdrr.io/bioc/xcms/man/groupChromPeaks-density.html#heading-2 .. _NearestPeaks_manual: https://rdrr.io/bioc/xcms/man/groupChromPeaks-nearest.html#heading-2 For details and explanations concerning all the parameters and workflow of xcms
 package, see its manual_ and this example_ .. 
xcms: https://bioconductor.org/packages/release/bioc/html/xcms.html .. _manual: http://www.bioconductor.org/packages/release/bioc/manuals/xcms/man/xcms.pdf .. _example: https://bioconductor.org/packages/release/bioc/vignettes/xcms/inst/doc/xcms.html .. _news: https://bioconductor.org/packages/release/bioc/news/xcms/NEWS Get a Peak List --------------- If 'true', the module generates two additional files corresponding to the peak list: - the variable metadata file (corresponding to information about extracted ions such as mass or retention time) - the data matrix (corresponding to related intensities) 
decimal places for [mass or retention time] values in identifiers
 | Ions' identifiers are constructed as MxxxTyyy where 'xxx' is the ion median mass and 'yyy' the ion median retention time. | Two parameters are used to adjust the number of decimal places wanted in identifiers for mass and retention time respectively. | Theses parameters do not affect decimal places in columns other than the identifier one. 
Reported intensity values
 | This parameter determines which values should be reported as intensities in the dataMatrix table; it correspond to xcms 'intval' parameter: | - into: integrated area of original (raw) peak | - maxo: maximum intensity of original (raw) peak | - intb: baseline corrected integrated peak area (only available if peak detection was done by ‘findPeaks.centWave’) ------------ Output files ------------ xset.groupChromPeaks.RData: rdata.xcms.group format | RData file that will be necessary in the third and fourth step of the workflow (xcms.adjustRtime and xcms.fillChromPeaks). xset.groupChromPeaks.plotChromPeakDensity.pdf | Density plot xset.variableMetadata.tsv : tabular format | Table containing information about ions; can be used as one input of 
Quality_Metrics
 or 
Generic_filter
 modules. xset.dataMatrix.tsv : tabular format | Table containing ions' intensities; can be used as one input of 
Quality_Metrics
 or 
Generic_filter
 modules. --------------------------------------------------- Changelog/News -------------- 
Version 3.12.0+galaxy* - 03/03/2020
 - UPGRADE: upgrade the xcms version from 3.6.1 to 3.12.0 (see XCMS news
) 
Version 3.6.1+galaxy1 - 22/04/2020
 - BUGFIX: sample group colours were not displayed in plots. 
Version 3.6.1+galaxy* - 03/09/2019
 - UPGRADE: upgrade the xcms version from 3.4.4 to 3.6.1 (see XCMS news_) 
Version 3.4.4.0 - 08/02/2019
 - UPGRADE: upgrade the xcms version from 3.0.0 to 3.4.4 (see XCMS news_) - BUGFIX: groupChromPeaks wasn't pass to the ChromPeakDensity plots - BUGFIX: issue with Inf values in the exported DataMatrix: https://github.com/sneumann/xcms/issues/323#issuecomment-433044378 
Version 3.0.0.1 - 09/11/2018
 - BUGFIX: issue when the vector at peakidx is too long and is written in a new line during the export of the peaklist 
Version 3.0.0.0 - 08/03/2018
 - UPGRADE: upgrade the xcms version from 1.46.0 to 3.0.0. So refactoring of a lot of underlying codes and methods. Some parameters may have been renamed. - NEW: a bunch of new options: PeakDensity.minSamples), MzClust.minSamples) - NEW: a new density plot - IMPROVEMENT: the advanced options are now in sections. It will allow you to access to all the parameters and to know their default values. 
Version 2.1.1 - 29/11/2017
 - BUGFIX: To avoid issues with accented letter in the parentFile tag of the mzXML files, we changed a hidden mechanim to LC_ALL=C 
Version 2.1.0 - 07/02/2017
 - IMPROVEMENT: Add an option to export the peak list at this step without have to wait camara.annotate - IMPROVEMENT: xcms.group can deal with merged individual data from ""xcms.xcmsSet Merger"" - BUGFIX: the default value of ""density"" -> ""Maximum number of groups to identify in a single m/z slice"" which was of 5 have been changed to fix with the XMCS default values to 50 
Version 2.0.6 - 06/07/2016
 - UPGRADE: upgrate the xcms version from 1.44.0 to 1.46.0 
Version 2.0.5 - 04/04/2016
 - TEST: refactoring to pass planemo test using conda dependencies 
Version 2.0.4 - 10/02/2016
 - BUGFIX: better management of errors. Datasets remained green although the process failed - UPDATE: refactoring of internal management of inputs/outputs - UPDATE: refactoring to feed the new report tool 
Version 2.0.2 - 02/06/2015
 - IMPROVEMENT: new datatype/dataset formats (rdata.xcms.raw, rdata.xcms.group, rdata.xcms.retcor ...) will facilitate the sequence of tools and so avoid incompatibility errors. - IMPROVEMENT: parameter labels have changed to facilitate their reading."
toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_plot_chromatogram/xcms_plot_chromatogram/3.12.0+galaxy3	".. class:: infomark 
Authors
 Colin A. Smith csmith@scripps.edu, Ralf Tautenhahn rtautenh@gmail.com, Steffen Neumann sneumann@ipb-halle.de, Paul Benton hpaul.benton08@imperial.ac.uk and Christopher Conley cjconley@ucdavis.edu .. class:: infomark 
Galaxy integration
 ABiMS TEAM - SU/CNRS - Station biologique de Roscoff and Yann Guitton - LABERCA Part of Workflow4Metabolomics.org [W4M] | Contact support@workflow4metabolomics.org for any questions or concerns about the Galaxy implementation of this tool. --------------------------------------------------- ====================== xcms plot chromatogram ====================== ----------- Description ----------- This tool will plot Base Peak Intensity chromatogram (BPI) and Total Ion Current chromatogram (TIC) from xcms experiments. ----------------- Workflow position ----------------- 
Upstream tools
 =========================== ======================== ============================== Name Output file Format =========================== ======================== ============================== MSnbase.readMSData 
*
.raw.RData rdata.msnbase.raw --------------------------- ------------------------ ------------------------------ xcms.findChromPeaks 
*
.raw.xset.RData rdata.xcms.findchrompeaks --------------------------- ------------------------ ------------------------------ xcms.findChromPeaks Merger xset.merged.RData rdata.xcms.retcor --------------------------- ------------------------ ------------------------------ xcms.adjustRtime 
*
.adjustRtime.RData rdata.xcms.retcor =========================== ======================== ============================== .. image:: xcms_plot_chromatogram_workflow.png --------------------------------------------------- ------------ Output files ------------ 
Total Ion Current (TIC) chromatogram
 | Sum of intensity (Y) of all ions detected at each retention time (X) 
Base Peak Intensity Chromatogram (BPI)
 | Sum of intensity (Y) of the most intense peaks at each retention time (X) For details and explanations concerning all the parameters and workflow of xcms_ package, see its manual_ and this example_ .. 
xcms: https://bioconductor.org/packages/release/bioc/html/xcms.html .. _manual: http://www.bioconductor.org/packages/release/bioc/manuals/xcms/man/xcms.pdf .. _example: https://bioconductor.org/packages/release/bioc/vignettes/xcms/inst/doc/xcms.html .. _news: https://bioconductor.org/packages/release/bioc/news/xcms/NEWS --------------------------------------------------- Changelog/News -------------- 
Version 3.12.0+galaxy* - 03/03/2020
 - UPGRADE: upgrade the xcms version from 3.6.1 to 3.12.0 (see XCMS news
) 
Galaxy Version 3.6.1+galaxy2 - 23/09/2020
 - BUGFIX: sample group colours did not match group labels. 
Version 3.6.1+galaxy* - 03/09/2019
 - UPGRADE: upgrade the xcms version from 3.4.4 to 3.6.1 (see XCMS news_) 
Version 3.4.4.0 - 08/02/2019
 - UPGRADE: upgrade the xcms version from 3.0.0 to 3.4.4 (see XCMS news_) 
Version 3.0.0.0 - 07/03/2018
 - NEW: This new tool will plot base peak intensity chromatogram (BPI) and total ion chromatogram (TIC) from xcms experience. It will replace those created by xcmsSet and retcor tools."
toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_summary/abims_xcms_summary/3.12.0+galaxy3	".. class:: infomark 
Authors
 Colin A. Smith csmith@scripps.edu, Ralf Tautenhahn rtautenh@gmail.com, Steffen Neumann sneumann@ipb-halle.de, Paul Benton hpaul.benton08@imperial.ac.uk and Christopher Conley cjconley@ucdavis.edu .. class:: infomark 
Galaxy integration
 ABiMS TEAM - SU/CNRS - Station biologique de Roscoff and Yann Guitton - LABERCA Part of Workflow4Metabolomics.org [W4M] | Contact support@workflow4metabolomics.org for any questions or concerns about the Galaxy implementation of this tool. --------------------------------------------------- ==================== xcms process history ==================== ----------- Description ----------- This tool provide a HTML summary which summarizes your analysis using the [W4M] XCMS and CAMERA tools ----------------- Workflow position ----------------- 
Upstream tools
 ========================= =========================== ================================ Name Output file Format ========================= =========================== ================================ xcms.findChromPeaks xset.RData rdata.xcms.findchrompeaks ------------------------- --------------------------- -------------------------------- xcms.groupChromPeaks 
*
.groupChromPeaks.RData rdata.xcms.group ------------------------- --------------------------- -------------------------------- xcms.adjustRtime 
*
.adjustRtime.RData rdata.xcms.retcor ------------------------- --------------------------- -------------------------------- xcms.fillChromPeaks 
*
.fillChromPeaks.RData rdata.xcms.fillpeaks ------------------------- --------------------------- -------------------------------- CAMERA.annotate 
*
.annotate.
.RData rdata.camera.
*
 ========================= =========================== ================================ .. image:: xcms_summary_workflow.png For details and explanations concerning all the parameters and workflow of xcms_ package, see its manual_ and this example_ .. _xcms: https://bioconductor.org/packages/release/bioc/html/xcms.html .. _manual: http://www.bioconductor.org/packages/release/bioc/manuals/xcms/man/xcms.pdf .. _example: https://bioconductor.org/packages/release/bioc/vignettes/xcms/inst/doc/xcms.html .. _news: https://bioconductor.org/packages/release/bioc/news/xcms/NEWS --------------------------------------------------- Changelog/News -------------- 
Version 3.12.0+galaxy
 - 03/03/2020
 - UPGRADE: upgrade the xcms version from 3.6.1 to 3.12.0 (see XCMS news_) 
Version 3.6.1+galaxy2 - 23/09/2020
 - BUGFIX: ""Error: object 'sampleNamesList' not found"" in case of .RData input from CAMERA 
Version 3.6.1+galaxy1 - 05/04/2020
 - UPGRADE: upgrade the CAMERA version from 1.38.0 to 1.42.0 - BUGFIX: ""Error in chromPeaks(from)[, ""is_filled""] : subscript out of bounds"" 
Version 3.6.1+galaxy
 - 03/09/2019
 - UPGRADE: upgrade the xcms version from 3.4.4 to 3.6.1 (see XCMS news_) 
Version 3.4.4.0 - 08/02/2019
 - UPGRADE: upgrade the xcms version from 3.0.0 to 3.4.4 (see XCMS news_) 
Version 3.0.0.0 - 14/02/2018
 - UPGRADE: upgrade the xcms version from 1.46.0 to 3.0.0. So refactoring of a lot of underlying codes and methods - IMPROVEMENT: the tool now shows all the parameters and not only those which were set. 
Version 1.0.4 - 13/02/2018
 - UPGRADE: upgrate the CAMERA version from 1.26.0 to 1.32.0 
Version 1.0.3 - 03/02/2017
 - IMPROVEMENT: xcms.summary can deal with merged individual data 
Version 1.0.2 - 06/07/2016
 - UPGRADE: upgrate the xcms version from 1.44.0 to 1.46.0 
Version 1.0.1 - 04/04/2016
 - TEST: refactoring to pass planemo test using conda dependencies 
Version 1.0.0 - 10/02/2016
* - NEW: Create a summary of XCMS analysis"
toolshed.g2.bx.psu.edu/repos/iuc/biom_add_metadata/biom_add_metadata/2.1.15+galaxy1	"Usage: biom add-metadata [OPTIONS] Add metadata to a BIOM table. Add sample and/or observation metadata to BIOM-formatted files. See examples here: http://biom-format.org/documentation/adding_metadata.html Example usage: Add sample metadata to a BIOM table: $ biom add-metadata -i otu_table.biom -o table_with_sample_metadata.biom -m sample_metadata.txt Options: -i, --input-fp PATH The input BIOM table [required] -o, --output-fp PATH The output BIOM table [required] -m, --sample-metadata-fp PATH The sample metadata mapping file (will add sample metadata to the input BIOM table, if provided). --observation-metadata-fp PATH The observation metadata mapping file (will add observation metadata to the input BIOM table, if provided). --sc-separated TEXT Comma-separated list of the metadata fields to split on semicolons. This is useful for hierarchical data such as taxonomy or functional categories. --sc-pipe-separated TEXT Comma-separated list of the metadata fields to split on semicolons and pipes (""|""). This is useful for hierarchical data such as functional categories with one-to-many mappings (e.g. x;y;z|x;y;w)). --int-fields TEXT Comma-separated list of the metadata fields to cast to integers. This is useful for integer data such as ""DaysSinceStart"". --float-fields TEXT Comma-separated list of the metadata fields to cast to floating point numbers. This is useful for real number data such as ""pH"". --sample-header TEXT Comma-separated list of the sample metadata field names. This is useful if a header line is not provided with the metadata, if you want to rename the fields, or if you want to include only the first n fields where n is the number of entries provided here. --observation-header TEXT Comma-separated list of the observation metadata field names. This is useful if a header line is not provided with the metadata, if you want to rename the fields, or if you want to include only the first n fields where n is the number of entries provided here. --output-as-json Write the output file in JSON format. --help Show this message and exit."
toolshed.g2.bx.psu.edu/repos/iuc/scikit_bio_diversity_beta_diversity/scikit_bio_diversity_beta_diversity/0.4.2.0	Calculates beta diversity using the selected metric.
toolshed.g2.bx.psu.edu/repos/iuc/binette/binette/1.2.1+galaxy0	".. class:: infomark 
What does Binette
 Binette is a fast and accurate binning refinement tool to constructs high quality MAGs from the output of multiple binning tools. 
Inputs
 - At least 2 different contig tables. .. class:: infomark The contig tables can be generate by the tool 
Converts genome bins in fasta format
. This tool only need the bins which where created by any binner as input. - The contig file .. class:: infomark This file should contain all reads used to create the bins. The format of this file should be either fasta or fasta.gz. - A CheckM2 diamond database .. class::infomark This database can be download with using the CheckM2 package and the followed command: 
checkm2 database --download --path <checkm2/database/>
 or it is possible to use a database cached on Galaxy. - An optional (fasta/fasta.gz) file with predicted genes .. class:: infomark This file, in a fasta format, is generate with the tool 
Prodigal
 Example: :: >Chlamydia_trachomatis_part1_1 # 1 # 1776 # 1 # ID=1_1;partial=10;start_type=Edge;rbs_motif=None;rbs_spacer=None;gc_cont=0.466 MSIRGVGGNGNSRIPSHNGDGSNRRSQNTKGNNKVEDRVCSLYSSRSNENRESPYAVVDV SSMIESTPTSGETTRASRGVFSRFQRGLVRIADKVRRAVQCAWSSVSTSRSSATRAAESG SSSRTARGASSGYREYSPSAARGLRLMFTDFWRTRVLRQTSPMAGVFGNLDVNEARLMAA YTSECADHLEAKELAGPDGVAAAREIAKRWEKRVRDLQDKGAARKLLNDPLGRRTPNYQS KNPGEYTVGNSMFYDGPQVANLQNVDTGFWLDMSNLSDVVLSREIQTGLRARATLEESMP MLENLEERFRRLQETCDAARTEIEESGWTRESASRMEGDEAQGPSRAQQAFQSFVNECNS IEFSFGSFGEHVRVLCARVSRGLAAAGEAIRRCFSCCKGSTHRYAPRDDLSPEGASLAET LARFADDMGIERGADGTYDIPLVDDWRRGVPSIEGEGSDSIYEIMMPIYEVMNMDLETRR SFAVQQGHYQDPRASDYDLPRASDYDLPRSPYPTPPLPPRYQLQNMDVEAGFREAVYASF VAGMYNYVVTQPQERIPNSQQVEGILRDMLTNGSQTFRDLMKRWNREVDRE
 
Outputs
* - A collection (list) with all the selected bins in fasta format. - A final quality report file containing quality information about the final selected bins. - A collection (list) storing quality reports for the input bin sets, with files following the same structure as the final quality report file."
toolshed.g2.bx.psu.edu/repos/iuc/biobox_add_taxid/biobox_add_taxid/1.2+galaxy0	"USAGE OF THIS TOOL
 This tool can be used to add the TaxID column to a biobox file. 
INPUTS
 - A biobox file. This file can be generated with the CAMI AMBER utility Tool named 'convert to biobox' - Either a Contig2TaxID file which is generated from Kraken2 (the classification file from Kraken2) or a BinID2TaxID which has to be done manualy Example for a BinID2TaxID file :: #BinID TaxID test1 11056 test2 444944 ABC 888 
OUTPUT
 - A CAMI AMBER biobox file with a taxid column"
toolshed.g2.bx.psu.edu/repos/iuc/bracken/est_abundance/3.1+galaxy0	Bracken relies on Bayesian probabilities that derive from the knowledge about the Kraken classification of each read-length kmer from all genomes within the provided Kraken database. It takes the tabular report output of kraken/kraken2 representing abundance of all detected taxa and provides as output a table representing the re-estimated abundances of different taxa at the taxonomy level pre-determined by the user. For more information about the operation behind the scene, visit http://ccb.jhu.edu/software/bracken/index.shtml?t=manual. Prior to abundance estimation with bracken, we must divide each genome in the Kraken database into read-length kmers, classify each of those kmers and store as a data structure. This indexing step has already been performed for you and it suffices to select the name of the correct kraken DB that you used for read classification. Bracken output file format (tabular): * Taxon name * Taxonomy ID * Level ID (S=Species, G=Genus, O=Order, F=Family, P=Phylum, K=Kingdom) * Kraken assigned reads * Added reads with abundance re-estimation * Total reads after abundance re-estimation * Fraction of total reads
toolshed.g2.bx.psu.edu/repos/iuc/cami_amber/cami_amber/2.0.7+galaxy0	".. class:: infomark 
What is AMBER
 AMBER is an evaluation package for the comparative assessment of genome reconstructions and taxonomic assignments from metagenome benchmark datasets. .. class:: infomark 
What it does
 AMBER calculate multiple metrics per bin and multiple metrics per sample. Each of them are provided then in results rankings, and comparative visualizations for assessing multiple programs or parameter effects. For more information please visit 
AMBER in GitHub &lt;https://github.com/CAMI-challenge/AMBER&gt;
. 
Input
 AMBER use only 2 required inputs: - The golden standard file (biobox format) This file can be created via the add_length tool .. class:: infomark Example(tab separated) :: @Version:0.9.1 @SampleID:CAMI_low @@SEQUENCEID BINID _LENGTH RL|S1|C10817 Sample18_57 20518 RL|S1|C11497 Sample22_57 37672 RL|S1|C6571 evo_1286_AP.033 69914 RL|S1|C10560 evo_1286_AP.033 995657 RL|S1|C13546 evo_1286_AP.033 626775 Note: This file looks similar to the binning files but the only different which is also is important is the length column. - Multiple binning files (biobox format) Files can be created via the convert_to_biobox tool .. class:: infomark Example(tab separated): :: #CAMI Format for Binning @Version:0.9.0 @SampleID:CAMI_low @@SEQUENCEID BINID RL|S1|C10 Bin_034 RL|S1|C100 Bin_023 RL|S1|C1000 Bin_034 RL|S1|C10000 Bin_019 RL|S1|C10002 Bin_035 RL|S1|C10004 Bin_035 RL|S1|C10008 Bin_034 RL|S1|C10011 Bin_035 RL|S1|C10012 Bin_013 RL|S1|C10014 Bin_035 There are also additional inputs which can be used: - A genome list which should be removed(tabular format) .. class:: infomark Example(tab separated): :: evo_1035930.029 common strain 1035930 common strain evo_1035930.032 common strain evo_1035930.011 common strain evo_1286_AP.033 common strain 1286_AP common strain evo_1286_AP.026 common strain evo_1286_AP.037 common strain evo_1286_AP.008 common strain 1052944 common strain 1053058 common strain 1052947 common strain evo_1049056.013 common strain evo_1049056.031 common strain evo_1049056.011 common strain 1049056 common strain evo_1049056.039 common strain Note: The first column contain the BINID and the second contain any kind of string. IMPORTANT: The argument where to state a keyword has to match to the anything in the second column to filter these kind of genomes out. If there is no keyword stated it can happen if the remove list contain all genomes which should be used then AMBER will fail since there are no genomes left to use! - Genome coverage file (tabular format) .. class:: infomark Example(tab separated): :: @SampleID:gsa_pooled @@GENOMEID COVERAGE 4378740.0 82.85111527272727 4378740.1 27.159305090909097 denovo10559.0 2.1596957142857143 179927.0 1.6946866666666667 denovo8373.1 2.07144 136604.0 9.150489565217391 denovo8373.0 1.1413460000000002 269378.0 8.051563333333332 190114.0 18.253119629629627 228140.0 3.078681818181818 135956.0 121.52672015625001 259846.0 12.298210588235296 162576.0 9.57867191489362 184966.0 14.461031521739134 There is also an option to include the NCBI database in AMBER. This can be used when including the link which is stated in the data manager: 
data_manager_fetch_ncbi_taxonomy
. This data manager download the current version of the database and store all files for you. If there are questions about data manager maybe have a look at this 
Tutorial &lt;https://usegalaxy.eu/training-material/topics/admin/tutorials/reference-genomes/tutorial.html&gt;
. 
Output
 AMBER will output 3 tsv files where each metrics value is stated. The important output is the HTML file where all data are included and also can be visualized with certain plots! 
Additional information
 The package 
Bokeh
 will create warnings which can be ignored since the only tell you that certain functions in the code are swapped with the new functions. AMBER was tested with the stated version of 
Bokeh
 and with the newest version of it and both generate the same output!"
toolshed.g2.bx.psu.edu/repos/iuc/cami_amber_add/cami_amber_add/2.0.7+galaxy0	".. class:: infomark 
What it does
 This tool can create the gold standard file which has to be used for the CAMI AMBER. 
Input
 This tool required 2 inputs: - A binning file (biobox format) This file can be created via the convert_to_biobox tool .. class:: infomark Example(tab separated): :: #CAMI Format for Binning @Version:0.9.0 @SampleID:CAMI_low @@SEQUENCEID BINID RL|S1|C10 Bin_034 RL|S1|C100 Bin_023 RL|S1|C1000 Bin_034 RL|S1|C10000 Bin_019 RL|S1|C10002 Bin_035 RL|S1|C10004 Bin_035 RL|S1|C10008 Bin_034 RL|S1|C10011 Bin_035 RL|S1|C10012 Bin_013 RL|S1|C10014 Bin_035 - A fasta/fastq file (fasta,fasta.gz,fastq,fastq.gz format) This file need the identical sequences which are in the gold standard file to create the finish gold standard file for CAMI AMBER 
Output
 As output the tool give you out a file in biobox format with the added column 'Length'. This file then can be used for AMBER as example!"
toolshed.g2.bx.psu.edu/repos/iuc/cami_amber_convert/cami_amber_convert/2.0.7+galaxy0	".. class:: infomark 
What it does
 This tool can create a tsv file in biobox format or a collection of multiple files. 
Input
 This tool required only fasta files. You can submit n files manually or a collection of fasta files. 
Output
 This tool either give out 1 binning file in biobox format or for each inputted file 1 binning file in biobox format."
toolshed.g2.bx.psu.edu/repos/iuc/cat_add_names/cat_add_names/5.2.3+galaxy0	"CAT/BAT add_names
 Add taxonomic names to CAT or BAT output files. Names can be added to ORF2LCA.txt, contig2classification.txt, and bin2classification.txt outputs. The Contig Annotation Tool (CAT) and Bin Annotation Tool (BAT) workflows are described at: https://github.com/dutilh/CAT - CAT contigs/CAT bins - runs Prodigal_ prokaryotic protein prediction on the fasta input. - CAT contigs/CAT bins - runs Diamond_ to align predicted proteins to the reference proteins in the CAT database. - CAT contigs/CAT bins - assigns taxonomic classification to fasta entries and ORFs based on alignments. - CAT add_names - annotates outputs with taxonomic names. - CAT summerise - reports number of assignments to each taxonomic name. A CAT database can either be installed by data_manager_cat or in the local history by CAT prepare tool. .. _Prodigal: https://github.com/hyattpd/Prodigal .. _Diamond: https://github.com/bbuchfink/diamond 
INPUT
 Example: contig2classification.txt from CAT conitgs :: # contig classification reason lineage lineage scores contig_44250 classified based on 1/2 ORFs 1;131567;2;1224;1236;135623;641;662;666 1.00;1.00;1.00;1.00;1.00;1.00;1.00;1.00;1.00 contig_9952 classified based on 1/5 ORFs 1;131567;2;1783272;1239;91061
 1.00;1.00;1.00;1.00;1.00;1.00 
OUTPUT
 :: # contig classification reason lineage lineage scores superkingdom phylum class order family genus species contig_44250 classified based on 1/2 ORFs 1;131567;2;1224;1236;135623;641;662;666 1.00;1.00;1.00;1.00;1.00;1.00;1.00;1.00;1.00 Bacteria: 1.00 Proteobacteria: 1.00 Gammaproteobacteria: 1.00 Vibrionales: 1.00 Vibrionaceae: 1.00 Vibrio: 1.00 Vibrio cholerae: 1.00 contig_9952 classified based on 1/5 ORFs 1;131567;2;1783272;1239;91061
 1.00;1.00;1.00;1.00;1.00;1.00 Bacteria: 1.00 Firmicutes: 1.00 Bacilli*: 1.00 not classified not classified not classified not classified Required arguments: -i, --input_file Path to input file. Can be either classification output file or ORF2LCA output file. -t, --taxonomy_folder Path to folder that contains taxonomy files. Optional arguments: --only_official Only output official level names. --exclude_scores Do not include bit-score support scores in the lineage."
toolshed.g2.bx.psu.edu/repos/iuc/cat_bins/cat_bins/5.2.3+galaxy0	predicted_proteins.faa and alignment.diamond from previous CAT run.
toolshed.g2.bx.psu.edu/repos/iuc/cat_contigs/cat_contigs/5.2.3+galaxy0	predicted_proteins.faa and alignment.diamond from previous CAT run.
toolshed.g2.bx.psu.edu/repos/iuc/cat_prepare/cat_prepare/5.2.3+galaxy0	"CAT prepare
 Prepare CAT reference data for classifying metagomic contigs or genome assemblies. 
NOTE:
 This requires over a 100GB of RAM, 250GB of disk space, and up to 24 hours. The Contig Annotation Tool (CAT) and Bin Annotation Tool (BAT) workflows are described at: https://github.com/dutilh/CAT - CAT contigs/CAT bins - runs Prodigal_ prokaryotic protein prediction on the fasta input. - CAT contigs/CAT bins - runs Diamond_ to align predicted proteins to the reference proteins in the CAT database. - CAT contigs/CAT bins - assigns taxonomic classification to fasta entries and ORFs based on alignments. - CAT add_names - annotates outputs with taxonomic names. - CAT summerise - reports number of assignments to each taxonomic name. A CAT database can either be installed by data_manager_cat or in the local history by CAT prepare tool. .. _Prodigal: https://github.com/hyattpd/Prodigal .. _Diamond: https://github.com/bbuchfink/diamond"
toolshed.g2.bx.psu.edu/repos/iuc/cat_summarise/cat_summarise/5.2.3+galaxy0	"CAT summarise
 Summarise taxonomic asignemts from a CAT or BAT classification file that has official taxonomic names added by CAT add_names. 
INPUT
 :: # bin classification reason lineage lineage scores superkingdom phylum class order family genus species genome2.fna classified based on 6/17 ORFs 1;131567;2;1224;1236;135623;641;662;666 1.00;1.00;1.00;0.89;0.89;0.89;0.89;0.89;0.69 Bacteria: 1.00 Proteobacteria: 0.89 Gammaproteobacteria: 0.89 Vibrionales: 0.89 Vibrionaceae: 0.89 Vibrio: 0.89 Vibrio cholerae: 0.69 genome3.fna classified based on 4/9 ORFs 1;131567;2;1783272;1239;91061;1385;186820;1637;1639 1.00;1.00;1.00;0.76;0.76;0.76;0.76;0.76;0.76;0.76 Bacteria: 1.00 Firmicutes: 0.76 Bacilli: 0.76 Bacillales: 0.76 Listeriaceae: 0.76 Listeria: 0.76 Listeria monocytogenes: 0.76 
OUTPUT
 :: # total number of bins is 2, of which 2 (100.00%) are classified. # # rank clade number of bins superkingdom Bacteria 2 phylum Proteobacteria 1 phylum Firmicutes 1 class Gammaproteobacteria 1 class Bacilli 1 order Vibrionales 1 order Bacillales 1 family Vibrionaceae 1 family Listeriaceae 1 genus Vibrio 1 genus Listeria 1 species Vibrio cholerae 1 species Listeria monocytogenes 1 Required arguments: -i, --input_file Path to named CAT contig classification file or BAT bin classification file. Currently only official ranks are supported, and only classification files containing a single classification per contig / bin. Optional arguments: -c, --contigs_fasta Path to contigs fasta file. This is required if you want to summarise a contig classification file. The Contig Annotation Tool (CAT) and Bin Annotation Tool (BAT) workflows are described at: https://github.com/dutilh/CAT - CAT contigs/CAT bins - runs Prodigal_ prokaryotic protein prediction on the fasta input. - CAT contigs/CAT bins - runs Diamond_ to align predicted proteins to the reference proteins in the CAT database. - CAT contigs/CAT bins - assigns taxonomic classification to fasta entries and ORFs based on alignments. - CAT add_names - annotates outputs with taxonomic names. - CAT summerise - reports number of assignments to each taxonomic name. A CAT database can either be installed by data_manager_cat or in the local history by CAT prepare tool. .. _Prodigal: https://github.com/hyattpd/Prodigal .. _Diamond: https://github.com/bbuchfink/diamond"
toolshed.g2.bx.psu.edu/repos/iuc/concoct/concoct/1.1.0+galaxy2	"What it does
 CONCOCT (Clustering cONtigs with COverage and ComposiTion) performs unsupervised binning of metagenomic contigs by using nucleotide composition - kmer frequencies - and coverage data for multiple samples. CONCOCT can accurately (up to species level) bin metagenomic contigs. The tool accepts 2 inputs; a tabular file where each row corresponds to a contig and each column corresponds to a sample (the values are the average coverage for this contig in that sample) and a file containing sequences in fasta format. Three outputs are produced; clustering of the > 1000 kmer count, the PCA transformed matrix and the PCA components. The intended use of the CONCOCT tools is shown in the following image. .. image:: pipeline.png More information may be found on the CONCOCT homepage:: https://github.com/BinPro/CONCOCT"
toolshed.g2.bx.psu.edu/repos/iuc/concoct_cut_up_fasta/concoct_cut_up_fasta/1.1.0+galaxy2	"What it does
 Accepts a fasta file containing contigs, cuts them into non-overlapping or overlapping parts of equal length, and produces a fasta file containing the cut contigs. An optional output BED file can be produced, where the cut contigs are specified in terms of the original contigs. Using this file as input to a BED coverage tool (e.g., bedtools Compute both the length and depth of coverage) will produce a file that can be used as input to the CONCOCT Create coverage table tool. The intended use of the CONCOCT tools is shown in the following image. .. image:: pipeline.png More information may be found on the CONCOCT homepage:: https://github.com/BinPro/CONCOCT"
toolshed.g2.bx.psu.edu/repos/iuc/concoct_extract_fasta_bins/concoct_extract_fasta_bins/1.1.0+galaxy2	"What it does
 Performs metagenomic binning of fasta contigs by extracting a fasta file for each cluster defined in a CONCOCT clustering file. The tool accepts two inputs; the fasta contigs file and the CONCOCT clustering file that was produced using the same fasta contigs input. A collection of fasta files is produced. The intended use of the CONCOCT tools is shown in the following image. .. image:: pipeline.png More information may be found on the CONCOCT homepage:: https://github.com/BinPro/CONCOCT"
toolshed.g2.bx.psu.edu/repos/iuc/concoct_coverage_table/concoct_coverage_table/1.1.0+galaxy2	"What it does
 Accepts an assembled (and possibly cut by the Cut fasta contigs tool) fasta contigs file and a tabular coverage histogram file (produced by the bedtools Genomve Coverage tool) and outputs a tabular coverage file for use as the input to the CONCOCT metagenome binning tool. The intended use of the CONCOCT tools is shown in the following image. .. image:: pipeline.png More information may be found on the CONCOCT homepage:: https://github.com/BinPro/CONCOCT"
toolshed.g2.bx.psu.edu/repos/iuc/concoct_merge_cut_up_clustering/concoct_merge_cut_up_clustering/1.1.0+galaxy2	"What it does
 This tool merges the clustering created by concoct by looking at cluster assignments per contig part and assigning a concensus cluster for the original contig. The output contains a header line and contig_id and cluster_id per line, separated by a comma. The intended use of the CONCOCT tools is shown in the following image. .. image:: pipeline.png More information may be found on the CONCOCT homepage:: https://github.com/BinPro/CONCOCT"
toolshed.g2.bx.psu.edu/repos/iuc/metabat2_jgi_summarize_bam_contig_depths/metabat2_jgi_summarize_bam_contig_depths/2.17+galaxy0	"What it does
 Calculates coverage depth for each sequence in one or more selected BAM files, producing a tabular file (for each input) having mean and variance of base coverage depth that can be used as one of the inputs for the MetaBAT2 metagenome binning tool. The algorithm used for calculating the coverage depth is adjusted by a few factors to improve the fidelity of the metrics when correlating abundance coverage in the binning stage. By default the following adjustments are applied. 
Edge bases are ignored
 Edge bases are not counted as coverage, by the lesser of 1 AverageReadLength or (--maxEdgeBases=75). This is because most mappers can not reliably place a read that would extend off the edge of a sequence, and coverage depth tends to drop towards 0 at the edge of a contig or scaffold. Use --includeEdgeBases to include the coverage in this region. 
Reads with high mapping errors are skipped
 Reads that map imperfectly are excluded when the %ID of the mapping drops below a threshold (--percentIdentity=97). MetaBAT2 is designed to resolve strain variation and mapping reads with low %ID indicate that the read actually came from a different strain/species. %ID is calculated from the CIGAR string and/or NM/MD fields and == 100 * MatchedBases / (MatchedBases + Substituions + Insertions + Deletions). This ensures that clips, insertions, deletions and mismatches are excluded from the coverage count. Only the read bases that exactly match the reference are counted as coverage. This generally has a small effect, except in the case of long reads from PacBio and Nanopore. 
More information
 https://bitbucket.org/berkeleylab/metabat/src/master/ 
Options
 * 
Select a reference genome
 - optionally select the reference genome that was used to map the input bam file(s) and 3 additional outputs will be produced; gc coverage histogram, read statistics and perfect kmer counts."
toolshed.g2.bx.psu.edu/repos/iuc/checkm_analyze/checkm_analyze/1.2.4+galaxy2	What it does ============ CheckM provides a set of tools for assessing the quality of genomes recovered from isolates, single cells, or metagenomes. It provides robust estimates of genome completeness and contamination by using collocated sets of genes that are ubiquitous and single-copy within a phylogenetic lineage. Assessment of genome quality can also be examined using plots depicting key genomic characteristics (e.g., GC, coding density) which highlight sequences outside the expected distributions of a typical genome. CheckM also provides tools for identifying genome bins that are likely candidates for merging based on marker set compatibility, similarity in genomic characteristics, and proximity within a reference genome tree. This command identifies marker genes in bins and calculates genome statistics
toolshed.g2.bx.psu.edu/repos/iuc/checkm_lineage_set/checkm_lineage_set/1.2.4+galaxy2	What it does ============ CheckM provides a set of tools for assessing the quality of genomes recovered from isolates, single cells, or metagenomes. It provides robust estimates of genome completeness and contamination by using collocated sets of genes that are ubiquitous and single-copy within a phylogenetic lineage. Assessment of genome quality can also be examined using plots depicting key genomic characteristics (e.g., GC, coding density) which highlight sequences outside the expected distributions of a typical genome. CheckM also provides tools for identifying genome bins that are likely candidates for merging based on marker set compatibility, similarity in genomic characteristics, and proximity within a reference genome tree. This command places bins in the genome tree.
toolshed.g2.bx.psu.edu/repos/iuc/checkm_lineage_wf/checkm_lineage_wf/1.2.4+galaxy2	What it does ============ CheckM provides a set of tools for assessing the quality of genomes recovered from isolates, single cells, or metagenomes. It provides robust estimates of genome completeness and contamination by using collocated sets of genes that are ubiquitous and single-copy within a phylogenetic lineage. Assessment of genome quality can also be examined using plots depicting key genomic characteristics (e.g., GC, coding density) which highlight sequences outside the expected distributions of a typical genome. CheckM also provides tools for identifying genome bins that are likely candidates for merging based on marker set compatibility, similarity in genomic characteristics, and proximity within a reference genome tree. This command runs the recommended workflow for assessing the completeness and contamination of genome bins is to use lineage-specific marker sets. This workflow consists of 4 mandatory (M) steps and 1 recommended (R) step: - (M) The tree command places genome bins into a reference genome tree - (R) The tree_qa command indicates the number of phylogenetically informative marker genes found in each genome bin along with a taxonomic string indicating its approximate placement in the tree. If desired, genome bins with few phylogenetically marker genes may be removed in order to reduce the computational requirements of the following commands. Alternatively, if only genomes from a particular taxonomic group are of interest these can be moved to a new directory and analyzed separately. - (M) The lineage_set command creates a marker file indicating lineage-specific marker sets suitable for evaluating each genome. - (M) The analyze command identifies marker genes and estimates the completeness and contamination of each genome bin. - (M) The qa command can be used to produce different tables summarizing the quality of each genome bin.
toolshed.g2.bx.psu.edu/repos/iuc/checkm_plot/checkm_plot/1.2.4+galaxy2	What it does ============ CheckM provides a set of tools for assessing the quality of genomes recovered from isolates, single cells, or metagenomes. It provides robust estimates of genome completeness and contamination by using collocated sets of genes that are ubiquitous and single-copy within a phylogenetic lineage. Assessment of genome quality can also be examined using plots depicting key genomic characteristics (e.g., GC, coding density) which highlight sequences outside the expected distributions of a typical genome. CheckM also provides tools for identifying genome bins that are likely candidates for merging based on marker set compatibility, similarity in genomic characteristics, and proximity within a reference genome tree. This command produces a number of plots for assessing the quality of genome bins. Here we describe each of these plots and provide an example. - gc_plot: Provides a 3 pane plot suitable for assessing the GC distribution of sequences within a genome bin. The first pane is a histogram of the number of non-overlapping 5 kbp windows with a give percent GC. A typical genome will produce a unimodal distribution. The second pane plots each sequence in the genome bin as a function of its deviation from the average GC of the entire genome (x-axis) and sequence length (y-axis). The dashed red lines indicate the expected deviation from the mean GC as a function of length. This expected deviation is pre-calculated from a set of trusted reference genomes and the percentile plotted is provided as an argument to this command. A good default value to use for this distribution parameter is 95. - coding_plot: Provides a plot analogous to the gc_plot suitable for assessing the coding density of sequences within a genome bin. - tetra_plot: Provides a plot analogous to the gc_plot suitable for assessing the tetranucleotide signatures of sequences within a genome bin. The Manhattan distance is used for determine the different between each sequence's tetranucleotide signature and the tetranucleotide signature of the entire genome bin. This plot requires a file indicating the tetranucleotide signature of all sequences within the genome bins. This file can be creates with the tetra command. - dist_plot: Produces a single figure combining the plots produced by gc_plot, coding_plot, and tetra_plot. This plot requires a file indicating the tetranucleotide signature of all sequences within the genome bins. This file can be creates with the tetra command. - nx_plot: Produces a plot indicating the Nx value of a genome bin for all values of x. This provides a more comprehensive view of the quality of an assembly than simply considering N50. - len_hist: Produce a histogram of the number of sequences within a genome bin at different sequence length intervals. This provides additional information regarding the quality of an assembled genome. - marker_plot: Plots the position of marker genes on sequences within a genome bin. This provides information regarding the extent to which marker genes are collocated. The number of marker genes within a fixed size window (2.8 kbps in this example) is indicated by with different colours. Sequences without any marker genes are not shown. - gc_bias_plot:
toolshed.g2.bx.psu.edu/repos/iuc/checkm_qa/checkm_qa/1.2.4+galaxy2	What it does ============ CheckM provides a set of tools for assessing the quality of genomes recovered from isolates, single cells, or metagenomes. It provides robust estimates of genome completeness and contamination by using collocated sets of genes that are ubiquitous and single-copy within a phylogenetic lineage. Assessment of genome quality can also be examined using plots depicting key genomic characteristics (e.g., GC, coding density) which highlight sequences outside the expected distributions of a typical genome. CheckM also provides tools for identifying genome bins that are likely candidates for merging based on marker set compatibility, similarity in genomic characteristics, and proximity within a reference genome tree. This command identifies marker genes in bins and calculates genome statistics Adjacent called genes matching the same marker gene may indicate a true duplication event, a gene calling error, or an assembly error. If adjacent genes hit distinct regions of the same marker gene HMM, CheckM assumes a gene calling error has occurred and concatenate the two genes. When this occurs, CheckM concatenates the gene ids of the two genes with a pair of ampersands (&&). Outputs ======= Output in function of selection output format 1. Summary of bin completeness, contamination, and strain heterogeneity Bin Id: bin identifier derived from input FASTA file Marker lineage: indicates lineage used for inferring marker set (a precise indication of where a bin was placed in CheckM's reference tree can be obtained with the tree_qa command) No. genomes: number of reference genomes used to infer marker set No. markers: number of inferred marker genes No. marker sets: number of inferred co-located marker sets 0-5+: number of times each marker gene is identified Completeness: estimated completeness Contamination: estimated contamination Strain heterogeneity: estimated strain heterogeneity 2. Extended summary of bin quality (includes GC, genome size, coding density, ...) 3. Summary of bin quality for increasingly basal lineage-specific marker sets Node Id: unique id of internal node in genome tree from which lineage-specific markers were inferred 4. ist of marker genes for each bin along with the number of times each marker was identified Node Id: unique id of internal node in genome tree from which lineage-specific markers were inferred Marker lineage: indicates lineage used for inferring marker set Useful for identifying lineage-specific gene loss or duplication 5. List of bin id, marker gene id, and called gene id for each identified marker gene 6. List of marker genes present multiple times in a bin 7. List of marker genes present multiple times on the same scaffold Useful for identifying true gene duplication events, gene calling errors, or assembly errors. See note below. 8. List indicating the position of each marker genes within a bin 9. Marker genes identified in each bin and their sequence
toolshed.g2.bx.psu.edu/repos/iuc/checkm_taxon_set/checkm_taxon_set/1.2.4+galaxy2	What it does ============ CheckM provides a set of tools for assessing the quality of genomes recovered from isolates, single cells, or metagenomes. It provides robust estimates of genome completeness and contamination by using collocated sets of genes that are ubiquitous and single-copy within a phylogenetic lineage. Assessment of genome quality can also be examined using plots depicting key genomic characteristics (e.g., GC, coding density) which highlight sequences outside the expected distributions of a typical genome. CheckM also provides tools for identifying genome bins that are likely candidates for merging based on marker set compatibility, similarity in genomic characteristics, and proximity within a reference genome tree. This command generates taxonomic-specific marker set
toolshed.g2.bx.psu.edu/repos/iuc/checkm_taxonomy_wf/checkm_taxonomy_wf/1.2.4+galaxy2	What it does ============ CheckM provides a set of tools for assessing the quality of genomes recovered from isolates, single cells, or metagenomes. It provides robust estimates of genome completeness and contamination by using collocated sets of genes that are ubiquitous and single-copy within a phylogenetic lineage. Assessment of genome quality can also be examined using plots depicting key genomic characteristics (e.g., GC, coding density) which highlight sequences outside the expected distributions of a typical genome. CheckM also provides tools for identifying genome bins that are likely candidates for merging based on marker set compatibility, similarity in genomic characteristics, and proximity within a reference genome tree. This command runs recommended workflow to analyze all genome bins with the same marker set. A common example would be a set of genomes from the same taxonomic group. The workflow for using a taxonomic-specific marker set consists of 3 mandatory (M) steps and 1 recommended (R) step: - (R) The taxon_list command produces a table indicating all taxa for which a marker set can be produced. All support taxa at a given taxonomic rank can be produced by passing taxon_list the --rank flag - (M) The taxon_set command is used to produce marker sets for a specific taxon: - (M) The analyze command identifies marker genes within each genome bin and estimate completeness and contamination. All putative genomes to be analyzed must be provided. - (M) The qa command is used to produce different tables summarizing the quality of each genome bin. Inputs ====== Outputs =======
toolshed.g2.bx.psu.edu/repos/iuc/checkm_tetra/checkm_tetra/1.2.4+galaxy2	What it does ============ CheckM provides a set of tools for assessing the quality of genomes recovered from isolates, single cells, or metagenomes. It provides robust estimates of genome completeness and contamination by using collocated sets of genes that are ubiquitous and single-copy within a phylogenetic lineage. Assessment of genome quality can also be examined using plots depicting key genomic characteristics (e.g., GC, coding density) which highlight sequences outside the expected distributions of a typical genome. CheckM also provides tools for identifying genome bins that are likely candidates for merging based on marker set compatibility, similarity in genomic characteristics, and proximity within a reference genome tree. This command generates taxonomic-specific marker set Inputs ====== Outputs =======
toolshed.g2.bx.psu.edu/repos/iuc/checkm_tree/checkm_tree/1.2.4+galaxy2	What it does ============ CheckM provides a set of tools for assessing the quality of genomes recovered from isolates, single cells, or metagenomes. It provides robust estimates of genome completeness and contamination by using collocated sets of genes that are ubiquitous and single-copy within a phylogenetic lineage. Assessment of genome quality can also be examined using plots depicting key genomic characteristics (e.g., GC, coding density) which highlight sequences outside the expected distributions of a typical genome. CheckM also provides tools for identifying genome bins that are likely candidates for merging based on marker set compatibility, similarity in genomic characteristics, and proximity within a reference genome tree. This command places bins in the genome tree. The following heuristic is used to establish the translation table used by Prodigal: use table 11 unless the coding density using table 4 is 5% higher than when using table 11 and the coding density under table 4 is >70%. Distinguishing between tables 4 and 25 is challenging so CheckM does not attempt to distinguish between these two tables. If you know the correct translation table for your genomes, it is recommended that you call genes outside of CheckM and provide CheckM with the protein sequences
toolshed.g2.bx.psu.edu/repos/iuc/checkm_tree_qa/checkm_tree_qa/1.2.4+galaxy2	What it does ============ CheckM provides a set of tools for assessing the quality of genomes recovered from isolates, single cells, or metagenomes. It provides robust estimates of genome completeness and contamination by using collocated sets of genes that are ubiquitous and single-copy within a phylogenetic lineage. Assessment of genome quality can also be examined using plots depicting key genomic characteristics (e.g., GC, coding density) which highlight sequences outside the expected distributions of a typical genome. CheckM also provides tools for identifying genome bins that are likely candidates for merging based on marker set compatibility, similarity in genomic characteristics, and proximity within a reference genome tree. This command assesses phylogenetic markers in the genome tree Inputs ====== Outputs ======= Output in function of selection output format 1. brief summary of genome tree placement indicating the number of unique phylogenetically informative markers found, the number of markers found multiple times, and a taxon string indicating the placement of each bin within the genome tree 2. detailed summary of genome tree placement giving a more detailed indication of where each bin is within the genome tree, general characteristics about each bin (e.g., GC, genome size, coding density), and general characteristics about all reference genomes descendant from the parental node of each bin (e.g., mean and standard deviation of GC) 3. genome tree in Newick format decorated with IMG genome ids which can be used to examine the phylogenetic neighbours of each bin 4. genome tree in Newick format decorated with taxonomy strings which can be used to examine the phylogenetic neighbours of each bin 5. multiple sequence alignment of reference genomes and bins which can be used to infer a de novo genome tree
toolshed.g2.bx.psu.edu/repos/ufz/checkv_end_to_end/checkv_end_to_end/1.0.3+galaxy0	".. class:: infomark 
What it does
 CheckV is a fully automated command-line pipeline for assessing the quality of single-contig viral genomes, including identification of host contamination for integrated proviruses, estimating completeness for genome fragments, and identification of closed genomes. There are 4 steps: 1. Remove host contamination on proviruses - Genes are first annotated as viral or microbial based on comparison to a custom database of HMMs - CheckV scans over the contig (5' to 3') comparing gene annotations and GC content between a pair of adjacent gene windows - This information is used to compute a score at each intergenic position and identify host-virus breakpoints - Works best for contigs that are mostly viral 2. Estimate genome completeness - Proteins are first compared to the CheckV genome database using AAI (average amino acid identity) - After identifying the top hits, completeness is computed as a ratio between the contig length (or viral region length for proviruses) and the length of matched reference - A confidence level is reported based on the strength of the alignment - Generally, high- and medium-confidence estimates are quite accurate - Less frequently, your viral genome may not have a close match to the CheckV database; in these cases CheckV estimates the completeness based on the viral HMMs identified on the contig - Based on the HMMs found, CheckV returns the estimated range for genome completeness (e.g. 35% to 60% completeness), which represents the 90% confidence interval based on the distribution of lengths of reference genomes with the same viral HMMs 3.: Predict closed genomes - Direct terminal repeats (DTRs) - Repeated sequence of >20-bp at start/end of contig - Most trusted signature in our experience - May indicate circular genome or linear genome replicated from a circular template (i.e. concatamer) - Proviruses - Viral region with predicted host boundaries at 5' and 3' ends (see panel A) - Note: CheckV will not detect proviruses if host regions have already been removed (e.g. using VIBRANT or VirSorter) - Inverted terminal repeats (ITRs) - Repeated sequence of >20-bp at start/end of contig (3' repeat is inverted) - Least trusted signature - For all the methods above, CheckV also checks whether the contig is approximately the correct sequence length based on estimated completeness; this is important because terminal repeats can represent artifacts of metagenomic assembly 4. Summarize quality. - Based on the results of 1-3, CheckV generates a report file and assigns query contigs to one of five quality tiers (consistent with and expand upon the MIUViG quality tiers): - Complete - High-quality (>90% completeness) - Medium-quality (50-90% completeness) - Low-quality (<50% completeness) - Undetermined quality Usage ..... 
Input
 - Viral contigs in fasta (or gz, bz2 compressed fasta). - CheckV reference data 
Output
 - Quality Summary: Tabular file showing integrated results from the three main modules and should be the main output referred to. - Complete genomes: Tabular overview of putative complete genomes identified. - Viruses: Virus sequences - Proviruses: Provirus sequences Optional outputs: - Completeness: detailed overview of how completeness was estimated - Contamination: detailed overview of how contamination was estimated"
toolshed.g2.bx.psu.edu/repos/bebatut/combine_metaphlan2_humann2/combine_metaphlan_humann/0.3.0	"What it does
 This tool combine MetaPhlAn outputs and HUMANnN outputs. For each gene families/pathways and the corresponding taxonomic stratification, you get relative abundance of this gene family/pathway and the relative abundance of corresponding species and genus."
toolshed.g2.bx.psu.edu/repos/bebatut/combine_metaphlan2_humann2/combine_metaphlan2_humann2/0.2.0	"What it does
 This tool combine MetaPhlAn2 outputs and HUMANnN2 outputs. For each gene families/pathways and the corresponding taxonomic stratification, you get relative abundance of this gene family/pathway and the relative abundance of corresponding species and genus."
toolshed.g2.bx.psu.edu/repos/iuc/biom_convert/biom_convert/2.1.15+galaxy1	"Usage: biom convert [OPTIONS]:: Convert to/from the BIOM table format. Convert between BIOM table formats. See examples here: http://biom-format.org/documentation/biom_conversion.html Example usage: Convert a ""classic"" BIOM file (tab-separated text) to an HDF5 BIOM formatted OTU table: $ biom convert -i table.txt -o table.biom --to-hdf5 Options: -i, --input-fp PATH The input BIOM table [required] -o, --output-fp PATH The output BIOM table [required] -m, --sample-metadata-fp PATH The sample metadata mapping file (will add sample metadata to the input BIOM table, if provided). --observation-metadata-fp PATH The observation metadata mapping file (will add observation metadata to the input BIOM table, if provided). --to-json Output as JSON-formatted table. --to-hdf5 Output as HDF5-formatted table. --to-tsv Output as TSV-formatted (classic) table. table is a BIOM table with collapsed samples, this will update the sample metadata of the table to the supported HDF5 collapsed format. --collapsed-observations If --to_hdf5 is passed and the original table is a BIOM table with collapsed observations, this will update the supported HDF5 collapsed format. --header-key TEXT The observation metadata to include from the input BIOM table file when creating a tsv table file. By default no observation metadata will be included. --output-metadata-id TEXT The name to be given to the observation metadata column when creating a tsv table file if the column should be renamed. --table-type OTU table,Pathway table,Function table,Ortholog table,Gene table,Metabolite table,Taxon table,Table The type of the table. --process-obs-metadata taxonomy,naive,sc_separated Process metadata associated with observations when converting from a classic table. --tsv-metadata-formatter naive,sc_separated Method for formatting the observation --help Show this message and exit."
toolshed.g2.bx.psu.edu/repos/devteam/kraken2tax/Kraken2Tax/1.2+galaxy0	".. class:: infomark Use 
Filter and Sort->Filter
 to restrict output of this tool to desired taxonomic ranks. You can also use 
Text Manipulation->Cut
 to remove unwanted columns from the output. ------ 
What it does
 This tool is designed to translate results of the Kraken metagenomic classifier (see citations below) to the full representation of NCBI taxonomy. It does so by using Taxonomic ID field provided by Kraken. The output of this tool can be directly visualized by the Krona tool. It is based on 
gb_taxonomy_tools
 developed by https://github.com/spond. ------- 
Example
 Suppose you have Kraken output that looks like this (here the second field is the name of a sequencing read and the third is the taxonomic ID):: C Read_1 9606 465 Q:1 and you want to obtain the full taxonomic representation for this read. Setting 
Read name
 and 
Taxonomy ID field
 parameters to 
2
 and 
3
, respectively, will produce the following output (you may need to scroll sideways to see the entire line):: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Read_1 9606 root Eukaryota Metazoa n n Chordata Craniata Gnathostomata Mammalia n Euarchontoglires Primates Haplorrhini Hominoidea Hominidae n n n Homo n Homo sapiens n In other words the tool printed 
Read name
, 
Taxonomy ID field
, and appended 22 columns containing taxonomic ranks from Superkingdom to Subspecies. Below is a formal definition of the output columns:: Column Definition ------- ------------------------------------------------- 1 Name (specified by 'Read name' dropdown) 2 taxID (specified by 'Taxonomy ID field' dropdown) 3 root 4 superkingdom 5 kingdom 6 subkingdom 7 superphylum 8 phylum 9 subphylum 10 superclass 11 class 12 subclass 13 superorder 14 order 15 suborder 16 superfamily 17 family 18 subfamily 19 tribe 20 subtribe 21 genus 22 subgenus 23 species 24 subspecies ------ .. class:: warningmark 
Why do I have these ""n"" things?
 Be aware that the NCBI taxonomy (ftp://ftp.ncbi.nih.gov/pub/taxonomy/) this tool relies upon is incomplete. This means that for many species one or more ranks are absent and represented as ""
n
"". In the above example 
subkingdom
, 
superphylum
 etc. are missing."
toolshed.g2.bx.psu.edu/repos/iuc/fasta_to_contig2bin/Fasta_to_Contig2Bin/1.1.7+galaxy1	What it does ============ DAS Tool is an automated method that integrates the results of a flexible number of binning algorithms to calculate an optimized, non-redundant set of bins from a single assembly.
toolshed.g2.bx.psu.edu/repos/iuc/coverm_contig/coverm_contig/0.7.0+galaxy0	".. class:: infomark 
Method for calculating coverage
 Calculation of genome-wise coverage (genome mode) is similar to calculating contig-wise (contig mode) coverage, except that the unit of reporting is per-genome rather than per-contig. For calculation methods which exclude base positions based on their coverage, all positions from all contigs are considered together. - Relative abundance: Percentage relative abundance of each genome, and the unmapped read percentage - Mean: Average number of aligned reads overlapping each position on the genome - Trimmed mean: Average number of aligned reads overlapping each position after removing the most deeply and shallowly covered positions. - Covered fraction: Fraction of genome covered by 1 or more reads - Covered bases: Number of bases covered by 1 or more reads - Variance: Variance of coverage depths - Length: Length of each genome in base pairs - Count: Number of reads aligned toq each genome. Note that a single read may be aligned to multiple genomes with supplementary alignments - Reads per base: Number of reads aligned divided by the length of the genome - MetaBAT: Reproduction of the 
MetaBAT &lt;https://bitbucket.org/berkeleylab/metabat&gt;
 tool output - Coverage histogram: Histogram of coverage depths - RPKM: Reads mapped per kilobase of genome, per million mapped reads - TPM: Transcripts Per Million as described in 
Li et al 2010 &lt;https://doi.org/10.1093/bioinformatics/btp692&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/coverm_genome/coverm_genome/0.7.0+galaxy0	".. class:: infomark 
Dereplication quality formula
 Scoring function for genome quality: - Parks2020_reduced: A quality formula described in 
Parks et. al. 2020 &lt;https://doi.org/10.1038/s41587-020-0501-8&gt;
 (Supplementary Table 19) but only including those scoring criteria that can be calculated from the sequence without homology searching: 
completeness-5
contamination-5
num_contigs/100-5
num_ambiguous_bases/100000
. - completeness-4contamination: 
completeness-4
contamination
 - completeness-5contamination: 
completeness-5
contamination
 - dRep: 
completeness-5
contamination+contamination
(strain_heterogeneity/100)+0.5
log10(N50)
 ----- .. class:: infomark 
Method for calculating coverage
 Calculation of genome-wise coverage (genome mode) is similar to calculating contig-wise (contig mode) coverage, except that the unit of reporting is per-genome rather than per-contig. For calculation methods which exclude base positions based on their coverage, all positions from all contigs are considered together. - Relative abundance: Percentage relative abundance of each genome, and the unmapped read percentage - Mean: Average number of aligned reads overlapping each position on the genome - Trimmed mean: Average number of aligned reads overlapping each position after removing the most deeply and shallowly covered positions. - Covered fraction: Fraction of genome covered by 1 or more reads - Covered bases: Number of bases covered by 1 or more reads - Variance: Variance of coverage depths - Length: Length of each genome in base pairs - Count: Number of reads aligned toq each genome. Note that a single read may be aligned to multiple genomes with supplementary alignments - Reads per base: Number of reads aligned divided by the length of the genome - MetaBAT: Reproduction of the 
MetaBAT &lt;https://bitbucket.org/berkeleylab/metabat&gt;
 tool output - Coverage histogram: Histogram of coverage depths - RPKM: Reads mapped per kilobase of genome, per million mapped reads - TPM: Transcripts Per Million as described in 
Li et al 2010 &lt;https://doi.org/10.1093/bioinformatics/btp692&gt;
_"
toolshed.g2.bx.psu.edu/repos/iuc/phyloseq_from_biom/phyloseq_from_biom/1.50.0+galaxy3	"What it does
 Read a BIOM file (and optionally sequences and a phylogeny) and store them in a phyloseq object."
toolshed.g2.bx.psu.edu/repos/iuc/phyloseq_from_dada2/phyloseq_from_dada2/1.50.0+galaxy3	"What it does
 Accepts a sequence table produced by the dada2 removeBimeraDenovo tool and a taxonomy table produced by the dada2 assignTaxonomyAndAddSpecies tool and uses them to create a phyloseq object which is output in a phyloseq dataset."
toolshed.g2.bx.psu.edu/repos/iuc/das_tool/das_tool/1.1.7+galaxy1	What it does ============ DAS Tool is an automated method that integrates the results of a flexible number of binning algorithms to calculate an optimized, non-redundant set of bins from a single assembly. Inputs ====== - Bins: Tab-separated files of contig-IDs and bin-IDs. Contigs to bin file example: :: Contig_1 bin.01 Contig_8 bin.01 Contig_42 bin.02 Contig_49 bin.03 - Contigs: Assembled contigs in fasta format: :: >Contig_1 ATCATCGTCCGCATCGACGAATTCGGCGAACGAGTACCCCTGACCATCTCCGATTA... >Contig_2 GATCGTCACGCAGGCTATCGGAGCCTCGACCCGCAAGCTCTGCGCCTTGGAGCAGG... - [Optional] Proteins: Predicted proteins in prodigal fasta format. The header contains contig-ID and gene number: :: >Contig_1_1 MPRKNKKLPRHLLVIRTSAMGDVAMLPHALRALKEAYPEVKVTVATKSLFHPFFEG... >Contig_1_2 MANKIPRVPVREQDPKVRATNFEEVCYGYNVEEATLEASRCLNCKNPRCVAACPVN... Outputs ======= - Summary of output bins including quality and completeness estimates - Contigs to bin file of output bins - [Optional] Quality and completeness estimates of input bin sets - [Optional] Bins in fasta format - [Optional] Unbinned contigs
toolshed.g2.bx.psu.edu/repos/rplanel/defense_finder/defense_finder/2.0.1+galaxy1	Search for all known anti-phage defense systems in the target protein sequences.
toolshed.g2.bx.psu.edu/repos/bgruening/diamond/bg_diamond/2.1.16+galaxy0	"What it does
 DIAMOND_ is a new alignment tool for aligning short DNA sequencing reads to a protein reference database such as NCBI-NR. On Illumina reads of length 100-150bp, in fast mode, DIAMOND is about 20,000 times faster than BLASTX, while reporting about 80-90% of all matches that BLASTX finds, with an e-value of at most 1e-5. In sensitive mode, DIAMOND ist about 2,500 times faster than BLASTX, finding more than 94% of all matches. The DIAMOND algorithm is designed for the alignment of large datasets. The algorithm is not efficient for a small number of query sequences or only a single one of them, and speed will be low. BLAST is recommended for small datasets. .. _DIAMOND: http://ab.inf.uni-tuebingen.de/software/diamond/ 
Input
 Input data is a large protein or nucleotide sequence file. 
Output
 Diamond gives you a tabular output file with 12 columns: Column Description 1 Query Seq-id (ID of your sequence) 2 Subject Seq-id (ID of the database hit) 3 Percentage of identical matches 4 Alignment length 5 Number of mismatches 6 Number of gap openings 7 Start of alignment in query 8 End of alignment in query 9 Start of alignment in subject (database hit) 10 End of alignment in subject (database hit) 11 Expectation value (E-value) 12 Bit score Supported values for gap open and gap extend parameters depending on the selected scoring matrix. ======== ============================================ Matrix Supported values for (gap open)/(gap extend) ======== ============================================ BLOSUM45 (10-13)/3; (12-16)/2; (16-19)/1 BLOSUM50 (9-13)/3; (12-16)/2; (15-19)/1 BLOSUM62 (6-11)/2; (9-13)/1 BLOSUM80 (6-9)/2; 13/2; 25/2; (9-11)/1 BLOSUM90 (6-9)/2; (9-11)/1 PAM250 (11-15)/3; (13-17)/2; (17-21)/1 PAM70 (6-8)/2; (9-11)/1 PAM30 (5-7)/2; (8-10)/1 ======== ============================================"
toolshed.g2.bx.psu.edu/repos/bgruening/diamond/bg_diamond_makedb/2.1.16+galaxy0	".. class:: infomark 
What it does
 DIAMOND_ is a new alignment tool for aligning short DNA sequencing reads to a protein reference database such as NCBI-NR. On Illumina reads of length 100-150bp, in fast mode, DIAMOND is about 20,000 times faster than BLASTX, while reporting about 80-90% of all matches that BLASTX finds, with an e-value of at most 1e-5. In sensitive mode, DIAMOND is about 2,500 times faster than BLASTX, finding more than 94% of all matches. .. _DIAMOND: http://ab.inf.uni-tuebingen.de/software/diamond/ - taxonmap: Path to mapping file that maps NCBI protein accession numbers to taxon ids (gzip compressed). This parameter is optional and needs to be supplied in order to provide taxonomy features. The file can be downloaded from NCBI: ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.gz - taxonnames: Path to the names.dmp file from the NCBI taxonomy. This parameter is optional and needs to be supplied in order to provide taxonomy features. The file is contained within this archive downloadable at NCBI: ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip - taxonnodes: Path to the nodes.dmp file from the NCBI taxonomy. This parameter is optional and needs to be supplied in order to provide taxonomy features. The file is contained within this archive downloadable at NCBI: ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip"
toolshed.g2.bx.psu.edu/repos/bgruening/diamond/bg_diamond_view/2.1.16+galaxy0	"What it does
 Converts diamond daa files to multiple other formats. 
Input
 Input data is a daa file. 
Output
 Alignment results in BLAST format (pairwise/tabular), xml, sam, taxonomic (Note the latter does not work with the current diamond version. ) BLAST tables contain the following columns. Column Description 1 Query Seq-id (ID of your sequence) 2 Subject Seq-id (ID of the database hit) 3 Percentage of identical matches 4 Alignment length 5 Number of mismatches 6 Number of gap openings 7 Start of alignment in query 8 End of alignment in query 9 Start of alignment in subject (database hit) 10 End of alignment in subject (database hit) 11 Expectation value (E-value) 12 Bit score"
toolshed.g2.bx.psu.edu/repos/iuc/gtdbtk_classify_wf/gtdbtk_classify_wf/2.6.1+galaxy0	"What it does
 GTDB-Tk is a software toolkit for assigning objective taxonomic classifications to bacterial and archaeal genomes based on the Genome Database Taxonomy GTDB. It is designed to work with recent advances that allow hundreds or thousands of metagenome-assembled genomes (MAGs) to be obtained directly from environmental samples. It can also be applied to isolate and single-cell genomes. This tool accepts one or more fasta (genome) files and determines taxonomic classification of genomes by maximum-likelihood (ML) placement. The classification workflow consists of three steps: identify, align, and classify. The identify step calls genes using Prodigal, and uses HMM models and the HMMER package to identify the 120 bacterial and 122 archaeal marker genes used for phylogenetic inference. Multiple sequence alignments (MSA) are obtained by aligning marker genes to their respective HMM model. The align step concatenates the aligned marker genes and filters the concatenated MSA to approximately 5,000 amino acids. Finally, the classify step uses pplacer to find the maximum-likelihood placement of each genome in the GTDB-Tk reference tree. GTDB-Tk classifies each genome based on its placement in the reference tree, its relative evolutionary divergence, and/or average nucleotide identity (ANI) to reference genomes. Results can be impacted by a lack of marker genes or contamination."
toolshed.g2.bx.psu.edu/repos/iuc/humann/humann/3.9+galaxy0	What it does ============ HUMAnN is a pipeline for efficiently and accuretly profiling the presence/absence and abundance of microbial pathways in a community from metagenomic or metatranscriptomic sequencing data. Read more about the tool: http://huttenhower.sph.harvard.edu/humann This tool corresponds to the main tool in HUMAnN pipeline: 1. Taxomonic prescreen Reads are mapped (with MetaPhlAn) to clade-specific marker genes to rapidly identify community species 2. Pangenome search (nucleotide search) Reads are mapped (with Bowtie2) to pangenomes of identified species 3. Translated search Unclassified reads are aligned to a comprehensive and non-redundant protein database 4. Gene family and pathway quantification - Gene abundance estimation Mapping results are processed to estimate per-species and community total gene family abundance, weighting by - alignment Quality - gene length - gene coverage - Per-species and community-level metabolic network reconstruction Genes are mapped to metabolic reactions to identify a parsiomonious set of pathways that explains each species' observed reactions Pathway abundance and coverage are quantified by: 1. optimizing over alternative subpathways 2. imputing abundance for conspicuously depleted reactions Inputs ====== HUMAnN can start from a few different types of input data each in a few different types of formats: - Quality-controlled shotgun sequencing reads This is the most common starting point : A metagenome (DNA reads) or metatranscriptome (RNA reads) - Pre-computed mappings of reads to database sequences - Pre-computed (typically gene) abundance tables HUMAnN uses 3 reference databases Locally cached databases have to be downloaded before using them (using the dedicated tool). Custom databases can also be used after upload. Outputs ======= HUMAnN creates three output files: - Gene families and their abundance - Pathways and their abundance - Pathways and their coverage Ten intermediate temp output files can also be retrieved.
toolshed.g2.bx.psu.edu/repos/devteam/kraken/kraken/1.3.1	"What it does
 Kraken is a taxonomic sequence classifier that assigns taxonomic labels to short DNA reads. It does this by examining the k-mers within a read and querying a database with those k-mers. This database contains a mapping of every k-mer in Kraken's genomic library to the lowest common ancestor (LCA) in a taxonomic tree of all genomes that contain that k-mer. The set of LCA taxa that correspond to the k-mers in a read are then analyzed to create a single taxonomic label for the read; this label can be any of the nodes in the taxonomic tree. Kraken is designed to be rapid, sensitive, and highly precise. ----- 
Output Format
 Each sequence classified by Kraken results in a single line of output. Output lines contain five tab-delimited fields; from left to right, they are:: 1. ""C""/""U"": one letter code indicating that the sequence was either classified or unclassified. 2. The sequence ID, obtained from the FASTA/FASTQ header. 3. The taxonomy ID Kraken used to label the sequence; this is 0 if the sequence is unclassified. 4. The length of the sequence in bp. 5. A space-delimited list indicating the LCA mapping of each k-mer in the sequence. For example, ""562:13 561:4 A:31 0:1 562:3"" would indicate that: a) the first 13 k-mers mapped to taxonomy ID #562 b) the next 4 k-mers mapped to taxonomy ID #561 c) the next 31 k-mers contained an ambiguous nucleotide d) the next k-mer was not in the database e) the last 3 k-mers mapped to taxonomy ID #562"
toolshed.g2.bx.psu.edu/repos/iuc/kraken_taxonomy_report/kraken_taxonomy_report/0.0.3+galaxy1	".. class:: warningmark 
Note
: the database used must be the same as the one used in the original Kraken run ----- 
What is Does
 Summarizes read counts across taxonomic ranks for multiple samples. This is convenient for comparing results across multiple experiments, conditions, locations, etc. ----- 
Output
 The output is tab-delimited, with one line per taxon. Will optionally output a newick tree built from the kraken database taxonomy using the specified options. Tree branch lengths will be set to ""1.00000""."
toolshed.g2.bx.psu.edu/repos/iuc/kraken_biom/kraken_biom/1.2.0+galaxy1	Kraken-biom =========== Create BIOM-format tables (http://biom-format.org) from Kraken output (http://ccb.jhu.edu/software/kraken/). Input ===== The program takes as input, one or more files output from the kraken-report tool. Each file is parsed and the counts for each OTU (operational taxonomic unit) are recorded, along with database ID (e.g. NCBI), and lineage. The extracted data are then stored in a BIOM table where each count is linked to the Sample and OTU it belongs to. Sample IDs are extracted from the input filenames (everything up to the '.'). OTUs are defined by the --max and --min arguments. By default these are set to Order and Species respectively. This means that counts assigned directly to an Order, Family, or Genus are recorded under the associated OTU ID, and counts assigned at or below the Species level are assigned to the OTU ID for the species. Setting a minimum rank below Species is not yet available. Output ====== The BIOM format currently has two major versions. Version 1.0 uses the JSON (JavaScript Object Notation) format as a base. Version 2.x uses the HDF5 (Hierarchical Data Format v5) as a base. The output format can be specified with the --fmt option. Note that a tab-separated (tsv) output format is also available. The resulting file will not contain most of the metadata, but can be opened by spreadsheet programs. Version 2 of the BIOM format is used by default for output, but requires the Python library 'h5py'. If the library is not installed, kraken-biom will automatically switch to using version 1.0. Note that the output can optionally be compressed with gzip (--gzip) for version 1.0 and TSV files. Version 2 files are automatically compressed.
toolshed.g2.bx.psu.edu/repos/devteam/kraken_filter/kraken-filter/1.3.1	".. class:: warningmark 
Note
: the database used must be the same as the one used in the original Kraken run ----- 
What it does
 At present, we have not yet developed a confidence score with a solid probabilistic interpretation for Kraken. However, we have developed a simple scoring scheme that has yielded good results for us, and we've made that available in the kraken-filter script. The approach we use allows a user to specify a threshold score in the [0,1] interval; the 
kraken-filter
 script then will adjust labels up the tree until the label's score (described below) meets or exceeds that threshold. If a label at the root of the taxonomic tree would not have a score exceeding the threshold, the sequence is called unclassified by 
kraken-filter
. A sequence label's score is a fraction C/Q, where C is the number of k-mers mapped to LCA values in the clade rooted at the label, and Q is the number of k-mers in the sequence that lack an ambiguous nucleotide (i.e., they were queried against the database). Consider the example of the LCA mappings in Kraken's output:: 562:13 561:4 A:31 0:1 562:3 would indicate that:: the first 13 k-mers mapped to taxonomy ID #562 the next 4 k-mers mapped to taxonomy ID #561 the next 31 k-mers contained an ambiguous nucleotide the next k-mer was not in the database the last 3 k-mers mapped to taxonomy ID #562 In this case, ID #561 is the parent node of #562. Here, a label of #562 for this sequence would have a score of C/Q = (13+3)/(13+4+1+3) = 16/21. A label of #561 would have a score of C/Q = (13+4+3)/(13+4+1+3) = 20/21. If a user specified a threshold over 16/21, kraken-filter would adjust the original label from #562 to #561; if the threshold was greater than 20/21, the sequence would become unclassified."
toolshed.g2.bx.psu.edu/repos/devteam/kraken_report/kraken-mpa-report/1.2.4	".. class:: warningmark 
Note
: the database used must be the same as the one used in the original Kraken run ----- 
What is Does
 Kraken-mpa-report summarizes read counts across taxonomic ranks for multiple samples. This is convenient for comparing results across multiple experiments, conditions, locations, etc. ----- 
Output
 The output of kraken-mpa-report is a tab-delimited table, with one line per taxon."
toolshed.g2.bx.psu.edu/repos/devteam/kraken_report/kraken-report/1.3.1	".. class:: warningmark 
Note
: the database used must be the same as the one used in the original Kraken run ----- 
Output
 The output of kraken-report is tab-delimited, with one line per taxon. The fields of the output, from left-to-right, are as follows:: 1. Percentage of reads covered by the clade rooted at this taxon 2. Number of reads covered by the clade rooted at this taxon 3. Number of reads assigned directly to this taxon 4. A rank code, indicating (U)nclassified, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. All other ranks are simply filled with a dash. 5. NCBI taxonomy ID 6. Indented scientific name The scientific names are indented using spaces, according to the tree structure specified by the taxonomy."
toolshed.g2.bx.psu.edu/repos/devteam/kraken_translate/kraken-translate/1.3.1	".. class:: warningmark 
Note
: the database used must be the same as the one used in the original Kraken run ------- 
What it does
 The file sequences.labels generated by the above example is a text file with two tab-delimited columns, and one line for each classified sequence in sequences.fa; unclassified sequences are not reported by kraken-translate. The first column of kraken-translate's output are the sequence IDs of the classified sequences, and the second column contains the taxonomy of the sequence. For example, an output line from kraken:: C SEQ1 562 36 562:6 Would result in a corresponding output line from kraken-translate:: SEQ1 root;cellular organisms;Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacteriales;Enterobacteriaceae;Escherichia;Escherichia coli Alternatively, kraken-translate accepts the option 
--mpa-format
 which will report only levels of the taxonomy with standard rank assignments (superkingdom, kingdom, phylum, class, order, family, genus, species), and uses pipes to delimit the various levels of the taxonomy. For example, 
kraken-translate --mpa-format
 with the above example output from kraken would result in the following line of output:: SEQ1 d__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacteriales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia_coli Taxonomy assignments above the superkingdom rank are represented as just ""root"" when using the 
--mpa-report
 option with kraken-translate."
toolshed.g2.bx.psu.edu/repos/iuc/kraken2/kraken2/2.17.1+galaxy0	"What it does
 Kraken2 is a taxonomic sequence classifier that assigns taxonomic labels to short DNA reads. It does this by examining the k-mers within a read and querying a database with those k-mers. This database contains a mapping of every k-mer in Kraken's genomic library to the lowest common ancestor (LCA) in a taxonomic tree of all genomes that contain that k-mer. The set of LCA taxa that correspond to the k-mers in a read are then analyzed to create a single taxonomic label for the read; this label can be any of the nodes in the taxonomic tree. Kraken is designed to be rapid, sensitive, and highly precise. ----- 
Output Format
 Each sequence classified by Kraken results in a single line of output. Output lines contain five tab-delimited fields; from left to right, they are:: 1. ""C""/""U"": a one letter code indicating that the sequence was either classified or unclassified. 2. The sequence ID, obtained from the FASTA/FASTQ header. 3. The taxonomy ID Kraken 2 used to label the sequence; this is 0 if the sequence is unclassified. 4. The length of the sequence in bp. 5. A space-delimited list indicating the LCA mapping of each k-mer in the sequence. For example, ""562:13 561:4 A:31 0:1 562:3"" would indicate that: a) the first 13 k-mers mapped to taxonomy ID #562 b) the next 4 k-mers mapped to taxonomy ID #561 c) the next 31 k-mers contained an ambiguous nucleotide d) the next k-mer was not in the database e) the last 3 k-mers mapped to taxonomy ID #562"
toolshed.g2.bx.psu.edu/repos/iuc/krakentools_alpha_diversity/krakentools_alpha_diversity/1.2.1+galaxy0	KrakenTools is a suite of scripts to be used alongside the Kraken, KrakenUniq, Kraken 2, or Bracken programs. These scripts are designed to help Kraken users with downstream analysis of Kraken results. This program calculates alpha diversity, from the Bracken abundance estimation file. User must specify Bracken output file, and type of alpha diversity to be calculated. Specific options are specified below. Input ------ Bracken abundance estimation file. Input Bracken file must be in standard Bracken output file format and must be run specifically for Abundance Estimation. Example format: name tax_id tax_lvl kraken... added... new... fraction... Homo sapiens 9606 S ... .... 999000 0.999000 Streptococcus pyogenes 1314 S ... .... 10 0.000001 Streptococcus agalactiae 1311 S ... .... 5 0.000000 Streptococcus pneumoniae 1313 S ... .... 3 0.000000 Bordetella pertussis 520 S ... .... 20 0.000002 Output --------- Alpha diversity with the specified type.
toolshed.g2.bx.psu.edu/repos/iuc/krakentools_combine_kreports/krakentools_combine_kreports/1.2.1+galaxy2	KrakenTools is a suite of scripts to be used alongside the Kraken, KrakenUniq, Kraken 2, or Bracken programs. These scripts are designed to help Kraken users with downstream analysis of Kraken results. This script combines multiple Kraken reports into a combined report file. Input ------ Kraken-style reports to combine Output ------ Combine Kraken reports Input file format (tab-delimited) - percentage of total reads - number of reads (including reads within subtree) - number of reads (only at this level) - taxonomic classification level (U, D, P, C, O, F, G, S,...etc) - NCBI taxonomic ID - name of level Output file format (tab-delimited) - percentage of total reads (for summed reads) - combined number of reads (including reads within subtree) - combined number of reads (only at this level) - S1_all_reads, S1_lvl_reads, S2_all_reads, S2_lvl_reads, ...etc. - taxonomic classification level (U, D, P, C, O, F, G, S,...etc) - NCBI taxonomic ID - name of level
toolshed.g2.bx.psu.edu/repos/iuc/krakentools_kreport2krona/krakentools_kreport2krona/1.2.1+galaxy0	"KrakenTools is a suite of scripts to be used alongside the Kraken, KrakenUniq, Kraken 2, or Bracken programs. These scripts are designed to help Kraken users with downstream analysis of Kraken results. This program takes a Kraken report file and prints out a krona-compatible TEXT file Input ------ A Kraken report file Output ------ A krona-compatible TEXT file 
Example output:
 
--no-intermediate-ranks
 :: 6298 Unclassified 8 k__Bacteria 4 k__Bacteria p_Proteobacteria 6 k__Bacteria p_Proteobacteria c__Gammaproteobacteria 
--intermediate-ranks
 :: 6298 Unclassified 79 x__root 0 x__root x__cellular_organisms 8 x__root x__cellular organisms k__Bacteria 4 x__root x__cellular organisms k__Bacteria p__Proteobacteria 6 x__root x__cellular organisms k__Bacteria p__Proteobacteria c__Gammaproteobacteria"
toolshed.g2.bx.psu.edu/repos/iuc/krakentools_kreport2mpa/krakentools_kreport2mpa/1.2.1+galaxy0	"KrakenTools is a suite of scripts to be used alongside the Kraken, KrakenUniq, Kraken 2, or Bracken programs. These scripts are designed to help Kraken users with downstream analysis of Kraken results. This program takes a Kraken report file and prints out a MetaPhlAn-style TEXT file. Input ------ A Kraken report file Output ------ A MetaPhlAn-style TEXT file 
Example output:
 
--no-intermediate-ranks
 :: k__Eukaryota 756 k__Eukaryota|k__Metazoa 402 k__Eukaryota|k__Metazoa|p__Chordata 402 k__Eukaryota|k__Metazoa|p__Chordata|c__Mammalia 402 
--intermediate-ranks
 :: x__cellular_organisms 836 x__cellular_organisms|k__Eukaryota 756 x__cellular_organisms|k__Eukaryota|x__Opisthokonta 747 x__cellular_organisms|k__Eukaryota|x__Opisthokonta|k__Metazoa 402 x__cellular_organisms|k__Eukaryota|x__Opisthokonta|k__Metazoa|x__Eumetazoa 402 
--percentages
 :: k__Eukaryota 56.0 k__Eukaryota|k__Metazoa 29.78 k__Eukaryota|k__Metazoa|p__Chordata 29.78 k__Eukaryota|k__Metazoa|p__Chordata|c__Mammalia 29.78 k__Eukaryota|k__Metazoa|p__Chordata|c__Mammalia|o__Primates 29.78 k__Eukaryota|k__Metazoa|p__Chordata|c__Mammalia|o__Primates|f__Hominidae 29.78 k__Eukaryota|k__Metazoa|p__Chordata|c__Mammalia|o__Primates|f__Hominidae|g__Homo 29.78"
toolshed.g2.bx.psu.edu/repos/iuc/krakentools_extract_kraken_reads/krakentools_extract_kraken_reads/1.2.1+galaxy0	"What it does
 ------------------- After running Kraken, Kraken2, or KrakenUniq, users may use the 
extract_kraken_reads.py
 program to extract the FASTA or FASTQ reads classified as a specific taxonomy ID. For example, this program can be used to extract all bacterial reads or only reads assigned to Escherichia coli. Users must provide (at minimum) the original sequence file(s), at least one taxonomy ID, and the Kraken output file."
toolshed.g2.bx.psu.edu/repos/iuc/krakentools_beta_diversity/krakentools_beta_diversity/1.2.1+galaxy0	KrakenTools is a suite of scripts to be used alongside the Kraken, KrakenUniq, Kraken 2, or Bracken programs. These scripts are designed to help Kraken users with downstream analysis of Kraken results. This program calculates the beta diversity (Bray-Curtis dissimilarity) from kraken, krona and bracken files. Input ------ Bracken files, Kraken and Kracken2 report files and Krona files. Additionally, you can pass any tab-separated file and specify the columns with the taxonomical information and read counts (--cols). Output ------ Beta diversity of the input files
toolshed.g2.bx.psu.edu/repos/crs4/taxonomy_krona_chart/taxonomy_krona_chart/2.7.1	"What it does
 This tool renders results of a metagenomic profiling as a zoomable pie chart using Krona_. ------ 
Krona options
 The Galaxy version supports the following options:: -n Name of the highest level. -c Combine data from each file, rather than creating separate datasets within the chart. -d Maximum depth of wedges to include in the chart. ----- 
Input format
 
Tabular
 input format should be a tab-delimited file with the first column containing a count and the remaining columns describing the hierarchy. For example:: 2 Fats Saturated fat 3 Fats Unsaturated fat Monounsaturated fat 3 Fats Unsaturated fat Polyunsaturated fat 13 Carbohydrates Sugars 4 Carbohydrates Dietary fiber 21 Carbohydrates 5 Protein 4 which would yield this 
Krona plot
. .. _Krona plot: https://marbl.github.io/Krona/examples/xml.krona.html ----- 
License and citation
 This Galaxy tool is Copyright © 2013-2014 
CRS4 Srl.
 and is released under the 
MIT license
. .. _CRS4 Srl.: http://www.crs4.it/ .. _MIT license: https://opensource.org/licenses/MIT You can use this tool only if you agree to the license terms of: 
Krona
. .. _Krona: https://github.com/marbl/Krona/wiki"
toolshed.g2.bx.psu.edu/repos/iuc/mapseq/mapseq/2.1.1+galaxy0	"MAPseq ====== MAPseq is a set of fast and accurate sequence read classification tools designed to assign taxonomy and OTU classifications to ribosomal RNA sequences. This is done by using a reference set of full-length ribosomal RNA sequences for which known taxonomies are known, and for which a set of high quality OTU clusters has been previously generated. For each read, the best guess and correspoding confidence in the assignment is shown at each taxonomic and OTU level. Mapseq2biom =========== This downstream script summaries the mapseq output as an OTU table (including taxon information) as reads per OTU. This requires as input an OTU to taxon mapping, for the taxonomy used to run the mapseq tool. Example ------- Mapseq output: :: # mapseq v1.2.3 (Oct 2 2018) #query dbhit bitscore identity matches mismatches gaps query_start query_end dbhit_start dbhit_end strand ITS2 test.1 355527192 204 0.9863636493682861 217 1 2 0 218 0 220 - sk__Eukaryota;k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Didymellaceae;g__Ectophoma;s__Ectophoma_multirostrata test.2 555948006 248 0.8478803038597107 340 42 19 200 582 192 593 - sk__Eukaryota;k__Fungi test.4 406352048 217 0.9127272963523865 251 22 2 106 381 169 442 - sk__Eukaryota;k__Fungi;p__ OTU to taxon mapping: :: 1 sk__Eukaryota;k__Fungi 2 sk__Eukaryota;k__Fungi;p__;c__;o__;f__;g__;s__uncultured_fungus 3 sk__Eukaryota;k__Fungi;p__Ascomycota 4 sk__Eukaryota;k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales 5 sk__Eukaryota;k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Astrosphaeriellaceae;g__Pithomyces 6 sk__Eukaryota;k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Coniothyriaceae;g__Coniothyrium 7 sk__Eukaryota;k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Didymellaceae 8 sk__Eukaryota;k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Didymellaceae;g__Ectophoma 9 sk__Eukaryota;k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Didymellaceae;g__Ectophoma;s__Ectophoma_multirostrata 10 sk__Eukaryota;k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Didymosphaeriaceae;g__Paraconiothyrium;s__Paraconiothyrium_cyclothyrioides OTU output: :: # Constructed from biom file # OTU ID label taxonomy 1 2.0 sk__Eukaryota;k__Fungi 9 1.0 sk__Eukaryota;k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Didymellaceae;g__Ectophoma;s__Ectophoma_multirostrata Taxon output for Krona: :: 2 sk__Eukaryota k__Fungi 1 sk__Eukaryota k__Fungi p__Ascomycota c__Dothideomycetes o__Pleosporales f__Didymellaceae g__Ectophoma s__Ectophoma_multirostrata Source ------ * 
GitHub &lt;https://github.com/EBI-Metagenomics/pipeline-v5/blob/master/tools/RNA_prediction/mapseq2biom/mapseq2biom.pl&gt;
 License ------- * 
Apache-2.0 license &lt;https://raw.githubusercontent.com/EBI-Metagenomics/pipeline-v5/master/LICENSE&gt;"
toolshed.g2.bx.psu.edu/repos/mbernt/maxbin2/maxbin2/2.2.7+galaxy6	"MaxBin is a software that clusters metagenomic contigs into different bins, each consists (hopefully) of contigs from one species. MaxBin uses the nucleotide composition information and contig abundance information to do achieve binning through an Expectation-Maximization algorithm. 
Input
: MaxBin need the contigs and contig abundance information. The contig abundance information can be provided in two ways: the user can choose to provide - the abundance file or - the sequencing reads in fasta format (and MaxBin will use Bowtie2 to map the sequencing reads against the contigs and generate the abundance information) The abundance information can be provided as tabular file: For example, assume I have three contigs named A0001, A0002, and A0003, then my abundance file will look like ===== ===== A0001 30.89 A0002 20.02 A0003 78.93 ===== ===== Reads/Abundundance files can be given in multiple files. By default MaxBin will look for 107 marker genes present in >95% of bacteria. Alternatively you can also choose 40 marker gene sets that are universal among bacteria and archaea (Wu et al., PLoS ONE 2013). This option may be better suited for environment dominated by archaea; however it tend to split genomes into more bins. You can choose between different marker gene sets and see which one works better. 
Outputs
 - bins: binned sequences - summary: a summary file describing which contigs are being classified into which bin. - log: a log file recording the core steps of MaxBin algorithm - abundances (only if reads are used as input): a summary file describing which contigs are being classified into which bin - marker: marker gene presence numbers for each bin. This table is ready to be plotted by R or other 3rd-party software. - marker plot (anly present if selected in the advanced options): visualization of the marker gene presence numbers using R. Will only appear if -plotmarker is specified. - unclassified sequences: this file stores all sequences that pass the minimum length threshold but are not classified successfully. - to short sequences: this file stores all sequences that do not meet the minimum length threshold. - markers prediced for bins: these data sets store all markers predicted from the individual bins. 
Reassembly
 This is an experimental feature of MaxBin. It calls for each read bin IDBA_UD with the pre_correction parameter. Of course this IDBA_UD call can be done also with the corresponding Galaxy tool ** More information ** https://web.archive.org/web/20190417100740/https://downloads.jbei.org/data/microbial_communities/MaxBin/MaxBin.html"
toolshed.g2.bx.psu.edu/repos/bgruening/mgnify_seqprep/mgnify_seqprep/1.2+galaxy0	".. class:: warningmark 
Caution
 ----------- :: This is a modified version of the 1.2 release. Made for use with the MGnify pipeline. Difference in 
utils.h
: :: -#define MAX_SEQ_LEN (256) +#define MAX_SEQ_LEN (1024) 
SeqPrep
 ----------- :: SeqPrep is a versatile tool designed for merging overlapping paired-end Illumina reads into a single, longer read. Additionally, it offers the functionality to trim adapter sequences from reads, making it a needful tool for preprocessing Illumina sequencing data. 
Usage
 ========= :: To utilize SeqPrep, start by selecting your input FASTQ files: one for the first set of reads and another for the second set. SeqPrep provides several options to customize your data processing: - Adapter Sequences: You can provide specific sequences for adapter trimming if they are known. SeqPrep will remove these sequences from the reads. - Quality Score Cutoff: Set a threshold for the quality score. Reads with quality scores below this threshold can be discarded or trimmed. - Minimum Read Length: Define the minimum length for reads to be retained after trimming. Reads shorter than this length will be discarded. If the merging feature is enabled, SeqPrep will combine overlapping reads into longer sequences, thereby enhancing the data quality for downstream analysis. 
Outputs
 =========== :: SeqPrep generates outputs in gzipped FASTQ format. See more details on 
SeqPrep GitHub repository &lt;https://github.com/jstjohn/SeqPrep&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/metabat2/metabat2/2.17+galaxy0	"What it does
 MetaBAT2 (Metagenome Binning based on Abundance and Tetranucleotide frequency) is an automated metagenome binning software that integrates empirical probabilistic distances of genome abundance and tetranucleotide frequency. The tool accepts a fast file containing contigs and produces a collection (i.e., bins) of fasta files. MetaBAT2 includes optionizations to MetaBAT. It requires virtually no parameter optimization. Default parameter values are more reliable to use in most cases since MetaBAT2 adapts to the given data to find the best parameter. Some parameter settings are still available for advanced users, helping to manage some exceptional cases by changing the amount of data used for the analysis. 
More information
 https://bitbucket.org/berkeleylab/metabat/src/master/ 
Options
 * 
Use base coverage depth file
 - optionally select a base coverage depth file that was either generated by the Calculate contig depths for MetaBAT2 tool or another 3rd party tool."
toolshed.g2.bx.psu.edu/repos/iuc/metaphlan/metaphlan/4.2.4+galaxy0	"What it does ============ MetaPhlAn is a computational tool for profiling the composition of microbial communities (Bacteria, Archaea and Eukaryotes) from metagenomic shotgun sequencing data (i.e. not 16S) at species-level. MetaPhlAn relies on ~1.1M unique clade-specific marker genes identified from ~100,000 reference genomes (~99,500 bacterial and archaeal and ~500 eukaryotic), allowing: - unambiguous taxonomic assignments; - accurate estimation of organismal relative abundance; - species-level resolution for bacteria, archaea, eukaryotes and viruses; - strain identification and tracking - orders of magnitude speedups compared to existing methods. - microbiota strain-level population genomics MetaPhlAn clade-abundance estimation ------------------------------------ The basic usage of MetaPhlAn consists in the identification of the clades (from phyla to species and strains in particular cases) present in the microbiota obtained from a microbiome sample and their relative abundance. Marker level analysis --------------------- MetaPhlAn introduces the capability of characterizing organisms at the strain level using non aggregated marker information. Such capability comes with several slightly different flavours and are a way to perform strain tracking and comparison across multiple samples. Usually, MetaPhlAn is first ran with default parameters for the type of analysis to profile the species present in the community, and then a strain-level profiling can be performed to zoom-in on specific species of interest. This operation can be performed quickly as it exploits the mapout intermediate file saved during the execution of the default analysis type. Inputs ====== Metaphlan takes as input either: - one or several sequence files in Fasta, FastQ (whether compressed or not) - a BowTie2 produced SAM file - an intermediary mapping file of the microbiota generated by a previous MetaPhlAn run It also needs the reference database, which can be locally installed or customized using the dedicated tools. Outputs ======= The main output is a tab-separated file with the predicted taxon relative abundances. It also generates a BIOM file and some intermediate files (SAM and mapout) if sequence files are given as inputs. More help and use cases ======================= To get more information about MetaPhlAn usage and use cases, please refer to the 
Metaphlan documentation
_. .. _Metaphlan documentation: https://github.com/biobakery/MetaPhlAn/wiki/MetaPhlAn-4#Basic-Usage"
toolshed.g2.bx.psu.edu/repos/iuc/gtdb_to_taxdump/gtdb_to_taxdump/0.1.9+galaxy0	".. class:: infomark 
Where to download the metadata
 The metadata can be download from 
GTDB &lt;https://data.gtdb.ecogenomic.org/releases/&gt;
_. If you want to use this tool to convert gtdb naming from a GTDB tool such as 
gtdbtk classify_wf
 use the corresponding GTDB release of the metadata. After download it dont forget to uncompressed it! 
What does this tool
 This tool was desinged to map the GTDB taxonomy to the NCBI taxonomy since both are using different ID systems. This means this tool map either GTDB names to the NCBI names or the other way around. If the names contain prefixes like 
s__
 the mapped name also will contain there corresponded prefix. If you use the names without prefix then the mapped names also don't contain the prefixes. 
Prefixes
 The prefixes stand for the taxonomic rank for example s__Bacteroides thetaiotaomicron. If you use them directly with in the file where you list the names which should be queried then you dont need the prefix option but if you use names without them you can either manully include them with the prefix option (read the help section there too) or you can use the tool without them but then you have to check the no-prefix option! This means when you use the no-prefix option s__Bacteroides thetaiotaomicron will not be used in this tool but Bacteroides thetaiotaomicron will work fine!"
toolshed.g2.bx.psu.edu/repos/iuc/name2taxid/name2taxid/0.20.0+galaxy0	"This tool can convert a NCBI name to its corresponding taxid. Input a tsv or txt file and state the column where the name are written .. class:: infomark Example :: Homo sapiens Akkermansia muciniphila ATCC BAA-835 Akkermansia muciniphila Mouse Intracisternal A-particle 
sci_name option
 .. class:: infomark For example, the name ""Enterococcus coli"" is not a scientific name which means with this option you can remove it from the query to find a taxid to it but it will still be in the output. In contrast, for example, Drosophila is a scientific name which means that this will always be searched in the query even if the option is on or off. 
show_rank option
 ..class:: infomark Here is an example of the output if you use the option: :: Homo sapiens 9606 species Akkermansia muciniphila ATCC BAA-835 349741 strain Akkermansia muciniphila 239935 species Mouse Intracisternal A-particle 11932 species without this option the output will be: :: Homo sapiens 9606 Akkermansia muciniphila ATCC BAA-835 349741 Akkermansia muciniphila 239935 Mouse Intracisternal A-particle 11932"
toolshed.g2.bx.psu.edu/repos/iuc/nonpareil/nonpareil/3.5.5+galaxy1	"Nonpareil uses the redundancy of the reads in metagenomic datasets to estimate the average coverage and predict the amount of sequences that will be required to achieve ""nearly complete coverage"". Nonpareil outputs three files and one optional log file: - Redundancy summary is a tab-delimited file with six columns. The first column indicates the sequencing effort (in number of reads), and the remaining columns indicate the summary of the distribution of redundancy (from the replicates, 1,024 by default) at the given sequencing effort. These five columns are: average redundancy, standard deviation, quartile 1, median (quartile 2), and quartile 3. - Redundancy values is a tab-delimited file with three columns. Similar to the redundancy summary file, it contains information about the redundancy at each sequencing effort, but it provides ALL the results from the replicates, not only the summary at each point. The first column indicates the sequencing effort (as a fraction of the dataset), the second column indicates the ID of the replicate (a number used only to introduce some controlled noise in plots), and the third column indicates the estimated redundancy value. - Mates distribution is a raw list with the number of reads in the dataset matching a query read. A set of query reads is randomly drawn by Nonpareil (1,000 by default), and compared against all reads in the dataset. Each line on this file corresponds to a query read (the order is not important). We have seen certain correspondance between these numbers and the distribution of abundances in the community (compared, for example, as rank-abundance plots), but this file is provided only for quality-control purposes and comparisons with other tools. - Log is a verbose log of internal Nonpareil processing. The number to the left (inside squared brackets) indicate the CPU time (in minutes). This file also provide quality assessment of the Nonpareil run (automated consistency evaluation). Ideally, the last line should read ""Everything seems correct"". Otherwise, it suggests alternative parameters that may improve the estimation. For more details about the tool, please check: http://nonpareil.readthedocs.io/en/latest/index.html"
toolshed.g2.bx.psu.edu/repos/ufz/phi_toolkit_report/phi_toolkit_report/0.2.0+galaxy0	".. class:: infomark 
What it does
 Create a report for the PHI toolkit workflow. Usage ..... 
Input
 - Genomes - Checkm2 quality report - geNomad Virus Summary - geNomad Virus genes - DefenseFinder systems - GTDB-Tk summary - CheckV Quality summary - drep compare Cdb.csv - drep compare clustering dendrogram - iPHop Host prediction to genome - Abricate results - Vibrant Individual predicted virus AMGs 
Output
 A html report summarizing the results."
toolshed.g2.bx.psu.edu/repos/iuc/phyloseq_plot_richness/phyloseq_plot_richness/1.50.0+galaxy3	"What it does
 Accepts a dataset containing a phyloseq object created from a dada2 taxonomy table and a dada2 sequence table, estimates a number of alpha-diversity metrics, and generates a colored plot that includes the alpha diverstiy measure of each sample."
toolshed.g2.bx.psu.edu/repos/iuc/phyloseq_plot_ordination/phyloseq_plot_ordination/1.50.0+galaxy3	"What it does
 Accepts a dataset containing a phyloseq object created from a dada2 taxonomy table and a dada2 sequence table, and generates an ordination plot of the samples. 
Options
 
Ordination method
 * 
DCA
 - Performs detrended correspondence analysis using decorana. * 
CCA
 - Performs correspondence analysis, or optionally, constrained correspondence analysis (a.k.a. canonical correspondence analysis) via vegan cca. * 
RDA
 - Performs redundancy analysis, or optionally principal components analysis, via vegan rda. * 
CAP
 - [Partial] Constrained Analysis of Principal Coordinates or distance-based RDA, via vegan capscale. * 
NMDS
 - Performs Non-metric MultiDimenstional Scaling of a sample-wise ecological distance matrix onto a user-specified number of axes (k). * 
MDS/PCoA
 - Performs principal coordinate analysis (also called principle coordinate decomposition, multidimensional scaling (MDS), or classical scaling) of a distance matrix including two correction methods for negative eigenvalues. 
Distance method
 - Utilized only if a distance matrix is required by the Ordination method documented above. 
Plot type
 * 
biplot
 - Produces a combined plot with both taxa and samples. * 
samples
 - Produces a single plot of just the samples of the ordination. * 
scree
 - Produces an ordered bar plot of the normalized eigenvalues associated with each ordination axis. * 
species
 - Produces a single plot of just the species of the ordination. * 
split
 - Produces a plot with both taxa and samples separated in two facet panels respectively."
toolshed.g2.bx.psu.edu/repos/iuc/plasflow/PlasFlow/1.1.0+galaxy0	PlasFlow is a set of scripts used for prediction of plasmid sequences in metagenomic contigs. It relies on the neural network models trained on full genome and plasmid sequences and is able to differentiate between plasmids and chromosomes with accuracy reaching 96%. It outperforms other available solutions for plasmids recovery from metagenomes and incorporates the thresholding which allows for exclusion of incertain predictions.
toolshed.g2.bx.psu.edu/repos/iuc/recentrifuge/recentrifuge/1.16.1+galaxy0	"What it does
 With Recentrifuge, researchers can interactively explore what organisms are in their samples and at which level of confidence, enabling robust comparative analysis of multiple samples in any metagenomic study. * Removes diverse contaminants, including crossovers, using a novel robust contamination removal algorithm. * Provides a confidence level for every result, since the calculated score propagates to all the downstream analysis and comparisons. * Unveils the generalities and specificities in the metagenomic samples, thanks to a new comparative analysis engine. Recentrifuge is especially useful when a more reliable detection of minority organisms is needed (e.g. in the case of low microbial biomass metagenomic studies) in clinical, environmental, or forensic analysis. Beyond the standard confidence levels, Recentrifuge implements others devoted to variable length reads, very convenient for complex datasets generated by nanopore sequencers. 
Input option
 Recentrifuge can deal with some different taxonomic output files. Input files can come from centrifuge, kraken, clark of lmat software. A generic fonction to accept other files is available but need to add information of the file content. If generic is choose, the option format need a string like : 'TYP:csv,TID:1,LEN:3,SCO:6,UNC:0'. Where TYP are csv/tsv/ssv, and the rest of fields indicate the number of column used (starting in 1) for the TaxIDs assigned,the LENgth of the read, the SCOre given to the assignment"" 
Database for recentrifuge
 Recentrifuge first need the taxonomic database from NCBI (nodes.dmp and names.dmp). We also provide the option to directly load necessary files from history as a dataset list. 1. cached for already installed taxonomic databases 2. history to load from your history 
Output options
 1. Depending of the option provided, the file output format can be csv, tsv or xlsx and be combine in one or more files (extra). 3. By default a html file is generated to visualize data, could be remove using the nohtml option 
Advanced options
 1. Recentrifuge can integrate sample in the data which are negative control to normalize the data 2. Scoring is an option to choose the score method for the read classified by taxonomic tools : SHEL (Single Hit Equivalent Length): This is a score value in pair bases roughly equivalent to a single hit to the database. KRAKEN: This scoring scheme is only available for this classifier. It divides the k-mer hit count of the top assignment by the total k-mers in the read and multiplies the result by 100 to give a percentage of coverage (the fraction of the read k-mers covered by k-mers belonging to the read final assignment). This is the default scoring scheme for Kraken samples, and it supports the mixing of samples with different read length. LENGTH: The score of a read will be its length (or the combined length of mate pairs). LOGLENGTH: Logarithm (base 10) of the length score. NORMA: This score is the normalized score SHEL / LENGTH in percentage, so it takes into account both the assignment quality and the length of the read. Very useful when both the score assignments and lengths are variable among the reads. LMAT: This scoring scheme is only available for this classifier. CLARK_C: This scoring scheme is not available for other classifiers. It takes the confidence score as the score for a read, conf=h1/(h1+h2), or 1-conf=h2/(h1+h2) in case the majority of a read is not classified (1st assignment unclassified). See CLARK's README file for details on how h1 and h2 are calculated. If you use this scoring, you will probably want to filter to a minimum of 0.5 (-y 0.5) or beyond, as under 0.5 the assignments have very low confidence. CLARK_G: This scheme scores every read with its CLARK gamma score, so it is only available for this classifier. 3. You can choose a filter for read quality using the minscore option (--minscore) 4. You can include or exclude specific taxa using the NCBI taxid code 
More advanced options
 1. You can choose a filter for read quality specifically on the control samples 2. You cans specify the minimum taxa value to avoid collapsing one level into parent 3. A summary option is available produce a summary file Some other options are available and explicite in the more advanced panel of the tool rcf - Release 1.8.1 - Mar 2022 Copyright (C) 2017–2022, Jose Manuel Martí Martínez This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details."
toolshed.g2.bx.psu.edu/repos/iuc/humann_regroup_table/humann_regroup_table/3.9+galaxy0	"What it does ============ HUMAnN is a pipeline for efficiently and accuretly profiling the presence/absence and abundance of microbial pathways in a community from metagenomic or metatranscriptomic sequencing data. Read more about the tool: http://huttenhower.sph.harvard.edu/humann This tool takes a table of feature values and a mapping of groups to component features to produce a new table with group values in place of feature values. HUMAnN gene family output can contain a very large number of features depending on the complexity of your underlying sample. One way to explore this information in a simplified manner is via HUMAnN's own pathway coverage and abundance, which summarize the values of their member genes. However, this approach does not apply to gene families that are not associated with metabolic pathways. To further simplify the exploration of gene family abundance data, users can regroup gene families into other functional categories using the current tool. This tool takes as arguments a gene family abundance table and a mapping (groups) file that indicates which gene families belong to which groups. Out of the box, HUMAnN can regroup gene families to MetaCyc reactions (a step which is also used internally as part of MetaCyc pathway quantification). Users can use additional mapping files for both UniRef90 and UniRef50 gene families to the following systems: - MetaCyc Reactions - KEGG Orthogroups (KOs) - Pfam domains - Level-4 enzyme commission (EC) categories - EggNOG (including COGs) - Gene Ontology (GO) - Informative GO In most cases, mappings are directly inferred from the annotation of the corresponding UniRef centroid sequence in UniProt. One exception to this are the ""informative GO"" (infogo1000) maps: These are informative subsets of GO computed from UniProt's annotations and the structure of the GO hierarchy specifically for HUMAnN (each informative GO term has >1,000 UniRef centroids annotated to it, but none of its progeny terms have >1,000 centroids so annotated). If the ""UNMAPPED"" gene abundance feature is included in a user's input, it will automatically be carried forward to the final output. In addition, genes that do not group with a non-trivial feature are combined as an ""UNGROUPED"" group. By default, UNGROUPED reflects the total abundance of genes that did not belong to another group (similar in spirit to the ""UNINTEGRATED"" value reported in the pathway abundance file). Some groups are not associated by default with human-readable names. To attach names to a regrouped table, use the HUMAnN rename tool (The ""GO"" name map can be used for both raw GO and informative GO.) Inputs ====== Users are free to create and use additional mapping files and pass them to this tool. The format of a mapping file is: 
group1 uniref1 uniref2 uniref3 ...
 
group2 uniref1 uniref5 ...
 Where spaces between items above denote TABS. By default, feature abundances (such as gene families) are summed to produce group abundances."
toolshed.g2.bx.psu.edu/repos/iuc/humann_rename_table/humann_rename_table/3.9+galaxy0	What it does ============ HUMAnN is a pipeline for efficiently and accuretly profiling the presence/absence and abundance of microbial pathways in a community from metagenomic or metatranscriptomic sequencing data. Read more about the tool: http://huttenhower.sph.harvard.edu/humann By default, HUMAnN outputs do not attach names (glosses) to individual feature IDs to keep file sizes down. This tool is used to attach those names. The most common use for this tool is attaching gene names to UniRef90/UniRef50 features in the default gene family output files. Features without names (common for many UniRef90/50 families) are assigned the default name: NO_NAME.
toolshed.g2.bx.psu.edu/repos/iuc/humann_renorm_table/humann_renorm_table/3.9+galaxy0	"What it does ============ HUMAnN is a pipeline for efficiently and accuretly profiling the presence/absence and abundance of microbial pathways in a community from metagenomic or metatranscriptomic sequencing data. Read more about the tool: http://huttenhower.sph.harvard.edu/humann HUMAnN quantifies genes and pathways in units of RPKs (reads per kilobase). These account for gene length but not sample sequencing depth. While there are some applications, e.g. strain profiling, where RPK units are superior to depth-normalized units, most of the time a user will renormalize their samples prior to downstream analysis. This tool provides the choice to normalize to relative abundance or copies per million (CPM) units. Both of these represent ""total sum scaling (TSS)""-style normalization: in the former case, each sample is constrained to sum to 1, whereas in the latter case (CPMs) samples are constrained to sum to 1 million. Units out of 1 million are often more convenient for tables with many, many features (such as genefamilies.tsv tables). Note: CPM as used here does not refer to unnormalized COUNTS per million, but rather copies per million. CPMs as used here are a generic analog of the TPM (transcript per million) unit in RNA-seq. You may wish to use the abbreviation CoPM for added clarity. By default, this tool normalizes all stratification levels to the sum of all community feature totals, but other options (such as level-wise normalization) are supported. ""Special"" features (such as UNMAPPED) can be included or excluded in the normalization process."
toolshed.g2.bx.psu.edu/repos/iuc/bio_hansel/bio_hansel/2.6.1+galaxy0	"*
*
*
*
*
*
*
*
 bio_hansel - Heidelberg And eNteritidis Snp ELucidation 
*
*
*
*
*
*
*
*
 Subtype 
Salmonella enterica
 subsp. enterica serovar Heidelberg and Enteritidis genomes using 
in-silico
 33 bp k-mer SNP subtyping schemes developed by Genevieve Labbe et al. Subtype 
Salmonella
 genome assemblies (FASTA files) and/or whole-genome sequencing reads (FASTQ files)! Usage ===== 1) Enter your FASTA/FASTQ file(s) 2) Select which scheme you would like to use (e.g. heidelberg, enteritidis, or specify your own) 3) Click Execute For more information visit 
https://github.com/phac-nml/bio_hansel
 Example Usage ============= Analysis of a single FASTA file ------------------------------- Contents of 
results.tab
: +------------+------------+-------------+------------------------------------------------+---------------------------------------------------------------+-------------------------+-----------------------+----------------------+----------------------------+---------------------------+---------------------------------+--------------------------+--------------------------------+------------+ | sample | scheme | subtype | all_subtypes | tiles_matching_subtype | are_subtypes_consistent | inconsistent_subtypes | n_tiles_matching_all | n_tiles_matching_all_total | n_tiles_matching_positive | n_tiles_matching_positive_total | n_tiles_matching_subtype | n_tiles_matching_subtype_total | file_path | +------------+------------+-------------+------------------------------------------------+---------------------------------------------------------------+-------------------------+-----------------------+----------------------+----------------------------+---------------------------+---------------------------------+--------------------------+--------------------------------+------------+ | file.fasta | heidelberg | 2.2.2.2.1.4 | 2; 2.2; 2.2.2; 2.2.2.2; 2.2.2.2.1; 2.2.2.2.1.4 | 1037658-2.2.2.2.1.4; 2154958-2.2.2.2.1.4; 3785187-2.2.2.2.1.4 | True | | 202 | 202 | 17 | 17 | 3 | 3 | file.fasta | +------------+------------+-------------+------------------------------------------------+---------------------------------------------------------------+-------------------------+-----------------------+----------------------+----------------------------+---------------------------+---------------------------------+--------------------------+--------------------------------+------------+ Contents of 
match_results.tab
: +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | tilename | stitle | pident | length | mismatch | gapopen | qstart | qend | sstart | send | evalue | bitscore | qlen | slen | seq | coverage | is_trunc | refposition | subtype | is_pos_tile | sample | file_path | scheme | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | 775920-2.2.2.2 | NODE_2_length_512016_cov_46.4737_ID_3 | 100 | 33 | 0 | 0 | 1 | 33 | 474875 | 474907 | 2E-11 | 62.1 | 33 | 512016 | GTTCAGGTGCTACCGAGGATCGTTTTTGGTGCG | 1 | False | 775920 | 2.2.2.2 | True | out | file.fasta | heidelberg | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | negative3305400-2.1.1.1 | NODE_3_length_427905_cov_48.1477_ID_5 | 100 | 33 | 0 | 0 | 1 | 33 | 276235 | 276267 | 2E-11 | 62.1 | 33 | 427905 | CATCGTGAAGCAGAACAGACGCGCATTCTTGCT | 1 | False | negative3305400 | 2.1.1.1 | False | out | file.fasta | heidelberg | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | negative3200083-2.1 | NODE_3_length_427905_cov_48.1477_ID_5 | 100 | 33 | 0 | 0 | 1 | 33 | 170918 | 170950 | 2E-11 | 62.1 | 33 | 427905 | ACCCGGTCTACCGCAAAATGGAAAGCGATATGC | 1 | False | negative3200083 | 2.1 | False | out | file.fasta | heidelberg | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | negative3204925-2.2.3.1.5 | NODE_3_length_427905_cov_48.1477_ID_5 | 100 | 33 | 0 | 0 | 1 | 33 | 175760 | 175792 | 2E-11 | 62.1 | 33 | 427905 | CTCGCTGGCAAGCAGTGCGGGTACTATCGGCGG | 1 | False | negative3204925 | 2.2.3.1.5 | False | out | file.fasta | heidelberg | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | negative3230678-2.2.2.1.1.1 | NODE_3_length_427905_cov_48.1477_ID_5 | 100 | 33 | 0 | 0 | 1 | 33 | 201513 | 201545 | 2E-11 | 62.1 | 33 | 427905 | AGCGGTGCGCCAAACCACCCGGAATGATGAGTG | 1 | False | negative3230678 | 2.2.2.1.1.1 | False | out | file.fasta | heidelberg | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | negative3233869-2.1.1.1.1 | NODE_3_length_427905_cov_48.1477_ID_5 | 100 | 33 | 0 | 0 | 1 | 33 | 204704 | 204736 | 2E-11 | 62.1 | 33 | 427905 | CAGCGCTGGTATGTGGCTGCACCATCGTCATTA | 1 | False | negative3233869 | 2.1.1.1.1 | False | out | file.fasta | heidelberg | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | negative3254229-2.2.3.1.3 | NODE_3_length_427905_cov_48.1477_ID_5 | 100 | 33 | 0 | 0 | 1 | 33 | 225064 | 225096 | 2E-11 | 62.1 | 33 | 427905 | CGCCACCACGCGGTTAGCGTCACGCTGACATTC | 1 | False | negative3254229 | 2.2.3.1.3 | False | out | file.fasta | heidelberg | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | negative3257074-2.2.1 | NODE_3_length_427905_cov_48.1477_ID_5 | 100 | 33 | 0 | 0 | 1 | 33 | 227909 | 227941 | 2E-11 | 62.1 | 33 | 427905 | CGGCAACCAGACCGACTACGCCGCCAAGCAGAC | 1 | False | negative3257074 | 2.2.1 | False | out | file.fasta | heidelberg | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | negative3264474-2.2.2.1.1.1 | NODE_3_length_427905_cov_48.1477_ID_5 | 100 | 33 | 0 | 0 | 1 | 33 | 235309 | 235341 | 2E-11 | 62.1 | 33 | 427905 | AATGGCGCCGATCGTCGCCAGATAACCGTTGCC | 1 | False | negative3264474 | 2.2.2.1.1.1 | False | out | file.fasta | heidelberg | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | negative3267927-2.2.2.2.2.1 | NODE_3_length_427905_cov_48.1477_ID_5 | 100 | 33 | 0 | 0 | 1 | 33 | 238762 | 238794 | 2E-11 | 62.1 | 33 | 427905 | AAAGAGAAATATGATGCCAGGCTGATACATGAC | 1 | False | negative3267927 | 2.2.2.2.2.1 | False | out | file.fasta | heidelberg | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | negative3278067-1.1 | NODE_3_length_427905_cov_48.1477_ID_5 | 100 | 33 | 0 | 0 | 1 | 33 | 248902 | 248934 | 2E-11 | 62.1 | 33 | 427905 | TGTGAGTAAGTTGCGCGATATTCTGCTGGATTC | 1 | False | negative3278067 | 1.1 | False | out | file.fasta | heidelberg | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | negative3299717-2.2.3.1.4 | NODE_3_length_427905_cov_48.1477_ID_5 | 100 | 33 | 0 | 0 | 1 | 33 | 270552 | 270584 | 2E-11 | 62.1 | 33 | 427905 | ATGCCGGACAGCAGGCGAAACTCGAACCGGATA | 1 | False | negative3299717 | 2.2.3.1.4 | False | out | file.fasta | heidelberg | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ | negative3373069-2.2.2.2.1.1 | NODE_3_length_427905_cov_48.1477_ID_5 | 100 | 33 | 0 | 0 | 1 | 33 | 344011 | 344043 | 2E-11 | 62.1 | 33 | 427905 | CTCTCCAGAAGATGAAGCCCGTGATGCGGCGCA | 1 | False | negative3373069 | 2.2.2.2.1.1 | False | out | file.fasta | heidelberg | +-----------------------------+---------------------------------------+--------+--------+----------+---------+--------+------+--------+--------+--------+----------+------+--------+-----------------------------------+----------+----------+-----------------+-------------+-------------+--------+------------+------------+ Next 196 lines omitted. Analysis of a single FASTQ readset ---------------------------------- Contents of 
results.tab
: +--------+------------+-------------+------------------------------------------------+------------------------------------------+-------------------------+-----------------------+----------------------+----------------------------+---------------------------+---------------------------------+--------------------------+--------------------------------+------------------------------------------+ | sample | scheme | subtype | all_subtypes | tiles_matching_subtype | are_subtypes_consistent | inconsistent_subtypes | n_tiles_matching_all | n_tiles_matching_all_total | n_tiles_matching_positive | n_tiles_matching_positive_total | n_tiles_matching_subtype | n_tiles_matching_subtype_total | file_path | +--------+------------+-------------+------------------------------------------------+------------------------------------------+-------------------------+-----------------------+----------------------+----------------------------+---------------------------+---------------------------------+--------------------------+--------------------------------+------------------------------------------+ | 564 | heidelberg | 2.2.1.1.1.1 | 2; 2.2; 2.2.1; 2.2.1.1; 2.2.1.1.1; 2.2.1.1.1.1 | 1983064-2.2.1.1.1.1; 4211912-2.2.1.1.1.1 | True | | 202 | 202 | 20 | 20 | 2 | 2 | forward.fastqsanger; reverse.fastqsanger | +--------+------------+-------------+------------------------------------------------+------------------------------------------+-------------------------+-----------------------+----------------------+----------------------------+---------------------------+---------------------------------+--------------------------+--------------------------------+------------------------------------------+ Contents of 
match_results.tab
: +-----------------------------------+------+--------+------------------------------------------+------------------+-------------+-----------+-------------+-------------------+------------+ | seq | freq | sample | file_path | tilename | is_pos_tile | subtype | refposition | is_kmer_freq_okay | scheme | +-----------------------------------+------+--------+------------------------------------------+------------------+-------------+-----------+-------------+-------------------+------------+ | ACGGTAAAAGAGGACTTGACTGGCGCGATTTGC | 68 | 564 | forward.fastqsanger; reverse.fastqsanger | 21097-2.2.1.1.1 | True | 2.2.1.1.1 | 21097 | True | heidelberg | +-----------------------------------+------+--------+------------------------------------------+------------------+-------------+-----------+-------------+-------------------+------------+ | AACCGGCGGTATTGGCTGCGGTAAAAGTACCGT | 77 | 564 | forward.fastqsanger; reverse.fastqsanger | 157792-2.2.1.1.1 | True | 2.2.1.1.1 | 157792 | True | heidelberg | +-----------------------------------+------+--------+------------------------------------------+------------------+-------------+-----------+-------------+-------------------+------------+ | CCGCTGCTTTCTGAAATCGCGCGTCGTTTCAAC | 67 | 564 | forward.fastqsanger; reverse.fastqsanger | 293728-2.2.1.1 | True | 2.2.1.1 | 293728 | True | heidelberg | +-----------------------------------+------+--------+------------------------------------------+------------------+-------------+-----------+-------------+-------------------+------------+ | GAATAACAGCAAAGTGATCATGATGCCGCTGGA | 91 | 564 | forward.fastqsanger; reverse.fastqsanger | 607438-2.2.1 | True | 2.2.1 | 607438 | True | heidelberg | +-----------------------------------+------+--------+------------------------------------------+------------------+-------------+-----------+-------------+-------------------+------------+ | CAGTTTTACATCCTGCGAAATGCGCAGCGTCAA | 87 | 564 | forward.fastqsanger; reverse.fastqsanger | 691203-2.2.1.1 | True | 2.2.1.1 | 691203 | True | heidelberg | +-----------------------------------+------+--------+------------------------------------------+------------------+-------------+-----------+-------------+-------------------+------------+ | CAGGAGAAAGGATGCCAGGGTCAACACGTAAAC | 33 | 564 | forward.fastqsanger; reverse.fastqsanger | 944885-2.2.1.1.1 | True | 2.2.1.1.1 | 944885 | True | heidelberg | +-----------------------------------+------+--------+------------------------------------------+------------------+-------------+-----------+-------------+-------------------+------------+ Next 200 lines omitted."
toolshed.g2.bx.psu.edu/repos/iuc/semibin/semibin/2.1.0+galaxy1	"Please note that there is a known issue with Semibin2 where results may be inconsistent across runs on different, despite a set seed. This may cause issues with reproducibility.
 For more information, see this [issue]{https://github.com/BigDataBiology/SemiBin/issues/186} on their repository: https://github.com/BigDataBiology/SemiBin/issues/186 What it does ============ SemiBin is a Semi-supervised siamese neural network for metagenomic binning Inputs ====== - Contigs in fasta for 1 or several samples from single or co-assembly - BAM with reads mapping to the contigs"
toolshed.g2.bx.psu.edu/repos/iuc/humann_split_stratified_table/humann_split_stratified_table/3.9+galaxy0	What it does ============ HUMAnN is a pipeline for efficiently and accuretly profiling the presence/absence and abundance of microbial pathways in a community from metagenomic or metatranscriptomic sequencing data. Read more about the tool: http://huttenhower.sph.harvard.edu/humann This tool splits a stratified table into two files (one stratified and one unstratified).
toolshed.g2.bx.psu.edu/repos/iuc/humann_unpack_pathways/humann_unpack_pathways/3.9+galaxy0	What it does ============ HUMAnN is a pipeline for efficiently and accuretly profiling the presence/absence and abundance of microbial pathways in a community from metagenomic or metatranscriptomic sequencing data. Read more about the tool: http://huttenhower.sph.harvard.edu/humann This tool unpacks the pathways to show the genes for each. It adds another level of stratification to the pathway abundance table by including the gene families (or EC) abundances.
toolshed.g2.bx.psu.edu/repos/iuc/valet/valet/1.0.0	"What it does
 VALET is a de novo pipeline for detecting all types of mis-assemblies in metagenomic data sets. Its primarily adapts the approaches developed in the context of isolate genomes. To avoid false positives and false negatives because of uneven depth of coverage, VALET bins contig by coverage before applying these methods. Possible break points in the assembly are found by examining regions, where a large number of parts of the reads are unable to align. To identify break points, VALET uses the first and last third of each unaligned read, called sister reads. The sister reads are aligned independently to the reference genome, and then regions where the sister reads align to nonadjacent segments of the genome are flagged as mis-assemblies. For more details about the tool, please check: https://github.com/marbl/VALET"
toolshed.g2.bx.psu.edu/repos/iuc/vapor/vapor/1.0.3+galaxy0	"What it does
 VAPOR is a tool for classification of Influenza samples from raw short read sequence data for downstream bioinformatics analysis. VAPOR works on a fasta file of full-length reference sequences for a given genome segment and a set of sequenced reads, and attempts to retrieve the reference that is closest to the sequenced strain. The 
-s
 option of the command-line tool for sub-sampling input reads is not exposed here since you can always build a workflow that preprocesses your reads to a (random) subsample. You can use this output as your reads file for VAPOR."
toolshed.g2.bx.psu.edu/repos/iuc/vsearch/vsearch_alignment/2.8.3.0	"What it does
 Pairwise alignments of all sequences. Alignment options (most searching options also apply) --allpairs_global FILENAME perform global alignment of all sequence pairs --alnout FILENAME filename for human-readable alignment output --acceptall output all pairwise alignments For details about this tool, please refer to the 
GitHub repository &lt;https://github.com/torognes/vsearch&gt;
 or the 
vsearch manual &lt;https://github.com/torognes/vsearch/releases/download/v2.8.3/vsearch_manual.pdf&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/vsearch/vsearch_chimera_detection/2.8.3.0	"What it does
 Sequence chimera detection based on a different scoring functions. Chimera detection options --abskew REAL min abundance ratio of parent vs chimera (2.0) --alignwidth INT width of alignment in uchimealn output (80) --chimeras FILENAME output chimeric sequences to file --db FILENAME reference database for --uchime_ref --dn REAL 'no' vote pseudo-count (1.4) --mindiffs INT minimum number of differences in segment (3) --mindiv REAL minimum divergence from closest parent (0.8) --minh REAL minimum score (0.28) --nonchimeras FILENAME output non-chimeric sequences to file --self exclude identical labels for --uchime_ref --selfid exclude identical sequences for --uchime_ref --uchime_denovo FILENAME detect chimeras de novo --uchime_ref FILENAME detect chimeras using a reference database --uchimealns FILENAME output chimera alignments to file --uchimeout FILENAME output to chimera info to tab-separated file --uchimeout5 make output compatible with uchime version 5 --xn REAL 'no' vote weight (8.0) For details about this tool, please refer to the 
GitHub repository &lt;https://github.com/torognes/vsearch&gt;
 or the 
vsearch manual &lt;https://github.com/torognes/vsearch/releases/download/v2.8.3/vsearch_manual.pdf&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/vsearch/vsearch_clustering/2.8.3.0	"What it does
 vsearch implements a single-pass, greedy star-clustering algorithm, similar to the algorithms implemented in usearch, DNAclust and sumaclust for example. Clustering options (most searching options also apply) --centroids FILENAME output centroid sequences to FASTA file --cluster_fast FILENAME cluster sequences after sorting by length --cluster_size FILENAME cluster sequences after sorting by abundance --cluster_smallmem FILENAME cluster already sorted sequences (see -usersort) --clusters STRING output each cluster to a separate FASTA file --consout FILENAME output cluster consensus sequences to FASTA file --cons_truncate do not ignore terminal gaps in MSA for consensus --id REAL reject if identity lower --iddef INT id definition, 0-4=CD-HIT,all,int,MBL,BLAST (2) --msaout FILENAME output multiple seq. alignments to FASTA file --qmask seqs with dust, soft or no method (dust) --sizein propagate abundance annotation from input --sizeout write cluster abundances to centroid file --strand cluster using plus or both strands (plus) --uc FILENAME filename for UCLUST-like output --usersort indicate sequences not presorted by length For details about this tool, please refer to the 
GitHub repository &lt;https://github.com/torognes/vsearch&gt;
 or the 
vsearch manual &lt;https://github.com/torognes/vsearch/releases/download/v2.8.3/vsearch_manual.pdf&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/vsearch/vsearch_dereplication/2.8.3.0	"What it does
 Merge strictly identical sequences contained in filename. Identical sequences are defined as having the same length and the same string of nucleotides (case insensitive, T and U are considered the same). Dereplication options --derep_fulllength FILENAME dereplicate sequences in the given FASTA file --maxuniquesize INT maximum abundance for output from dereplication --minuniquesize INT minimum abundance for output from dereplication --output FILENAME output FASTA file --sizein read abundance annotation from input --sizeout write abundance annotation to output --strand dereplicate ""plus"" or ""both"" strands (plus) --topn INT output just the n most abundant sequences --uc FILENAME filename for UCLUST-like output For details about this tool, please refer to the 
GitHub repository &lt;https://github.com/torognes/vsearch&gt;
 or the 
vsearch manual &lt;https://github.com/torognes/vsearch/releases/download/v2.8.3/vsearch_manual.pdf&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/vsearch/vsearch_masking/2.8.3.0	"What it does
 An input sequence can be composed of lower- or uppercase nucleotides. Lowercase nucleotides are silently set to uppercase before masking, unless the −−qmask soft option is used. Here are the results of combined masking options −−qmask (or −−dbmask for database sequences) and −−hardmask, assuming each input sequences contains both lower and uppercase nucleotides: ===== ======== ================================================ qmask hardmask action ===== ======== ================================================ none off no masking, all symbols uppercased none on no masking, all symbols uppercased dust off masked symbols lowercased, others uppercased dust on masked symbols changed to Ns, others uppercased soft off lowercase symbols masked, no case changes soft on lowercase symbols masked and changed to Ns ===== ======== ================================================ Masking options --hardmask mask by replacing with N instead of lower case --maskfasta FILENAME mask sequences in the given FASTA file --output FILENAME output to specified FASTA file --qmask mask seqs with ""dust"", ""soft"" or ""none"" method (dust) For details about this tool, please refer to the 
GitHub repository &lt;https://github.com/torognes/vsearch&gt;
 or the 
vsearch manual &lt;https://github.com/torognes/vsearch/releases/download/v2.8.3/vsearch_manual.pdf&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/vsearch/vsearch_search/2.8.3.1	"What it does
 Sequence search based on vsearch. 
Available fields in user defined tabular output
 ========= ================ Key Description ========= ================ aln Print a string of M (match), D (delete, i.e. a gap in the query) and I (insert, i.e. a gap in the target) representing the pairwise alignment. Empty field if there is no alignment. alnlen Print the length of the query-target alignment (number of columns). The field is set to 0 if there is no alignment. bits Bit score (not computed for nucleotide alignments). Always set to 0. caln Compact representation of the pairwise alignment using the CIGAR format (Compact Idiosyncratic Gapped Alignment Report): M (match), D (deletion) and I (insertion). Empty field if there is no alignment. evalue E-value (not computed for nucleotide alignments). Always set to -1. exts Number of columns containing a gap extension (zero or positive integer value). gaps Number of columns containing a gap (zero or positive integer value). id Percentage of identity (real value ranging from 0.0 to 100.0). The percentage identity is defined as 100 * (matching columns) / (alignment length - terminal gaps). id0 CD-HIT definition of the percentage of identity (real value ranging from 0.0 to 100.0) using the length of the shortest sequence in the pairwise alignment as denominator: 100 * (matching columns) / (shortest sequence length). id1 The percentage of identity (real value ranging from 0.0 to 100.0) is defined as the edit distance: 100 * (matching columns) / (alignment length). id2 The percentage of identity (real value ranging from 0.0 to 100.0) is defined as the edit distance, excluding terminal gaps. The field id2 is an alias for the field id. id3 Marine Biological Lab definition of the percentage of identity (real value ranging from 0.0 to 100.0), counting each extended gap (internal or terminal) as a single difference and using the length of the longest sequence in the pairwise alignment as denominator: 100 * (1.0 - [(mismatches + gaps) / (longest sequence length)]). id4 BLAST definition of the percentage of identity (real value ranging from 0.0 to 100.0), equivalent to −−iddef 2 in a context of global pairwise alignment. ids Number of matches in the alignment (zero or positive integer value). mism Number of mismatches in the alignment (zero or positive integer value). opens Number of columns containing a gap opening (zero or positive integer value). pairs Number of columns containing only nucleotides. That value corresponds to the length of the alignment minus the gap-containing columns (zero or positive integer value). pctgaps Number of columns containing gaps expressed as a percentage of the alignment length (real value ranging from 0.0 to 100.0). pctpv Percentage of positive columns. When working with nucleotide sequences, this is equivalent to the percentage of matches (real value ranging from 0.0 to 100.0). pv Number of positive columns. When working with nucleotide sequences, this is equivalent to the number of matches (zero or positive integer value). qcov Fraction of the query sequence that is aligned with the target sequence (real value ranging from 0.0 to 100.0). The query coverage is computed as 100.0 * (matches + mismatches) / query sequence length. Internal or terminal gaps are not taken into account. The field is set to 0.0 if there is no alignment. qframe Query frame (-3 to +3). That field only concerns coding sequences and is not computed by vsearch. Always set to +0. qhi Last nucleotide of the query aligned with the target. Always equal to the length of the pairwise alignment. The field is set to 0 if there is no alignment. qihi Last nucleotide of the query aligned with the target (ignoring terminal gaps). Nucleotide numbering starts from 1. The field is set to 0 if there is no alignment. qilo First nucleotide of the query aligned with the target (ignoring initial gaps). Nucleotide numbering starts from 1. The field is set to 0 if there is no alignment. ql Query sequence length (positive integer value). The field is set to 0 if there is no alignment. qlo First nucleotide of the query aligned with the target. Always equal to 1 if there is an alignment, 0 otherwise. qrow Print the sequence of the query segment as seen in the pairwise alignment (i.e. with gap insertions if need be). Empty field if there is no alignment. qs Query segment length. Always equal to query sequence length. qstrand Query strand orientation (+ or - for nucleotide sequences). Empty field if there is no alignment. query Query label. raw Raw alignment score (negative, null or positive integer value). The score is the sum of match rewards minus mismatch penalties, gap openings and gap extensions. The field is set to 0 if there is no alignment. target Target label. The field is set to ""*"" if there is no alignment. tcov Fraction of the target sequence that is aligned with the query sequence (real value rang-ing from 0.0 to 100.0). The target coverage is computed as 100.0 * (matches + mis-matches) / target sequence length. Internal or terminal gaps are not taken into account. The field is set to 0.0 if there is no alignment. tframe Target frame (-3 to +3). That field only concerns coding sequences and is not computed by vsearch. Always set to +0. thi Last nucleotide of the target aligned with the query. Always equal to the length of the pairwise alignment. The field is set to 0 if there is no alignment. tihi Last nucleotide of the target aligned with the query (ignoring terminal gaps). Nucleotide numbering starts from 1. The field is set to 0 if there is no alignment. tilo First nucleotide of the target aligned with the query (ignoring initial gaps). Nucleotide numbering starts from 1. The field is set to 0 if there is no alignment. tl Target sequence length (positive integer value). The field is set to 0 if there is no alignment. tlo First nucleotide of the target aligned with the query. Always equal to 1 if there is an alignment, 0 otherwise. trow Print the sequence of the target segment as seen in the pairwise alignment (i.e. with gap insertions if need be). Empty field if there is no alignment. ts Target segment length. Always equal to target sequence length. The field is set to 0 if there is no alignment. tstrand Target strand orientation (+ or - for nucleotide sequences). Always set to ""+"", so reverse strand matches have tstrand ""+"" and qstrand ""-"". Empty field if there is no alignment. ========= ================ Searching options --alnout FILENAME filename for human-readable alignment output --blast6out FILENAME filename for blast-like tab-separated output --db FILENAME filename for FASTA formatted database for search --dbmask mask db with ""dust"", ""soft"" or ""none"" method (dust) --dbmatched FILENAME FASTA file for matching database sequences --dbnotmatched FILENAME FASTA file for non-matching database sequences --fastapairs FILENAME FASTA file with pairs of query and target --fulldp full dynamic programming alignment (always on) --gapext STRING penalties for gap extension (2I/1E) --gapopen STRING penalties for gap opening (20I/2E) --hardmask mask by replacing with N instead of lower case --id REAL reject if identity lower --iddef INT id definition, 0-4=CD-HIT,all,int,MBL,BLAST (2) --idprefix INT reject if first n nucleotides do not match --idsuffix INT reject if last n nucleotides do not match --leftjust reject if terminal gaps at alignment left end --match INT score for match (2) --matched FILENAME FASTA file for matching query sequences --maxaccepts INT number of hits to accept and show per strand (1) --maxdiffs INT reject if more substitutions or indels --maxgaps INT reject if more indels --maxhits INT maximum number of hits to show (unlimited) --maxid REAL reject if identity higher --maxqsize INT reject if query abundance larger --maxqt REAL reject if query/target length ratio higher --maxrejects INT number of non-matching hits to consider (32) --maxsizeratio REAL reject if query/target abundance ratio higher --maxsl REAL reject if shorter/longer length ratio higher --maxsubs INT reject if more substitutions --mid REAL reject if percent identity lower, ignoring gaps --mincols INT reject if alignment length shorter --minqt REAL reject if query/target length ratio lower --minsizeratio REAL reject if query/target abundance ratio lower --minsl REAL reject if shorter/longer length ratio lower --mintsize INT reject if target abundance lower --mismatch INT score for mismatch (-4) --notmatched FILENAME FASTA file for non-matching query sequences --output_no_hits output non-matching queries to output files --qmask mask query with ""dust"", ""soft"" or ""none"" method (dust) --query_cov REAL reject if fraction of query seq. aligned lower --rightjust reject if terminal gaps at alignment right end --rowlen INT width of alignment lines in alnout output (64) --self reject if labels identical --selfid reject if sequences identical --sizeout write abundance annotation to output --strand search ""plus"" or ""both"" strands (plus) --target_cov REAL reject if fraction of target seq. aligned lower --top_hits_only output only hits with identity equal to the best --uc FILENAME filename for UCLUST-like output --uc_allhits show all, not just top hit with uc output --usearch_global FILENAME filename of queries for global alignment search --userfields STRING fields to output in userout file --userout FILENAME filename for user-defined tab-separated output --weak_id REAL include aligned hits with >= id; continue search --wordlength INT length of words for database index 3-15 (8) For details about this tool, please refer to the 
GitHub repository &lt;https://github.com/torognes/vsearch&gt;
 or the 
vsearch manual &lt;https://github.com/torognes/vsearch/releases/download/v2.8.3/vsearch_manual.pdf&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/vsearch/vsearch_shuffling/2.8.3.0	"What it does
 Sequence shuffling to obtain new random sequences. Shuffling options --output FILENAME output to specified FASTA file --seed INT seed for PRNG, zero to use random data source (0) --shuffle FILENAME shuffle order of sequences pseudo-randomly --topn INT output just first n sequences For details about this tool, please refer to the 
GitHub repository &lt;https://github.com/torognes/vsearch&gt;
 or the 
vsearch manual &lt;https://github.com/torognes/vsearch/releases/download/v2.8.3/vsearch_manual.pdf&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/vsearch/vsearch_sorting/2.8.3.0	"What it does
 Fasta entries are sorted by decreasing abundance (−−sortbysize) or sequence length (−−sort- bylength). To obtain a stable sorting order, ties are sorted by decreasing abundance and label increasing alpha-numerical order (−−sortbylength), or just by label increasing alpha-numerical order (−−sortbysize). Label sorting assumes that all sequences have unique labels. The same applies to the automatic sorting performed during chimera checking (−−uchime_denovo), derepli- cation (−−derep_fulllength), and clustering (−−cluster_fast and −−cluster_size). Sorting options --maxsize INT maximum abundance for sortbysize --minsize INT minimum abundance for sortbysize --output FILENAME output FASTA file --relabel STRING relabel with this prefix string after sorting --sizeout add abundance annotation to output --sortbylength FILENAME sort sequences by length in given FASTA file --sortbysize FILENAME abundance sort sequences in given FASTA file --topn INT output just top n seqs after sorting For details about this tool, please refer to the 
GitHub repository &lt;https://github.com/torognes/vsearch&gt;
 or the 
vsearch manual &lt;https://github.com/torognes/vsearch/releases/download/v2.8.3/vsearch_manual.pdf&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/vegan_diversity/vegan_diversity/2.4-3	Calculate Diversity index using vegan and selected method.
toolshed.g2.bx.psu.edu/repos/iuc/vegan_fisher_alpha/vegan_fisher_alpha/2.4-3	Calculate Diversity index using vegan and selected method.
toolshed.g2.bx.psu.edu/repos/iuc/vegan_rarefaction/vegan_rarefaction/2.4-3	Gives the expected species richness in random subsamples of size sample from the community. The size of sample should be smaller than total community size, but the function will work for larger sample as well (with a warning) and return non-rarefied species richness (and standard error = 0). Rarefaction can be performed only with genuine counts of individuals. The function rarefy is based on Hurlbert’s (1971) formulation, and the standard errors on Heck et al. (1975). Returns probabilities that species occur in a rarefied community of size sample. Draws a rarefaction curve for each row of the input data. The rarefaction curves are evaluated using the interval of step sample sizes, always including 1 and total sample size. If sample is specified, a vertical line is drawn at sample with horizontal lines for the rarefied species richnesses. .. class:: warningmark When subsampling by community size, slope of the rarefaction curve and species probabilities are computed using community size-1
toolshed.g2.bx.psu.edu/repos/iuc/ampvis2_load/ampvis2_load/2.8.11+galaxy0	"What it does ============ This tool reads an OTU or ASV table and corresponding sample metadata, and returns a RDS data set for use in all ampvis2 tools. It is therefore required to load data with this tool before any other ampvis2 tools can be used. The Galaxy tool calls the 
amp_load &lt;https://kasperskytte.github.io/ampvis2/reference/amp_load.html&gt;
 function of the ampvis2 package. This function validates and corrects the provided data frames in different ways to make it suitable for the rest of the ampvis2 tools. It is important that the provided data sets match the requirements as described in the following to work properly. Input ===== 
The OTU-table
 contains information about the OTU/ASVs, their read counts in each sample, and optionally their assigned taxonomy. The OTU table can be given as - Tabular data set - BIOM version (1 and 2) Metadata and taxonomy in the tabular or BIOM files that are given via the 
OTU table
 parameter can is overwritten if by data presented via the 
Sample metadata
 or 
Taxonomy table
 parameters. If given in tabular format the provided OTU-table must be a table with the following requirements: - The rows are OTU IDs and the columns are samples. - The OTU IDs are by default expected to be in a column called ""OTU"", ""ASV"", or ""#OTU ID"". For data using an empty header for the OTU/ASV colum enable the option 
OTU/ASV column has empty header
 (this allows to process data as produced e.g. by dada2). - The column names of the table are the sample IDs, exactly matching those in the metadata - The last 7 columns are optionally the corresponding taxonomy assigned to the OTUs, named ""Kingdom"", ""Phylum"", ""Class"", ""Order"", ""Family"", ""Genus"", ""Species"". If the ASV IDs are actually the ASV Sequences then enabling 
ASV identifiers are the ASV sequences
 will rename the identifiers to ASV1, ASV2,... (and save the sequences in the ampvis2 object). Generally avoid special characters and spaces in row- and column names. The OTU table can also contain the taxonomic information in additional columns: Kingdom, Phylum, Class, Order, Family, Genus. Check 
here &lt;https://biom-format.org/&gt;
 for information on the BIOM formats. 
The metadata
 contains additional information about the samples, for example where each sample was taken, date, pH, treatment etc, which is used to compare and group the samples during analysis. The amount of information in the metadata is unlimited, it can contain any number of columns (variables), however there are a few requirements: - The sample IDs must be in the first column. The sample IDs must match exactly to those in the OTU-table. Any unmatched samples between the otutable and metadata will be removed with a warning. - Generally avoid special characters and spaces in row- and column names. By default the data types of metadata columns are guessed with 
readr::type_convert
. The guessed column types can be seen in the last (4th) column of the 
metadata list
 output and also stdout of the tool. Guessing of data types can be disabled using the parameter 
Guess metadata column types
. If disabled matadata from separate tabular input is treated as character data, and if loaded from biom files that data is used as is. Metadata types can be set manually using the tool 
ampvis2: set metadata
 Dates should be given in the format 
YYYY-MM-DD
 (Y: year, M: month, D: day). In addition to the RDS data set a metadata (resp. taxonomy) list data set is returned if metadata (resp. taxonomic information) is given to this tool. It contains restructured metadata (taxonomic information) that is used in downstream ampvis2 Galaxy tools in order to select metadata / metadata values (resp. taxonomic levels). 
Taxonomy
 is a tabular data set with 7 columns and one row per ASV/OTU: - the 1st column is identical to the 1st column of the OTU table parameter - the remaining columns contain data for Kingdom, Phylum, Class, Order, Family, Genus Note that the taxonomic information can also be embedded in the OTU table. 
Tree
 a tree with branch lengths in Newick format. This is needed / usefull only if the data is used as input of: 
ampvis2: ordination plot
 for ordination methods NNDS / MMDS with (un)weighted UniFrac distances. Note that the loaded tree is also filtered by the 
ampvis2: subset ...
 tools. 
Fasta
 a fasta file containing the sequences of the OTUs. Note that this information is only used in 
ampvis2: export fasta
. If the OTU table is modified by 
ampvis2: mergereplicates
 or the 
ampvis2: subset ...
 tools this might be useful to obtain a filtered list of sequences. Output ====== 
RDS
 The main output of the tool is an RDS data set that contains the R representation of the ampvis2 object containing the provided data (OTU table, metadata, taxonomy, phylogenetic tree, and fasta). 
List files
 Summarize the metadata and taxonomy information: - the taxonomy list file lists all taxa in a 1 column tabular data set - the metadata list file lists the Metadata variables (column 1), and the corresponding available metadata values (column 2), if the variable is the SampleID (column 3), and the data type of the corresponding metadata variable (column 4) These files are auxilliary files that are needed in downstream 
ampvis2
 Galaxy tools to allow selecting metadata and taxonomy. They are not passed to the underlying R functions. Note that, if the no taxonomy (or metadata) is given then the underlying 
ampvis2
 R function adds dummy taxonomy (resp. metadata). In this case the output of the list datasets can be disabled with the 
Output list data sets
 parameter."
toolshed.g2.bx.psu.edu/repos/devteam/cd_hit_dup/cd_hit_dup/0.0.1	"What it does
 cd-hit-dup is a simple tool for removing duplicates from sequencing reads, with optional step to detect and remove chimeric reads. 
Options
 cd-hit-dup provides a number of options to tune how the duplicates are removed:: -d Description length (default 0, truncate at the first whitespace character) -u Length of prefix to be used in the analysis (default 0, for full/maximum length) -m Match length (true/false, default true) -e Maximum number/percent of mismatches allowed -f Filter out chimeric clusters (true/false, default false) -s Minimum length of common sequence shared between a chimeric read and each of its parents (default 30, minimum 20) -a Abundance cutoff (default 1 without chimeric filtering, 2 with chimeric filtering) -b Abundance ratio between a parent read and a chimeric read (default 1) -p Dissimilarity control for chimeric filtering (default 1)"
toolshed.g2.bx.psu.edu/repos/iuc/checkm2/checkm2/1.1.0+galaxy0	Unlike CheckM1, CheckM2 has universally trained machine learning models it applies regardless of taxonomic lineage to predict the completeness and contamination of genomic bins. This allows it to incorporate many lineages in its training set that have few - or even just one - high-quality genomic representatives, by putting it in the context of all other organisms in the training set. As a result of this machine learning framework, CheckM2 is also highly accurate on organisms with reduced genomes or unusual biology, such as the Nanoarchaeota or Patescibacteria. CheckM2 uses two distinct machine learning models to predict genome completeness. The 'general' gradient boost model is able to generalize well and is intended to be used on organisms not well represented in GenBank or RefSeq (roughly, when an organism is novel at the level of order, class or phylum). The 'specific' neural network model is more accurate when predicting completeness of organisms more closely related to the reference training set (roughly, when an organism belongs to a known species, genus or family). CheckM2 uses a cosine similarity calculation to automatically determine the appropriate completeness model for each input genome, but you can also force the use of a particular completeness model, or get the prediction outputs for both. There is only one contamination model (based on gradient boost) which is applied regardless of taxonomic novelty and works well across all cases.
toolshed.g2.bx.psu.edu/repos/iuc/drep_compare/drep_compare/3.6.2+galaxy1	"dRep compare
 
dRep &lt;https://drep.readthedocs.io/en/latest/overview.html&gt;
 performs rapid pair-wise comparison of genome sets. 
dRep compare &lt;https://drep.readthedocs.io/en/latest/overview.html#genome-comparison&gt;
 can rapidly and accurately compare a list of genomes in a pair-wise manner. This allows identification of groups of organisms that share similar DNA content in terms of Average Nucleotide Identity (ANI). dRep performs this in two steps: - first with a rapid primary algorithm (Mash) - second with a more sensitive algorithm (ANIm). We can't just use Mash because, while incredibly fast, it is not robust to genome incompletenss (see 
Choosing parameters &lt;https://drep.readthedocs.io/en/latest/choosing_parameters.html&gt;
 and 
Module Descriptions &lt;https://drep.readthedocs.io/en/latest/module_descriptions.html&gt;
) and only provides an “estimate” of ANI. ANIm is robust to genome incompleteness and is more accurate, but too slow to perform pair-wise comparisons of longer genome lists. dRep first compares all genomes using Mash, and then only runs the secondary algorithm (ANIm or gANI) on sets of genomes that have at least 90% Mash ANI. This results in a great decrease in the number of (slow) secondary comparisons that need to be run while maintaining the sensitivity of ANIm. 
INPUTS
 - Genome sets in fasta format. 
OUTPUTS
 - 
Figures &lt;https://drep.readthedocs.io/en/latest/example_output.html#figures&gt;
 that show the relationship of the Genome inputs. - 
Warnings &lt;https://drep.readthedocs.io/en/latest/example_output.html#warnings&gt;
 report two things: de-replicated genome similarity and secondary clusters that were almost different."
toolshed.g2.bx.psu.edu/repos/iuc/drep_dereplicate/drep_dereplicate/3.6.2+galaxy1	"A CSV dataset that must contain: [ ""genome""(history dataset name of .fasta dataset of that genome), ""completeness""(0-100 value for completeness of the genome), ""contamination""(0-100 value of the contamination of the genome)]"
toolshed.g2.bx.psu.edu/repos/iuc/dada2_assigntaxonomyaddspecies/dada2_assignTaxonomyAddspecies/1.34.0+galaxy1	"Description ........... This tool implements dada2's assignTaxonomy and assignSpecies functions. - assignTaxonomy assigns taxonomy to the sequence variants. The DADA2 package provides a native implementation of the naive Bayesian classifier method for this purpose (see Wang et al. 2007, kmer size 8 and 100 bootstrap replicates). The assignTaxonomy function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least minBoot bootstrap confidence. Properly formatted reference files for several popular taxonomic databases are available http://benjjneb.github.io/dada2/training.html - assignSpecies makes species level assignments based on exact matching between ASVs and sequenced reference strains. Recent analysis suggests that exact matching (or 100% identity) is the only appropriate way to assign species to 16S gene fragments. Currently, species-assignment training fastas are available for the Silva and RDP 16S databases. Usage ..... 
Input
 - A list of sequences contained in the results of removeBimeraDenovo or sequenceTable (note that also the results of dada, and mergePairs are accepted). - Reference data bases for taxonomic and species/genus level assignment. Several cached data bases can be chosen (ask your Galaxy admin if they are missing). For using custom data bases see below. 
Output
 - A table containing the assigned taxonomies exceeding the minBoot level of bootstrapping confidence. Rows correspond to the provided sequences, columns to the taxonomic levels. NA indicates that the sequence was not consistently classified at that level at the minBoot threshold. - Optionally two columns for the genus and species taxonomic levels can be added. NA indicates that the sequence was not classified at that level. - If outputBootstraps checked, a table containing the assigned taxonomies (named ""taxa"") and the bootstrap values (named ""boot"") will be returned. Overview ........ The intended use of the dada2 tools for paired sequencing data is shown in the following image. .. image:: pairpipe.png Note: In particular for the analysis of paired collections the collections should be sorted lexicographical before the analysis. For single end data you the steps ""Unzip collection"" and ""mergePairs"" are not necessary. More information may be found on the dada2 homepage:: https://benjjneb.github.io/dada2/index.html (in particular tutorials) or the documentation of dada2's R package https://bioconductor.org/packages/release/bioc/html/dada2.html (in particular the pdf which contains the full documentation of all parameters) Custom Reference data sets .......................... For ** taxonomy assignment ** the following is needed: - a reference fasta data base - a comma separated list of taxonomic ranks present in the reference data base The reference fasta data base for taxonomic assignment (fasta or compressed fasta) needs to encode the taxonomy corresponding to each sequence in the fasta header lines in the following fashion (note, the second sequence is not assigned down to level 6): :: >Level1;Level2;Level3;Level4;Level5;Level6; ACCTAGAAAGTCGTAGATCGAAGTTGAAGCATCGCCCGATGATCGTCTGAAGCTGTAGCATGAGTCGATTTTCACATTCAGGGATACCATAGGATAC >Level1;Level2;Level3;Level4;Level5; CGCTAGAAAGTCGTAGAAGGCTCGGAGGTTTGAAGCATCGCCCGATGGGATCTCGTTGCTGTAGCATGAGTACGGACATTCAGGGATCATAGGATAC The list of required taxonomic ranks could be for instance: ""Kingdom,Phylum,Class,Order,Family,Genus"" The reference data base for ** species assignment ** is a fasta file (or compressed fasta file), with the id line formatted as follows: :: >ID Genus species ACCTAGAAAGTCGTAGATCGAAGTTGAAGCATCGCCCGATGATCGTCTGAAGCTGTAGCATGAGTCGATTTTCACATTCAGGGATACCATAGGATAC >ID Genus species CGCTAGAAAGTCGTAGAAGGCTCGGAGGTTTGAAGCATCGCCCGATGGGATCTCGTTGCTGTAGCATGAGTACGGACATTCAGGGATCATAGGATAC"
toolshed.g2.bx.psu.edu/repos/iuc/dada2_dada/dada2_dada/1.34.0+galaxy1	"Description ........... The dada function takes as input amplicon sequencing reads and returns the inferred composition of the sample (or samples). Put another way, dada removes all sequencing errors to reveal the members of the sequenced community. Usage ..... 
Input:
 - A number of fastq(.gz) files (given as collection or multiple data sets) - An dada2_errorrates data set computed with learnErrors You can decide to compute the data jointly or in batches. - Jointly (Process ""samples in batches""=no): A single Galaxy job is started that processes all fastq data sets jointly. You may chose different pooling strategies: if the started dada job processes the samples individually, pooled, or pseudo pooled. - In batches (Process ""samples in batches""=yes): A separate Galaxy job is started for earch fastq data set. This is equivalent to joint processing and choosing to process samples individually. While the single dada job (in case of joint processing) can use multiple cores on one compute node, batched processing distributes the work on a number of jobs (equal to the number of input fastq data sets) where each can use multiple cores. Hence, if you intend to or need to process the data sets individually, batched processing is more efficient -- in particular if Galaxy has access to a larger number of compute resources. A typical use case of individual processing of the samples are large data sets for which the pooled strategy needs to much time or memory. Pseudo-pooling is recommended for those interested in detecting singleton ASVs in their samples 
Output
: a data set of type dada2_dada (which is a RData file containing the output of dada2's dada function). The output of this tool can serve as input for 
dada2: mergePairs
, 
dada2: removeBimeraDinovo
, and ""dada2: makeSequenceTable"" Details ....... Briefly, dada implements a statistical test for the notion that a specific sequence was seen too many times to have been caused by amplicon errors from currently inferred sample sequences. Overly abundant sequences are used as the seeds of new partitions of sequencing reads, and the final set of partitions is taken to represent the denoised composition of the sample. A more detailed explanation of the algorithm is found in the dada2 puplication (see below) and https://doi.org/10.1186/1471-2105-13-283. dada depends on a parametric error model of substitutions. Thus the quality of its sample inference is affected by the accuracy of the estimated error rates. All comparisons between sequences performed by dada depend on pairwise alignments. This step is the most computationally intensive part of the algorithm, and two alignment heuristics have been implemented in dada for speed: A kmer-distance screen and banded Needleman-Wunsch alignmemt. Overview ........ The intended use of the dada2 tools for paired sequencing data is shown in the following image. .. image:: pairpipe.png Note: In particular for the analysis of paired collections the collections should be sorted lexicographical before the analysis. For single end data you the steps ""Unzip collection"" and ""mergePairs"" are not necessary. More information may be found on the dada2 homepage:: https://benjjneb.github.io/dada2/index.html (in particular tutorials) or the documentation of dada2's R package https://bioconductor.org/packages/release/bioc/html/dada2.html (in particular the pdf which contains the full documentation of all parameters)"
toolshed.g2.bx.psu.edu/repos/iuc/dada2_filterandtrim/dada2_filterAndTrim/1.34.0+galaxy1	"Description ........... Filters and trims a FASTQ dataset (can be compressed) based on several user-definable criteria, and outputs a compressed FASTQ data set containing those trimmed reads which passed the filters. For paired end data forward and reverse FASTQ datasets can be provided as pair of FASTQ datasets (or two separate data sets), in which case filtering is performed on the forward and reverse reads independently, and both reads must pass for the read pair to be in the output. Usage ..... 
Input
 is a FASTQ dataset (or a pair in case of paired end data) containing all reads of a sample. It is suggested to organize them in a (paired) collection (in particular if you have multiple samples). 
Output
 is a (paired) collection of filtered and trimmed paired FASTQ datasets (again one data set or pair per sample). Upstream dada2 tools are 
dada2: learnErrorRates
 and 
dada2: dada
. Note that these tools do not work on paired end data. So, if you have paired end data you need to split the generated paired collection into one containing the forward reads and one containing the reverse reads. This can be done by the 
unzip collection
 tool. An additional tabular output gives the number of reads before and after trimming. This can data set can be used as input for 
dada2: sequence counts
 to track the sequence counts for each sample through all dada2 pipeline step. Details ....... 
Trimming and filtering
: - Truncation of the read length is enforced after trimming of the right end. - The long read filter is applied before trimming and the short read filter after trimming. - For details on the calculation of the number of expected errors see also Calahan et al. 2016 
String present at the start of valid reads
 (orient.fwd): This string is compared to the start of each read, and the reverse complement of each read. If it exactly matches the start of the read, the read is kept. If it exactly matches the start of the reverse complement read, the read is reverse-complemented and kept. Otherwise the read if filtered out. For paired reads, the string is compared to the start of the forward and reverse reads, and if it matches the start of the reverse read the reads are swapped and kept. The primary use of this parameter is to unify the orientation of amplicon sequencing libraries that are a mixture of forward and reverse orientations, and that include the forward primer on the reads. 
Low complexity filter kmer threshold""
 If greater than 0, reads with an effective number of kmers less than this value will be removed. The effective number of kmers is determined as a Shannon information approximation. The default kmer-size is 2, and therefore perfectly random sequences will approachan effective kmer number of 16 = 4 (nucleotides) ^ 2 (kmer size). Notes ..... This step may be replaced by alternative tools to filter and trim short read data if the following is ensured: - For paired end data unpaired reads must be removed. - There must not be a read containing a non-canonical nucleotide (N). Overview ........ The intended use of the dada2 tools for paired sequencing data is shown in the following image. .. image:: pairpipe.png Note: In particular for the analysis of paired collections the collections should be sorted lexicographical before the analysis. For single end data you the steps ""Unzip collection"" and ""mergePairs"" are not necessary. More information may be found on the dada2 homepage:: https://benjjneb.github.io/dada2/index.html (in particular tutorials) or the documentation of dada2's R package https://bioconductor.org/packages/release/bioc/html/dada2.html (in particular the pdf which contains the full documentation of all parameters)"
toolshed.g2.bx.psu.edu/repos/iuc/dada2_learnerrors/dada2_learnErrors/1.34.0+galaxy1	"Description ........... Error rates are learned by alternating between sample inference and error rate estimation until convergence. Additionally a plot is generated that shows the observed frequency of each transition (eg. A->C) as a function of the associated quality score, the final estimated error rates (if they exist), the initial input rates, and the expected error rates under the nominal definition of quality scores. In addition a plot is generated (with plotErrors) that shows the observed frequency of each transition (eg. A->C) as a function of the associated quality score. Also the final estimated error rates (if they exist) are shown. Optionally also the initial input rates and the expected error rates under the nominal definition of quality scores can be added to the plot. Usage ..... 
Input
 are the FASTQ dataset containing the filtered and trimmed reads of the samples. 
Output
 a dataset with type 
dada2_errorrates
 (which is a RData file containing the output of dada2's learnErrors function) and a 
plot
 showing the error rates for each possible transition (A→C, A→G,...) - Points are the observed error rates for each consensus quality score. - The black line shows the estimated error rates after convergence of the machine-learning algorithm. - The red line shows the error rates expected under the nominal definition of the Q-score. The learned error rates are input the the 
dada2: dada
 tool. Details ....... The learnErrors method learns a parametric error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors). It is expected that the estimated error rates (black lines in the plot) are in a good fit to the observed rates (points in the plot), and that the error rates drop with increased quality. Try to increase the 
number of bases to use for learning
 if this is not the case. Error functions: - loessErrfun: accepts a matrix of observed transitions, with each transition corresponding to a row (eg. row 2 = A->C) and each column to a quality score (eg. col 31 = Q30). It returns a matrix of estimated error rates of the same shape. Error rates are estimates by a loess fit of the observed rates of each transition as a function of the quality score. Self-transitions (i.e. A->A) are taken to be the left-over probability. - noqualErrfun: accepts a matrix of observed transitions, groups together all observed transitions regardless of quality scores, and estimates the error rate for that transition as the observed fraction of those transitions. The effect is that quality scores will be effectively ignored. - PacBioErrfun: This function accepts a matrix of observed transitions from PacBio CCS amplicon sequencing data, with each transition corresponding to a row (eg. row 2 = A->C) and each column to a quality score (eg. col 31 = Q30). It returns a matrix of estimated error rates of the same shape. Error rates are estimates by loessErrfun for quality scores 0-92, and individually by the maximum likelihood estimate for the maximum quality score of 93. Overview ........ The intended use of the dada2 tools for paired sequencing data is shown in the following image. .. image:: pairpipe.png Note: In particular for the analysis of paired collections the collections should be sorted lexicographical before the analysis. For single end data you the steps ""Unzip collection"" and ""mergePairs"" are not necessary. More information may be found on the dada2 homepage:: https://benjjneb.github.io/dada2/index.html (in particular tutorials) or the documentation of dada2's R package https://bioconductor.org/packages/release/bioc/html/dada2.html (in particular the pdf which contains the full documentation of all parameters)"
toolshed.g2.bx.psu.edu/repos/iuc/dada2_makesequencetable/dada2_makeSequenceTable/1.34.0+galaxy1	"Description ........... This function constructs a sequence table -- more precisely an amplicon sequence variant table (ASV) table -- a higher-resolution version of the OTU table produced by traditional methods. The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. Usage ..... 
Input
: The result of dada, or mergePairs. 
Output
: A data set of type dada2_sequencetable, i.e. a tabular with a row for each sample, and a column for each unique sequence across all the samples. The columns are named by the sequence. Details ....... Sequences that are much longer or shorter than expected may be the result of non-specific priming. You can remove non-target-length by applying a length filter. This is analogous to “cutting a band” in-silico to get amplicons of the targeted length. Overview ........ The intended use of the dada2 tools for paired sequencing data is shown in the following image. .. image:: pairpipe.png Note: In particular for the analysis of paired collections the collections should be sorted lexicographical before the analysis. For single end data you the steps ""Unzip collection"" and ""mergePairs"" are not necessary. More information may be found on the dada2 homepage:: https://benjjneb.github.io/dada2/index.html (in particular tutorials) or the documentation of dada2's R package https://bioconductor.org/packages/release/bioc/html/dada2.html (in particular the pdf which contains the full documentation of all parameters)"
toolshed.g2.bx.psu.edu/repos/iuc/dada2_mergepairs/dada2_mergePairs/1.34.0+galaxy1	"Description ........... This function attempts to merge each denoised pair of forward and reverse reads, rejecting any pairs which do not sufficiently overlap (at least 12bp by default) or which contain too many (>0 by default) mismatches in the overlap region. Usage ..... 
Input
 - reads and learned error rates of the forward reads - reads and learned error rates of the reverse reads 
Output
 - a data set of type dada2_mergepairs (which is a RData file containing the output of dada2's mergePairs function). Details ....... Overview ........ The intended use of the dada2 tools for paired sequencing data is shown in the following image. .. image:: pairpipe.png Note: In particular for the analysis of paired collections the collections should be sorted lexicographical before the analysis. For single end data you the steps ""Unzip collection"" and ""mergePairs"" are not necessary. More information may be found on the dada2 homepage:: https://benjjneb.github.io/dada2/index.html (in particular tutorials) or the documentation of dada2's R package https://bioconductor.org/packages/release/bioc/html/dada2.html (in particular the pdf which contains the full documentation of all parameters)"
toolshed.g2.bx.psu.edu/repos/iuc/dada2_plotcomplexity/dada2_plotComplexity/1.34.0+galaxy1	"Summary ....... This function plots a histogram of the distribution of sequence complexities in the form of effective numbers of kmers as determined by seqComplexity. By default, kmers of size 2 are used, in which case a perfectly random sequences will approach an effective kmer number of 16 = 4 (nucleotides)^ 2 (kmer size). Details ....... This function calculates the kmer complexity of input sequences. Complexity is quantified as the Shannon richness of kmers, which can be thought of as the effective number of kmers if they were all at equal frequencies. If a window size is provided, the minimum Shannon richness observed over sliding window along the sequence is returned. Overview ........ The intended use of the dada2 tools for paired sequencing data is shown in the following image. .. image:: pairpipe.png Note: In particular for the analysis of paired collections the collections should be sorted lexicographical before the analysis. For single end data you the steps ""Unzip collection"" and ""mergePairs"" are not necessary. More information may be found on the dada2 homepage:: https://benjjneb.github.io/dada2/index.html (in particular tutorials) or the documentation of dada2's R package https://bioconductor.org/packages/release/bioc/html/dada2.html (in particular the pdf which contains the full documentation of all parameters)"
toolshed.g2.bx.psu.edu/repos/iuc/dada2_plotqualityprofile/dada2_plotQualityProfile/1.34.0+galaxy1	"Summary ....... This function plots a visual summary of the distribution of quality scores as a function of sequence position for the input fastq datasets. Details ....... The distribution of quality scores at each position is shown as a grey-scale heat map, with dark colors corresponding to higher frequency. The plotted lines show positional summary statistics: green is the mean, orange is the median, and the dashed orange lines are the 25th and 75th quantiles. If the sequences vary in length, a red line will be plotted showing the percentage of reads that extend to at least that position. Overview ........ The intended use of the dada2 tools for paired sequencing data is shown in the following image. .. image:: pairpipe.png Note: In particular for the analysis of paired collections the collections should be sorted lexicographical before the analysis. For single end data you the steps ""Unzip collection"" and ""mergePairs"" are not necessary. More information may be found on the dada2 homepage:: https://benjjneb.github.io/dada2/index.html (in particular tutorials) or the documentation of dada2's R package https://bioconductor.org/packages/release/bioc/html/dada2.html (in particular the pdf which contains the full documentation of all parameters)"
toolshed.g2.bx.psu.edu/repos/iuc/dada2_removebimeradenovo/dada2_removeBimeraDenovo/1.34.0+galaxy1	"Description ........... This tool can be used to remove chimeric sequences, i.e. sequences that can be constructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.. Two methods to identify chimeras are supported: Identification from pooled sequences and identification by consensus across samples. - from 
pooled
 sequences: Each sequence is evaluated against a set of ""parents"" drawn from the sequence collection that are sufficiently more abundant than the sequence being evaluated. Sequences that are bimera are removed, i.e. a two-parent chimera, in which the left side is made up of one parent sequence, and the right-side made up of a second parent sequence. - by 
consensus
: In short, bimeric sequences are flagged on a sample-by-sample basis. Then, a vote is performed for each sequence across all samples in which it appeared. If the sequence is flagged in a sufficiently high fraction of samples, it is identified as a bimera. A logical vector is returned, with an entry for each sequence in the table indicating whether it was identified as bimeric by this consensus procedure. 
Note
: pooled should only be used in combination with pooled denoising. Usage ..... 
Input
 - the results of makeSequenceTable (note that also the results of dada, and mergePairs are accepted) 
Output
 A data set of type: - dada2_sequenceTable (resp. dada2_mergepairs) if the input is of type dada2_sequenceTable (resp. dada2_mergepairs) - dada2_uniques otherwise Details ....... The frequency of chimeric sequences varies substantially from dataset to dataset, and depends on on factors including experimental procedures and sample complexity. Here chimeras make up about 21% of the merged sequence variants, but when we account for the abundances of those variants we see they account for only about 4% of the merged sequence reads. Considerations for your own data: Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to beginning the DADA2 pipeline. You can check for present primer sequences with the tool 
dada2: primer check
 Overview ........ The intended use of the dada2 tools for paired sequencing data is shown in the following image. .. image:: pairpipe.png Note: In particular for the analysis of paired collections the collections should be sorted lexicographical before the analysis. For single end data you the steps ""Unzip collection"" and ""mergePairs"" are not necessary. More information may be found on the dada2 homepage:: https://benjjneb.github.io/dada2/index.html (in particular tutorials) or the documentation of dada2's R package https://bioconductor.org/packages/release/bioc/html/dada2.html (in particular the pdf which contains the full documentation of all parameters)"
toolshed.g2.bx.psu.edu/repos/iuc/dada2_seqcounts/dada2_seqCounts/1.34.0+galaxy1	"Description ........... Get the counts of sequences per sample for the different stages of the dada pipeline. Usage ..... 
Inputs:
 Any number of results of dada2 steps in the following form: - a collection of results from dada, mergePairs, or the collection of statistics from filterAndTrim (the identifiers of the collection elements are used as sample names) - the results of dada in non-batch mode - the result of makeSequenceTable or removeBimeraDenovo 
Output:
 A table containing the number of sequences per sample (rows) for each input (columns) Details ....... For results from - dada, and mergePairs the sum of the result of dada2's getUniques function is used - makeSequenceTable, and removeBimeraDenovo R's rowSums function is used Overview ........ The intended use of the dada2 tools for paired sequencing data is shown in the following image. .. image:: pairpipe.png Note: In particular for the analysis of paired collections the collections should be sorted lexicographical before the analysis. For single end data you the steps ""Unzip collection"" and ""mergePairs"" are not necessary. More information may be found on the dada2 homepage:: https://benjjneb.github.io/dada2/index.html (in particular tutorials) or the documentation of dada2's R package https://bioconductor.org/packages/release/bioc/html/dada2.html (in particular the pdf which contains the full documentation of all parameters)"
toolshed.g2.bx.psu.edu/repos/ufz/genomad_end_to_end/genomad_end_to_end/1.11.1+galaxy0	"geNomad is a tool that identifies virus and plasmid genomes from nucleotide sequences. For details on the function refer to the citation or https://portal.nersc.gov/genomad/index.html. This Galaxy tool executed the ent-to-end geNomad workflow. geNomad is free to use for internal use, research &amp; development, non-commercial use, purposes 
only
. Check out the full 
License
. Usage ..... 
Input
 Any FASTA file containing nucleotide sequences as input. geNomad will work for isolate genomes, metagenomes, and metatranscriptomes. In addition you need to select a reference database that has to be installed by your Galaxy admin. 
Output
 plasmid/virus summary with the following columns * seq_name: The identifier of the sequence in the input FASTA file. Proviruses will have the following name scheme: <sequence_identifier>|provirus
<start_coordinate>
<end_coordinate>. * length: Length of the sequence (or the provirus, in the case of integrated viruses). * topology: Topology of the viral sequence. Possible values are: No terminal repeats, DTR (direct terminal repeats), ITR (inverted terminal repeats), or Provirus (viruses integrated in host genomes). * coordinates: 1-indexed coordinates of the provirus region within host sequences. Will be NA for viruses that were not predicted to be integrated. * n_genes: Number of genes encoded in the sequence. * genetic_code: Predicted genetic code. Possible values are: 11 (standard code for Bacteria and Archaea), 4 (recoded TGA stop codon), or 15 (recoded TAG stop codon). * virus_score: A measure of how confident geNomad is that the sequence is a virus. Sequences that have scores close to 1.0 are more likely to be viruses than the ones that have lower scores. * fdr: The estimated false discovery rate (FDR) of the classification (that is, the expected proportion of false positives among the sequences up to this row). To estimate FDRs geNomad requires score calibration, which is turned off by default. Therefore, this column will only contain NA values in this example. * n_hallmarks: Number of genes that matched a hallmark geNomad marker. Hallmarks are genes that were previously associated with viral function and their presence is a strong indicative that the sequence is indeed a virus. * marker_enrichment: A score that represents the total enrichment of viral markers in the sequence. The value goes as the number of virus markers in the sequence increases, so sequences with multiple markers will have higher score. Chromosome and plasmid markers will reduce the score. * taxonomy: Taxonomic assignment of the virus genome. Lineages follow the taxonomy contained in ICTV's VMR number 19. Viruses can be taxonomically assigned up to the family level, but not to specific genera or species within that family. The taxonomy is presented with a fixed number of fields (corresponding to taxonomic ranks) separated by semicolons, with empty fields left blank. The virus specific summary misses the coordinates and taxonomy columns and there are wto additional columns: * conjugation_genes genes that might be involved in conjugation. It's important to note that the presence of such genes is not sufficient to tell whether a given plasmid is conjugative or mobilizible. If you are interested in identifying conjugative plasmids, we recommend you to analyze the plasmids you identified using geNomad with CONJscan. * amr_genes genes annotated with antimicrobial resistance function. You can check the specific functions associated with each accession in AMRFinderPlus website. plasmid/virus genes: During its execution, geNomad annotates the genes encoded by the input sequences using a database of chromosome, plasmid, and virus-specific markers. The <prefix>_virus_genes.tsv file summarizes the annotation of the genes encoded by the identified viruses. * gene: Identifier of the gene (<sequence_name>
<gene_number>). Usually, gene numbers start with 1 (first gene in the sequence). However, genes encoded by prophages integrated in the middle of the host chromosome may start with a different number, depending on it's position within the chromosome. * start: 1-indexed start coordinate of the gene. * end: 1-indexed end coordinate of the gene. * length: Length of the gene locus (in base pairs). * strand: Strand that encodes the gene. Can be 1 (direct strand) or -1 (reverse strand). * gc_content: GC content of the gene locus. * genetic_code: Predicted genetic code (see details in the explanation of the summary file). * rbs_motif: Detected motif of the ribosome-binding site. * marker: Best matching geNomad marker. If this gene doesn't match any markers, the value will be NA. * evalue: E-value of the alignment between the protein encoded by the gene and the best matching geNomad marker. * bitscore: Bitscore of the alignment between the protein encoded by the gene and the best matching geNomad marker. * uscg: Whether the marker assigned to this gene corresponds to a universal single-copy gene (UCSG, as defined in BUSCO v5). These genes are expected to be found in chromosomes and are rare in plasmids and viruses. Can be 1 (gene is USCG) or 0 (gene is not USCG). * plasmid_hallmark: Whether the marker assigned to this gene represents a plasmid hallmark. * virus_hallmark: Whether the marker assigned to this gene represents a virus hallmark. * taxid: Taxonomic identifier of the marker assigned to this gene (you can ignore this as it is meant to be used internally by geNomad). * taxname: Name of the taxon associated with the assigned geNomad marker. In this example, we can see that the annotated proteins are all characteristic of Caudoviricetes (which is why the provirus was assigned to this class). * annotation_conjscan: If the marker that matched the gene is a conjugation-related gene (as defined in CONJscan) this field will show which CONJscan acession was assigned to the marker. * annotation_amr: If the marker that matched the gene was annotated with an antimicrobial resistance (AMR) function (as defined in NCBIfam-AMRFinder), this field will show which NCBIfam acession was assigned to the marker. * annotation_accessions: Some of the geNomad markers are functionally annotated. This column tells you which entries in Pfam, TIGRFAM, COG, and KEGG were assigned to the marker. * annotation_description: A text describing the function assigned to the marker. plasmid/virus genes/proteins: gives the nucleotide and aminoaced sequences of the annotated genes .. _License: https://raw.githubusercontent.com/apcamargo/genomad/refs/heads/main/LICENSE"
toolshed.g2.bx.psu.edu/repos/ufz/iphop_predict/iphop_predict/1.3.3+galaxy0	"iPHoP stands for integrated Phage Host Prediction. It is an automated command-line pipeline for predicting host genus of novel bacteriophages and archaeoviruses based on their genome sequences. Usage ..... 
Input
 Virus sequences in FASTA format. 
Output
 Host prediction to genus - contains integrated results from host-based and phage-based tools at the host genus level - lists for each prediction - the virus sequence ID, - the level of amino-acid similarity (AAI) between the query and the genomes in the RaFAH phage database, - the predicted host genus, - the confidence score calculated from all tools, and - the list of scores for individual classifiers obtained for this virus-host pair. - for the detailed score by classifier, ""RaFAH"" represents the score derived from RaFAH (https://pubmed.ncbi.nlm.nih.gov/34286299/), iPHoP-RF is the score derived from all host-based tools, CRISPR the score derived only from CRISPR hits, and blast the score derived only from blastn hits - all virus-host pairs for which the confidence score is higher than the selected score cutoff (default = 90) are included, so each virus may be associated with multiple predictions - when multiple predictions are available for a query virus, typical standard practices is to use the one with the highest score Host prediction to genome - contains integrated results from host-based tools only (i.e., no RaFAH) at the host genome representative level - lists for each host-based prediction - the virus sequence ID, - the representative host genome ID, - the corresponding host genome taxonomy, - the main method supporting this prediction (i.e., highest score), - the confidence score for this main method, and - the list of additional methods and scores obtained for this virus-host pair. Detailed by tool - This output files lists the 5 best hits for each method for each input virus - When no hits were obtained, the corresponding method is not listed in this output file for the query virus."
toolshed.g2.bx.psu.edu/repos/iuc/metasbt_index/metasbt_index/0.1.5+galaxy1	"What it does
 MetaSBT is a scalable framework for the characterization of known and still unknown microbial genomes with Sequence Bloom Trees. This tool act as an interface to the 
index
 and 
update
 subroutines of MetaSBT for the generation and update of new or predefined public databases. ----- .. class:: infomark Please visit the official GitHub repository_ for additional information about MetaSBT. Public MetaSBT Databases are available at the official MetaSBT-DBs_ repository. .. _repository: https://github.com/cumbof/MetaSBT .. _MetaSBT-DBs: https://github.com/cumbof/MetaSBT-DBs"
toolshed.g2.bx.psu.edu/repos/iuc/khmer_abundance_distribution/khmer_abundance_distribution/3.0.0a3+galaxy3	Calculate abundance distribution of the k-mers in the sequence file using a pre-made k-mer countgraph. The columns of the k-mer abundance histogram are: (1) k-mer abundance, (2) k-mer count, (3) cumulative count, (4) fraction of total distinct k-mers. (from the khmer project: http://khmer.readthedocs.org/en/v2.0/ )
toolshed.g2.bx.psu.edu/repos/iuc/khmer_abundance_distribution_single/khmer_abundance_distribution_single/3.0.0a3+galaxy3	"Calculate the abundance distribution of k-mers from a single sequence file. Note that with 
-b
 this script is constant memory; in exchange, k-mer counts will stop at 255. The memory usage of this script with 
-b
 will be about 1.15x the product of the 
-x
 and 
-N
 numbers. The columns of the k-mer abundance histogram are: (1) k-mer abundance, (2) k-mer count, (3) cumulative count, (4) fraction of total distinct k-mers. (from the khmer project: http://khmer.readthedocs.org/en/v2.0/ )"
toolshed.g2.bx.psu.edu/repos/iuc/khmer_count_median/khmer_count_median/3.0.0a3+galaxy3	"Count the median/avg k-mer abundance for each sequence in the input file, based on the k-mer counts in the given k-mer countgraph. Can be used to estimate expression levels (mRNAseq) or coverage (genomic/metagenomic). The output file contains sequence id, median, average, stddev, and seq length. For khmer 1.x count-median.py will split sequence names at the first space which means that some sequence formats (e.g. paired FASTQ in Casava 1.8 format) will yield uninformative names. Use 
--csv
 to fix this behavior. (from the khmer project: http://khmer.readthedocs.org/en/v2.0/ )"
toolshed.g2.bx.psu.edu/repos/iuc/khmer_extract_partitions/khmer_extract_partitions/3.0.0a3+galaxy3	Separate sequences that are annotated with partitions into grouped files. (from the khmer project: http://khmer.readthedocs.org/en/v2.0/ )
toolshed.g2.bx.psu.edu/repos/iuc/khmer_filter_abundance/khmer_filter_abundance/3.0.0a3+galaxy3	"Trims fastq/fasta sequences at k-mers of a given abundance based on a provided k-mer countgraph If the input sequences are from RNAseq or metagenome sequencing then 
--variable-coverage
 should be used. (from the khmer project: http://khmer.readthedocs.org/en/v2.0/ )"
toolshed.g2.bx.psu.edu/repos/iuc/khmer_filter_below_abundance_cutoff/khmer_filter_below_abundance_cutoff/3.0.0a3+galaxy3	Trims fastq/fasta sequences at k-mers with abundance below 50 based on a provided k-mer countgraph. (from the khmer project: http://khmer.readthedocs.org/en/v2.0/ )
toolshed.g2.bx.psu.edu/repos/iuc/khmer_normalize_by_median/khmer_normalize_by_median/3.0.0a3+galaxy3	"Do digital normalization (remove mostly redundant sequences) Discard sequences based on whether or not their median k-mer abundance lies above a specified cutoff. Kept sequences will be placed in <fileN>.keep. By default, Paired end reads will be considered together; if either read will be kept, then both will be kept. (This keeps both reads from a fragment, and helps with retention of repeats.) Unpaired reads are treated individually. If 
--paired
 is set then proper pairing is required and the tool will exit on unpaired reads, although 
--unpaired-reads
 can be used to supply a file of orphan reads to be read after the paired reads. 
--force_single
 will ignore all pairing information and treat reads individually. With 
-s
/
--savegraph
, the k-mer countgraph will be saved to the specified file after all sequences have been processed. 
--loadgraph
 will load the specified k-mer countgraph before processing the specified files. Note that the countgraph is in same format as those produced by 
load-into-counting.py
 and consumed by 
abundance-dist.py
. (from the khmer project: http://khmer.readthedocs.org/en/v2.0/ )"
toolshed.g2.bx.psu.edu/repos/iuc/khmer_partition/khmer_partition/3.0.0a3+galaxy3	"Load in a set of sequences, partition them, merge the partitions, and annotate the original sequences files with the partition information. This script combines the functionality of 
load-graph.py
, 
partition-graph.py
, 
merge-partitions.py
, and 
annotate-partitions.py
 into one script. This is convenient but should probably not be used for large data sets, because 
do-partition.py
 doesn't provide save/resume functionality."
toolshed.g2.bx.psu.edu/repos/iuc/mash_dist/mash_dist/2.3+galaxy0	"What it does
 Estimate the distance of each query sequence to the reference. Both the reference and queries can be fasta or fastq, gzipped or not, or Mash sketch files (.msh) with matching k-mer sizes. Query files can also be files of file names (see -l). Whole files are compared by default (see -i). The output fields are [reference-ID, query-ID, distance, p-value, shared-hashes]."
toolshed.g2.bx.psu.edu/repos/iuc/mash_paste/mash_paste/2.3+galaxy0	Create a single sketch file from multiple sketch files.
toolshed.g2.bx.psu.edu/repos/iuc/mash/mash_screen/2.3+galaxy4	"What it does
 Determine how well query sequences are contained within a pool of sequences. The queries must be formatted as a single Mash sketch file (.msh), created with the 
mash sketch
 command. The <pool> files can be contigs or reads, in fasta or fastq, gzipped or not, and ""-"" can be given for <pool> to read from standard input. The <pool> sequences are assumed to be nucleotides, and will be 6-frame translated if the <queries> are amino acids. The output fields are [identity, shared-hashes, median-multiplicity, p-value, query-ID, query-comment], where median-multiplicity is computed for shared hashes, based on the number of observations of those hashes within the pool."
toolshed.g2.bx.psu.edu/repos/iuc/mash_sketch/mash_sketch/2.3+galaxy3	"What it does
 Create a sketch file, which is a reduced representation of a sequence or set of sequences (based on min-hashes) that can be used for fast distance estimations. Inputs can be fasta or fastq files (gzipped or not), and ""-"" can be given to read from standard input. Input files can also be files of file names (see -l). For output, one sketch file will be generated, but it can have multiple sketches within it, divided by sequences or files (see -i). By default, the output file name will be the first input file with a '.msh' extension, or 'stdin.msh' if standard input is used (see -o)."
toolshed.g2.bx.psu.edu/repos/iuc/megahit_contig2fastg/megahit_contig2fastg/1.1.3+galaxy1	"MEGAHIT toolkit's contig2fastg
 Contig2fastg is a subprogram within the MEGAHIT toolkit. It converts MEGAHIT's contigs (.fa) to assembly graphs (.fastg) that can be utilized for protein/peptide identification via graph2pro and can also be visualized via Bandage. ----- 
EXAMPLE
 
Command:
 megahit_toolkit contig2fastg 99 intermediate_contigs/k99.contigs.fa > k99.contigs.fastg 
k99.contigs.fa
 >k21_1 flag=3 multi=1.0486 len=576 CGGTCGAAAAACTGCTGGCAGTGGGGCATTACCTC... 
k99.contigs.fastg
 >NODE_1_length_576_cov_1.0486_ID_1:NODE_1_length_576_cov_1.0486_ID_1; CGGTCGAAAAACTGCTGGCAGTGGGGCATTACCTC... ----- 
MEGAHIT
 MEGAHIT is a single node assembler for large and complex metagenomics NGS reads, such as soil. It makes use of succinct de Bruijn graph (SdBG) to achieve low memory assembly. MEGAHIT can optionally utilize a CUDA-enabled GPU to accelerate its SdBG contstruction. The GPU-accelerated version of MEGAHIT has been tested on NVIDIA GTX680 (4G memory) and Tesla K40c (12G memory) with CUDA 5.5, 6.0 and 6.5. -------- 
Project links:
 MEGAHIT and MEGAHIT Toolkit: https://github.com/voutcn/megahit Graph2Pro-Var: https://github.com/COL-IU/graph2pro-var Bandage: http://rrwick.github.io/Bandage/"
toolshed.g2.bx.psu.edu/repos/iuc/metagenomeseq_normalization/metagenomeseq_normalizaton/1.16.0-0.0.1	metagenomeSeq Cumulative sum scaling ==================================== Info ---- :: Cumulative sum scaling based upon percentile selection. You can manually specificy the percentile or calculate it using cumNormStat or cumNormStatFast. Inputs ------ :: Requires a BIOM formatted file for input. Outputs ------- :: Creates a normalized and scaled output abundance matrix in Tabular format. Additionally can create simple statistics and the RScript.
toolshed.g2.bx.psu.edu/repos/iuc/metasbt_profile/metasbt_profile/0.1.5+galaxy1	"What it does
 This tool wraps the 
profile
 subcommand of MetaSBT to infer the closest taxonomic assignments for a set of genomes using a MetaSBT database. 
Input
 A list of genomes in FASTA or compressed format. 
Output
 A collection of 3-column tables (level, closest, ANI) per input genome. Each table reports the most likely taxonomic assignments at different levels (e.g., kingdom, phylum), with their estimated Average Nucleotide Identity (ANI). ----- .. class:: infomark Please visit the official GitHub repository_ for additional information about MetaSBT. Public MetaSBT Databases are available at the official MetaSBT-DBs_ repository. .. _repository: https://github.com/cumbof/MetaSBT .. _MetaSBT-DBs: https://github.com/cumbof/MetaSBT-DBs"
toolshed.g2.bx.psu.edu/repos/nml/staramr/staramr_search/0.11.0+galaxy0	"staramr ======= staramr_ scans bacterial genome contigs against both the ResFinder_, PlasmidFinder_ and PointFinder_ databases (used by their respective webservices_) and compiles a summary report of detected antimicrobial resistance genes. Usage ----- 1. Select your genome contigs (in FASTA format). 2. Select whether or not you wish to scan your genome for point mutations giving antimicrobial resistance using the PointFinder database. This requires you to specify the specific organism you are scanning. 3. Run the tool. Input ----- Genomes 
`````` staramr_ takes as input one or more assembled genomes (in FASTA format) to search for AMR genes. Exclude genes file `````````````````` Setting **Provide a custom list of AMR genes to exclude** in the **Advanced options** allows you to pass a file containing a list of genes to exclude from the results. The file must start with a line **#gene_id** and the gene names must correspond to the sequence IDs in the ResFinder/PointFinder databases. For example: :: #gene_id aac(6')-Iaa_1_NC_003197 ColpVC_1__JX133088 Complex mutations file `````````````````````` Complex mutations describe multiple point mutations that must be simultaneously present in order to confer resistance. Complex mutations may be specified by the user using a TSV-formatted file with the following format: :: positions mandatory phenotype mutation(s) mutation(s) phenotype Where ""positions"" are all the point mutations to group into the complex mutation (optional and mandatory), ""mandatory"" are all the point mutations that must be present for the complex mutation to be reported (mandatory is a subset of positions), and phenotype is the phenotype that is conferred when this set of mutations is present. Output ------ There are 6 different output files produced by
staramr
as well as a collection of additional files. mlst.tsv ```````````````````` A tabular file of each multi-locus sequence type (MLST) and it's corresponding locus/alleles, one genome per line. +------------+---------------------+---------------+---------+---------+---------+---------+---------+---------+----------+ | Isolate ID | Scheme | Sequence Type | Locus 1 | Locus 2 | Locus 3 | Locus 4 | Locus 5 | Locus 6 | Locus 7 | +============+=====================+===============+=========+=========+=========+=========+=========+=========+==========+ | SRR1952908 | senterica_achtman_2 | 11 | aroC(5) | dnaN(2) | hemD(3) | hisD(7) | purE(6) | sucA(6) | thrA(11) | | SRR1952926 | senterica_achtman_2 | 11 | aroC(5) | dnaN(2) | hemD(3) | hisD(7) | purE(6) | sucA(6) | thrA(11) | +------------+---------------------+---------------+---------+---------+---------+---------+---------+---------+----------+ summary.tsv ``````````` A summary of all detected AMR genes/mutations in each genome, one genome per line. +------------+----------------+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------+---------------------+---------------+---------------+-----------+---------------------------------------------------+-------------------------+ | Isolate ID | Quality Module | Genotype | Predicted Phenotype | CGE Predicted Phenotype | Plasmid | Scheme | Sequence Type | Genome Length | N50 value | Number of Contigs Greater Than Or Equal To 300 bp | Quality Module Feedback | +============+================+===========================================================+===========================================================================================================+===========================================================================================================================================================================================+==============================================+=====================+===============+===============+===========+===================================================+=========================+ | SRR1952908 | Passed | aadA1, aadA2, blaTEM-57, cmlA1, gyrA (S83Y), sul3, tet(A) | streptomycin, ampicillin, chloramphenicol, ciprofloxacin I/R, nalidixic acid, sulfisoxazole, tetracycline | Spectinomycin, Streptomycin, Amoxicillin, Ampicillin, Cephalothin, Piperacillin, Ticarcillin, Chloramphenicol, Nalidixic acid, Ciprofloxacin, Sulfamethoxazole, Doxycycline, Tetracycline | ColpVC, IncFIB(S), IncFII(S), IncI1-I(Alpha) | senterica_achtman_2 | 11 | 4785500 | 250423 | 41 | | | SRR1952926 | Passed | blaTEM-57, gyrA (S83Y), tet(A) | ampicillin, ciprofloxacin I/R, nalidixic acid, tetracycline | Amoxicillin, Ampicillin, Cephalothin, Piperacillin, Ticarcillin, Nalidixic acid, Ciprofloxacin, Doxycycline, Tetracycline | ColpVC, IncFIB(S), IncFII(S), IncI1-I(Alpha) | senterica_achtman_2 | 11 | 4785451 | 228311 | 40 | | +------------+----------------+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------+---------------------+---------------+---------------+-----------+---------------------------------------------------+-------------------------+ detailed_summary.tsv ```````````````````` A detailed summary of all detected AMR genes/mutations/plasmids in each genome/type, one gene/type per line. +------------+----------------------------+------------+---------------------+-----------------------------+-----------+----------+-------------------------+-------------+-------+------+-----------+ | Isolate ID | Data | Data Type | Predicted Phenotype | CGE Predicted Phenotype | %Identity | %Overlap | HSP Length/Total Length | Contig | Start | End | Accession | +============+============================+============+=====================+=============================+===========+==========+=========================+=============+=======+======+===========+ | SRR1952908 | ST11 (senterica_achtman_2) | MLST | | | | | | | | | | | SRR1952908 | ColpVC | Plasmid | | | 98.96 | 100.0 | 193/193 | contig00038 | 1618 | 1426 | JX133088 | | SRR1952908 | aadA1 | Resistance | streptomycin | Spectinomycin, Streptomycin | 100.0 | 100.0 | 792/792 | contig00030 | 5355 | 4564 | JQ414041 | +------------+----------------------------+------------+---------------------+-----------------------------+-----------+----------+-------------------------+-------------+-------+------+-----------+ resfinder.tsv ````````````` A tabular file of each AMR gene and additional BLAST information from the **ResFinder** database, one gene per line. +------------+--------+---------------------+---------------------------+-----------+----------+-------------------------+-------------+-------+------+-----------+-----------+-----------+ | Isolate ID | Gene | Predicted Phenotype | CGE Predicted Phenotype | %Identity | %Overlap | HSP Length/Total Length | Contig | Start | End | Accession | Sequence | CGE Notes | +============+========+=====================+===========================+===========+==========+=========================+=============+=======+======+===========+===========+===========+ | SRR1952908 | sul3 | sulfisoxazole | Sulfamethoxazole | 100.00 | 100.00 | 792/792 | contig00030 | 2091 | 2882 | AJ459418 | ATGA[...] | | | SRR1952908 | tet(A) | tetracycline | Doxycycline, Tetracycline | 99.92 | 97.80 | 1247/1275 | contig00032 | 1476 | 2722 | AF534183 | ATGT[...] | | +------------+--------+---------------------+---------------------------+-----------+----------+-------------------------+-------------+-------+------+-----------+-----------+-----------+ plasmidfinder.tsv ````````````````` A tabular file of each AMR plasmid type and additional BLAST information from the **PlasmidFinder** database, one plasmid type per line. +------------+-----------+-----------+----------+-------------------------+-------------+-------+------+-----------+ | Isolate ID | Plasmid | %Identity | %Overlap | HSP Length/Total Length | Contig | Start | End | Accession | +============+===========+===========+==========+=========================+=============+=======+======+===========+ | SRR1952908 | ColpVC | 98.96 | 100 | 193/193 | contig00038 | 1618 | 1426 | JX133088 | | SRR1952908 | IncFIB(S) | 98.91 | 100 | 643/643 | contig00024 | 10302 | 9660 | FN432031 | +------------+-----------+-----------+----------+-------------------------+-------------+-------+------+-----------+ pointfinder.tsv ``````````````` A tabular file of each AMR point mutation and additional BLAST information from the **PointFinder** database, one gene per line. +------------+-------------+-----------------------------------+------------------------------+-------+----------+---------------------+-----------+----------+-------------------------+-------------+--------+--------+----------------------+-----------+-----------------------+---------------------+------------------+ | Isolate ID | Gene | Predicted Phenotype | CGE Predicted Phenotype | Type | Position | Mutation | %Identity | %Overlap | HSP Length/Total Length | Contig | Start | End | Pointfinder Position | CGE Notes | CGE Required Mutation | CGE Mechanism | CGE PMID | +============+=============+===================================+==============================+=======+==========+=====================+===========+==========+=========================+=============+========+========+======================+===========+=======================+=====================+==================+ | SRR1952908 | gyrA (S83Y) | ciprofloxacin I/R, nalidixic acid | Nalidixic acid,Ciprofloxacin | codon | 83 | TCC -&gt; TAC (S -&gt; Y) | 99.96 | 100.00 | 2637/2637 | contig00008 | 22801 | 20165 | S83Y | | | Target modification | 7492118,10471553 | | SRR1952926 | gyrA (S83Y) | ciprofloxacin I/R, nalidixic acid | Nalidixic acid,Ciprofloxacin | codon | 83 | TCC -&gt; TAC (S -&gt; Y) | 99.96 | 100.00 | 2637/2637 | contig00011 | 157768 | 160404 | S83Y | | | Target modification | 7492118,10471553 | +------------+-------------+-----------------------------------+------------------------------+-------+----------+---------------------+-----------+----------+-------------------------+-------------+--------+--------+----------------------+-----------+-----------------------+---------------------+------------------+ settings.txt ```````````` The command-line, database versions, and other settings used to run
staramr`. :: command_line = staramr search --pointfinder-organism enterococcus_faecium -o out pbp5.fa version = 0.10.0 start_time = 2023-08-23 13:37:41 end_time = 2023-08-23 13:37:42 total_minutes = 0.02 resfinder_db_dir = staramr/databases/data/dist/resfinder resfinder_db_url = https://bitbucket.org/genomicepidemiology/resfinder_db.git resfinder_db_commit = fa32d9a3cf0c12ec70ca4e90c45c0d590ee810bd resfinder_db_date = Tue, 24 May 2022 06:51 pointfinder_db_dir = staramr/databases/data/dist/pointfinder pointfinder_db_url = https://bitbucket.org/genomicepidemiology/pointfinder_db.git pointfinder_db_commit = 8c694b9f336153e6d618b897b3b4930961521eb8 pointfinder_db_date = Mon, 01 Feb 2021 15:46 pointfinder_organisms_all = campylobacter, enterococcus_faecalis, enterococcus_faecium, escherichia_coli, helicobacter_pylori, klebsiella, mycobacterium_tuberculosis, neisseria_gonorrhoeae, plasmodium_falciparum, salmonella, staphylococcus_aureus pointfinder_organisms_valid = campylobacter, enterococcus_faecalis, enterococcus_faecium, escherichia_coli, helicobacter_pylori, salmonella plasmidfinder_db_dir = staramr/databases/data/dist/plasmidfinder plasmidfinder_db_url = https://bitbucket.org/genomicepidemiology/plasmidfinder_db.git plasmidfinder_db_commit = c18e08c17a5988d4f075fc1171636e47546a323d plasmidfinder_db_date = Wed, 18 Jan 2023 09:45 mlst_version = 2.23.0 pointfinder_organism = enterococcus_faecium pointfinder_gene_drug_version = 072621.2 resfinder_gene_drug_version = 072621 results.xlsx 
`` An Excel spreadsheet containing the previous 5 files as separate worksheets. BLAST Hits
 The dataset collection 
hits
 stores fasta files of the specific blast hits. Galaxy wrapper written by Aaron Petkau and Eric Marinier at the National Microbiology Laboratory, Public Health Agency of Canada. .. _staramr: https://github.com/phac-nml/staramr .. _ResFinder: https://bitbucket.org/genomicepidemiology/resfinder_db .. _PlasmidFinder: https://bitbucket.org/genomicepidemiology/plasmidfinder_db .. _PointFinder: https://bitbucket.org/genomicepidemiology/pointfinder_db .. _webservices: http://www.genomicepidemiology.org/services/"
toolshed.g2.bx.psu.edu/repos/bgruening/sylph_profile/sylph_profile/0.8.1+galaxy0	"What is sylph?
 Sylph is an extremely fast and memory efficient program for profiling and searching metagenomic samples against databases. It is 10-100x faster than other popular software such as MetaPhlAn or Kraken and more memory efficient too. 
What can sylph do?
 - Profile metagenomes: sylph can calculate the abundances of genomes in a sample using a reference database. This is the same type of output as Kraken or MetaPhlAn. - Search genomes against metagenomes: sylph can check if a genome is contained in your sample (e.g. is this E. coli genome in my sample?). - ANI querying: sylph can estimate the containment average nucleotide identity (ANI) of a reference genome to the genomes in your sample. - Use custom reference databases: Eukaryotes, viruses, and any collections of fasta files are ok. - Long-reads are usable: sylph is primarily optimized for short-reads, but it can utilize nanopore or PacBio reads with high precision. - Calculate coverage: sylph can estimate the coverage (not just the abundance) of genomes in your database. 
[See here for more information on what sylph can and can not do]. &lt;https://github.com/bluenote-1577/sylph/wiki/Introduction:-what-is-sylph-and-how-does-it-work%3F&gt;
 ---- 
Output
 Sylph profile outputs a TSV (tab-separated values) file. Each row is one genome detected in the metagenome sample. - Sample_file: the filename of the reads/sample. - Genome_file: the filename of the detected genome. - Taxonomic_abundance: normalized taxonomic abundance as a percentage. Coverage-normalized - same as MetaPhlAn abundance - Sequence_abundance: normalized sequence abundance as a percentage. The ""number of reads"" assigned to each genome - same as Kraken abundance - Adjusted_ANI: adjusted containment ANI estimate. - If coverage adjustment is possible (cov is < 3x cov): returns coverage-adjusted ANI - If coverage is too low/high: returns Naive_ANI (see below) - Eff_cov/True_cov: an estimate of the effective, or if -u specified, the true coverage. Always a decimal number. - ANI_5-95_percentile: [5%,95%] confidence intervals. Not always a decimal number. - If coverage adjustment is possible: float-float e.g. 98.52-99.55 - If coverage is too low/high: NA-NA is given. - Eff_lambda: estimate of the effective coverage parameter. Not always a decimal number. - If coverage adjustment is possible: lambda estimate is given - If coverage is too low/high: LOW or HIGH is output - Lambda_5-95_percentile: [5%, 95%] confidence intervals for lambda. Same format rules as ANI_5-95_percentile. - Median_cov: median k-mer multiplicity for k-mers with >= 1 multiplicity. - Mean_cov_geq1: mean k-mer multiplicity for k-mers with >= 1 multiplicity. - Containment_ind: int/int showing the containment index (number of k-mers found in sample divided by total k-mers), e.g. 959/1053. - Naive_ANI: containment ANI without coverage adjustment. - kmers_reassigned: the number of k-mers reassigned away from the genome. - Contig_name: name of the first contig in the genome Additional files are able to be output. The metaphlan-style output is formatted similarly to that output by the 
[Metaphlan &lt;toolshed.g2.bx.psu.edu/repos/iuc/metaphlan/metaphlan/4.1.1+galaxy3&gt;
 This output is 
NOT
 compatible with Krona directly. For that, please select the Krona-style output option."
toolshed.g2.bx.psu.edu/repos/bgruening/sylph_query/sylph_query/0.8.1+galaxy0	"What is sylph?
 Sylph is an extremely fast and memory efficient program for profiling and searching metagenomic samples against databases. It is 10-100x faster than other popular software such as MetaPhlAn or Kraken and more memory efficient too. 
What can sylph do?
 - Profile metagenomes: sylph can calculate the abundances of genomes in a sample using a reference database. This is the same type of output as Kraken or MetaPhlAn. - Search genomes against metagenomes: sylph can check if a genome is contained in your sample (e.g. is this E. coli genome in my sample?). - ANI querying: sylph can estimate the containment average nucleotide identity (ANI) of a reference genome to the genomes in your sample. - Use custom reference databases: Eukaryotes, viruses, and any collections of fasta files are ok. - Long-reads are usable: sylph is primarily optimized for short-reads, but it can utilize nanopore or PacBio reads with high precision. - Calculate coverage: sylph can estimate the coverage (not just the abundance) of genomes in your database. 
[See here for more information on what sylph can and can not do]. &lt;https://github.com/bluenote-1577/sylph/wiki/Introduction:-what-is-sylph-and-how-does-it-work%3F&gt;
 ---- 
Output
 Sylph Query outputs a TSV (tab-separated values) file. Each row is one genome detected in the metagenome sample. - Sample_file: the filename of the reads/sample. - Genome_file: the filename of the detected genome. - Adjusted_ANI: adjusted containment ANI estimate. - If coverage adjustment is possible (cov is < 3x cov): returns coverage-adjusted ANI - If coverage is too low/high: returns Naive_ANI (see below) - Eff_cov/True_cov: an estimate of the effective, or if -u specified, the true coverage. Always a decimal number. - ANI_5-95_percentile: [5%,95%] confidence intervals. Not always a decimal number. - If coverage adjustment is possible: float-float e.g. 98.52-99.55 - If coverage is too low/high: NA-NA is given. - Eff_lambda: estimate of the effective coverage parameter. Not always a decimal number. - If coverage adjustment is possible: lambda estimate is given - If coverage is too low/high: LOW or HIGH is output - Lambda_5-95_percentile: [5%, 95%] confidence intervals for lambda. Same format rules as ANI_5-95_percentile. - Median_cov: median k-mer multiplicity for k-mers with >= 1 multiplicity. - Mean_cov_geq1: mean k-mer multiplicity for k-mers with >= 1 multiplicity. - Containment_ind: int/int showing the containment index (number of k-mers found in sample divided by total k-mers), e.g. 959/1053. - Naive_ANI: containment ANI without coverage adjustment. - Contig_name: name of the first contig in the genome Additional files are able to be output. The metaphlan-style output is formatted similarly to that output by the 
[Metaphlan &lt;toolshed.g2.bx.psu.edu/repos/iuc/metaphlan/metaphlan/4.1.1+galaxy3&gt;
 This output is 
NOT
 compatible with Krona directly. For that, please select the Krona-style output option."
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_convert/0.1.9	".. class:: infomark 
What it does
 The tool converts between different file formats used for storing next-generation sequencing data. As input file types it can handle fastq, SAM or BAM format, which it can convert to SAM or BAM format. 
Notes:
 1) The tool can convert fastq files representing data from paired-end sequencing runs to appropriate SAM/BAM format provided that the mate information is split over two fastq files in corresponding order. 
TIP:
 If your paired-end data is arranged differently, you may look into the 
fastq splitter
 and 
fastq de-interlacer
 tools for Galaxy from the 
Fastq Manipulation category
_ of the Galaxy Tool Shed to see if they can convert your files to the expected format. 2) Merging partial fastq (or gzipped fastq) files into a single SAM/BAM file is supported both for single-end and paired-end data. Simply add additional input datasets and select the appropriate files (pairs of files in case of paired-end data). Concatenation of SAM/BAM file during conversion is currently not supported. 3) For input in fastq format a SAM header file providing run metadata 
has to be specified
. The information in this file will be used as the header data of the new SAM/BAM file. You can use the 
NGS Run Annotation
 tool to generate a new header file for your data. For input in SAM/BAM format the tool will simply copy the existing header data to the new file. To modify the header of an existing SAM/BAM file, use the 
Reheader BAM file
 tool instead. .. _Fastq Manipulation category: https://toolshed.g2.bx.psu.edu/repository/browse_repositories_in_category?id=310ff67d4caf6531 .. _recipe for using gzipped fastq files in Galaxy: http://mimodd.readthedocs.org/en/latest/recipes.html#use-gzipped-fastq-files-in-galaxy .. _MiModD user guide: http://mimodd.readthedocs.org/en/latest ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_covstats/0.1.9	".. class:: infomark 
What it does
 The tool takes as input a BCF file produced by the 
Variant Calling
 tool, and calculates per-chromosome read coverage from it. .. class:: warningmark The tool treats genome positions missing from the BCF input as zero coverage, so it is safe to use ONLY with BCF files produced by the 
Variant Calling
 tool or through other commands that keep the information for all sites. ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_delcall/0.1.9	".. class:: infomark 
What it does
 The tool predicts deletions from paired-end data in a two-step process: 1) It finds regions of low-coverage, i.e., candidate regions for deletions, by scanning a BCF file produced by the 
Variant Calling
 tool. The 
maximal coverage allowed inside a low-coverage region
 and the 
minimal deletion size
 parameters are used at this step to define what is considered a low-coverage region. .. class:: warningmark The tool treats genome positions missing from the BCF input as zero coverage, so it is safe to use ONLY with BCF files produced by the 
Variant Calling
 tool or through other commands that keep the information for all sites. 2) It assesses every low-coverage region statistically for evidence of it being a real deletion. 
This step requires paired-end data
 since it relies on shifts in the distribution of read pair insert sizes around real deletions. By default, the tool only reports Deletions, i.e., the subset of low-coverage regions that pass the statistical test. If 
include low-coverage regions
 is selected, regions that failed the test will also be reported. With 
group reads based on read group id only
 selected, as it is by default, grouping of reads into samples is done strictly based on their read group IDs. With the option deselected, grouping is done based on sample names in the first step of the analysis, i.e. the reads of all samples with a shared sample name are used to identify low-coverage regions. In the second step, however, reads will be regrouped by their read group IDs again, i.e. the statistical assessment for real deletions is always done on a per read group basis. 
TIP:
 Deselecting 
group reads based on read group id only
 can be useful, for example, if you have both paired-end and single-end sequencing data for the same sample. In this case, the two sets of reads will usually share a common sample name, but differ in their read groups. With grouping based on sample names, the single-end data can be used together with the paired-end data to identify low-coverage regions, thus increasing overall coverage and reliability of this step. Still, the assessment of deletions will use only the paired-end data (auto-detecting that the single-end reads do not provide insert size information). ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_varextract/0.1.9	".. class:: infomark 
What it does
 The tool takes as input a BCF dataset like the ones produced by the 
MiModD Variant Calling
 tool, extracts just the variant sites from it and reports them in VCF format. If the BCF input file specifies multiple samples, sites are included if they qualify as variant sites in at least one sample. ---------- 
Options:
 
keep all sites with alternate bases
 By default, a variant site is considered to be a position in the genome for which a non-reference allele appears in the inferred genotype of any sample. You can select the 
keep all sites with alternate bases
 option, if instead you want to extract all sites, for which at least one non-reference base has been observed (whether resulting in a non-reference allele call or not). Using this option should rarely be necessary, but could be occassionally helpful for closer inspection of candidate genomic regions. 
include information from pre-calculated vcf dataset
 During the process of variant extraction the tool can take into account genome positions specified in one or more independently generated VCF datasets. If such additional VCF input is provided, the tool output will contain the samples found in these files as additional samples and sites from the main BCF dataset will be included not only if they qualify as variant sites in at least one sample specified in the BCF, but also if they are listed in any of the additional VCF datasets. Optional VCF input can be particularly useful in one of the following situations: 1) you have prior information that leads you to think that certain genome positions are of special relevance for your project and, thus, you are interested in the statistics produced by the variant caller for these positions even if they are not considered variant sites. In this case you can use a minimal VCF dataset to guide the variant extraction process to include these positions. This dataset needs a minimal header of the form: 
##fileformat=VCFv4.2
 followed by positional information like in this example:: #CHROM POS ID REF ALT QUAL FILTER INFO chrI 1222 . . . . . . chrI 2651 . . . . . . chrI 3659 . . . . . . chrI 3731 . . . . . . , where columns are tab-separated and . serves as a placeholder for missing information. 2) you have actual variant calls from an additional sample, but you do not have access to the original sequenced reads data (if you had, the recommended approach would be to include that data in the 
MiModD Variant Calling
 step. This situation is often encountered with published datasets. Assume you have obtained a list of known single nucleotide variants (SNVs) found in one particular strain of your favorite model organism and you would like to know which of these SNVs are present in the related strains you have sequenced. You have aligned the sequenced reads from your samples and have used the 
MiModD Variant Calling
 tool, which has generated a BCF dataset ready for variant extraction. If the SNV list for the previously sequenced strain is in VCF format already, you can now just plug it into the analysis process by specifying it in the tool interface as an 
independently generated vcf dataset
. The resulting vcf output will contain all SNV sites along with the variant sites found in the BCF alone. You can then proceed to the 
MiModD VCF Filter
 tool to look at the original SNV sites only or to investigate any other interesting subset of sites. If the SNV list is in some other format, you will have o convert it to VCF first. At a minimum, the dataset must have a 
##fileformat
 header line like the previous example and have the 
REF
 and 
ALT
 column filled in like so:: #CHROM POS ID REF ALT QUAL FILTER INFO chrI 1897409 . A G . . . chrI 1897492 . C T . . . chrI 1897616 . C A . . . chrI 1897987 . A T . . . chrI 1898185 . C T . . . chrI 1898715 . G A . . . chrI 1898729 . T C . . . chrI 1900288 . T A . . . , in which case the tool will assume that the corresponding sample is homozygous for each of the SNVs. If you need to distinguish between homozygous and heterozygous SNVs you will have to extend the format to include a format and a sample column with genotype (GT) information like in this example:: #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT sampleX chrI 1897409 . A G . . . GT 1/1 chrI 1897492 . C T . . . GT 0/1 chrI 1897616 . C A . . . GT 0/1 chrI 1897987 . A T . . . GT 0/1 chrI 1898185 . C T . . . GT 0/1 chrI 1898715 . G A . . . GT 0/1 chrI 1898729 . T C . . . GT 0/1 chrI 1900288 . T A . . . GT 0/1 , in which sampleX would be heterozygous for all SNVs except the first. .. class:: warningmark If the optional VCF input contains INDEL calls, these will be ignored by the tool. ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_info/0.1.9	".. class:: infomark 
What it does
 The tool inspects the input datasets and generates a report summarizing its contents. It autodetects and works with most file formats produced by MiModD, i.e., 
SAM / BAM, vcf / bcf and fasta
, and produces a standardized report for all of them. ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_map/0.1.9	".. class:: infomark 
What it does
 This is the most downstream tool in 
mapping-by-sequencing analysis workflows in MiModD
. It can be used to analyze and visualize the inheritance pattern of variants detected and selected with other MiModD tools or as an alternative (and more versatile) plotting engine for data generated with 
CloudMap
. ------------- 
Usage Modes:
 This tool can be run in one of three different modes depending on the type of mapping analysis that should be performed: 1) 
Simple Variant Density (SVD) Mapping
 mode analyzes the density of variants along the reference genome by dividing each chromosome into regions of user-defined size (bins) and counting the variants found in each bin. All variants listed in the input file are analyzed in this mode, which means that as input you will typically want to use filtered lists of variants (as produced by the VCF Filter tool). The aim of SVD analysis is to identify clusters of variants in an outcrossed strain carrying a selectable unknown mutation, which is interpreted as linkage between the corresponding genomic region and the unknown mutation. This mode corresponds roughly to EMS Variant Density Mapping in CloudMap. 2) 
Variant Allele Frequency (VAF) Mapping
 mode analyzes the inheritance pattern in cross-progeny at sites, at which the parents are homozygous for different alleles. The aim of VAF analysis is to identify clusters of variants with (near) homozygous inheritance in a F2 (or later generation) population obtained from a cross between a strain carrying a selectable unknown mutation and an unrelated mapping strain. Such a cluster is interpreted as linkage between the corresponding genomic region and the unknown mutation selected for in the F2 generation. This mode corresponds roughly to Hawaiian Variant Mapping in CloudMap, but can simultaneously take into account non-reference alleles found in either parent strain (CloudMap users may think of this as a combined Hawaiian Variant and Variant Discovery Mapping analysis). 3) 
Variant Allele Contrast (VAC) Mapping
 mode analyzes and visualizes the divergence between two samples at all sites in the input dataset. It works independent of any parent strain information and can be used if you have two samples selected for contrary phenotypes, but also with a selected and a non-selected sample. ------------- 
Input:
 Valid input for this tool are VCF datasets (any such dataset in SVD mode, a MiModD-generated multi-sample VCF dataset in VAF and VAC modes) or a tabular report as generated by the CloudMap Hawaiian Variant Mapping tool. Alternatively, the tool can generate (in both modes) its own tabular reports, which can be used as input instead of the original VCF dataset, when rerunning the tool with different plotting parameters, to reduce analysis time. ------------- 
Output:
 The tool produces up to three output files: 1) a default tabular report of binned variant counts that can be used to plot the data with external software such as Excel, 2) an optional pdf containing linkage plots, and 3) an optional tabular per-variant report, which can be configured to be either valid input for the corresponding original CloudMap tool (for users who really, really want to continue using CloudMap for plotting) or to be reusable in fast reruns of the tool (which can be useful to experiment with different plotting parameters). ------------- 
Settings:
 1) Analysis settings 
bin size to analyze variants in
 - determines the width of the regions along each chromosome, in which variants are counted and analyzed together. Several bin sizes can be specified and for each size you will get a corresponding results section in the binned variant counts report and a linkage histogram plot. 
sample names (in VAF and VAC modes only)
 - to analyze inheritance patterns, the VAF and VAC modes need information about the relationship between the samples defined in the input. While VAC mode simply requires you to name the two contrasting samples for the analysis, the sample roles in VAF mode are a bit more complicated to understand. Specifically: The 
mapping sample name
 should be set to the name of the sample for which the inheritance pattern is to be analyzed (the pooled progeny population). The 
name of the related sample
 should indicate the parent sample that carried and brought in the unknown mutation to be mapped (or, alternatively, a closely related ancestor). The 
name of the unrelated sample
 should be that of the other parent strain used in the cross. At least one of the parent samples MUST be specified, but if the input contains variant information for both parents, they can be analyzed together for higher mapping accuracy. If you are reanalyzing a tabular report from a previous tool run or from CloudMap, the association between variants and samples is already stored in the input dataset and cannot be specified again. 2) Graphical output settings .. class:: warningmark To be able to generate plots, the system running MiModD needs to have the statistical programming environment R and its Python interface rpy2 installed. Disable graphical output if this is not the case. 
y-axes scaling
 - if you want to override the defaults 
x-axis scaling
 - choose 
preserve relative contig sizes
 if you want the largest chromosome to fit the page width and smaller chromosomes to appear according to their relative size or choose 
scale each contig to fit the plot width
 if all chromosomes should exploit the available space 
span value to be used in calculating the Loess regression line
 - this value determines the degree of smoothing of the regression line through the scatterplot data. Information on loess regression and the loess span parameter can be found at http://en.wikipedia.org/wiki/Local_regression. 
colors used for plotting
 - can be selected freely from the offered palette. For histogram colors, the list of selected colors will be used to provide the colors for the different histograms plotted. If less colors than histograms (determined by the number of bin sizes selected) are specified, colors from the list will be recycled. .. _CloudMap: https://usegalaxy.org/u/gm2123/p/cloudmap .. _mapping-by-sequencing analysis workflows in MiModD: http://mimodd.readthedocs.io/en/latest/nacreousmap.html .. _CloudMap-style sequence dictionary: http://mimodd.readthedocs.io/en/latest/fileformats.html#cloudmap-style-sequence-dictionary ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_aln/mimodd_align/0.1.9	".. class:: infomark 
What it does
 The tool aligns the sequenced reads in an arbitrary number of input datasets against a common reference genome and stores the results in a single, possibly multi-sample output dataset. Internally, the tool uses the ultrafast, hashtable-based aligner SNAP (http://snap.cs.berkeley.edu). ---------- 
Notes:
 
Input formats
 - The tool accepts SAM, BAM, fastq and fastq.gz input datasets of sequenced reads and supports both single-end and paired-end data. The recommended approach with MiModD is to store NGS datasets in SAM/BAM format with 
Run Metadata
 (see below) stored in the file header. You can use the 
MiModD Run Annotation
 and 
MiModD Convert
 tools to convert data from fastq format to SAM/BAM format while attaching run metadata to it. While alignments 
directly from fastq format
 are supported, this 
is less reliable
 due to less strict specifications of this format. If you find the tool complaining about malformed fastq input, it is likely that you can fix this problem by converting the data to SAM/BAM format first. - If you wish to align paired-end data directly from fastq format, the mate sequence data has to be split over two datasets as is mostly standard today. If you have your paired-end data as a single dataset you may look into the 
FASTQ splitter
 and 
FASTQ de-interlacer
 tools for Galaxy, which are available from the 
Fastq Manipulation category
 of the Galaxy Tool Shed and may be able to convert your files to the expected format. 
Run Metadata
 - 
Every input file requires accompanying Run Metadata!
 Most importantly, this includes a 
read-group ID
 (an identifier of the sequencing run that produced the data) and a 
sample name
 (identifying the biological sample sequenced in the run). - If an input dataset does not provide this information directly (fastq datasets never do; SAM/BAM datasets may provide it in their header), you need to specify a separate SAM/BAM dataset with an appropriate header as the source of the Run Metadata. You can use the 
MiModD Run Annotation
 tool to generate such a file. - If a SAM/BAM input dataset already provides Run Metadata, you can still specify a different Run Metadata source, which will then overwrite the information already present in the input. This is useful, for example, to resolve read-group ID conflicts between multiple input datasets. - Every input dataset can only contain reads from a single read-group. If you would like, for example, to realign the reads in a multi-sample SAM/BAM dataset. You should first use the 
MiModD Sort
 tool to sort the data by read names (this step is only necessary for paired-end data), then split the reads into new per-read-group datasets using the 
MiModD Convert
 tool. - Several input datasets can declare identical read-group IDs and/or sample names. Identical read-group IDs mean that the datasets were produced in the same sequencing run, as is the case, for example, with partial fastq sequencing data. In the output dataset, the corresponding reads will be merged and it will not be possible to trace back their source. Identical sample names (but different read-group IDs) indicate that the same sample has been sequenced multiple times. In the output dataset, the corresponding reads will be tagged appropriately and tools like the 
MiModD Variant Calling
 tool will let you decide whether you want to treat them together or separately. ---------- 
Tool Options
 The section 
Alignment parameters
 lets you configure global settings for the alignment job that will be applied to all input datasets. For each input dataset, however, you can overwrite some or all of these settings by specifying new values in the section 
Alignment options for this sample
. Some of the alignment parameters may have 
big
 effects on the alignment quality, but these effects are very dependent on the type of input sequences. You are strongly encouraged to consult the in-depth 
tool documentation
 for detailed explanations of the available options. .. _Fastq Manipulation category: https://toolshed.g2.bx.psu.edu/repository/browse_repositories_in_category?id=310ff67d4caf6531 .. _recipe for using gzipped fastq files in Galaxy: http://mimodd.readthedocs.org/en/latest/recipes.html#use-gzipped-fastq-files-in-galaxy .. _tool documentation: http://mimodd.readthedocs.io/en/0.1.9/tool_doc.html#snap ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_rebase/0.1.9	".. class:: infomark 
What it does
 The tool takes as input a VCF file like the ones produced by the 
Extract Variant
 tool and a Genome Browser chain format, and maps the variant positions found in the VCF file to a different reference genome coordinate system according to the mapping defined in a UCSC chain file. ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_reheader/0.1.9	".. class:: infomark 
What it does
 The tool generates a copy of the BAM input file with a modified header (i.e., metadata). It can update or replace read-group information (i.e., information about the samples in the file), add or replace comment lines, and rename reference sequences declared in the header. The tool ensures that the resulting BAM file is valid and can be further processed by other MiModD tools and standard software like samtools. It aborts with an error message if a valid BAM file cannot be generated with the user-specified settings. The template information used to modify or replace the input file metadata is provided through forms or, in the case of read-group information, can be taken from an existing SAM file as can be generated, for example, with the 
NGS Run Annotation
 tool. ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_varreport/0.1.9	".. class:: infomark 
What it does
 The tool turns a variant list in VCF format into a more readable summary table listing variant sites and effects. 
Html output enriched with species-specific hyperlinks
 If you select html as the output format, the tool can insert species- and variant-specific hyperlinks to public genome browsers and databases into the report. This is a useful feature to explore medium-size lists of variants, but requires that the tool knows: 1) the species that you are analyzing data for The tool can autodetect the species if the input dataset has been generated with the 
MiModD Variant Annotation
 tool. Alternatively, you can declare the species you are working with explicitly. 2) how to generate hyperlinks for this species The tool has built-in support for a number of standard model organisms. If your organism is not in that list or if you find that the default hyperlinks for a supported species are outdated, you can provide your own recipe to generate correct hyperlinks through a 
custom hyperlink template file
. 
TIP:
 MiModD's built-in hyperlink formatting tables are actively maintained and extended with every new version! If you find the tool produces outdated hyperlinks for any supported species or if you would like to see additional species supported, do not hesitate to 
tell us about it
. If you have a custom hyperlink template file that is working for you, that is even better. We may use it as a starting point for a built-in recipe and, while we are working on that and with your permission, we can post it on the package home page for other users who may need it. .. _custom hyperlink template file: http://mimodd.readthedocs.io/en/0.1.9/recipes.html#hyperlink-template-file .. _tell us about it: mailto:mimodd@googlegroups.com ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_header/0.1.9	".. class:: infomark 
What it does
 This tool takes user-provided information about a next-generation sequencing run and constructs a valid SAM format header from it. The resulting dataset can be used by the 
MiModD Convert
, 
MiModD Reheader
 and the 
MiModD Read Alignment
 tool to add run metadata to sequenced reads input datasets (or to overwrite pre-existing information). 
Note:
 
MiModD requires run metadata for every input file at the Alignment step !
 
Tip:
 While you can do Alignments from fastq file format by providing a custom header file directly to the 
SNAP Read Alignment
 tool, we 
recommend
 you to first convert all input files to and archive all datasets in SAM/BAM format with appropriate header information prior to any downstream analysis. Although a bit more time-consuming, this practice protects against information loss and ensures that the input datasets will remain useful for others in the future. ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_sort/0.1.9	".. class:: infomark 
What it does
 The tool sorts an aligned reads input dataset, typically by the reference genome coordinates that the reads have been mapped to. Coordinate-sorted input files are expected by most downstream MiModD tools, but note that the 
MiModD Read Alignment
 produces coordinate-sorted output by default and it is only necessary to sort files that come from other sources or from 
MiModD Read Alignment
 jobs with a custom sort order. The option 
Sort by read names instead of coordinates
 is useful if you want to re-align coordinate-sorted paired-end data. In 
paired-end mode
, the 
MiModD Read Alignment
 tool expects the reads in the input file to be arranged in read pairs, 
i.e.
, the forward read information of a pair must be followed immediately by its reverse mate information, which is typically not the case in coordinate-sorted data. Resorting such data by read names fixes this problem. ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_vcf_filter/0.1.9	".. class:: infomark 
What it does
 The tool filters a variant file in VCF format to generate a new VCF file with only a subset of the original variants. The following types of variant filters can be set up: 1) Sample-specific filters: Filter variants based on their characteristics in the sequenced reads of a specific sample. Multiple sample-specific filters are combined by logical AND, i.e., only variants that pass ALL sample-specific filters are kept. 2) Region filters: Filter variants based on the genomic region they affect. Multiple region filters are combined by logical OR, i.e., variants passing ANY region filter are kept. 3) Variant type filter: Filter variants by their type, i.e. whether they are single nucleotide variations (SNVs) or indels In addition, the 
sample
 filter can be used to reduce the samples encoded in a multi-sample VCF file to just those specified by the filter. The 
sample
 filter is included mainly for compatibility reasons: if an external tool cannot deal with the multisample file format, but instead looks only at the first sample-specific column of the file, you can use the filter to turn the multi-sample file into a single-sample file. Besides, the filter can also be used to change the order of the samples since it will sort the samples in the order specified in the filter field. 
Examples of sample-specific filters:
 
Simple genotype pattern
 genotype pattern: 1/1 ==> keep all variants in the vcf input file for which the specified sample's genotype is homozygous mutant 
Complex genotype pattern
 genotype pattern: 0/1, 0/0 ==> keep all variants for which the sample's genotype is either heterozygous or homozygous wildtype 
Multiple sample-specific filters
 Filter 1: genotype pattern: 0/0, Filter 2: genotype pattern 1/1: ==> keep all variants for which the first sample's gentoype is homozygous wildtype 
and
 the second sample's genotype is homozygous mutant 
Combining sample-specific filter criteria
 genotype pattern: 1/1, depth of coverage: 3, genotype quality: 9 ==> keep variants for which the sample's genotype is homozygous mutant 
and
 for which this genotype assignment is corroborated by a genotype quality score of at least 9 
and
 at least three reads from the sample cover the variant site 
TIP:
 As in the example above, genotype quality is typically most useful in combination with a genotype pattern. It acts then, effectively, to make the genotype filter more stringent. ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_varcall/0.1.9	".. class:: infomark 
What it does
 The tool transforms the read-centered information in the aligned reads input datasets into position-centered information including variant call statistics (using samtools mpileup and bcftools internally). 
It produces a BCF file that serves as the basis for all further variant analyses with MiModD
. ----- 
Notes on Advanced Settings:
 
MD5 checksums
 By default, the tool will check whether the input BAM dataset(s) provide(s) MD5 checksums for the reference genome contig/chromosome sequences used during read alignment (e.g., the 
MiModD Read Alignment
 tool stores these in the BAM file header). If it finds MD5 sums for all sequences, it will compare them to the checksums of the reference genome sequences used in the current tool run and abort with an error message if there is a discrepancy between them. If it finds contigs/chromosomes with matching checksum, but different names in the aligned reads dataset(s) and the reference genome dataset, it will use the name from the reference genome in its output. This behavior has two benefits: 1) It protects from accidental variant calling against a wrong reference genome (
i.e.
, a different one than that used during the alignment step), which would result in wrong calls. This is the primary reason why we recommend to leave the check activated. 2) It provides an opportunity to change sequence names between aligned reads files and variant call files by providing a reference genome file with altered sequence names (but identical sequence data). Since there may be rare cases where you 
really
 want to align against a reference genome with different checksums (e.g., you may have edited the reference sequence based on the alignment results), the check can be turned off, but only do this if you know 
exactly
 why. 
Average sample depth cap limit
 For each of a total of 
M
 BAM input datasets, the tool will only pile up a maximum number of reads 
N
 per position to avoid excessive memory usage with very large numbers of samples sequenced at high coverage. N will be calculated as the maximum of 
8000/M
 and 
DEPTH*S
, where 
S
 is the maximum number of samples found in a single input dataset and 
DEPTH
 is the 
average sample depth cap limit
 specified in the tool form. This parameter, thus sets the average depth of the pile-up per sample that is guaranteed to be used even when there is a very large number of samples. As can be seen from the formula above, however, it will rarely become relevant for any regular-size analysis. ---- .. class:: infomark For 
additional help
 see these resources: - The complete 
MiModD User Guide &lt;http://mimodd.readthedocs.io/en/doc0.1.9/usage_toc.html&gt;
 - The 
MiModD help forum &lt;https://groups.google.com/forum/#!forum/mimodd&gt;
 reachable also via 
email &lt;mailto:mimodd@googlegroups.com&gt;
__"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/monocle3_create/monocle3_create/0.1.4+galaxy2	"=========================================================================== Create Monocle3 object from input expression/metadata (
new_cell_data_set
) =========================================================================== Create a Monocle3 cds3 object from input expression data and optional metadata. The expression matrix, with genes as rows and cells as columns, is the only mandatory input. The cell metadata (with row names matching the column names of the expression matrix) and gene annotation (with row names matching the row names of the expression matrix) are optional. These files can be provided as RDS objects, or CSV/TSV. Select the format of your input files in the corresponding drop-downs. More information can be found at https://cole-trapnell-lab.github.io/monocle3/ and https://github.com/ebi-gene-expression-group/monocle-scripts 
Version history
 0.1.5+galaxy1: Updated to monocle3-cli 0.0.9 for plot cells to allow for multiple genes plotting. 0.1.5+galaxy0: Updated to monocle3-cli 0.0.8 to add top marker genes and fix container plots for plotCells (only those two modules upgraded). 0.1.4+galaxy0: Updated to monocle3-cli 0.0.7 to fix bug with column headers in tsv. 0.1.3+galaxy0: Updated to monocle3-cli 0.0.5 to fix bug with tsv inputs. 0.1.2+galaxy0: Initial version based on monocle3-cli 0.0.3 and monocle3 0.1.2"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/monocle3_diffexp/monocle3_diffExp/0.1.4+galaxy1	"======================================================================== Monocle3 differential expression testing along trajectory (
graph_test
) ======================================================================== Identify differentially expressed genes along the inferred trajectory. More information can be found at https://cole-trapnell-lab.github.io/monocle3/ and https://github.com/ebi-gene-expression-group/monocle-scripts 
Version history
 0.1.5+galaxy1: Updated to monocle3-cli 0.0.9 for plot cells to allow for multiple genes plotting. 0.1.5+galaxy0: Updated to monocle3-cli 0.0.8 to add top marker genes and fix container plots for plotCells (only those two modules upgraded). 0.1.4+galaxy0: Updated to monocle3-cli 0.0.7 to fix bug with column headers in tsv. 0.1.3+galaxy0: Updated to monocle3-cli 0.0.5 to fix bug with tsv inputs. 0.1.2+galaxy0: Initial version based on monocle3-cli 0.0.3 and monocle3 0.1.2"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/monocle3_learngraph/monocle3_learnGraph/0.1.4+galaxy0	"========================================================== Monocle3 identification of cell-cell graph (
learn_graph
) ========================================================== Identify the underlying cell-cell graph in the data. More information can be found at https://cole-trapnell-lab.github.io/monocle3/ and https://github.com/ebi-gene-expression-group/monocle-scripts 
Version history
 0.1.5+galaxy1: Updated to monocle3-cli 0.0.9 for plot cells to allow for multiple genes plotting. 0.1.5+galaxy0: Updated to monocle3-cli 0.0.8 to add top marker genes and fix container plots for plotCells (only those two modules upgraded). 0.1.4+galaxy0: Updated to monocle3-cli 0.0.7 to fix bug with column headers in tsv. 0.1.3+galaxy0: Updated to monocle3-cli 0.0.5 to fix bug with tsv inputs. 0.1.2+galaxy0: Initial version based on monocle3-cli 0.0.3 and monocle3 0.1.2"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/monocle3_ordercells/monocle3_orderCells/0.1.4+galaxy0	"========================================== Monocle3 ordering of cells (
order_cells
) ========================================== Order cells using the inferred graph More information can be found at https://cole-trapnell-lab.github.io/monocle3/ and https://github.com/ebi-gene-expression-group/monocle-scripts 
Version history
 0.1.5+galaxy1: Updated to monocle3-cli 0.0.9 for plot cells to allow for multiple genes plotting. 0.1.5+galaxy0: Updated to monocle3-cli 0.0.8 to add top marker genes and fix container plots for plotCells (only those two modules upgraded). 0.1.4+galaxy0: Updated to monocle3-cli 0.0.7 to fix bug with column headers in tsv. 0.1.3+galaxy0: Updated to monocle3-cli 0.0.5 to fix bug with tsv inputs. 0.1.2+galaxy0: Initial version based on monocle3-cli 0.0.3 and monocle3 0.1.2"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/monocle3_partition/monocle3_partition/0.1.4+galaxy0	"============================================== Monocle3 clustering of cells (
cluster_cells
) ============================================== Cluster the primary dimensionality reduced space More information can be found at https://cole-trapnell-lab.github.io/monocle3/ and https://github.com/ebi-gene-expression-group/monocle-scripts 
Version history
 0.1.5+galaxy1: Updated to monocle3-cli 0.0.9 for plot cells to allow for multiple genes plotting. 0.1.5+galaxy0: Updated to monocle3-cli 0.0.8 to add top marker genes and fix container plots for plotCells (only those two modules upgraded). 0.1.4+galaxy0: Updated to monocle3-cli 0.0.7 to fix bug with column headers in tsv. 0.1.3+galaxy0: Updated to monocle3-cli 0.0.5 to fix bug with tsv inputs. 0.1.2+galaxy0: Initial version based on monocle3-cli 0.0.3 and monocle3 0.1.2"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/monocle3_plotcells/monocle3_plotCells/0.1.5+galaxy1	"===================================== Monocle3 cell plotting (
plot_cells
) ===================================== Display various information about cells (clustering, pseudotime, the inferred graph, user-provided annotations, gene expression) in reduced dimensions. More information can be found at https://cole-trapnell-lab.github.io/monocle3/ and https://github.com/ebi-gene-expression-group/monocle-scripts 
Version history
 0.1.5+galaxy1: Updated to monocle3-cli 0.0.9 for plot cells to allow for multiple genes plotting. 0.1.5+galaxy0: Updated to monocle3-cli 0.0.8 to add top marker genes and fix container plots for plotCells (only those two modules upgraded). 0.1.4+galaxy0: Updated to monocle3-cli 0.0.7 to fix bug with column headers in tsv. 0.1.3+galaxy0: Updated to monocle3-cli 0.0.5 to fix bug with tsv inputs. 0.1.2+galaxy0: Initial version based on monocle3-cli 0.0.3 and monocle3 0.1.2"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/monocle3_preprocess/monocle3_preprocess/0.1.4+galaxy0	"==================================================================================== Preprocess Monocle3 object up to initial dimensionality reduction (
preprocess_cds
) ==================================================================================== Preprocess a Monocle3 object with size factor scaling of the count data, optionally log transforming, and computing an initial dimensionality reduction. This space is used as a starting point for a number of subsequent analyses. More information can be found at https://cole-trapnell-lab.github.io/monocle3/ and https://github.com/ebi-gene-expression-group/monocle-scripts 
Version history
 0.1.5+galaxy1: Updated to monocle3-cli 0.0.9 for plot cells to allow for multiple genes plotting. 0.1.5+galaxy0: Updated to monocle3-cli 0.0.8 to add top marker genes and fix container plots for plotCells (only those two modules upgraded). 0.1.4+galaxy0: Updated to monocle3-cli 0.0.7 to fix bug with column headers in tsv. 0.1.3+galaxy0: Updated to monocle3-cli 0.0.5 to fix bug with tsv inputs. 0.1.2+galaxy0: Initial version based on monocle3-cli 0.0.3 and monocle3 0.1.2"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/monocle3_reducedim/monocle3_reduceDim/0.1.4+galaxy0	"============================================================== Primary Monocle3 dimensionality reduction (
reduce_dimension
) ============================================================== Use the results of preprocessing to propose a primary dimensionally reduced space for use in downstream analysis. More information can be found at https://cole-trapnell-lab.github.io/monocle3/ and https://github.com/ebi-gene-expression-group/monocle-scripts 
Version history
 0.1.5+galaxy1: Updated to monocle3-cli 0.0.9 for plot cells to allow for multiple genes plotting. 0.1.5+galaxy0: Updated to monocle3-cli 0.0.8 to add top marker genes and fix container plots for plotCells (only those two modules upgraded). 0.1.4+galaxy0: Updated to monocle3-cli 0.0.7 to fix bug with column headers in tsv. 0.1.3+galaxy0: Updated to monocle3-cli 0.0.5 to fix bug with tsv inputs. 0.1.2+galaxy0: Initial version based on monocle3-cli 0.0.3 and monocle3 0.1.2"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/monocle3_topmarkers/monocle3_topmarkers/0.1.5+galaxy0	"========================================================== Monocle3 top markers genes ========================================================== Identify the genes most specifically expressed in groups of cells. See inline help for options. More information can be found at https://cole-trapnell-lab.github.io/monocle3/ and https://github.com/ebi-gene-expression-group/monocle-scripts 
Version history
 0.1.5+galaxy1: Updated to monocle3-cli 0.0.9 for plot cells to allow for multiple genes plotting. 0.1.5+galaxy0: Updated to monocle3-cli 0.0.8 to add top marker genes and fix container plots for plotCells (only those two modules upgraded). 0.1.4+galaxy0: Updated to monocle3-cli 0.0.7 to fix bug with column headers in tsv. 0.1.3+galaxy0: Updated to monocle3-cli 0.0.5 to fix bug with tsv inputs. 0.1.2+galaxy0: Initial version based on monocle3-cli 0.0.3 and monocle3 0.1.2"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_align_check/mothur_align_check/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The align.check
 command allows you to calculate the number of potentially misaligned bases in a 16S rRNA gene sequence alignment using a secondary_structure_map_. If you are familiar with the editor window in ARB, this is the same as counting the number of ~, #, -, and = signs. .. _secondary_structure_map: https://www.mothur.org/wiki/Secondary_structure_map .. _align.check: https://www.mothur.org/wiki/Align.check"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_align_seqs/mothur_align_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The align.seqs
 command aligns a user-supplied fasta-formatted fasta sequence file to a user-supplied fasta-formatted template_alignment_. The general approach is to i) find the closest template for each fasta using kmer searching, blastn, or suffix tree searching; ii) to make a pairwise alignment between the fasta and de-gapped template sequences using the Needleman-Wunsch, Gotoh,or blastn algorithms; and iii) to re-insert gaps to the fasta and template pairwise alignments using the NAST algorithm so that the fasta sequence alignment is compatible with the original template alignment. In general the alignment is very fast - we are able to align over 186,000 full-length sequences to the SILVA alignment in less than 3 hrs with a quality as good as the SINA aligner. Furthermore, this rate can be accelerated using multiple processors. While the aligner doesn't explicitly take into account the secondary structure of the 16S rRNA gene, if the template database is based on the secondary structure, then the resulting alignment will at least be implicitly based on the secondary structure. .. _template_alignment: https://www.mothur.org/wiki/Alignment_database .. _align.seqs: https://www.mothur.org/wiki/Align.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_amova/mothur_amova/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The amova
 command calculates the analysis of molecular variance from a phylip_distance_matrix_, a nonparametric analog of traditional analysis of variance. This method is widely used in population genetics to test the hypothesis that genetic diversity within two populations is not significantly different from that which would result from pooling the two populations. A design file partitions a list of names into groups. It is a tab-delimited file with 2 columns: name and group, e.g. : ======= ======= duck bird cow mammal pig mammal goose bird cobra reptile ======= ======= The Make_Design tool can construct a design file from a Mothur dataset that contains group names. .. _phylip_distance_matrix: https://www.mothur.org/wiki/Phylip-formatted_distance_matrix .. _amova: https://www.mothur.org/wiki/Amova"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_anosim/mothur_anosim/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The anosim
 command uses a phylip_distance_matrix_ and a design file to calculate the non-parametric multivariate analysis of changes in community structure. A design file partitions a list of names into groups. It is a tab-delimited file with 2 columns: name and group, e.g. : ======= ======= duck bird cow mammal pig mammal goose bird cobra reptile ======= ======= The Make_Design tool can construct a design file from a Mothur dataset that contains group names. .. _phylip_distance_matrix: https://www.mothur.org/wiki/Phylip-formatted_distance_matrix .. _anosim: https://www.mothur.org/wiki/Anosim"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_bin_seqs/mothur_bin_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The bin.seqs
 command generates fasta-formatted files where sequences are ordered according to the OTU from the list_file_ that they belong to. Such an output may be helpful for generating primers specific to an OTU or for classification of sequences. .. _list_file: https://www.mothur.org/wiki/List_file .. _bin.seqs: https://www.mothur.org/wiki/Bin.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_biom_info/mothur_biom_info/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. _Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The biom.info command reads a biom file creates a shared file. If your biom file contains metadata mothur will also create taxonomy or constaxonomy along with tax.summary files."
toolshed.g2.bx.psu.edu/repos/iuc/mothur_chimera_bellerophon/mothur_chimera_bellerophon/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The chimera.bellerophon
 command identifies putative chimeras using the bellerophon_ approach. Advantages of Bellerophon: 1) You can process all sequences from a PCR-clone library in a single analysis and don't have to inspect outputs for every sequence in the dataset. 2) The approximate putative breakpoint is calculated using a sliding window (see above) and will help verification of the chimera manually. 3) A chimeric sequence is not only tested against two (putative) parent sequences but rather is assessed by how well it fits into the complete phylogenetic environment of a multiple sequence alignment. Hence sequences do not become invisible to the program as is the case with CHIMERA_CHECK (see Ref 1 below). 4) The calculations Bellerophon uses to detect chimeric sequences are computationally relatively cheap and results are quickly calculated for datasets with up 50 sequences (~1 min). Larger datasets take longer - 100 sequences ~30 min, 300 sequences ~8 hours. Tips for using Bellerophon: 1) Bellerophon works most efficiently if the parent sequences or non-chimeric sequences closely related to the parent sequences are present in the dataset analyzed. Therefore, as many sequences as possible from the one PCR-clone library should be included in the analysis since the parent sequences of any chimera are most likely to be in that dataset. Addition of non-chimeric outgroup sequences (e.g. from isolates) may help refine an analysis by providing reference points (and a broader phylogenetic context) in the analysis, but be aware of increasing analysis time with bigger datasets. 2) Bellerophon is compromised by using sequences of different lengths as this can produce artificial skews in distance matrices of fragments of the alignment. Datasets containing sequences of the same length and covering the same portion of the gene should be used (usually not an issue with sequences from a PCR-clone library). The filter will automatically remove sequences too short for the window size, i.e. less than 600 bp for a window size of 300. 3) If possible multiple window sizes should be used as the number of identified chimeras can vary with the choice of the window size. 4) Re-running the dataset without the first reported chimeras may identify additional putative chimeras by reducing noise in the analysis. Ideally, the dataset should continue to be re-run removing previously reported chimeras until no chimeras are identified. 5) Bellerophon should be used in concert with other detection methods such as CHIMERA_CHECK and putatively identified chimeras should always be confirmed by manual inspection of the sequences for signature shifts. .. _bellerophon: http://comp-bio.anu.edu.au/Bellerophon/doc/doc.html .. _chimera.bellerophon: https://www.mothur.org/wiki/Chimera.bellerophon"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_chimera_ccode/mothur_chimera_ccode/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The chimera.ccode
 command identifies putative chimeras using the ccode approach (Chimera and Cross-Over Detection and Evaluation). Ccode_ compares differences in distances, for each word, between query sequence and reference sequences, and reference sequences and themselves. This method was written using the algorithms described in the paper_ ""Evaluating putative chimeric sequences from PCR-amplified products"" by Juan M. Gonzalez, Johannes Zimmerman and Cesareo Saiz-Jimenez. The program can analyze sequences for any required word length. Generally, values of 5-20% of sequence length appear to deliver accurate results, for example, working on 16S rDNA sequences with a full-length of #1500 nt. It should be noted that the use of fragments either too long or too short might result in a reduction of sensitivity. .. _Ccode: http://www.microextreme.net/downloads.html .. _paper: http://bioinformatics.oxfordjournals.org/content/21/3/333.full.pdf .. _chimera.ccode: https://www.mothur.org/wiki/Chimera.ccode"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_chimera_check/mothur_chimera_check/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The chimera.check
 command identifies putative chimeras using the chimeraCheck approach. It looks at distance of left side of query to it's closest match + distance of right side of query to it's closest match - distance of whole query and its closest match over several windows. Note: following the RDP model this method does not determine whether or not a sequence is chimeric, but allows you to determine that based on the IS values produced. .. _chimera.check: https://www.mothur.org/wiki/Chimera.check"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_chimera_perseus/mothur_chimera_perseus/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The chimera.perseus
 command reads a fasta and name file, and outputs potentially chimeric sequences. .. _chimera.perseus: https://www.mothur.org/wiki/Chimera.perseus"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_chimera_pintail/mothur_chimera_pintail/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The chimera.pintail
 command identifies putative chimeras using the pintail approach. It looks at the variation between the expected differences and the observed differences in the query sequence over several windows. This method was written using the algorithms described in the paper_ ""At Least 1 in 20 16S rRNA Sequence Records Currently Held in the Public Repositories is Estimated To Contain Substantial Anomalies"" by Kevin E. Ashelford 1, Nadia A. Chuzhanova 3, John C. Fry 1, Antonia J. Jones 2 and Andrew J. Weightman 1. The Pintail algorithm is a technique for determining whether a 16S rDNA sequence is anomalous. It is based on the idea that the extent of local base differences between two aligned 16S rDNA sequences should be roughly the same along the length of the alignment (having allowed for the underlying pattern of hypervariable and conserved regions known to exist within the 16S rRNA gene). In other words, evolutionary distance between two reliable sequences should be constant along the length of the gene. In contrast, if an error-free sequence is compared with an anomalous sequence, evolutionary distance along the alignment is unlikely to be constant, especially if the anomaly in question is a chimera and formed from phylogenetically different parental sequences. The Pintail algorithm is designed to detect and quantify such local variations and in doing so generates the Deviation from Expectation (DE) statistic. The higher the DE value, the greater the likelihood that the query is anomalous. The algorithm works as follows The sequence to be checked (the query) is first globally aligned with a phylogenetically similar sequence known to be error-free (the subject). At regular intervals along the resulting alignment, the local evolutionary distance between query and subject is estimated by recording percentage base mismatches within a sampling window of fixed length. The resulting array of percentages (observed percentage differences) reflects variations in evolutionary distance between the query and subject along the length of the 16S rRNA gene. Subtracting observed percentage differences from an equivalent array of expected percentage differences (predicted values for error-free sequences), we obtain a set of deviations, the standard deviation of which (Deviation from Expectation, DE) summarises the variation between observed and expected datasets. The greater the DE value, the greater the disparity there is between observed and expected percentage differences, and the more likely it is that the query sequence is anomalous. .. _paper: http://www.ncbi.nlm.nih.gov/pubmed/16332745 .. _chimera.pintail: https://www.mothur.org/wiki/Chimera.pintail"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_chimera_slayer/mothur_chimera_slayer/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The chimera.slayer
 command identifies putative chimeras using the slayer approach. ChimeraSlayer_ is a chimeric sequence detection utility, compatible with near-full length Sanger sequences and shorter 454-FLX sequences (~500 bp). Chimera Slayer involves the following series of steps that operate to flag chimeric 16S rRNA sequences: (A) the ends of a query sequence are searched against an included database of reference chimera-free 16S sequences to identify potential parents of a chimera; (B) candidate parents of a chimera are selected as those that form a branched best scoring alignment to the NAST-formatted query sequence; (C) the NAST alignment of the query sequence is improved in a 'chimera-aware' profile-based NAST realignment to the selected reference parent sequences; and (D) an evolutionary framework is used to flag query sequences found to exhibit greater sequence homology to an in silico chimera formed between any two of the selected reference parent sequences. Note: It is not recommended to blindly discard all sequences flagged as chimeras. Some may represent naturally formed chimeras that do not represent PCR artifacts. Sequences flagged may warrant further investigation. .. _ChimeraSlayer: http://microbiomeutil.sourceforge.net/ .. _chimera.slayer: https://www.mothur.org/wiki/Chimera.slayer"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_chimera_uchime/mothur_chimera_uchime/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The chimera.uchime
 command reads a fasta file and reference file and outputs potentially chimeric sequences. The original uchime program was written by Robert C. Edgar and donated to the public domain, http://drive5.com/uchime .. _chimera.uchime: https://www.mothur.org/wiki/Chimera.uchime Version 1.23.0: Upgrades tool dependency to mothur 1.33 and adds support for count (mothur 1.28) and dereplicate (mothur 1.29) options."
toolshed.g2.bx.psu.edu/repos/iuc/mothur_chimera_vsearch/mothur_chimera_vsearch/1.39.5.2	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The chimera.vsearch
 command reads a fasta file and reference file and outputs potentially chimeric sequences. .. _chimera.vsearch: https://www.mothur.org/wiki/Chimera.vsearch"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_chop_seqs/mothur_chop_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The chop.seqs
 command reads a fasta file of sequences and outputs a .chop.fasta file containing the trimmed sequences. It works on both aligned and unaligned sequences. .. _chop.seqs: https://www.mothur.org/wiki/Chop.seqs v1.20.0: Updated to 1.33. Added name, group and count options for mothur version 1.31.0"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_classify_otu/mothur_classify_otu/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The classify.otu
 command assigns sequences to chosen taxonomy outline. The basis parameter allows you indicate what you want the summary file to represent, options are otu and sequence. Default is otu. For example consider the following basis=sequence could give Clostridiales 3 105 16 43 46, where 105 is the total number of sequences whose otu classified to Clostridiales. 16 is the number of sequences in the otus from groupA, 43 is the number of sequences in the otus from groupB, and 46 is the number of sequences in the otus from groupC. Now for basis=otu could give Clostridiales 3 7 6 1 2, where 7 is the number of otus that classified to Clostridiales. 6 is the number of otus containing sequences from groupA, 1 is the number of otus containing sequences from groupB, and 2 is the number of otus containing sequences from groupC. .. _classify.otu: https://www.mothur.org/wiki/Classify.otu v1.21.0: Updated to use Mothur 1.33. Added count parameter (1.28.0) and persample parameter (1.29.0)"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_classify_rf/mothur_classify_rf/1.36.1.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. _Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 .. _classify.rf: https://www.mothur.org/wiki/Classify.rf"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_classify_seqs/mothur_classify_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The classify.seqs
 command assigns sequences to chosen taxonomy outline. .. _classify.seqs: https://www.mothur.org/wiki/Classify.seqs v1.22.0: Updated for Mothur 1.33. Added count parameter (1.28), added relabund parameter (1.33), bayesian term changed to wang."
toolshed.g2.bx.psu.edu/repos/iuc/mothur_classify_tree/mothur_classify_tree/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The classify.tree
 command is used to get a consensus taxonomy for each node on a tree. Input is a taxonomy_ and a tree_ with optional name_ or group_ reference. The output is a tree_ and a summary.. TreeNode NumRep Taxonomy 243 2 Bacteria(100);""Firmicutes""(100);""Clostridia""(100);Clostridiales(100);""Ruminococcaceae""(100);Faecalibacterium(100); 244 3 Bacteria(100);""Firmicutes""(100);""Clostridia""(100);Clostridiales(100);""Ruminococcaceae""(100);Faecalibacterium(100); 245 4 Bacteria(100);""Firmicutes""(100);""Clostridia""(100);Clostridiales(100);""Ruminococcaceae""(100);Faecalibacterium(100); ... .. _taxonomy: https://www.mothur.org/wiki/Taxonomy_outline .. _tree: http://evolution.genetics.washington.edu/phylip/newicktree.html .. _name: https://www.mothur.org/wiki/Name_file .. _group: https://www.mothur.org/wiki/Group_file .. _classify.tree: https://www.mothur.org/wiki/Classify.tree v.1.25.0: Trivial upgrade to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_clearcut/mothur_clearcut/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The clearcut
 command runs clearcut The clearcut command allows mothur users to run the clearcut program from within mothur. The clearcut program written by Initiative for Bioinformatics and Evolutionary Studies (IBEST) at the University of Idaho. Clearcut is a stand-alone reference implementation of relaxed neighbor joining (RNJ). Clearcut is capable of taking either a distance matrix or a multiple sequence alignment (MSA) as input. If necessary, Clearcut will compute corrected distances based on a configurable distance correction model (Jukes-Cantor or Kimura). Clearcut outputs a phylogenetic tree in Newick format and an optional corrected distance matrix. .. _clearcut: https://www.mothur.org/wiki/Clearcut v.1.20.0: Trivial upgrade to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_cluster/mothur_cluster/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The cluster
 command assign sequences to OTUs (Operational Taxonomy Unit). The assignment is based on a phylip-formatted_distance_matrix_ or a column-formatted_distance_matrix_ and name_ file. It generates a list_, a sabund_ (Species Abundance), and a rabund_ (Relative Abundance) file. .. _phylip-formatted_distance_matrix: https://www.mothur.org/wiki/Phylip-formatted_distance_matrix .. _column-formatted_distance_matrix: https://www.mothur.org/wiki/Column-formatted_distance_matrix .. _name: https://www.mothur.org/wiki/Name_file .. _list: https://www.mothur.org/wiki/List_file .. _rabund: https://www.mothur.org/wiki/Rabund_file .. _sabund: https://www.mothur.org/wiki/Sabund_file .. _cluster: https://www.mothur.org/wiki/Cluster"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_cluster_classic/mothur_cluster_classic/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The cluster.classic
 command assign sequences to OTUs (Operational Taxonomy Unit). .. _cluster.classic: https://www.mothur.org/wiki/Cluster.classic"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_cluster_fragments/mothur_cluster_fragments/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The cluster.fragments
 command groups sequences that are part of a larger sequence. .. _cluster.fragments: https://www.mothur.org/wiki/Cluster.fragments v1.21: Updated to Mothur 1.33. Added count parameter."
toolshed.g2.bx.psu.edu/repos/iuc/mothur_cluster_split/mothur_cluster_split/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The cluster.split
 command assign sequences to OTUs (Operational Taxonomy Unit). .. _cluster.split: https://www.mothur.org/wiki/Cluster.split v1.28.0: Upgraded to Mothur 1.33, introduced cluster boolean."
toolshed.g2.bx.psu.edu/repos/iuc/mothur_collect_shared/mothur_collect_shared/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The collect.shared
 command generates collector's curves for calculators_, which describe the similarity between communities or their shared richness. Collector's curves describe how richness or diversity change as you sample additional individuals. If a collector's curve becomes parallel to the x-axis, you can be reasonably confident that you have done a good job of sampling and can trust the last value in the curve. For calc parameter choices see: https://www.mothur.org/wiki/Calculators .. _calculators: https://www.mothur.org/wiki/Calculators .. _collect.shared: https://www.mothur.org/wiki/Collect.shared"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_collect_single/mothur_collect_single/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The collect.single
 command generates collector's curves using calculators_, that describe the richness, diversity, and other features of individual samples. Collector's curves describe how richness or diversity change as you sample additional individuals. If a collector's curve becomes parallel to the x-axis, you can be reasonably confident that you have done a good job of sampling and can trust the last value in the curve. Otherwise, you need to keep sampling. For calc parameter choices see: https://www.mothur.org/wiki/Calculators .. _calculators: https://www.mothur.org/wiki/Calculators .. _collect.single: https://www.mothur.org/wiki/Collect.single"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_consensus_seqs/mothur_consensus_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The consensus.seqs
 command can be used in 2 ways: create a consensus sequence from a fastafile, or with a listfile create a consensus sequence for each otu. Sequences must be aligned. .. _consensus.seqs: https://www.mothur.org/wiki/Consensus.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_cooccurrence/mothur_cooccurrence/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The cooccurrence
 command variance calculates four metrics and tests their significance to assess whether presence-absence patterns are different than what one would expect by chance. The input is a shared_ file. The output can be filtered by groups and labels. 
metric
 The metric parameter options are 
cscore
, 
checker
, 
combo
 and 
vratio
. Default=cscore. The cscore or checkerboard score [1] is a metric that measures species segregation. It is the mean number of checkerboard units per species pair. The checker metric [2] counts the number of species pairs forming a perfect checkerboard. The combo metric [3] is the number of unique species pairs. The vratio or variance ratio [4] is a measure of the species association calculated by the ratio of the variance in total species number to the sum of the variances of the species. :: [1] Stone, L., and A. Roberts. 1990. The checkerboard score and species distributions. Ocelogia. 85:74-79. [2] Diamond, J. M. 1975. Assembly of species communities. Pages 342-444 in M. L. Cody and J. M. Diamond, editors. Ecology and evolution of communities. Harvard University Press, Cambridge, Massachusetts, USA. [3] Pielou, D. P., and E. C. Pielou. 1968 Association among species of infrequent occurrence: the insect and spider fauna of Polypours betulinus (Bulliard) Fries. Journal of Theoretical Biology 21:202-216. [4] Schluter, D. 1984. A variance test for detecting species associations, with some example applications. Ecology 65:998-1005. [5] Gotelli, Nicholas J. 2000. NULL MODEL ANALYSIS OF SPECIES CO-OCCURRENCE PATTERNS. Ecology 81:2606-2621. 
matrixmodel
 The matrixmodel parameter allows you to select the model you would like to use. Options are sim1, sim2, sim3, sim4, sim5, sim6, sim7, sim8 and sim9. Default=sim2. Each sim implements a different algorithm for generating null matrices with constraints on the rows (species) and columns (sites).:: ===================== ====================== ======================= ====================== Rows Columns equiprobable Columns proportional Column totals fixed ===================== ====================== ======================= ====================== Rows equiprobable sim1 sim6 sim3 Rows proportional sim7 sim8 sim5 Row totals fixed sim2 sim4 sim9 ===================== ====================== ======================= ====================== Equiprobable rows or columns means that each row, column or both is not dependent on the original co-occurrence matrix. Each species or site has an equal change of occurring in the null matrix. Proportional rows or columns means that the proportion of occurrences in rows, columns or both in the original co-occurrence matrix are preserved but the totals may differ. Each species or site's chances of occurring are proportional to their occurrence in the original co-occurrence matrix. Fixed row or column totals preserves the total number of occurrences in rows, columns or both in the original co-occurrence matrix. Sim9 is a special case that is not probabilistic. Since both the row and column totals are preserved the only way to randomize the matrix is with a checkerboard swap. When a checkerboard appears in the matrix the 1s and 0s are swapped to their mirror image to preserve the species and site totals. Checkerboard:: 10 01 Swap:: 01 10 suggested metric/matrixmodel combinations:: ======== ======== ======== ======== cscore checker combo vratio ======== ======== ======== ======== sim9 sim9 sim9 sim2 sim2 sim2 sim2 sim4 - - sim4 sim8 - - sim8 - ======== ======== ======== ======== Careful readers will note that none of the suggested matrixmodels have equiprobable rows (species). This is because tests of co-occurrence are quite sensitive to the frequency of species occurrence. As such, rowtotals should be maintained or at least kept proportional in the null models. Sim9 is well suited to co-occurrence matrices that have an ""island list"" structure. Island lists are often found in classical ecology datasets that contain species with well defined habitat patches and are rarely degenerate (matrices that contain empty rows or columns). Sim2 is well suited for co-occurrence matrices that have a ""sample list"" structure. Sample list structured data are found where species have relatively homogeneous habitats and degenerate matrices are not uncommon. In these matrices species will often occur in only one site. The default values of cscore and sim2 have been selected because the c-score is not very sensitive to noise in the data and when used with sim9 or sim2 is not particularly prone to false positives. Sim2 has been chosen because of the prevalence of degenerate matrices. These are just guidelines, however, be sure to select a metric and matrix model that is best suited to the type of data you are analyzing. It should be noted that sim9 cannot be used with vratio because in sim9 both the column and row totals are maintained, hence there will be no variance. Please see [5] for more details on metric/null model selection. .. _shared: https://www.mothur.org/wiki/Shared_file .. _cooccurrence: https://www.mothur.org/wiki/Cooccurrence v1.26.0: Updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_corr_axes/mothur_corr_axes/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The corr.axes
 command calculates the correlation of data to axes. .. _corr.axes: https://www.mothur.org/wiki/Corr.axes v.1.21.0: Updated to mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_count_groups/mothur_count_groups/1.39.5.0	All groups displayed if none are selected.
toolshed.g2.bx.psu.edu/repos/iuc/mothur_count_seqs/mothur_count_seqs/1.39.5.0	All groups displayed if none are selected.
toolshed.g2.bx.psu.edu/repos/iuc/mothur_create_database/mothur_create_database/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The create.database
 command reads a list_ or shared_ file, .cons.taxonomy, .rep.fasta, .rep.names and optional group file, and creates a database file. .. _list: https://www.mothur.org/wiki/List_file .. _shared: https://www.mothur.org/wiki/Shared_file .. _create.database: https://www.mothur.org/wiki/Create.database v.1.28.0: Updated to Mothur 1.33, added count paramter."
toolshed.g2.bx.psu.edu/repos/iuc/mothur_degap_seqs/mothur_degap_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The degap.seqs
 command reads a fasta file and outputs a fasta containing the sequences after all gap characters are removed. .. _degap.seqs: https://www.mothur.org/wiki/Degap.seqs v1.21.0: Updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_deunique_seqs/mothur_deunique_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The deunique.seqs
 command is the reverse of the unique.seqs command, and creates a fasta file from a fasta and name_ file. .. _name: https://www.mothur.org/wiki/Name_file .. _deunique.seqs: https://www.mothur.org/wiki/Deunique.seqs v.1.21.0: Updated to Mothur 1.33, added option to provide count instead of names file, new groups file as output"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_deunique_tree/mothur_deunique_tree/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The deunique.tree
 command is the reinserts the redundant sequence identiers back into a unique tree using a name_ file. .. _name: https://www.mothur.org/wiki/Name_file .. _deunique.tree: https://www.mothur.org/wiki/Deunique.tree v1.21.0: Updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_dist_seqs/mothur_dist_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The dist.seqs
 command will calculate uncorrected pairwise distances between aligned sequences. The command will generate a column-formatted_distance_matrix_ that is compatible with the column option in the read.dist command. The command is also able to generate a phylip-formatted_distance_matrix_. There are several options for how to handle gap comparisons and terminal gaps. .. _column-formatted_distance_matrix: https://www.mothur.org/wiki/Column-formatted_distance_matrix .. _phylip-formatted_distance_matrix: https://www.mothur.org/wiki/Phylip-formatted_distance_matrix .. _dist.seqs: https://www.mothur.org/wiki/Dist.seqs v.1.20.0: Updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_dist_shared/mothur_dist_shared/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The dist.shared
 command will generate a phylip-formatted_distance_matrix_ that describes the dissimilarity (1-similarity) among multiple groups from a shared_ file. For calc parameter choices see: https://www.mothur.org/wiki/Calculators .. _phylip-formatted_distance_matrix: https://www.mothur.org/wiki/Phylip-formatted_distance_matrix .. _shared: https://www.mothur.org/wiki/Shared_file .. _dist.shared: https://www.mothur.org/wiki/Dist.shared v1.26.0: Updated to Mothur 1.33. Omitted calculators since they do not appear to be available."
toolshed.g2.bx.psu.edu/repos/iuc/mothur_fastq_info/mothur_fastq_info/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The fastq.info
 command reads a fastq file and creates a fasta and quality file. .. _fastq.info: https://www.mothur.org/wiki/Fastq.info"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_filter_seqs/mothur_filter_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The filter.seqs
 command removes columns from alignments based on a criteria defined by the user. For example, alignments generated against reference alignments (e.g. from RDP, SILVA, or greengenes) often have columns where every character is either a '.' or a '-'. These columns are not included in calculating distances because they have no information in them. By removing these columns, the calculation of a large number of distances is accelerated. Also, people also like to mask their sequences to remove variable regions using a soft or hard mask (e.g. Lane's mask). This type of masking is only encouraged for deep-level phylogenetic analysis, not fine level analysis such as that needed with calculating OTUs. .. _filter.seqs: https://www.mothur.org/wiki/Filter.seqs v.1.20.0: Updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_filter_shared/mothur_filter_shared/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The filter.shared
 is used to remove OTUs based on various critieria. .. _filter.shared: https://www.mothur.org/wiki/Filter.shared"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_communitytype/mothur_get_communitytype/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. _Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 .. _get.communitytype: https://www.mothur.org/wiki/Get.communitytype"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_coremicrobiome/mothur_get_coremicrobiome/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.coremicrobiome
 command determines the fraction of OTUs that are found in varying numbers of samples for different minimum relative abundances. .. _get.coremicrobiome: https://www.mothur.org/wiki/Get.coremicrobiome v1.27.0: Updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_dists/mothur_get_dists/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.dists
 command selects distances from a phylip or column file related to groups or sequences listed in an accnos file. .. _get.dists: https://www.mothur.org/wiki/Get.dists v.1.20.0: Updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_group/mothur_get_group/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.group
 command generate principle components plot data. .. _get.group: https://www.mothur.org/wiki/Get.group v.1.20.0: Updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_groups/mothur_get_groups/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.groups
 command selects sequences from a specific group or set of groups from the following file types: fasta, fasta, name_, group_, list_, taxonomy_. .. _name: https://www.mothur.org/wiki/Name_file .. _group: https://www.mothur.org/wiki/Group_file .. _list: https://www.mothur.org/wiki/List_file .. _taxonomy: https://www.mothur.org/wiki/Taxonomy_outline .. _get.groups: https://www.mothur.org/wiki/Get.groups"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_label/mothur_get_label/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.label
 command generate principle components plot data. .. _get.label: https://www.mothur.org/wiki/Get.label v.1.20.0: Updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_lineage/mothur_get_lineage/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.lineage
 command reads a taxonomy_ file and a taxon and generates a new file that contains only the sequences in the that are from that taxon. You may also include either a fasta, name_, group_, list_, or align.report_ file to this command and mothur will generate new files for each of those containing only the selected sequences. .. _taxonomy: https://www.mothur.org/wiki/Taxonomy_outline .. _name: https://www.mothur.org/wiki/Name_file .. _group: https://www.mothur.org/wiki/Group_file .. _list: https://www.mothur.org/wiki/List_file .. _align.report: https://www.mothur.org/wiki/Align.seqs .. _get.lineage: https://www.mothur.org/wiki/Get.lineage"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_mimarkspackage/mothur_get_mimarkspackage/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.mimarkspackage
 command creates a mimarks package form with your groups. .. _get.mimarkspackage: https://www.mothur.org/wiki/Get.mimarkspackage"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_otulabels/mothur_get_otulabels/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.otulabels command selects otu labels from the output from classify.otu
, corr.axes_ and otu.association_. This can be useful especially with subsampled datasets or when groups have been selected. .. _classify.otu: https://www.mothur.org/wiki/Classify.otu .. _corr.axes: https://www.mothur.org/wiki/Corr.axes .. _otu.association: https://www.mothur.org/wiki/Otu.association v.1.27.0: Added list and shared parameters, updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_otulist/mothur_get_otulist/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.otulist
 command parses a list file and creates an .otu file for each distance containing 2 columns. The first column is the OTU number the second column is a list of sequences in that OTU. .. _get.otulist: https://www.mothur.org/wiki/Get.otulist v.1.20.0: Updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_oturep/mothur_get_oturep/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.oturep
 command generates a fasta-formatted sequence file containing only a representative sequence for each OTU. The opposite of the bin.seqs command. .. _get.oturep: https://www.mothur.org/wiki/Get.oturep v1.23.0: Updated to Mothur 1.33, added count and method parameter"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_otus/mothur_get_otus/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. _Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.otus command selects otus from a given list. .. _list: https://www.mothur.org/wiki/List_file .. _get.otus: https://www.mothur.org/wiki/Get.otus"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_rabund/mothur_get_rabund/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.rabund
 command generates an rabund_ file from a list_ or sabund_ file. .. _rabund: https://www.mothur.org/wiki/Rabund_file .. _list: https://www.mothur.org/wiki/List_file .. _sabund: https://www.mothur.org/wiki/Sabund_file .. _get.rabund: https://www.mothur.org/wiki/Get.rabund"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_relabund/mothur_get_relabund/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.relabund
 command calculates the relative abundance of each otu in a sample from a shared_ file. It outputs a .relabund_ file. .. _shared: https://www.mothur.org/wiki/Shared_file .. _get.relabund: https://www.mothur.org/wiki/Get.relabund v.1.21.0: Updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_sabund/mothur_get_sabund/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.sabund
 command generates an sabund_ file from a list_ or rabund_ file. .. _sabund: https://www.mothur.org/wiki/Sabund_file .. _list: https://www.mothur.org/wiki/List_file .. _rabund: https://www.mothur.org/wiki/Rabund_file .. _get.sabund: https://www.mothur.org/wiki/Get.sabund"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_seqs/mothur_get_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.seqs
 command takes a list of sequence names and either a fasta, name_, group_, list_, align.report_ or taxonomy_ file to generate a new file that contains only the sequences in the list. This command may be used in conjunction with the list.seqs_ command to help screen a sequence collection. .. _name: https://www.mothur.org/wiki/Name_file .. _group: https://www.mothur.org/wiki/Group_file .. _list: https://www.mothur.org/wiki/List_file .. _align.report: https://www.mothur.org/wiki/Align.seqs .. _taxonomy: https://www.mothur.org/wiki/Taxonomy_outline .. _list.seqs: https://www.mothur.org/wiki/list.seqs .. _get.seqs: https://www.mothur.org/wiki/Get.seqs v.1.27.0 : Updated to Mothur 1.33, added count and fastq params"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_sharedseqs/mothur_get_sharedseqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The get.sharedseqs
 command takes a list and group file and outputs a .shared.seqs file for each distance. This is useful for those cases where you might be interested in identifying sequences that are either unique or shared by specific groups, which you could then classify. .. _get.sharedseqs: https://www.mothur.org/wiki/Get.sharedseqs v1.21.0: Updated to Mothur 1.33, added shared file option"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_hcluster/mothur_hcluster/1.36.1.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The hcluster
 command assign sequences to OTUs (Operational Taxonomy Unit). The assignment is based on a phylip-formatted_distance_matrix_ or a column-formatted_distance_matrix_ and name_ file. It generates a list_, a sabund_ (Species Abundance), and a rabund_ (Relative Abundance) file. .. _phylip-formatted_distance_matrix: https://www.mothur.org/wiki/Phylip-formatted_distance_matrix .. _column-formatted_distance_matrix: https://www.mothur.org/wiki/Column-formatted_distance_matrix .. _name: https://www.mothur.org/wiki/Name_file .. _list: https://www.mothur.org/wiki/List_file .. _rabund: https://www.mothur.org/wiki/Rabund_file .. _sabund: https://www.mothur.org/wiki/Sabund_file"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_heatmap_bin/mothur_heatmap_bin/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The heatmap.bin
 command generates a heat map from data provided in either a list_ or a shared_ file. .. _list: https://www.mothur.org/wiki/List_file .. _shared: https://www.mothur.org/wiki/Shared_file .. _heatmap.bin: https://www.mothur.org/wiki/Heatmap.bin v.1.21.0: Updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_heatmap_sim/mothur_heatmap_sim/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The heatmap.sim
 command generates a heat map from data provided in either a shared_ file, a phylip_ distance matrix, or a column_ distance matrix and a name_ file. For calc parameter choices see: https://www.mothur.org/wiki/Calculators .. _shared: https://www.mothur.org/wiki/Shared_file .. _phylip: https://www.mothur.org/wiki/Phylip-formatted_distance_matrix .. _column: https://www.mothur.org/wiki/Column-formatted_distance_matrix .. _name: https://www.mothur.org/wiki/Name_file .. _heatmap.sim: https://www.mothur.org/wiki/Heatmap.sim v.1.24.0: Updated to Mothur 1.33, added count parameter"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_homova/mothur_homova/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The homova
 command calculates the homogeneity of molecular variance (HOMOVA) from a phylip_distance_matrix_, a nonparametric analog of Bartlett's test for homo- geneity of variance, which has been used in population genetics to test the hypothesis that the genetic diversity within two or more populations is homogeneous. A design file partitions a list of names into groups. It is a tab-delimited file with 2 columns: name and group, e.g. : ======= ======= duck bird cow mammal pig mammal goose bird cobra reptile ======= ======= The Make_Design tool can construct a design file from a Mothur dataset that contains group names. .. _phylip_distance_matrix: https://www.mothur.org/wiki/Phylip-formatted_distance_matrix .. _homova: https://www.mothur.org/wiki/Homova v.1.20.0: Updated to Mothur 1.33, added sets parameter"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_indicator/mothur_indicator/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The indicator
 command reads a shared_ or relabund_ file and a tree file, and outputs a .indicator.summary file and when a tree file is given a .indicator.tre file. The summary file lists the indicator value for each OTU for each node. The new tree contains labels at each internal node. The label is the node number so you can relate the tree to the summary file. .. _shared: https://www.mothur.org/wiki/Shared_file .. _relabund: https://www.mothur.org/wiki/Get.relabund .. _indicator: https://www.mothur.org/wiki/Indicator v.1.22.0: Updated to Mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_lefse/mothur_lefse/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. _Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 .. _lefse: https://www.mothur.org/wiki/Lefse"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_libshuff/mothur_libshuff/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The libshuff
 method is a generic test that describes whether two or more communities have the same structure using the Cramer-von Mises test statistic. The significance of the test statistic indicates the probability that the communities have the same structure by chance. Because each pairwise comparison requires two significance tests, a correction for multiple comparisons (e.g. Bonferroni's correction) must be applied. .. _libshuff: https://www.mothur.org/wiki/Libshuff"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_list_otulabels/mothur_list_otulabels/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The list.otulabels
 command lists otu labels from shared_ or relabund_ file. This list can be used especially with subsampled datasets when used with output from classify.otu_, otu.association_, or corr.axes_ to select specific otus using the get.otus_ or remove.otus_ commands .. _list.otulabels: https://www.mothur.org/wiki/List.otulabels .. _classify.otu: https://www.mothur.org/wiki/Classify.otu .. _otu.association: https://www.mothur.org/wiki/Otu.association .. _corr.axes: https://www.mothur.org/wiki/Corr.axes .. _get.otus: https://www.mothur.org/wiki/Get.otus .. _remove.otus: https://www.mothur.org/wiki/Remove.otus .. _shared: https://www.mothur.org/wiki/Shared_file .. _relabund: https://www.mothur.org/wiki/Get.relabund v.1.27.0: Updated to mothur 1.33, added list file for otu"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_list_seqs/mothur_list_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The list.seqs
 command writes out the names of the sequences found within a fasta, name_, group_, list_, align.report_ or taxonomy_ file. .. _name: https://www.mothur.org/wiki/Name_file .. _group: https://www.mothur.org/wiki/Group_file .. _list: https://www.mothur.org/wiki/List_file .. _align.report: https://www.mothur.org/wiki/Align.seqs .. _taxonomy: https://www.mothur.org/wiki/Taxonomy_outline .. _list.seqs: https://www.mothur.org/wiki/list.seqs v.1.20.0: Updated to mothur 1.33, added count and fastq option"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_make_design/mothur_make_design/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 Make Design creates a design file for use in mothur commands: merge.groups
, indicator_, and metastats_. A design file looks like the group file. It is a 2 column tab delimited file, where the first column is the group name and the second column is the set the group belongs to. .. _merge.groups: https://www.mothur.org/wiki/Merge.groups .. _indicator: https://www.mothur.org/wiki/Indicator .. _metastats: https://www.mothur.org/wiki/Metastats"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_make_biom/mothur_make_biom/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The make.biom command converts a shared
 shared file to biom_ files. The output can be filtered by groups and labels. .. _shared: https://www.mothur.org/wiki/Shared_file .. _biom: http://biom-format.org/documentation/biom_format.html .. _make.biom: https://www.mothur.org/wiki/Make.biom"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_make_contigs/mothur_make_contigs/1.39.5.1	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The make.contigs
 command reads a forward fastq file and a reverse fastq file and outputs new fasta and quality files. .. _make.contigs: https://www.mothur.org/wiki/Make.contigs v.1.27.0: Updated to use Mothur 1.33. Added findex and rindex parmaeters, optionally used with the oligos file."
toolshed.g2.bx.psu.edu/repos/iuc/mothur_make_fastq/mothur_make_fastq/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The make.fastq
 command reads a fasta file and quality file and creates a fastq. .. _make.fastq: https://www.mothur.org/wiki/Make.fastq"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_make_group/mothur_make_group/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The make.group
 command reads a fasta file or series of fasta files and creates a group_ file. .. _group: https://www.mothur.org/wiki/Group_file .. _make.group: https://www.mothur.org/wiki/Make.group"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_make_lefse/mothur_make_lefse/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The make.lefse
 allows you to create a lefse formatted input file from mothur's output files. .. _make.lefse: https://www.mothur.org/wiki/Make.lefse"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_make_lookup/mothur_make_lookup/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The make.lookup
 allows you to create custom lookup files for use with shhh.flows. .. _make.lookup: https://www.mothur.org/wiki/Make.lookup"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_make_shared/mothur_make_shared/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The make.shared
 command takes a list_ and a group_ file and outputs a shared_ file, as well as a rabund_ file for each group. .. _list: https://www.mothur.org/wiki/List_file .. _group: https://www.mothur.org/wiki/Group_file .. _shared: https://www.mothur.org/wiki/Shared_file .. _rabund: https://www.mothur.org/wiki/Rabund_file .. _make.shared: https://www.mothur.org/wiki/Make.shared"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_make_sra/mothur_make_sra/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The make.sra
 creates the necessary files for a NCBI submission. .. _make.sra: https://www.mothur.org/wiki/Make.sra"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_mantel/mothur_mantel/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The mantel
 command calculates the Mantel correlation coefficient between two matrices_. .. _matrices: //www.mothur.org/wiki/Phylip-formatted_distance_matrix .. _mantel: https://www.mothur.org/wiki/Mantel"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_merge_count/mothur_merge_count/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page .. _Mothur: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The merge.count
 command merges count files into a single count table. .. _merge.count: https://www.mothur.org/wiki/Merge.count"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_merge_files/mothur_merge_files/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The merge.files
 command merge inputs into a single output. .. _merge.files: https://www.mothur.org/wiki/Merge.files"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_merge_groups/mothur_merge_groups/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The merge.groups
 command reads a shared_ file and a design file and merges the groups in the shared file that are in the same grouping in the design file. A design file partitions a list of names into groups. It is a tab-delimited file with 2 columns: name and group, e.g. : ======= ======= duck bird cow mammal pig mammal goose bird cobra reptile ======= ======= The Make_Design tool can construct a design file from a Mothur dataset that contains group names. .. _shared: https://www.mothur.org/wiki/Shared_file .. _merge.groups: https://www.mothur.org/wiki/Merge.groups"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_merge_sfffiles/mothur_merge_sfffiles/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page .. _Mothur: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The merge.sfffiles
 command merge inputs into a single output. .. _merge.sfffiles: https://www.mothur.org/wiki/Merge.sfffiles"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_merge_taxsummary/mothur_merge_taxsummary/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. _Mothur-Wiki: https://www.mothur.org/wiki/Main_Page .. _Mothur: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The merge.taxsummary command takes a list of tax.summary files separated by dashes and merges them into one file. .. _merge.taxsummary: https://www.mothur.org/wiki/Merge.taxsummary"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_metastats/mothur_metastats/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The metastats
 command generate principle components plot data. .. _metastats: https://www.mothur.org/wiki/Metastats v.1.21.0: Updated to mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_mimarks_attributes/mothur_mimarks_attributes/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The mimarks.attributes
 command reads 
BioSample Attributes
 xml and generates source for get.mimarkspackage
 command. .. _BioSample Attributes: https://www.ncbi.nlm.nih.gov/biosample/docs/attributes/ .. _get.mimarkspackage: https://www.mothur.org/wiki/Get.mimarkspackage"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_nmds/mothur_nmds/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page .. _Mothur: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The nmds
 command generates non-metric multidimensional scaling data from a phylip_distance_matrix_. .. _phylip_distance_matrix: https://www.mothur.org/wiki/Phylip-formatted_distance_matrix .. _nmds: https://www.mothur.org/wiki/Nmds v1.20.0: Updated to mothur 1.33"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_normalize_shared/mothur_normalize_shared/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The normalize.shared
 command normalizes the number of sequences per group to a specified level. The input is a shared_ or relabund_ file. .. _shared: https://www.mothur.org/wiki/Shared_file .. _relabund: https://www.mothur.org/wiki/Get.relabund .. _normalize.shared: https://www.mothur.org/wiki/Normalize.shared"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_otu_association/mothur_otu_association/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The otu.association
 command calculates the correlation coefficient for the otus in a shared_ or relabund_ file. .. _shared: https://www.mothur.org/wiki/Shared_file .. _relabund: https://www.mothur.org/wiki/Get.relabund .. _otu.association: https://www.mothur.org/wiki/Otu.association v.1.25.0: Updated to mothur 1.33, added cutoff option"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_otu_hierarchy/mothur_otu_hierarchy/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The otu.hierarchy
 command relates OTUs from a list_ at different distances. .. _list: https://www.mothur.org/wiki/List_file .. _otu.hierarchy: https://www.mothur.org/wiki/Otu.hierarchy"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_pairwise_seqs/mothur_pairwise_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page .. _Mothur: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The pairwise.seqs
 command will calculate uncorrected pairwise distances between sequencesi as a column-formatted_distance_matrix_ or phylip-formatted_distance_matrix_. .. _column-formatted_distance_matrix: https://www.mothur.org/wiki/Column-formatted_distance_matrix .. _phylip-formatted_distance_matrix: https://www.mothur.org/wiki/Phylip-formatted_distance_matrix .. _pairwise.seqs: https://www.mothur.org/wiki/Pairwise.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_parse_list/mothur_parse_list/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The parse.list
 command reads a list_ file and group_ file and generates a list_ file for each group_ in the groupfile. .. _list: https://www.mothur.org/wiki/List_file .. _group: https://www.mothur.org/wiki/Group_file .. _parse.list: https://www.mothur.org/wiki/Parse.list v.1.19.0: Updated to mothur 1.33, added count parameter"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_parsimony/mothur_parsimony/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The parsimony
 command implements the parsimony method (aka P-test), which was previously implemented in TreeClimber and is also available in MacClade and on the UniFrac website. The parsimony method is a generic test that describes whether two or more communities have the same structure. The significance of the test statistic can only indicate the probability that the communities have the same structure by chance. The value does not indicate a level of similarity. .. _parsimony: https://www.mothur.org/wiki/Parsimony v.1.20.0: Added count parameter"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_pca/mothur_pca/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The pca
 command generate principle components plot data for a shared_ or relabund_ file. .. _shared: https://www.mothur.org/wiki/Shared_file .. _relabund: https://www.mothur.org/wiki/Get.relabund .. _pca: https://www.mothur.org/wiki/Pca"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_pcoa/mothur_pcoa/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The pcoa
 command performs principal coordinate analysis on a phylip-formatted_distance_matrix_. .. _phylip-formatted_distance_matrix: https://www.mothur.org/wiki/Phylip-formatted_distance_matrix .. _pcoa: https://www.mothur.org/wiki/Pcoa"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_pcr_seqs/mothur_pcr_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The pcr.seqs
 command assigns sequences to chosen taxonomy outline. .. _pcr.seqs: https://www.mothur.org/wiki/Pcr.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_phylo_diversity/mothur_phylo_diversity/1.39.5.0	All groups displayed if none are selected.
toolshed.g2.bx.psu.edu/repos/iuc/mothur_phylotype/mothur_phylotype/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The phylotype
 command assign sequences to OTUs based on their taxonomy and outputs a a list_, a sabund_ (Species Abundance), and a rabund_ (Relative Abundance) file. .. _list: https://www.mothur.org/wiki/List_file .. _rabund: https://www.mothur.org/wiki/Rabund_file .. _sabund: https://www.mothur.org/wiki/Sabund_file .. _phylotype: https://www.mothur.org/wiki/Phylotype"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_pre_cluster/mothur_pre_cluster/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The pre.cluster
 command implements a pseudo-single linkage algorithm with the goal of removing sequences that are likely due to pyrosequencing errors. The basic idea is that abundant sequences are more likely to generate erroneous sequences than rare sequences. With that in mind, the algorithm proceeds by ranking sequences in order of their abundance. Then we walk through the list of sequences looking for rarer sequences that are within some threshold of the original sequence. Those that are within the threshold are merged with the larger sequence. The original Huse method performs this task on a distance matrix, whereas we do it based on the original sequences. The advantage of our approach is that the algorithm works on aligned sequences instead of a distance matrix. This is advantageous because by pre-clustering you remove a large number of sequences making the distance calculation much faster. .. _pre.cluster: https://www.mothur.org/wiki/Pre.cluster v1.24.0: Updated to mothur 1.33, added count and topdown parameter"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_primer_design/mothur_primer_design/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The primer.design
 allows you to identify sequence fragments that are specific to particular OTUs. .. _primer.design: https://www.mothur.org/wiki/Primer.design"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_rarefaction_shared/mothur_rarefaction_shared/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The rarefaction.shared
 command generates inter-sample rarefaction curves using a re-sampling without replacement approach. The traditional way that ecologists use rarefaction is not to randomize the sampling order within a sample, rather between samples. For instance, if we wanted to know the number of OTUs in the human colon, we might sample from various sites within the colon, and sequence a bunch of 16S rRNA genes. By determining the number of OTUs in each sample and comparing the composition of those samples it is possible to determine how well you have sampled the biodiversity within the individual. For calc parameter choices see: https://www.mothur.org/wiki/Calculators .. _rarefaction.shared: https://www.mothur.org/wiki/Rarefaction.shared"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_rarefaction_single/mothur_rarefaction_single/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The rarefaction.single
 command generates intra-sample rarefaction curves using a re-sampling without replacement approach. Rarefaction curves provide a way of comparing the richness observed in different samples. For calc parameter choices see: https://www.mothur.org/wiki/Calculators .. _rarefaction.single: https://www.mothur.org/wiki/Rarefaction.single"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_remove_dists/mothur_remove_dists/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The remove.dists
 removes distances from a phylip or column file related to groups or sequences listed in an accnos file. .. _remove.dists: https://www.mothur.org/wiki/Remove.dists"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_remove_groups/mothur_remove_groups/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The remove.groups
 command removes sequences from a specific group or set of groups from the following file types: fasta, name_, group_, list_, taxonomy_. .. _name: https://www.mothur.org/wiki/Name_file .. _group: https://www.mothur.org/wiki/Group_file .. _list: https://www.mothur.org/wiki/List_file .. _taxonomy: https://www.mothur.org/wiki/Taxonomy_outline .. _remove.groups: https://www.mothur.org/wiki/Remove.groups"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_remove_lineage/mothur_remove_lineage/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The remove.lineage
 command reads a taxonomy_ file and a taxon and generates a new file that contains only the sequences in the that are not from that taxon. You may also include either a fasta, name_, group_, list_, or align.report_ file to this command and mothur will generate new files for each of those containing only the selected sequences. .. _taxonomy: https://www.mothur.org/wiki/Taxonomy_outline .. _name: https://www.mothur.org/wiki/Name_file .. _group: https://www.mothur.org/wiki/Group_file .. _list: https://www.mothur.org/wiki/List_file .. _align.report: https://www.mothur.org/wiki/Align.seqs .. _remove.lineage: https://www.mothur.org/wiki/Remove.lineage"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_remove_otulabels/mothur_remove_otulabels/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The remove.otulabels
 command removes otu labels from the output from classify.otu_, corr.axes_ and otu.association_. This can be useful especially with subsampled datasets or when groups have been selected. .. _classify.otu: https://www.mothur.org/wiki/Classify.otu .. _corr.axes: https://www.mothur.org/wiki/Corr.axes .. _otu.association: https://www.mothur.org/wiki/Otu.association .. _remove.otulabels: https://www.mothur.org/wiki/Remove.otus"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_remove_otus/mothur_remove_otus/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. _Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The remove.otus command removes otu labels from cons.taxonomy, corr.axes, otu.corr, shared and list files. This can be useful especially with subsampled datasets or when groups have been selected. .. _remove.otus: https://www.mothur.org/wiki/Remove.otus"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_remove_rare/mothur_remove_rare/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The remove.rare
 command reads one of the following file types: list_, rabund_, sabund_ or shared_ file. It outputs a new file after removing the rare otus. .. _list: https://www.mothur.org/wiki/List_file .. _sabund: https://www.mothur.org/wiki/Sabund_file .. _rabund: https://www.mothur.org/wiki/Rabund_file .. _shared: https://www.mothur.org/wiki/Shared_file .. _remove.rare: https://www.mothur.org/wiki/Remove.rare"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_remove_seqs/mothur_remove_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The remove.seqs
 command takes a list of sequence names and either a fasta, name_, group_, list_, align.report_ or taxonomy_ file to generate a new file that does not contain the sequences in the list. This command may be used in conjunction with the list.seqs_ command to help screen a sequence collection. .. _name: https://www.mothur.org/wiki/Name_file .. _group: https://www.mothur.org/wiki/Group_file .. _list: https://www.mothur.org/wiki/List_file .. _align.report: https://www.mothur.org/wiki/Align.seqs .. _taxonomy: https://www.mothur.org/wiki/Taxonomy_outline .. _list.seqs: https://www.mothur.org/wiki/list.seqs .. _remove.seqs: https://www.mothur.org/wiki/Remove.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_rename_seqs/mothur_rename_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The rename.seqs
 command takes fasta-formatted sequence file and group file, and renames the sequences by appending the group name to the sequence number. .. _rename.seqs: https://www.mothur.org/wiki/Rename.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_reverse_seqs/mothur_reverse_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The reverse.seqs
 command will generate a fasta containing the reverse complement of each sequence in the input fasta. .. _reverse.seqs: https://www.mothur.org/wiki/Reverse.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_screen_seqs/mothur_screen_seqs/1.39.5.1	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The screen.seqs
 command enables you to keep sequences that fulfill certain user defined criteria. Furthermore, it enables you to cull those sequences not meeting the criteria from a name_, group_, or align.report_ file. .. _name: https://www.mothur.org/wiki/Name_file .. _group: https://www.mothur.org/wiki/Group_file .. _align.report: https://www.mothur.org/wiki/Align.seqs .. _screen.seqs: https://www.mothur.org/wiki/Screen.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_sens_spec/mothur_sens_spec/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The sens.spec
 command takes a list_ and either a column_ or phylip_ distance matrix to determine the quality of OTU assignment. .. _list: https://www.mothur.org/wiki/List_file .. _column: https://www.mothur.org/wiki/Column-formatted_distance_matrix .. _phylip: https://www.mothur.org/wiki/Phylip-formatted_distance_matrix .. _sens.spec: https://www.mothur.org/wiki/Sens.spec"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_seq_error/mothur_seq_error/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The seq.error
 command evaluates error rate for sequences by comparing to the fasta-formatted template_alignment_. This is demonstrated in https://mothur.org/wiki/miseq_sop/#assessing-error-rates .. _template_alignment: https://www.mothur.org/wiki/Alignment_database .. _seq.error: https://www.mothur.org/wiki/Seq.error"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_sffinfo/mothur_sffinfo/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The sffinfo
 command will summarize the quality of sequences in an unaligned or aligned fasta-formatted sequence file. .. _sffinfo: https://www.mothur.org/wiki/Sffinfo"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_shhh_flows/mothur_shhh_flows/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The shhh.flows
 command is Pat Schloss's translation of Chris Quince's PyroNoise algorithm [1] from C to C++ with the incorporation of mothur's bells and whistles. Based on processing of test datasets provided by Quince, shhh.flows gives the same/similar output to AmpliconNoise. shhh.flows uses a expectation-maximization algorithm to correct flowgrams to identify the idealized form of each flowgram and translate that flowgram to a DNA sequence. Our testing has shown that when Titanium data are trimmed to 450 flows using trim.flows, shhh.flows provides the highest quality data for any other method available. In contrast, when we use the min/max number of flows suggested by Quince of 360/720, the error rate is not that great. This much improved error rate does come at a computational cost. Whereas the features in trim.seqs take on the order of minutes, shhh.flows can take on the order of hours. You will also need a lookup file that tells shhh.flows the probability of observing an intensity value for a given homopolymer length. You can get mothur-compatible files at: https://www.mothur.org/wiki/Lookup_files .. _shhh.flows: https://www.mothur.org/wiki/Shhh.flows"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_shhh_seqs/mothur_shhh_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The shhh.seqs
 command is a mothur-based rewrite of Chris Quince's sequence denoting program, SeqNoise. Schloss prefers pre.cluster for this operation. .. _shhh.seqs: https://www.mothur.org/wiki/Shhh.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_sort_seqs/mothur_sort_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The sort.seqs
 command puts sequences from a fasta, name, group, quality, flow or taxonomy file in the same order. You can provide an accnos file to indicate the order you want, otherwise mothur will use the order of the first file it reads. .. _sort.seqs: https://www.mothur.org/wiki/Sort.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_split_abund/mothur_split_abund/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The split.abund
 command reads a fasta file and a list_ or a name_ file and splits the sequences into rare and abundant groups. .. _list: https://www.mothur.org/wiki/List_file .. _name: https://www.mothur.org/wiki/Name_file .. _split.abund: https://www.mothur.org/wiki/Split.abund"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_split_groups/mothur_split_groups/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The split.groups
 command reads a fasta file and group_ file and generates a fasta file for each group in the groupfile. A name_ file can also be split into groups. .. _group: https://www.mothur.org/wiki/Group_file .. _name: https://www.mothur.org/wiki/Name_file .. _split.groups: https://www.mothur.org/wiki/Split.groups"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_sub_sample/mothur_sub_sample/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The sub.sample
 command can be used as a way to normalize your data, or to create a smaller set from your original set. It takes as an input the following file types: fasta, list_, shared_, rabund_ and sabund_ to generate a new file that contains a sampling of your original file. .. _list: https://www.mothur.org/wiki/List_file .. _shared: https://www.mothur.org/wiki/Shared_file .. _rabund: https://www.mothur.org/wiki/Rabund_file .. _sabund: https://www.mothur.org/wiki/Sabund_file .. _sub.sample: https://www.mothur.org/wiki/Sub.sample"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_summary_qual/mothur_summary_qual/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The summary.qual
 command reads a quality file and an optional name, and summarizes the quality information. .. _summary.qual: https://www.mothur.org/wiki/Summary.qual"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_summary_seqs/mothur_summary_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The summary.seqs
 command will summarize the quality of sequences in an unaligned or aligned fasta-formatted sequence file. .. _summary.seqs: https://www.mothur.org/wiki/Summary.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_summary_shared/mothur_summary_shared/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The summary.shared
 command produce a summary file that has the calculator value for each line in the OTU data of the shared_ file and for all possible comparisons between the different groups in the group_ file. This can be useful if you aren't interested in generating collector's or rarefaction curves for your multi-sample data analysis. It would be worth your while, however, to look at the collector's curves for the calculators you are interested in to determine how sensitive the values are to sampling. If the values are not sensitive to sampling, then you can trust the values. Otherwise, you need to keep sampling. For calc parameter choices see: https://www.mothur.org/wiki/Calculators .. _shared: https://www.mothur.org/wiki/Shared_file .. _group: https://www.mothur.org/wiki/Group_file .. _summary.shared: https://www.mothur.org/wiki/Summary.shared"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_summary_single/mothur_summary_single/1.39.5.2	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The summary.single
 command produce a summary file that has the calculator value for each line in the OTU data and for all possible comparisons between the different groups in the group_ file. This can be useful if you aren't interested in generating collector's or rarefaction curves for your multi-sample data analysis. It would be worth your while, however, to look at the collector's curves for the calculators you are interested in to determine how sensitive the values are to sampling. If the values are not sensitive to sampling, then you can trust the values. Otherwise, you need to keep sampling. For calc parameter choices see: https://www.mothur.org/wiki/Calculators .. _group: https://www.mothur.org/wiki/Group_file .. _summary.single: https://www.mothur.org/wiki/Summary.single"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_summary_tax/mothur_summary_tax/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The summary.tax
 command reads a taxonomy file and an optional name and or group file, and summarizes the taxonomy information. .. _summary.tax: https://www.mothur.org/wiki/Summary.tax"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_taxonomy_to_krona/mothur_taxonomy_to_krona/1.0	"Krona text input requires a tab-delimited file with first column being a count, and the rest representing the hierarchy, for example:: 2 Fats Saturated fat 3 Fats Unsaturated fat Monounsaturated fat 3 Fats Unsaturated fat Polyunsaturated fat 13 Carbohydrates Sugars 4 Carbohydrates Dietary fiber 21 Carbohydrates 5 Protein 4 This can be input into the Krona tool as generic text format, and would yield this 
Krona plot
_. .. _Krona plot: https://marbl.github.io/Krona/examples/xml.krona.html"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_tree_shared/mothur_tree_shared/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The tree.shared
 command will generate a newick-formatted tree file that describes the dissimilarity (1-similarity) among multiple groups. For calc parameter choices see: https://www.mothur.org/wiki/Calculators .. _tree.shared: https://www.mothur.org/wiki/Tree.shared"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_trim_flows/mothur_trim_flows/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The trim.flows
 command is analogous to the trim.seqs command, except that it uses the flowgram data that comes bundled in the sff file that is generated by 454 sequencing. It's primary usage is as a preliminary step to running shhh.seqs. Chris Quince has a series of perl scripts that fulfill a similar role [1]. This command will allow you to partition your flowgram data by sample based on the barcode, trim the flows to a specified length range, and cull sequences that are too short or have too many mismatches to barcodes and primers. .. _trim.flows: https://www.mothur.org/wiki/Trim.flows"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_trim_seqs/mothur_trim_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The trim.seqs
 command provides the preprocessing features needed to screen and sort pyrosequences. The command will enable you to trim off primer sequences and barcodes, use the barcode information to generate a group file and split a fasta file into sub-files, screen sequences based on the qual file that comes from 454 sequencers, cull sequences based on sequence length and the presence of ambiguous bases and get the reverse complement of your sequences. While this analysis is clearly geared towards pyrosequencing collections, it can also be used with traditional Sanger sequences. .. _trim.seqs: https://www.mothur.org/wiki/Trim.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_unique_seqs/mothur_unique_seqs/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The unique.seqs
 command returns only the unique sequences found in a fasta-formatted sequence file and a name_ file that indicates those sequences that are identical to the reference sequence. .. _name: https://www.mothur.org/wiki/Name_file .. _unique.seqs: https://www.mothur.org/wiki/Unique.seqs"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_venn/mothur_venn/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The venn
 command generates Venn diagrams to compare the richness shared among 2, 3, or 4 groups. For calc parameter choices see: https://www.mothur.org/wiki/Calculators .. _venn: https://www.mothur.org/wiki/Venn"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_unifrac_unweighted/mothur_unifrac_unweighted/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The unifrac.unweighted
 command the unweighted UniFrac algorithm. The unifrac.weighted command implements the weighted version of the command. Both of these methods are available through the UniFrac website. The UniFrac methods are generic tests that describes whether two or more communities have the same structure. The significance of the test statistic can only indicate the probability that the communities have the same structure by chance. The value does not indicate a level of similarity. .. _unifrac.unweighted: https://www.mothur.org/wiki/Unifrac.unweighted"
toolshed.g2.bx.psu.edu/repos/iuc/mothur_unifrac_weighted/mothur_unifrac_weighted/1.39.5.0	"Mothur Overview
 Mothur is a comprehensive suite of tools for microbial ecology community. It is initiated by Dr. Patrick Schloss and his software development team in the Department of Microbiology and Immunology at The University of Michigan. For more information, see Mothur-Wiki_. .. 
Mothur-Wiki: https://www.mothur.org/wiki/Main_Page 
Command Documentation
 The unifrac.weighted
 command implements the weighted UniFrac algorithm. The unifrac.unweighted command implements the unweighted version of the command. Both of these methods are available through the UniFrac website. The UniFrac methods are generic tests that describes whether two or more communities have the same structure. The significance of the test statistic can only indicate the probability that the communities have the same structure by chance. The value does not indicate a level of similarity. .. _unifrac.weighted: https://www.mothur.org/wiki/Unifrac.weighted"
toolshed.g2.bx.psu.edu/repos/iuc/meme_fimo/meme_fimo/5.5.8+galaxy0	".. class:: warningmark 
WARNING: This tool is only available for non-commercial use. Use for educational, research and non-profit purposes is permitted. Before using, be sure to review, agree, and comply with the license.
 FIMO scans a sequence database for individual matches to each of the motifs you provide (sample output for motifs and sequences). The name FIMO stands for 'Find Individual Motif Occurrences'. The program searches a database of sequences for occurrences of known motifs, treating each motif independently. Motifs must be in MEME Motif Format. You can define the statistical threshold (p-value) for motifs and whether FIMO scans just the given sequences or their reverse complements (where applicable). .. class:: infomark For detailed information on FIMO, click here_, or view the license_. .. _here: http://meme-suite.org/doc/fimo.html?man_type=web .. _license: http://meme-suite.org/doc/copyright.html?man_type=web"
toolshed.g2.bx.psu.edu/repos/iuc/meme_meme/meme_meme/5.5.8+galaxy0	".. class:: warningmark 
WARNING: This tool is only available for non-commercial use. Use for educational, research and non-profit purposes is permitted. Before using, be sure to review, agree, and comply with the license.
 If you want to specify sequence weights, you must include them at the top of your input FASTA file. MEME discovers novel, ungapped motifs (recurring, fixed-length patterns) in your sequences (sample output from sequences). MEME splits variable-length patterns into two or more separate motifs. A motif is a sequence pattern that occurs repeatedly in a group of related sequences. MEME represents motifs as position-dependent letter-probability matrices which describe the probability of each possible letter at each position in the pattern. Individual MEME motifs do not contain gaps. Patterns with variable-length gaps are split by MEME into two or more separate motifs. MEME takes as input a group of sequences and outputs as many motifs as requested. MEME uses statistical modeling techniques to automatically choose the best width, number of occurrences, and description for each motif. .. class:: infomark For detailed information on MEME, click here_, or view the license_. .. _here: http://meme-suite.org/doc/meme.html?man_type=web .. _license: http://meme-suite.org/doc/copyright.html?man_type=web"
toolshed.g2.bx.psu.edu/repos/iuc/meme_psp_gen/meme_psp_gen/5.5.8+galaxy0	".. class:: warningmark 
WARNING: This tool is only available for non-commercial use. Use for educational, research and non-profit purposes is permitted. Before using, be sure to review, agree, and comply with the license.
 psp-gen is used to allow MEME to perform discriminative motif discovery—to find motifs overrepresented in one set of sequences compared to in another set. It takes two files as input—the sequence file to be input to MEME, (the ""primary"" file) and a ""control"" sequence file of sequences believed not to contain the same motifs as in the ""primary"" file. psp-gen creates a file for use by MEME that encapsulates information about probable discriminative motifs. psp-gen records its chosen motif width in the file, and MEME is able to adjust the data when it tries different motif widths. .. class:: infomark For detailed information on psp-gen, click here_, or view the license_. .. _here: http://meme-suite.org/doc/psp-gen.html?man_type=web .. _license: http://meme-suite.org/doc/copyright.html?man_type=web ----- 
Required options
 * 
Primary sequence file
 - a file containing FASTA formatted sequences which are to be used as the primary set in PSP calculation. * 
Control sequence file
 - a file containing FASTA formatted sequences which are to be used as the control set in PSP calculation. 
Additional options
 * 
Minimum width to use for position specific priors
 - the minimum width to use with selecting the ""best"" width for PSPs. * 
Maximum width to use for position specific priors
 - the maximum width to use with selecting the ""best"" width for PSPs. * 
Alphabet
 - The alphabet to be used, one of DNA, protein or RNA. * 
Use spaced triples instead of whole-word matches
 - use spaced triples instead of whole-word matches (recommended when using the protein alphabet). * 
Allow triples to start anywhere within a site
 - when using the -triples option, select 'Yes' to only consider triples starting at the start of the site or 'No' to allow triples to start anywhere in a width 'w' site. * 
Match as equal sequences of letters that appear together
 - select 'Yes' to match as equal any sequence of letter that appears together. Separate letter groups using ""-"" (e.g. -equiv ""IVL-HKR"") means treat all occurrences of I, V or L as the same, and all occurrences of H, K or R as the same. * 
Consider both strands when calculating position specific priors for alphabets
 - select 'Yes' to consider both strands when calculating PSPs for complementable alphabets or 'No to consider only the given strand. * 
Set the lowest score value after scaling
 - select 'Yes' to set the lowest score to 0.1 unless the the following ""highest score"" option is selected, in which case the lowest score is highest score - 1. * 
Set the highest score value after scaling
 - select 'Yes' to set the highest score to 0.9 unless the previous ""lowest score"" option is selected, in which case the highest score is lowest score + 1. * 
Choose the width with the biggest difference between minimum and maximum scores before scaling
 - select 'Yes' to choose the width with the biggest difference between minimum and maximum scores before scaling, or 'No' to choose the width with the biggest maximum score before scaling. * 
Output scores instead of priors
 - select 'Yes' to output scores instead of position specific priors. * 
Output primary and control file names, scores and widths
 - select 'Yes' to produce an additional tabular output consisting of control file names, lowest and highest scores and lowest and highest widths. * 
Report frequency of each score
 - select 'Yes' to include the frequency of each score in the output."
toolshed.g2.bx.psu.edu/repos/iuc/meme_chip/meme_chip/4.11.2+galaxy1	".. class:: warningmark 
WARNING: This tool is only available for non-commercial use. Use for educational, research and non-profit purposes is permitted. Before using, be sure to review, agree, and comply with the license.
 MWMW-ChIP perform motif discovery, motif enrichment analysis and clustering on large nucleotide datasets. If you want to specify sequence weights, you must include them at the top of your input FASTA file. MEME discovers novel, ungapped motifs (recurring, fixed-length patterns) in your sequences (sample output from sequences). MEME splits variable-length patterns into two or more separate motifs. A motif is a sequence pattern that occurs repeatedly in a group of related sequences. MEME represents motifs as position-dependent letter-probability matrices which describe the probability of each possible letter at each position in the pattern. Individual MEME motifs do not contain gaps. Patterns with variable-length gaps are split by MEME into two or more separate motifs. MEME takes as input a group of sequences and outputs as many motifs as requested. MEME uses statistical modeling techniques to automatically choose the best width, number of occurrences, and description for each motif. .. class:: infomark For detailed information on MEME, click here_, or view the license_. .. _here: http://meme-suite.org/doc/meme.html?man_type=web .. _license: http://meme-suite.org/doc/copyright.html?man_type=web"
toolshed.g2.bx.psu.edu/repos/devteam/weblogo3/rgweblogo3/3.5.0	"Note
 This tool uses Weblogo3_ in Galaxy to generate a sequence logo. The input file must be a fasta file in your current history. It is recommended for (eg) viewing multiple sequence alignments output from the clustalw tool - set the output to fasta and feed it in to this tool. A typical output looks like this .. image:: ${static_path}/images/rgWebLogo3_test.jpg ---- 
Warning about input Fasta format files
 The Weblogo3 program used by this tool will fail if your fasta sequences are not all EXACTLY the same length. The tool will provide a warning and refuse to call the weblogo3 executable if irregular length sequences are detected. Fasta alignments from the companion ClustalW Galaxy tool will work but many other fasta files may cause this tool to fail - please do not file a Galaxy bug report - this is a feature of the tool and a problem with your source data - not a tool error - please make certain all your fasta sequences are the same length! ---- 
Attribution
 Weblogo attribution and associated documentation are available at Weblogo3_ This Galaxy wrapper was written by Ross Lazarus for the rgenetics project and the source code is licensed under the LGPL_ like other rgenetics artefacts .. _Weblogo3: http://weblogo.berkeley.edu/ .. _LGPL: http://www.gnu.org/copyleft/lesser.html"
toolshed.g2.bx.psu.edu/repos/iuc/longdust/longdust/1.4+galaxy0	".. class:: infomark 
What it does
 
longdust
 detects low-complexity (dusty) regions in long DNA sequences. It scans input FASTA sequences using k-mer statistics and reports regions that fall below a complexity threshold. These regions are often repetitive or homopolymeric stretches that may interfere with sequence analysis, alignment, or downstream bioinformatics pipelines. The method is tunable via parameters for k-mer size, window size, score threshold, and extension length, allowing you to control how strict or relaxed the detection should be. 
Input
 - A FASTA file containing DNA sequences (typically long reads or assembled contigs). - Optional parameters to configure detection: - 
-k
 : k-mer length (default 7) - 
-w
 : window size (default 5000) - 
-t
 : score threshold (default 0.6) - 
-e
 : extension X-drop length, 0 disables extension (default 50) - 
-f
 : forward strand only (optional flag) - 
-a
 : approximate O(Lw) algorithm (optional flag) * Recommend w < 4^k for performance, especially given large w * Use ""-k6 -w1000 -t.55"" for more relaxed but shorter regions 
Output
 - A BED file listing detected low-complexity regions"
toolshed.g2.bx.psu.edu/repos/iuc/episcanpy_cluster_embed/episcanpy_cluster_embed/0.3.2+galaxy1	"Automatically compute PCA coordinates (
pp.lazy
) ======================================================================================== This function automatically computes PCA coordinates, loadings and variance decomposition, a neighborhood graph of observations, t-distributed stochastic neighborhood embedding (tSNE) Uniform Manifold Approximation and Projection (UMAP). More details on the 
episcanpy documentation &lt;https://colomemaria.github.io/episcanpy_doc/api/episcanpy.api.pp.lazy.html&gt;
 Automatically obtain target number of clusters (
tl.getNClusters
) ======================================================================================== This function will test different settings of louvain to obtain the target number of clusters. More details on the 
episcanpy documentation &lt;https://colomemaria.github.io/episcanpy_doc/api/episcanpy.api.tl.getNClusters.html&gt;
 Perform kmeans clustering (
tl.kmeans
) ======================================================================================== This function will perform kmeans clustering using X_pca fits. More details on the 
episcanpy documentation &lt;https://colomemaria.github.io/episcanpy_doc/api/episcanpy.api.tl.kmeans.html&gt;
 Compute hierarchical clustering using X_pca fits (
tl.hc
) =================================================================== This function computes heirarchical clustering using X_pca fits using random_state=2019. More details on the 
episcanpy documentation &lt;https://colomemaria.github.io/episcanpy_doc/api/episcanpy.api.tl.hc.html&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/pipelign/pipelign/0.2+galaxy0	".. class:: infomark 
What it does
 ------------------- A pipeline for automated multiple sequence alignment, particularly of viral sequences."
toolshed.g2.bx.psu.edu/repos/devteam/clustalw/clustalw/2.1+galaxy1	".. class:: infomark 
Note
 This tool allows you to run a multiple sequence alignment with ClustalW_. You can align DNA or protein sequences in the input file which should be multiple sequences to be aligned in a FASTA file. The alignments will appear as a clustal format file or optionally, as PHYLIP or FASTA format files in your history. If you choose FASTA as the output format, you can create a 'Logo' image using the Sequence Logo tool. If Clustal format is chosen, you have the option of adding basepair counts to the output. A subsequence of the alignment can be output by setting the Output complete parameter to ""Partial"" and defining the offset and end of the subsequence to be output. ---- .. class:: infomark 
Attribution
 The first iteration of this Galaxy wrapper was written by Hans-Rudolf Hotz. It was modified by Ross Lazarus for the rgenetics project - tests and some additional parameters were added. Cristóbal Gallardo included the remaining parameters. This wrapper is released licensed under the LGPL_. .. _ClustalW: http://www.clustal.org/clustal2/ .. _LGPL: https://www.gnu.org/copyleft/lesser.html"
toolshed.g2.bx.psu.edu/repos/iuc/mummer_dnadiff/mummer_dnadiff/4.0.0+galaxy1	"This script is a wrapper around nucmer that builds an alignment using default parameters, and runs many of nucmer's helper scripts to process the output and report alignment statistics, SNPs, breakpoints, etc. It is designed for evaluating the sequence and structural similarity of two highly similar sequence sets. E.g. comparing two different assemblies of the same organism, or comparing two strains of the same species. 
Output files:
 * report: Summary of alignments, differences and SNPs * delta: Standard nucmer alignment output * 1delta: 1-to-1 alignment from delta-filter -1 * mdelta: M-to-M alignment from delta-filter -m * 1coords: 1-to-1 coordinates from show-coords -THrcl .1delta * mcoords: M-to-M coordinates from show-coords -THrcl .mdelta * snps: SNPs from show-snps -rlTHC .1delta * rdiff: Classified ref breakpoints from show-diff -rH .mdelta * qdiff: Classified qry breakpoints from show-diff -qH .mdelta"
toolshed.g2.bx.psu.edu/repos/iuc/mummer_delta_filter/mummer_delta_filter/4.0.0+galaxy1	"This program filters the alignment file produced by nucmer, leaving only the desired alignments. Its primary function is the LIS algorithm which calculates the longest increasing subset of alignments. This allows for the calculation of a global set of alignments (i.e. 1-to-1 and mutually consistent order) with the 1-1 global option or locally consistent with 1-1 with rearrangements or many-to-many alignment. Reference sequences can be mapped to query sequences with the reference option of the Overlaps parameter, or queries to references with the Query option. This allows the user to exclude chance and repeat induced alignments, leaving only the ""best"" alignments between the two data sets. Filtering can also be performed on length, identity, and uniqueness. An important distinction between the alignment options is that 1-1 global requires the alignments to be mutually consistent in their order, while the other options are not required to be mutually consistent and therefore tolerate translocations, inversions, etc. In general cases, the many-to-many option is the best choice, however 1-1 alignment allowing for rearrangements can be handy for applications such as SNP finding which require a 1-to-1 mapping. Finally, for mapping query contigs, or sequencing reads, to a reference genome, use the query option for the Overlaps parameter. 
Options
:: -m Many-to-many alignment allowing for rearrangements -1 1-to-1 alignment allowing for rearrangements -g 1-to-1 global alignment not allowing rearrangements -i Set the minimum alignment identity [0, 100], default 0 -l Set the minimum alignment length, default 0 -q Maps each position of each query to its best hit in the reference, allowing for reference overlaps -r Maps each position of each reference to its best hit in the query, allowing for query overlaps -u Set the minimum alignment uniqueness, i.e. percent of the alignment matching to unique reference AND query sequence [0,100], default 0 -o Set the maximum alignment overlap for -r and -q options as a percent of the alignment length [0, 100], default 100"
toolshed.g2.bx.psu.edu/repos/iuc/interval2maf/Interval2Maf1/1.0.1+galaxy1	"What it does
 This tool takes genomic coordinates, superimposes them on multiple alignments (in MAF format) stored on the Galaxy site or from your history, and excises alignment blocks corresponding to each set of coordinates. Alignment blocks that extend past START and/or END positions of an interval are trimmed. Note that a single genomic interval may correspond to two or more alignment blocks. ----- 
Example
 Here a single interval is superimposed on three MAF blocks. Blocks 1 and 3 are trimmed because they extend beyond boundaries of the interval: .. image:: ${static_path}/images/maf_icons/interval2maf.png ------- 
Split blocks by species
 This option examines each MAF block for multiple occurrences of a species in a single block. When this occurs, a block is split into multiple blocks where every combination of one sequence per species per block is represented. The interface for this option has two inputs: * 
MAF file to split
. Choose multiple alignments from history to be split by species. * 
Collapse empty alignment columns
. Should alignment columns containing only gaps in the new blocks be removed. 
Example 1
: 
Collapse empty alignment columns is Yes
: For the following alignment:: ##maf version=1 a score=2047408.0 s species1.chr1 147984545 85 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species1.chr1 147984545 83 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTT--GTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species1.chr1 147984645 79 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTT------AG s species1.chr1 147984645 79 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTC---GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTC---AG s species2.chr1 129723125 85 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species2.chr1 129723125 83 - 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCT--GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species2.chr1 129723925 79 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTC------AG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG the tool will create 
a single
 history item containing 12 alignment blocks (notice that no columns contain only gaps):: ##maf version=1 a score=2047408.0 s species1.chr1 147984545 85 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species2.chr1 129723125 85 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984545 83 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTT--GTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species2.chr1 129723125 85 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984645 79 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTT------AG s species2.chr1 129723125 85 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984645 79 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTC---GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTC---AG s species2.chr1 129723125 85 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984545 85 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species2.chr1 129723125 83 - 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCT--GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984545 83 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTT-GTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species2.chr1 129723125 83 - 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCT-GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC--GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984645 79 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTT------AG s species2.chr1 129723125 83 - 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCT--GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984645 79 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTC-GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTC---AG s species2.chr1 129723125 83 - 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC-GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984545 85 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species2.chr1 129723925 79 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTC------AG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984545 83 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTT--GTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species2.chr1 129723925 79 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTC------AG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984645 79 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTAG s species2.chr1 129723925 79 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGCAG a score=2047408.0 s species1.chr1 147984645 79 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTC---GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCAG s species2.chr1 129723925 79 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTC---AG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC---AG 
Example 2
: 
Collapse empty alignment columns is No
: For the following alignment:: ##maf version=1 a score=2047408.0 s species1.chr1 147984545 85 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species1.chr1 147984545 83 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTT--GTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species1.chr1 147984645 79 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTT------AG s species1.chr1 147984645 79 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTC---GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTC---AG s species2.chr1 129723125 85 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species2.chr1 129723125 83 - 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCT--GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species2.chr1 129723925 79 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTC------AG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG the tool will create 
a single
 history item containing 12 alignment blocks (notice that some columns contain only gaps):: ##maf version=1 a score=2047408.0 s species1.chr1 147984545 85 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species2.chr1 129723125 85 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984545 83 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTT--GTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species2.chr1 129723125 85 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984645 79 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTT------AG s species2.chr1 129723125 85 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984645 79 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTC---GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTC---AG s species2.chr1 129723125 85 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984545 85 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species2.chr1 129723125 83 - 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCT--GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984545 83 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTT--GTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species2.chr1 129723125 83 - 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCT--GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984645 79 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTT------AG s species2.chr1 129723125 83 - 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCT--GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984645 79 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTC---GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTC---AG s species2.chr1 129723125 83 - 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCT--GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTCGTCCTCAG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984545 85 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species2.chr1 129723925 79 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTC------AG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984545 83 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTT--GTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTCCTCAG s species2.chr1 129723925 79 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTC------AG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984645 79 + 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTT------AG s species2.chr1 129723925 79 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTC------AG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG a score=2047408.0 s species1.chr1 147984645 79 - 245522847 ATGGCGTCGGCCTCCTCCGGGCCGTCGTC---GGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTTGTC---AG s species2.chr1 129723925 79 + 229575298 ATGGCGTCGGCCTCCTCCGGGCCGTCGTCTTCGGTCGGTTTTTCATCCTTTGATCCCGCGGTCCCTTCCTGTACCTC------AG s species3.chr3 68255714 76 - 258222147 ATGGCGTCCGCCTCCTCAGGGCCAGCGGC---GGCGGGGTTTTCACCCCTTGATTCCGGGGTCCCTGCCGGTACCGC------AG"
toolshed.g2.bx.psu.edu/repos/iuc/kc_align/kc-align/1.0.2+galaxy1	"============ Kc-Align ============ Kc-Algin is a codon-aware multiple aligner that uses Kalgin2 to produce in-frame gapped codon alignments for selection analysis of small genomes (mostly viral and some smaller bacterial genomes). Takes nucleotide seqeunces as inputs, converts them to their in-frame amino acid sequences, performs multiple alignment with Kalign, and then converts the alignments back to their original codon sequence while preserving the gaps. Produces two outputs: the gapped nucleotide alignments in FASTA format and in CLUSTAL format. Kc-Align will also attempt to detect any frameshift mutations in the input reads. If a frameshift is detected, that sequence will not be included in the multiple alignment and its ID will be printed to stdout. Kc-Align also has functionality for genes that are are composed of more than one continuous sequence (currently only support for two segments). This can be achieved by entering each segments start coordinate in the Start Position parameter separated by a comma and then doing the same for each segments end coordinate in the End Position parameter (Ex: Start Postion: 12562,12591 End Position: 12592,13905) Modes: ------ Kc-Align can be run in three different modes, depending on your input data. * In 
genome
 mode, the ""reference"" and ""reads"" input parameters are all full genome FASTA files. This mode also requires the 1-based start and end position numbers corresponding to the gene you are interested in aligning from the reference input. * If both the ""reference"" and ""reads"" inputs are already in-frame genes, the 
gene
 mode should be used. This mode does not require start and end position parameters as the reference is already in-frame. * For the case when your ""reference"" is an in-frame gene while the ""reads"" are whole genomes, the 
mixed
 mode can be used. Like gene mode, this mode does not require the start and end point position parameters."
toolshed.g2.bx.psu.edu/repos/iuc/maf_stats/maf_stats1/1.0.2+galaxy0	"What it does
 This tool takes an MAF file and an interval file and relates coverage information by interval for each species. If a column does not exist in the reference genome, it is not included in the output. Consider the interval: ""chrX 1000 1100 myInterval"" Let's suppose we want to do stats on three way alignments for H, M, and R. The result look like this: chrX 1000 1100 myInterval H XXX YYY chrX 1000 1100 myInterval M XXX YYY chrX 1000 1100 myInterval R XXX YYY where XXX and YYY are: XXX = number of nucleotides YYY = number of gaps ---- Alternatively, you can request only summary information for a set of intervals: ======== =========== ======== #species nucleotides coverage ======== =========== ======== hg18 30639 0.2372 rheMac2 7524 0.0582 panTro2 30390 0.2353 ======== =========== ======== where 
coverage
 is the number of nucleotides divided by the total length of the intervals provided in the BED file."
toolshed.g2.bx.psu.edu/repos/rnateam/mafft/rbc_mafft/7.526+galaxy2	"What it does
 MAFFT is a multiple sequence alignment (MSA) program, which offers a range of multiple alignment methods. Input types and alignment scoring matrices ------------------------------------------ For the alignment of 
protein
 sequences, you can choose between: - different flavors of BLOSUM matrices (
Henikoff S and Henikoff JG, 1992 &lt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC50453/&gt;
) - JTT matrices with any point accepted mutation (PAM) rate (
Jones, Taylor and Thornton, 1992 &lt;https://pubmed.ncbi.nlm.nih.gov/1633570/&gt;
) - PAM-based matrices optimized for transmembrane proteins (
Jones, Taylor and Thornton, 1994 &lt;https://pubmed.ncbi.nlm.nih.gov/8112466/&gt;
) For nucleic acid sequence alignment, MAFFT uses Kimura's two parameter model (
Kimura 1980 &lt;https://pubmed.ncbi.nlm.nih.gov/7463489/&gt;
) with a transitions to transversions ratio of 2 (kappa 2), but lets you configure the PAM value. The tool can also try to autodetect the sequence type from the input(s). In this mode, it will use the BLOSUM 62 matrix if it detects amino acids input, and the Kimura kappa 2 PAM200 matrix for nucleic acids. Pre-configured MSA methods -------------------------- From the 
MAFFT man page &lt;https://mafft.cbrc.jp/alignment/software/manual/manual.html&gt;
__, an overview of the different predefined flavours of the tool. 
Accuracy-oriented methods:
 - 
L-INS-i
 (probably most accurate; recommended for <200 sequences; iterative refinement method incorporating local pairwise alignment information): - mafft --localpair --maxiterate 1000 input [> output] - 
G-INS-i
 (suitable for sequences of similar lengths; recommended for <200 sequences; iterative refinement method incorporating global pairwise alignment information): - mafft --globalpair --maxiterate 1000 input [> output] - 
E-INS-i
 (suitable for sequences containing large unalignable regions; recommended for <200 sequences): - mafft --ep 0 --genafpair --maxiterate 1000 input [> output]. For E-INS-i, the --ep 0 option is recommended to allow large gaps. 
Speed-oriented methods:
 - 
FFT-NS-i
 (iterative refinement method; two cycles only): - mafft --retree 2 --maxiterate 2 input [> output] - 
FFT-NS-2
 (fast; progressive method): - mafft --retree 2 --maxiterate 0 input [> output] - 
NW-NS-i
 (iterative refinement method without FFT approximation; two cycles only): - mafft --retree 2 --maxiterate 2 --nofft input [> output] - 
NW-NS-2
 (fast; progressive method without the FFT approximation): - mafft --retree 2 --maxiterate 0 --nofft input [> output] - 
NW-NS-PartTree-1
 (recommended for ~10,000 to ~50,000 sequences; progressive method with the PartTree algorithm): - mafft --retree 1 --maxiterate 0 --nofft --parttree input [> output] - 
FFT-NS-1
 (very fast; recommended for >2000 sequences; progressive method with a rough guide tree): - mafft --retree 1 --maxiterate 0 input [> output]"
toolshed.g2.bx.psu.edu/repos/rnateam/mafft/rbc_mafft_add/7.526+galaxy2	Add one of more sequences to an existing alignment. The new sequence(s) can be complete, fragments, or another alignement. - Sequences in new_sequences are ungapped and then aligned to existing_alignment. - new_sequences is a single multi-FASTA format file. - existing_alignment is a single multi-FASTA format file. - Gaps in existing_alignment are preserved by default (--add), but it can be deactivated (--seed). In that case the aligned letters in the seed alignment are preserved but gaps are not necessarily preserved. - The alignment length may be conserved if the --keeplength option is given. The alignment length is unchanged. Insertions at the new sequences are deleted. - --mapout options output a correspondence table of positions, new_sequences.map, between before and after the calculation. The --mapout option automatically turns on the --keeplength option, to keep the numbering of sites in the reference alignment. - Omit --reorder to preserve the original sequence order.
toolshed.g2.bx.psu.edu/repos/iuc/mmseqs2_easy_linclust_clustering/mmseqs2_easy_linclust_clustering/17-b804f+galaxy0	"MMseqs2: ultra fast and sensitive sequence search and clustering suite
 MMseqs2 (Many-against-Many sequence searching) is a software suite to search and cluster huge protein and nucleotide sequence sets. MMseqs2 is open source GPL-licensed software implemented in C++ for Linux, MacOS, and (as beta version, via cygwin) Windows. The software is designed to run on multiple cores and servers and exhibits very good scalability. MMseqs2 can run 10000 times faster than BLAST. At 100 times its speed it achieves almost the same sensitivity. It can perform profile searches with the same sensitivity as PSI-BLAST at over 400 times its speed. 
Usage
 MMseqs easy-linclust is useful to clusters entries from a FASTA/FASTQ file using the cascaded clustering algorithm. It offers an efficient clustering workflow, scaling linearly with input size. Similar to easy-cluster, but more suitable for handling very large datasets efficiently. https://github.com/soedinglab/MMseqs2"
toolshed.g2.bx.psu.edu/repos/iuc/msaboot/msaboot/0.1.2	=========== Description =========== .. class:: infomark A tool for creating bootstrapping replicates from Multiple Sequence Alignment data. .. _msaboot: https://github.com/phac-nml/msaboot ----- ----- Input ----- Input file location of FASTA file containing Multiple Sequence Alignment data(-i) ---------- Parameters ---------- Number of bootstrapping replicates (-n) ------ Output ------ This tool produces two output files, one of which is optional (the log file). (A) The bootstrapped replicates in Phylip format. (B) The optional log file, containing information about the msaboot run to bootstrap the multiple sequence alignment data.
toolshed.g2.bx.psu.edu/repos/iuc/mummer_mummer/mummer_mummer/4.0.0+galaxy1	"This is the core program of the MUMmer package. It is the suffix-tree based match finding routine, and the main part of every MUMmer script. By default, mummer now finds maximal matches regardless of their uniqueness. Limiting the output to only unique matches can be specified by choosing alternative Anchoring strategies. 
Options:
:: mummer -mumreference Compute maximal matches that are unique in the reference- sequence but not necessarily in the query-sequence (default) -maxmatch Compute all maximal matches regardless of their uniqueness -l Set the minimum length of a match -b Compute forward and reverse complement matches -r Compute only reverse complement matches -mum Compute maximal matches that are unique in both sequences -F Force 4 column output format regardless of the number of reference sequence inputs -n Match only the characters a, c, g, or t -L Print length of query sequence in header of matches -s Print first 53 characters of the matching substring -c Report the query position of a reverse complement match relative to the forward strand of the query sequence mummerplot -b Highlight alignments with breakpoints further than breaklen nucleotides from the nearest sequence end -color Color plot lines with a percent similarity gradient or turn off all plot color (default color by match dir) If the plot is very sparse, edit the .gp script to plot with 'linespoints' instead of 'lines' -c Generate a reference coverage plot (default for .tiling) --filter Only display .delta alignments which represent the ""best"" hit to any particular spot on either sequence, i.e. a one-to-one mapping of reference and query subsequences --fat Layout sequences using fattest alignment only -IdR Plot a particular reference sequence ID on the X-axis -IdQ Plot a particular query sequence ID on the Y-axis -s Set the output size to small, medium or large (--small) (--medium) (--large) (default 'small') --SNP Highlight SNP locations in each alignment -title Specify the gnuplot plot title (default none) -x Set the xrange for the plot '[min:max]' -y Set the yrange for the plot '[min:max]' -R Plot an ordered set of reference sequences from Rfile -Q Plot an ordered set of query sequences from Qfile --layout Layout a .delta multiplot in an intelligible fashion, this option requires the -R -Q options"
toolshed.g2.bx.psu.edu/repos/iuc/mummer_mummerplot/mummer_mummerplot/4.0.0+galaxy1	"Mummerplot is a perl script that generates gnuplot scripts and data collections for plotting with the gnuplot utility. It can generate 2-d dotplots and 1-d coverage plots for the output of mummer or nucmer. It can also color dotplots with an identity color gradient. 
Outputs:
 * gnuplot: The gnuplot script * fplot, rplot, hplot: The forward, reverse, and highlighted match information for plotting with gnuplot. * plot: The plotted image file 
Options:
:: -b Highlight alignments with breakpoints further than breaklen nucleotides from the nearest sequence end -color Color plot lines with a percent similarity gradient or turn off all plot color (default color by match dir) If the plot is very sparse, edit the .gp script to plot with 'linespoints' instead of 'lines' -c Generate a reference coverage plot (default for .tiling) --filter Only display .delta alignments which represent the ""best"" hit to any particular spot on either sequence, i.e. a one-to-one mapping of reference and query subsequences --fat Layout sequences using fattest alignment only -IdR Plot a particular reference sequence ID on the X-axis -IdQ Plot a particular query sequence ID on the Y-axis -s Set the output size to small, medium or large (--small) (--medium) (--large) (default 'small') --SNP Highlight SNP locations in each alignment -title Specify the gnuplot plot title (default none) -x Set the xrange for the plot '[min:max]' -y Set the yrange for the plot '[min:max]' -R Plot an ordered set of reference sequences from Rfile -Q Plot an ordered set of query sequences from Qfile --layout Layout a .delta multiplot in an intelligible fashion, this option requires the -R -Q options"
toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/4.0.0+galaxy1	"nucmer is for the all-vs-all comparison of nucleotide sequences contained in multi-FastA data files. It is best used for highly similar sequence that may have large rearrangements. Common use cases are: comparing two unfinished shotgun sequencing assemblies, mapping an unfinished sequencing assembly to a finished genome, and comparing two fairly similar genomes that may have large rearrangements and duplications. All output coordinates reference the forward strand of the involved sequence, regardless of the match direction. Also, nucmer now uses only matches that are unique in the reference sequence by default, use different Anchoring options to change this behavior. 
Options:
:: Defaults in parentheses nucmer --sam-long The original output format of nucmer, the delta format, contains only the minimum information necessary to quickly recreate the alignment. It contains the name of the matching sequences, the length of the match, number of errors and positions of indels. With --sam-long, it additionally reports the MD string (which specifies the mismatching positions), the sequence and, if applicable, the quality values of the matching sequence. The long format is more expensive to compute and it generates larger output files, but this option allows nucmer4 to match the behavior of other aligners such as Bowtie2 or BWA. --mum Use anchor matches that are unique in both the reference and query (false) --maxmatch Use all anchor matches regardless of their uniqueness (false) -b Set the distance an alignment extension will attempt to extend poor scoring regions before giving up (200) -c Sets the minimum length of a cluster of matches (65) -D Set the maximum diagonal difference between two adjacent anchors in a cluster (5) -d Set the maximum diagonal difference between two adjacent anchors in a cluster as a differential fraction of the gap length (0.12) --noextend Do not perform cluster extension step (false) -f Use only the forward strand of the Query sequences (false) -r Use only the reverse complement of the Query sequences (false) -g Set the maximum gap between two adjacent matches in a cluster (90) -l Set the minimum length of a single exact match (20) -L Minimum length of an alignment, after clustering and extension (0) --nooptimize No alignment score optimization, i.e. if an alignment extension reaches the end of a sequence, it will not backtrack to optimize the alignment score and instead terminate the alignment at the end of the sequence (false) --nosimplify Don't simplify alignments by removing shadowed clusters. Use this option when aligning a sequence to itself to look for repeats (false) --banded Enforce absolute banding of dynamic programming matrix based on diagdiff parameter (false) --large Force the use of large offsets (false) -G Map genome to genome (long query sequences) (false) -M Max chunk. Stop adding sequence for a thread if more than MAX already. (50000) mummerplot -b Highlight alignments with breakpoints further than breaklen nucleotides from the nearest sequence end -color Color plot lines with a percent similarity gradient or turn off all plot color (default color by match dir) If the plot is very sparse, edit the .gp script to plot with 'linespoints' instead of 'lines' -c Generate a reference coverage plot (default for .tiling) --filter Only display .delta alignments which represent the ""best"" hit to any particular spot on either sequence, i.e. a one-to-one mapping of reference and query subsequences --fat Layout sequences using fattest alignment only -IdR Plot a particular reference sequence ID on the X-axis -IdQ Plot a particular query sequence ID on the Y-axis -s Set the output size to small, medium or large (--small) (--medium) (--large) (default 'small') --SNP Highlight SNP locations in each alignment -title Specify the gnuplot plot title (default none) -x Set the xrange for the plot '[min:max]' -y Set the yrange for the plot '[min:max]' -R Plot an ordered set of reference sequences from Rfile -Q Plot an ordered set of query sequences from Qfile --layout Layout a .delta multiplot in an intelligible fashion, this option requires the -R -Q options"
toolshed.g2.bx.psu.edu/repos/iuc/sina/sina/1.7.2+galaxy0	".. class:: infomark 
What it does
 SINA aligns nucleotide sequences to match a pre-existing MSA using a graph-based alignment algorithm similar to PoA. The graph approach allows SINA to incorporate information from many reference sequences without blurring highly variable regions. While pure NAST implementations are highly dependent on finding a good match in the reference database, SINA is able to align sequences relatively distant to references with good quality and will yield a robust result for query sequences with many close references. While adding sequences to an MSA with SINA is usually faster than re-computing the entire MSA from an augmented set of unaligned sequences, the primary benefit lies in protecting investments made into the original MSA such as manual curation of the alignment, compute-intensive phylogenetic tree reconstruction and taxonomic annotation of the resulting phylogeny. Additionally, SINA includes a homology search which uses the previously computed alignment to determine the most similar sequences. Based on the search results, a LCA-based classification of the query sequence can be computed using taxonomic classifications assigned to the sequences comprising the reference MSA. SINA is used to compute the large and small subunit ribosomal RNA alignments provided by the 
SILVA project &lt;https://www.arb-silva.de/&gt;
 and is able to use the 
ARB format reference databases &lt;https://www.arb-silva.de/download/arb-files/&gt;
 released by the project. 
Input
 SINA requires sequences in FASTA file format, whereas libraries can be also provided as ARB files. Furthermore, reference databases can be added as data tables. See README.rst for more information. 
Output
 Results are provided in FASTA or ARB file format, whereas additional metadata is provided as tabular text file. .. class:: infomark 
References
 More information can be found on the 
project website &lt;https://sina.readthedocs.io/en/latest&gt;
 and on 
GitHub &lt;https://github.com/epruesse/SINA&gt;
. An 
online version &lt;https://www.arb-silva.de/aligner&gt;
_ of SINA is provided by the SILVA project."
toolshed.g2.bx.psu.edu/repos/iuc/mummer_show_coords/mummer_show_coords/4.0.0+galaxy1	"This program parses the delta alignment output of nucmer and displays the coordinates, and other useful information about the alignments. Output is tabular. Below is a description of each column (Default): * 
[S1]
 Start of the alignment region in the reference sequence * 
[E1]
 End of the alignment region in the reference sequence * 
[S2]
 Start of the alignment region in the query sequence * 
[E2]
 End of the alignment region in the query sequence * 
[LEN 1]
 Length of the alignment region in the reference sequence, measured in nucleotides * 
[LEN 2]
 Length of the alignment region in the query sequence, measured in nucleotides * 
[% IDY]
 Percent identity of the alignment, calculated as (number of exact matches) / ([LEN 1] + insertions in the query) * 
[LEN R]
 Length of the reference sequence * 
[LEN Q]
 Length of the query sequence * 
[COV R]
 Percent coverage of the alignment on the reference sequence, calculated as [LEN 1] / [LEN R] * 
[COV Q]
 Percent coverage of the alignment on the query sequence, calculated as [LEN 2] / [LEN Q] * 
[FRM R]
 Reading frame for the reference sequence (only with -d) * 
[FRM Q]
 Reading frame for the query sequence (only with -d) * 
[REF TAG]
 The reference FastA ID * 
[QUERY TAG]
 The query FastA ID There is also an optional final column (turned on with the ""Annotate"" parameter) that will contain some 'annotations'. The Annotate option will annotate alignments that represent overlaps between two sequences. Sometimes, nucmer will extend adjacent clusters past one another, thus causing a somewhat redundant output, this option will notify users of such rare occurrences. The Percent Coverage and Sequence Length options are useful when comparing two sets of assembly contigs, in that these options help determine if an alignment spans an entire contig, or is just a partial hit to a different read. The Merge option is useful when the user wishes to identify sytenic regions between two genomes, but is not particularly interested in the actual alignment similarity or appearance. This option also disregards match orientation, so should not be used if this information is needed. 
Options:
:: -b Merges overlapping alignments regardless of match dir or frame and does not display any identity information. -d Display the alignment direction in the additional FRM columns -I Set minimum percent identity to display -L Set minimum alignment length to display -o Annotate maximal alignments between two sequences, i.e. overlaps between reference and query sequences -q Sort output lines by query IDs and coordinates -r Sort output lines by reference IDs and coordinates"
toolshed.g2.bx.psu.edu/repos/iuc/genebed_maf_to_fasta/GeneBed_Maf_Fasta2/1.0.1+galaxy0	"What it does
 The coding sequence of genes are usually composed of several coding exons. Each of these coding exons is an individual genomic region, which when concatenated with each other constitutes the coding sequence. A single genomic region can be covered by multiple alignment blocks. In many cases it is desirable to stitch these alignment blocks together. This tool accepts a list of gene-based intervals, in the Gene BED format. For every interval it performs the following: * finds all MAF blocks that overlap the coding regions; * sorts MAF blocks by alignment score; * stitches blocks together and resolves overlaps based on alignment score; * outputs alignments in FASTA format."
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_mafaddirows/ucsc_mafaddirows/482+galaxy0	"mafAddIRows
 This tool adds 'i' rows to a Multiple Alignment Format (MAF) file. It requires a MAF file with a single target sequence. 
Usage
 - 
Input MAF file
: Provide a MAF file containing alignments with only one target sequence. - 
TwoBit reference file
: Specify a TwoBit file for the reference genome. - 
List of BED files
 (optional): Provide BED files (one per species) specifying N locations. - 
Add rows of N's
: Check to insert rows of N's into MAF blocks instead of annotating. - 
Add rows of -'s
: Check to insert rows of -'s into MAF blocks instead of annotating. 
Output
 - A modified MAF file with added 'i' rows or annotations based on the provided options. 
Note
 - The input MAF file must contain only a single target sequence. - Use either 
-addN
 or 
-addDash
, but not both, to modify the MAF blocks directly."
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_mafcoverage/ucsc_mafcoverage/482+galaxy0	mafCoverage is a command-line tool from the UCSC Genome Browser suite that analyses the coverage by MAF files chromosome by chromosome and genome-wide.
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_maffetch/ucsc_maffetch/482+galaxy0	mafFetch is a command-line tool from the UCSC Genome Browser suite that extracts MAF records overlapping regions in a BED file (minimum 3 columns: chrom, start, end) from a specified UCSC database table (e.g., multiz46way for hg19). Outputs alignments to a MAF file using an indexed lookup for efficiency.
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_maffilter/ucsc_mafFilter/482+galaxy0	"mafFilter
 Filters MAF (Multiple Alignment Format) files based on specified criteria. The filtered output is written to a new MAF file, and optionally, rejected blocks are saved to a separate file. 
Options:
 - 
Tolerate bad input
: Ignore bad input instead of aborting. - 
Minimum columns
: Filter out blocks with fewer than the specified number of columns (default: 1). - 
Minimum rows
: Filter out blocks with fewer than the specified number of rows (default: 2). - 
Maximum rows
: Filter out blocks with more than or equal to the specified number of rows (default: 100). - 
Factor-based score filtering
: Filter out scores below 
-minFactor * (ncol^2) * nrow
. - 
Minimum factor
: Factor to use with factor-based score filtering (default: 5). - 
Minimum score
: Minimum allowed score (alternative to factor-based filtering). - 
Rejected blocks output file
: Save rejected blocks to the specified file. - 
Required species component
: All alignments must include the specified species as a component. - 
Reject overlapping blocks
: Reject overlapping blocks in the reference (assumes ordered blocks). - 
Component filter file
: Filter out blocks without a component listed in the provided file. - 
Species filter file
: Filter out blocks without a species listed in the provided file."
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_maffrags/ucsc_maffrags/482+galaxy0	mafFrags is a command-line tool from the UCSC Genome Browser suite that extracts multiple alignment format (MAF) blocks from a specified alignment track (e.g., multiz46way) based on genomic regions defined in a BED file. It is useful for retrieving alignments for specific genes, exons, or regions across multiple species.
toolshed.g2.bx.psu.edu/repos/iuc/mashmap/mashmap/3.1.3+galaxy0	"MashMap
 implements a fast and approximate algorithm for computing local alignment boundaries between long DNA sequences. It can be useful for mapping genome assembly or long reads (PacBio/ONT) to reference genome(s). Given a minimum alignment length and an identity threshold for the desired local alignments, Mashmap computes alignment boundaries and identity estimates using k-mers. It does not compute the alignments explicitly, but rather estimates an unbiased k-mer based Jaccard similarity using a combination of minmers (a novel winnowing scheme) and MinHash. This is then converted to an estimate of sequence identity using the Mash distance. An appropriate k-mer sampling rate is automatically determined using the given minimum local alignment length and identity thresholds. As an example, Mashmap can map a human genome assembly to the human reference genome in about one minute total execution time and < 4 GB memory using just 8 CPU threads, achieving more than an order of magnitude improvement in both runtime and memory over alternative methods. We describe the algorithms associated with Mashmap, and report on speed, scalability, and accuracy of the software in the publications listed below. Unlike traditional mappers, MashMap does not compute exact sequence alignments. In future, we plan to add an optional alignment support to generate base-to-base alignments. The output is space-delimited with each line consisting of query name, length, 0-based start, end, strand, target name, length, start, end and mapping nucleotide identity."
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/blastxml_to_tabular/2.16.0+galaxy0	"What it does
 NCBI BLAST+ (and the older NCBI 'legacy' BLAST) can output in a range of formats including tabular and a more detailed XML format. A complex workflow may need both the XML and the tabular output - but running BLAST twice is slow and wasteful. This tool takes the BLAST XML output and can convert it into the standard 12 column tabular equivalent: ====== ========= ============================================ Column NCBI name Description ------ --------- -------------------------------------------- 1 qseqid Query Seq-id (ID of your sequence) 2 sseqid Subject Seq-id (ID of the database hit) 3 pident Percentage of identical matches 4 length Alignment length 5 mismatch Number of mismatches 6 gapopen Number of gap openings 7 qstart Start of alignment in query 8 qend End of alignment in query 9 sstart Start of alignment in subject (database hit) 10 send End of alignment in subject (database hit) 11 evalue Expectation value (E-value) 12 bitscore Bit score ====== ========= ============================================ The BLAST+ tools can optionally output additional columns of information, but this takes longer to calculate. Most (but not all) of these columns are included by selecting the extended tabular output. The extra columns are included 
after
 the standard 12 columns. This is so that you can write workflow filtering steps that accept either the 12 or 25 column tabular BLAST output. This tool now uses this extended 25 column output by default. ====== ============= =========================================== Column NCBI name Description ------ ------------- ------------------------------------------- 13 sallseqid All subject Seq-id(s), separated by a ';' 14 score Raw score 15 nident Number of identical matches 16 positive Number of positive-scoring matches 17 gaps Total number of gaps 18 ppos Percentage of positive-scoring matches 19 qframe Query frame 20 sframe Subject frame 21 qseq Aligned part of query sequence 22 sseq Aligned part of subject sequence 23 qlen Query sequence length 24 slen Subject sequence length 25 salltitles All subject title(s), separated by a '<>' ====== ============= =========================================== Beware that the XML file (and thus the conversion) and the tabular output direct from BLAST+ may differ in the presence of XXXX masking on regions low complexity (columns 21 and 22), and thus also calculated figures like the percentage identity (column 3). 
References
 If you use this Galaxy tool in work leading to a scientific publication please cite: Peter J.A. Cock, Björn A. Grüning, Konrad Paszkiewicz and Leighton Pritchard (2013). Galaxy tools and workflows for sequence analysis with applications in molecular plant pathology. PeerJ 1:e167 https://doi.org/10.7717/peerj.167 This wrapper is available to install into other Galaxy Instances via the Galaxy Tool Shed at http://toolshed.g2.bx.psu.edu/view/devteam/ncbi_blast_plus"
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_blastdbcmd_wrapper/2.16.0+galaxy0	"What it does
 Extracts FASTA formatted sequences from a BLAST database using the NCBI BLAST+ blastdbcmd command line tool. When giving a text file of entries, use one line per sequence. Optional valies should be space separate - the simplest syntax is 
identifier start-end
 (where 
end
 can be just 
-
), or 
identifier start-end strand
 (wheere the strand given as either 
+
 or 
-
). .. class:: warningmark 
BLAST assigned identifiers
 When a BLAST database is constructed from a FASTA file, the original identifiers can be replaced with BLAST assigned identifiers, partly to ensure uniqueness. e.g. Sometimes a prefix of 'lcl|' is added (lcl is short for local), or an arbitrary name starting 'gnl|BL_ORD_ID|' is created. If you are using the tabular output from BLAST, it will contain the original identifiers - not the BLAST assigned identifiers suitable for use with the blastdbcmd tool. If you are using the XML or plain text output, this will also contain the BLAST assigned identifiers. However, this means getting a list of BLAST assigned identifiers isn't straightforward. ------- 
Advanced Options
 For help with advanced options and their default values, visit the NCBI BLAST® Command Line Applications User Manual, Appendices, 
Options for the command-line applications &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_Options_for_the_commandline_a_&gt;
. For amino acid substitution matrices, see 
BLAST Substitution Matrices &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_BLAST_Substitution_Matrices_&gt;
 in the same appendices."
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_blastn_wrapper/2.16.0+galaxy0	".. class:: warningmark 
Note
. Database searches may take a substantial amount of time. For large input datasets it is advisable to allow overnight processing. ----- 
What it does
 Search a 
nucleotide database
 using a 
nucleotide query
, using the NCBI BLAST+ blastn command line tool. Algorithms include blastn, megablast, and discontiguous megablast. .. class:: warningmark You can also search against a FASTA file of subject (target) sequences. This is 
not
 advised because it is slower (only one CPU is used), but more importantly gives e-values for pairwise searches (very small e-values which will look overly signficiant). In most cases you should instead turn the other FASTA file into a database first using 
makeblastdb
 and search against that. ----- 
Output format
 Because Galaxy focuses on processing tabular data, the default output of this tool is tabular. The standard BLAST+ tabular output contains 12 columns: ====== ========= ============================================ Column NCBI name Description ------ --------- -------------------------------------------- 1 qaccver Query accession dot version 2 saccver Subject accession dot version (database hit) 3 pident Percentage of identical matches 4 length Alignment length 5 mismatch Number of mismatches 6 gapopen Number of gap openings 7 qstart Start of alignment in query 8 qend End of alignment in query 9 sstart Start of alignment in subject (database hit) 10 send End of alignment in subject (database hit) 11 evalue Expectation value (E-value) 12 bitscore Bit score ====== ========= ============================================ Until BLAST+ 2.5.0, the first two columns were 
qseqid
 and 
sseqid
, which were usually strings contained multiple pipe-separated entries. In BLAST+ 2.5.0, the first two columns became 
qacc
 and 
sacc
 (accesion only), while in BLAST+ 2.6.0 this was changed again to use 
qaccver
 and 
saccver
 (accession dot version). The BLAST+ tools can optionally output additional columns of information, but this takes longer to calculate. Many commonly used extra columns are included by selecting the extended tabular output. The extra columns are included 
after
 the standard 12 columns. This is so that you can write workflow filtering steps that accept either the 12 or 25 column tabular BLAST output. Galaxy now uses this extended 25 column output by default. ====== ============= =========================================== Column NCBI name Description ------ ------------- ------------------------------------------- 13 sallseqid All subject Seq-id(s), separated by a ';' 14 score Raw score 15 nident Number of identical matches 16 positive Number of positive-scoring matches 17 gaps Total number of gaps 18 ppos Percentage of positive-scoring matches 19 qframe Query frame 20 sframe Subject frame 21 qseq Aligned part of query sequence 22 sseq Aligned part of subject sequence 23 qlen Query sequence length 24 slen Subject sequence length 25 salltitles All subject title(s), separated by a '<>' ====== ============= =========================================== The third option is to customise the tabular output by selecting which columns you want, from the standard set of 12, the default set of 25, or any of the additional columns BLAST+ offers (including species name). The fourth option is BLAST XML output, which is designed to be parsed by another program, and is understood by some Galaxy tools. You can also choose several plain text or HTML output formats which are designed to be read by a person (not by another program). The HTML versions use basic webpage formatting and can include links to the hits on the NCBI website. The pairwise output (the default on the NCBI BLAST website) shows each match as a pairwise alignment with the query. The two query anchored outputs show a multiple sequence alignment between the query and all the matches, and differ in how insertions are shown (marked as insertions or with gap characters added to the other sequences). ------- 
Advanced Options
 For help with advanced options and their default values, visit the NCBI BLAST® Command Line Applications User Manual, Appendices, 
Options for the command-line applications &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_Options_for_the_commandline_a_&gt;
. For amino acid substitution matrices, see 
BLAST Substitution Matrices &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_BLAST_Substitution_Matrices_&gt;
 in the same appendices."
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_blastp_wrapper/2.16.0+galaxy0	".. class:: warningmark 
Note
. Database searches may take a substantial amount of time. For large input datasets it is advisable to allow overnight processing. ----- 
What it does
 Search a 
protein database
 using a 
protein query
, using the NCBI BLAST+ blastp command line tool. .. class:: warningmark You can also search against a FASTA file of subject (target) sequences. This is 
not
 advised because it is slower (only one CPU is used), but more importantly gives e-values for pairwise searches (very small e-values which will look overly signficiant). In most cases you should instead turn the other FASTA file into a database first using 
makeblastdb
 and search against that. ------- 
Output format
 Because Galaxy focuses on processing tabular data, the default output of this tool is tabular. The standard BLAST+ tabular output contains 12 columns: ====== ========= ============================================ Column NCBI name Description ------ --------- -------------------------------------------- 1 qaccver Query accession dot version 2 saccver Subject accession dot version (database hit) 3 pident Percentage of identical matches 4 length Alignment length 5 mismatch Number of mismatches 6 gapopen Number of gap openings 7 qstart Start of alignment in query 8 qend End of alignment in query 9 sstart Start of alignment in subject (database hit) 10 send End of alignment in subject (database hit) 11 evalue Expectation value (E-value) 12 bitscore Bit score ====== ========= ============================================ Until BLAST+ 2.5.0, the first two columns were 
qseqid
 and 
sseqid
, which were usually strings contained multiple pipe-separated entries. In BLAST+ 2.5.0, the first two columns became 
qacc
 and 
sacc
 (accesion only), while in BLAST+ 2.6.0 this was changed again to use 
qaccver
 and 
saccver
 (accession dot version). The BLAST+ tools can optionally output additional columns of information, but this takes longer to calculate. Many commonly used extra columns are included by selecting the extended tabular output. The extra columns are included 
after
 the standard 12 columns. This is so that you can write workflow filtering steps that accept either the 12 or 25 column tabular BLAST output. Galaxy now uses this extended 25 column output by default. ====== ============= =========================================== Column NCBI name Description ------ ------------- ------------------------------------------- 13 sallseqid All subject Seq-id(s), separated by a ';' 14 score Raw score 15 nident Number of identical matches 16 positive Number of positive-scoring matches 17 gaps Total number of gaps 18 ppos Percentage of positive-scoring matches 19 qframe Query frame 20 sframe Subject frame 21 qseq Aligned part of query sequence 22 sseq Aligned part of subject sequence 23 qlen Query sequence length 24 slen Subject sequence length 25 salltitles All subject title(s), separated by a '<>' ====== ============= =========================================== The third option is to customise the tabular output by selecting which columns you want, from the standard set of 12, the default set of 25, or any of the additional columns BLAST+ offers (including species name). The fourth option is BLAST XML output, which is designed to be parsed by another program, and is understood by some Galaxy tools. You can also choose several plain text or HTML output formats which are designed to be read by a person (not by another program). The HTML versions use basic webpage formatting and can include links to the hits on the NCBI website. The pairwise output (the default on the NCBI BLAST website) shows each match as a pairwise alignment with the query. The two query anchored outputs show a multiple sequence alignment between the query and all the matches, and differ in how insertions are shown (marked as insertions or with gap characters added to the other sequences). ------ 
Advanced Options
 For help with advanced options and their default values, visit the NCBI BLAST® Command Line Applications User Manual, Appendices, 
Options for the command-line applications &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_Options_for_the_commandline_a_&gt;
. For amino acid substitution matrices, see 
BLAST Substitution Matrices &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_BLAST_Substitution_Matrices_&gt;
 in the same appendices."
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_blastx_wrapper/2.16.0+galaxy0	".. class:: warningmark 
Note
. Database searches may take a substantial amount of time. For large input datasets it is advisable to allow overnight processing. ----- 
What it does
 Search a 
protein database
 using a 
translated nucleotide query
, using the NCBI BLAST+ blastx command line tool. .. class:: warningmark You can also search against a FASTA file of subject (target) sequences. This is 
not
 advised because it is slower (only one CPU is used), but more importantly gives e-values for pairwise searches (very small e-values which will look overly signficiant). In most cases you should instead turn the other FASTA file into a database first using 
makeblastdb
 and search against that. ------- 
Output format
 Because Galaxy focuses on processing tabular data, the default output of this tool is tabular. The standard BLAST+ tabular output contains 12 columns: ====== ========= ============================================ Column NCBI name Description ------ --------- -------------------------------------------- 1 qaccver Query accession dot version 2 saccver Subject accession dot version (database hit) 3 pident Percentage of identical matches 4 length Alignment length 5 mismatch Number of mismatches 6 gapopen Number of gap openings 7 qstart Start of alignment in query 8 qend End of alignment in query 9 sstart Start of alignment in subject (database hit) 10 send End of alignment in subject (database hit) 11 evalue Expectation value (E-value) 12 bitscore Bit score ====== ========= ============================================ Until BLAST+ 2.5.0, the first two columns were 
qseqid
 and 
sseqid
, which were usually strings contained multiple pipe-separated entries. In BLAST+ 2.5.0, the first two columns became 
qacc
 and 
sacc
 (accesion only), while in BLAST+ 2.6.0 this was changed again to use 
qaccver
 and 
saccver
 (accession dot version). The BLAST+ tools can optionally output additional columns of information, but this takes longer to calculate. Many commonly used extra columns are included by selecting the extended tabular output. The extra columns are included 
after
 the standard 12 columns. This is so that you can write workflow filtering steps that accept either the 12 or 25 column tabular BLAST output. Galaxy now uses this extended 25 column output by default. ====== ============= =========================================== Column NCBI name Description ------ ------------- ------------------------------------------- 13 sallseqid All subject Seq-id(s), separated by a ';' 14 score Raw score 15 nident Number of identical matches 16 positive Number of positive-scoring matches 17 gaps Total number of gaps 18 ppos Percentage of positive-scoring matches 19 qframe Query frame 20 sframe Subject frame 21 qseq Aligned part of query sequence 22 sseq Aligned part of subject sequence 23 qlen Query sequence length 24 slen Subject sequence length 25 salltitles All subject title(s), separated by a '<>' ====== ============= =========================================== The third option is to customise the tabular output by selecting which columns you want, from the standard set of 12, the default set of 25, or any of the additional columns BLAST+ offers (including species name). The fourth option is BLAST XML output, which is designed to be parsed by another program, and is understood by some Galaxy tools. You can also choose several plain text or HTML output formats which are designed to be read by a person (not by another program). The HTML versions use basic webpage formatting and can include links to the hits on the NCBI website. The pairwise output (the default on the NCBI BLAST website) shows each match as a pairwise alignment with the query. The two query anchored outputs show a multiple sequence alignment between the query and all the matches, and differ in how insertions are shown (marked as insertions or with gap characters added to the other sequences). ------- 
Advanced Options
 For help with advanced options and their default values, visit the NCBI BLAST® Command Line Applications User Manual, Appendices, 
Options for the command-line applications &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_Options_for_the_commandline_a_&gt;
. For amino acid substitution matrices, see 
BLAST Substitution Matrices &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_BLAST_Substitution_Matrices_&gt;
 in the same appendices."
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_convert2blastmask_wrapper/2.16.0+galaxy0	"What it does
 Convert masking information in lower-case masked FASTA input to file formats suitable for makeblastdb. More information about segmasker can be found in the 
BLAST Command Line Applications User Manual
_. .. _BLAST Command Line Applications User Manual: https://www.ncbi.nlm.nih.gov/books/NBK279690/"
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_blastdbcmd_info/2.16.0+galaxy0	"What it does
 Calls the NCBI BLAST+ blastdbcmd command line tool with the -info switch to give summary information about a BLAST database, such as the size (number of sequences and total length) and date."
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_dustmasker_wrapper/2.16.0+galaxy0	"What it does
 This tool identifies and masks out low complexity regions of a nucleotide database (or sequences in FASTA format) by using the symmetric DUST_ algorithm. If you select 
maskinfo ASN.1
 (binary or text) as output format, the output file can be used as masking data for NCBI BLAST+ makeblastdb tool. More information about dustmasker can be found in the 
BLAST Command Line Applications User Manual
_. .. _BLAST Command Line Applications User Manual: https://www.ncbi.nlm.nih.gov/books/NBK279690/ .. _DUST: https://www.ncbi.nlm.nih.gov/pubmed/16796549"
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_makeblastdb/2.16.0+galaxy0	"What it does
 Make BLAST database from one or more FASTA files and/or BLAST databases. This is a wrapper for the NCBI BLAST+ tool 'makeblastdb', which is the replacement for the 'formatdb' tool in the NCBI 'legacy' BLAST suite. More information about makeblastdb can be found in the 
BLAST Command Line Applications User Manual
_. .. _BLAST Command Line Applications User Manual: https://www.ncbi.nlm.nih.gov/books/NBK279690/"
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_makeprofiledb/2.16.0+galaxy0	"What it does
 Make a protein domain profile database (for use with RPS-BLAST or RSP-TBLASTN) from one or more Position Specific Scoring Matrices (PSSM) files in the NCBI ""scoremat"" ASN.1 format (usually named 
*.smp
). This is a wrapper for the NCBI BLAST+ tool 'makeprofiledb'. More information about makeprofiledb can be found in the 
BLAST Command Line Applications User Manual
_. .. _BLAST Command Line Applications User Manual: https://www.ncbi.nlm.nih.gov/books/NBK279690/"
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_rpsblast_wrapper/2.16.0+galaxy0	".. class:: warningmark 
Note
. Database searches may take a substantial amount of time. For large input datasets it is advisable to allow overnight processing. ----- 
What it does
 Search a 
protein domain database
 using a 
protein query
, using the NCBI BLAST+ rpsblast command line tool. The protein domain databases use position-specific scoring matrices (PSSMs) and are available for a number of domain collections including: 
CDD
 - NCBI curarated meta-collection of domains, see https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd_help.shtml#NCBI_curated_domains 
Kog
 - PSSMs from automatically aligned sequences and sequence fragments classified in the KOGs resource, the eukaryotic counterpart to COGs, see https://www.ncbi.nlm.nih.gov/COG/ 
Cog
 - PSSMs from automatically aligned sequences and sequence fragments classified in the COGs resource, which focuses primarily on prokaryotes, see https://www.ncbi.nlm.nih.gov/COG/ 
Pfam
 - PSSMs from Pfam-A seed alignment database, see http://xfam.org/ 
Smart
 - PSSMs from SMART domain alignment database, see http://smart.embl-heidelberg.de/ 
Tigr
 - PSSMs from TIGRFAM database of protein families, see ftp://ftp.jcvi.org/data/TIGRFAMs/ 
Prk
 - PSSms from automatically aligned stable clusters in the Protein Clusters database, see https://www.ncbi.nlm.nih.gov/proteinclusters?cmd=search&db=proteinclusters The exact list of domain databases offered will depend on how your local Galaxy has been configured. ----- 
Output format
 Because Galaxy focuses on processing tabular data, the default output of this tool is tabular. The standard BLAST+ tabular output contains 12 columns: ====== ========= ============================================ Column NCBI name Description ------ --------- -------------------------------------------- 1 qaccver Query accession dot version 2 saccver Subject accession dot version (database hit) 3 pident Percentage of identical matches 4 length Alignment length 5 mismatch Number of mismatches 6 gapopen Number of gap openings 7 qstart Start of alignment in query 8 qend End of alignment in query 9 sstart Start of alignment in subject (database hit) 10 send End of alignment in subject (database hit) 11 evalue Expectation value (E-value) 12 bitscore Bit score ====== ========= ============================================ Until BLAST+ 2.5.0, the first two columns were 
qseqid
 and 
sseqid
, which were usually strings contained multiple pipe-separated entries. In BLAST+ 2.5.0, the first two columns became 
qacc
 and 
sacc
 (accesion only), while in BLAST+ 2.6.0 this was changed again to use 
qaccver
 and 
saccver
 (accession dot version). The BLAST+ tools can optionally output additional columns of information, but this takes longer to calculate. Many commonly used extra columns are included by selecting the extended tabular output. The extra columns are included 
after
 the standard 12 columns. This is so that you can write workflow filtering steps that accept either the 12 or 25 column tabular BLAST output. Galaxy now uses this extended 25 column output by default. ====== ============= =========================================== Column NCBI name Description ------ ------------- ------------------------------------------- 13 sallseqid All subject Seq-id(s), separated by a ';' 14 score Raw score 15 nident Number of identical matches 16 positive Number of positive-scoring matches 17 gaps Total number of gaps 18 ppos Percentage of positive-scoring matches 19 qframe Query frame 20 sframe Subject frame 21 qseq Aligned part of query sequence 22 sseq Aligned part of subject sequence 23 qlen Query sequence length 24 slen Subject sequence length 25 salltitles All subject title(s), separated by a '<>' ====== ============= =========================================== The third option is to customise the tabular output by selecting which columns you want, from the standard set of 12, the default set of 25, or any of the additional columns BLAST+ offers (including species name). The fourth option is BLAST XML output, which is designed to be parsed by another program, and is understood by some Galaxy tools. You can also choose several plain text or HTML output formats which are designed to be read by a person (not by another program). The HTML versions use basic webpage formatting and can include links to the hits on the NCBI website. The pairwise output (the default on the NCBI BLAST website) shows each match as a pairwise alignment with the query. The two query anchored outputs show a multiple sequence alignment between the query and all the matches, and differ in how insertions are shown (marked as insertions or with gap characters added to the other sequences). ------- 
Advanced Options
 For help with advanced options and their default values, visit the NCBI BLAST® Command Line Applications User Manual, Appendices, 
Options for the command-line applications &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_Options_for_the_commandline_a_&gt;
. For amino acid substitution matrices, see 
BLAST Substitution Matrices &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_BLAST_Substitution_Matrices_&gt;
 in the same appendices."
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_rpstblastn_wrapper/2.16.0+galaxy0	".. class:: warningmark 
Note
. Database searches may take a substantial amount of time. For large input datasets it is advisable to allow overnight processing. ----- 
What it does
 Search a 
protein domain database
 using a 
nucleotide query
, using the NCBI BLAST+ rpstblastn command line tool. The protein domain databases use position-specific scoring matrices (PSSMs) and are available for a number of domain collections including: 
CDD
 - NCBI curarated meta-collection of domains, see https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd_help.shtml#NCBI_curated_domains 
Kog
 - PSSMs from automatically aligned sequences and sequence fragments classified in the KOGs resource, the eukaryotic counterpart to COGs, see https://www.ncbi.nlm.nih.gov/COG/ 
Cog
 - PSSMs from automatically aligned sequences and sequence fragments classified in the COGs resource, which focuses primarily on prokaryotes, see https://www.ncbi.nlm.nih.gov/COG/ 
Pfam
 - PSSMs from Pfam-A seed alignment database, see http://xfam.org/ 
Smart
 - PSSMs from SMART domain alignment database, see http://smart.embl-heidelberg.de/ 
Tigr
 - PSSMs from TIGRFAM database of protein families, see ftp://ftp.jcvi.org/data/TIGRFAMs/ 
Prk
 - PSSms from automatically aligned stable clusters in the Protein Clusters database, see https://www.ncbi.nlm.nih.gov/proteinclusters?cmd=search&db=proteinclusters The exact list of domain databases offered will depend on how your local Galaxy has been configured. ----- 
Output format
 Because Galaxy focuses on processing tabular data, the default output of this tool is tabular. The standard BLAST+ tabular output contains 12 columns: ====== ========= ============================================ Column NCBI name Description ------ --------- -------------------------------------------- 1 qaccver Query accession dot version 2 saccver Subject accession dot version (database hit) 3 pident Percentage of identical matches 4 length Alignment length 5 mismatch Number of mismatches 6 gapopen Number of gap openings 7 qstart Start of alignment in query 8 qend End of alignment in query 9 sstart Start of alignment in subject (database hit) 10 send End of alignment in subject (database hit) 11 evalue Expectation value (E-value) 12 bitscore Bit score ====== ========= ============================================ Until BLAST+ 2.5.0, the first two columns were 
qseqid
 and 
sseqid
, which were usually strings contained multiple pipe-separated entries. In BLAST+ 2.5.0, the first two columns became 
qacc
 and 
sacc
 (accesion only), while in BLAST+ 2.6.0 this was changed again to use 
qaccver
 and 
saccver
 (accession dot version). The BLAST+ tools can optionally output additional columns of information, but this takes longer to calculate. Many commonly used extra columns are included by selecting the extended tabular output. The extra columns are included 
after
 the standard 12 columns. This is so that you can write workflow filtering steps that accept either the 12 or 25 column tabular BLAST output. Galaxy now uses this extended 25 column output by default. ====== ============= =========================================== Column NCBI name Description ------ ------------- ------------------------------------------- 13 sallseqid All subject Seq-id(s), separated by a ';' 14 score Raw score 15 nident Number of identical matches 16 positive Number of positive-scoring matches 17 gaps Total number of gaps 18 ppos Percentage of positive-scoring matches 19 qframe Query frame 20 sframe Subject frame 21 qseq Aligned part of query sequence 22 sseq Aligned part of subject sequence 23 qlen Query sequence length 24 slen Subject sequence length 25 salltitles All subject title(s), separated by a '<>' ====== ============= =========================================== The third option is to customise the tabular output by selecting which columns you want, from the standard set of 12, the default set of 25, or any of the additional columns BLAST+ offers (including species name). The fourth option is BLAST XML output, which is designed to be parsed by another program, and is understood by some Galaxy tools. You can also choose several plain text or HTML output formats which are designed to be read by a person (not by another program). The HTML versions use basic webpage formatting and can include links to the hits on the NCBI website. The pairwise output (the default on the NCBI BLAST website) shows each match as a pairwise alignment with the query. The two query anchored outputs show a multiple sequence alignment between the query and all the matches, and differ in how insertions are shown (marked as insertions or with gap characters added to the other sequences). ------- 
Advanced Options
 For help with advanced options and their default values, visit the NCBI BLAST® Command Line Applications User Manual, Appendices, 
Options for the command-line applications &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_Options_for_the_commandline_a_&gt;
. For amino acid substitution matrices, see 
BLAST Substitution Matrices &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_BLAST_Substitution_Matrices_&gt;
 in the same appendices."
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_segmasker_wrapper/2.16.0+galaxy0	"What it does
 This tool identifies and masks out low complexity regions of a protein database (or proteins in FASTA format) by using the SEG_ algorithm. If you select 
maskinfo ASN.1
 (binary or text) as output format, the output file can be used as masking data for NCBI BLAST+ makeblastdb tool. More information about segmasker can be found in the 
BLAST Command Line Applications User Manual
_. .. _BLAST Command Line Applications User Manual: https://www.ncbi.nlm.nih.gov/books/NBK279690/ .. _SEG: https://www.ncbi.nlm.nih.gov/pubmed/8743706"
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_tblastn_wrapper/2.16.0+galaxy0	".. class:: warningmark 
Note
. Database searches may take a substantial amount of time. For large input datasets it is advisable to allow overnight processing. ----- 
What it does
 Search a 
translated nucleotide database
 using a 
protein query
, using the NCBI BLAST+ tblastn command line tool. .. class:: warningmark You can also search against a FASTA file of subject (target) sequences. This is 
not
 advised because it is slower (only one CPU is used), but more importantly gives e-values for pairwise searches (very small e-values which will look overly signficiant). In most cases you should instead turn the other FASTA file into a database first using 
makeblastdb
 and search against that. ------ 
Output format
 Because Galaxy focuses on processing tabular data, the default output of this tool is tabular. The standard BLAST+ tabular output contains 12 columns: ====== ========= ============================================ Column NCBI name Description ------ --------- -------------------------------------------- 1 qaccver Query accession dot version 2 saccver Subject accession dot version (database hit) 3 pident Percentage of identical matches 4 length Alignment length 5 mismatch Number of mismatches 6 gapopen Number of gap openings 7 qstart Start of alignment in query 8 qend End of alignment in query 9 sstart Start of alignment in subject (database hit) 10 send End of alignment in subject (database hit) 11 evalue Expectation value (E-value) 12 bitscore Bit score ====== ========= ============================================ Until BLAST+ 2.5.0, the first two columns were 
qseqid
 and 
sseqid
, which were usually strings contained multiple pipe-separated entries. In BLAST+ 2.5.0, the first two columns became 
qacc
 and 
sacc
 (accesion only), while in BLAST+ 2.6.0 this was changed again to use 
qaccver
 and 
saccver
 (accession dot version). The BLAST+ tools can optionally output additional columns of information, but this takes longer to calculate. Many commonly used extra columns are included by selecting the extended tabular output. The extra columns are included 
after
 the standard 12 columns. This is so that you can write workflow filtering steps that accept either the 12 or 25 column tabular BLAST output. Galaxy now uses this extended 25 column output by default. ====== ============= =========================================== Column NCBI name Description ------ ------------- ------------------------------------------- 13 sallseqid All subject Seq-id(s), separated by a ';' 14 score Raw score 15 nident Number of identical matches 16 positive Number of positive-scoring matches 17 gaps Total number of gaps 18 ppos Percentage of positive-scoring matches 19 qframe Query frame 20 sframe Subject frame 21 qseq Aligned part of query sequence 22 sseq Aligned part of subject sequence 23 qlen Query sequence length 24 slen Subject sequence length 25 salltitles All subject title(s), separated by a '<>' ====== ============= =========================================== The third option is to customise the tabular output by selecting which columns you want, from the standard set of 12, the default set of 25, or any of the additional columns BLAST+ offers (including species name). The fourth option is BLAST XML output, which is designed to be parsed by another program, and is understood by some Galaxy tools. You can also choose several plain text or HTML output formats which are designed to be read by a person (not by another program). The HTML versions use basic webpage formatting and can include links to the hits on the NCBI website. The pairwise output (the default on the NCBI BLAST website) shows each match as a pairwise alignment with the query. The two query anchored outputs show a multiple sequence alignment between the query and all the matches, and differ in how insertions are shown (marked as insertions or with gap characters added to the other sequences). ------ 
Advanced Options
 For help with advanced options and their default values, visit the NCBI BLAST® Command Line Applications User Manual, Appendices, 
Options for the command-line applications &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_Options_for_the_commandline_a_&gt;
. For amino acid substitution matrices, see 
BLAST Substitution Matrices &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_BLAST_Substitution_Matrices_&gt;
 in the same appendices."
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_tblastx_wrapper/2.16.0+galaxy0	".. class:: warningmark 
Note
. Database searches may take a substantial amount of time. For large input datasets it is advisable to allow overnight processing. ----- 
What it does
 Search a 
translated nucleotide database
 using a 
translated nucleotide query
, using the NCBI BLAST+ tblastx command line tool. .. class:: warningmark You can also search against a FASTA file of subject (target) sequences. This is 
not
 advised because it is slower (only one CPU is used), but more importantly gives e-values for pairwise searches (very small e-values which will look overly signficiant). In most cases you should instead turn the other FASTA file into a database first using 
makeblastdb
 and search against that. ----- 
Output format
 Because Galaxy focuses on processing tabular data, the default output of this tool is tabular. The standard BLAST+ tabular output contains 12 columns: ====== ========= ============================================ Column NCBI name Description ------ --------- -------------------------------------------- 1 qaccver Query accession dot version 2 saccver Subject accession dot version (database hit) 3 pident Percentage of identical matches 4 length Alignment length 5 mismatch Number of mismatches 6 gapopen Number of gap openings 7 qstart Start of alignment in query 8 qend End of alignment in query 9 sstart Start of alignment in subject (database hit) 10 send End of alignment in subject (database hit) 11 evalue Expectation value (E-value) 12 bitscore Bit score ====== ========= ============================================ Until BLAST+ 2.5.0, the first two columns were 
qseqid
 and 
sseqid
, which were usually strings contained multiple pipe-separated entries. In BLAST+ 2.5.0, the first two columns became 
qacc
 and 
sacc
 (accesion only), while in BLAST+ 2.6.0 this was changed again to use 
qaccver
 and 
saccver
 (accession dot version). The BLAST+ tools can optionally output additional columns of information, but this takes longer to calculate. Many commonly used extra columns are included by selecting the extended tabular output. The extra columns are included 
after
 the standard 12 columns. This is so that you can write workflow filtering steps that accept either the 12 or 25 column tabular BLAST output. Galaxy now uses this extended 25 column output by default. ====== ============= =========================================== Column NCBI name Description ------ ------------- ------------------------------------------- 13 sallseqid All subject Seq-id(s), separated by a ';' 14 score Raw score 15 nident Number of identical matches 16 positive Number of positive-scoring matches 17 gaps Total number of gaps 18 ppos Percentage of positive-scoring matches 19 qframe Query frame 20 sframe Subject frame 21 qseq Aligned part of query sequence 22 sseq Aligned part of subject sequence 23 qlen Query sequence length 24 slen Subject sequence length 25 salltitles All subject title(s), separated by a '<>' ====== ============= =========================================== The third option is to customise the tabular output by selecting which columns you want, from the standard set of 12, the default set of 25, or any of the additional columns BLAST+ offers (including species name). The fourth option is BLAST XML output, which is designed to be parsed by another program, and is understood by some Galaxy tools. You can also choose several plain text or HTML output formats which are designed to be read by a person (not by another program). The HTML versions use basic webpage formatting and can include links to the hits on the NCBI website. The pairwise output (the default on the NCBI BLAST website) shows each match as a pairwise alignment with the query. The two query anchored outputs show a multiple sequence alignment between the query and all the matches, and differ in how insertions are shown (marked as insertions or with gap characters added to the other sequences). ------- 
Advanced Options
 For help with advanced options and their default values, visit the NCBI BLAST® Command Line Applications User Manual, Appendices, 
Options for the command-line applications &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_Options_for_the_commandline_a_&gt;
. For amino acid substitution matrices, see 
BLAST Substitution Matrices &lt;https://www.ncbi.nlm.nih.gov/books/NBK279684/#_appendices_BLAST_Substitution_Matrices_&gt;
 in the same appendices."
toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/get_species_taxids/2.16.0+galaxy0	"What it does
 Returns a list of species taxids for a taxon. It relies on the get_species_taxids.sh script of the BLAST+ package https://www.ncbi.nlm.nih.gov/books/NBK546209/"
toolshed.g2.bx.psu.edu/repos/iuc/clair3/clair3/1.0.10+galaxy1	"Clair3 is a germline small variant caller for long-reads. Clair3 makes the best of two major method categories: pileup calling handles most variant candidates with speed, and full-alignment tackles complicated candidates to maximize precision and recall. Clair3 runs fast and has superior performance, especially at lower coverage. Clair3 is simple and modular for easy deployment and integration. The tool can use models from the Oxford Nanopore Technologies Rerio_ repository, which are covered by a license that restricts their use to non-commercial use. If you select one of these models, you must agree to the terms of the Oxford Nanopore Technologies, Ltd. Public License, which can be found in the Rerio repository in the file LICENSE.txt. https://github.com/HKU-BAL/Clair3 LICENSE: Copyright 2021 The University of Hong Kong, Department of Computer Science Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. .. _Rerio: https://github.com/nanoporetech/rerio"
toolshed.g2.bx.psu.edu/repos/iuc/poretools_yield_plot/poretools_yield_plot/0.6.1a1.1	Create a collector’s curve reflecting the sequencing yield over time.
toolshed.g2.bx.psu.edu/repos/iuc/poretools_tabular/poretools_tabular/0.6.1a1.0	Dump the length, name, sequence, and quality scores of the sequence in one or a set of FAST5 files.
toolshed.g2.bx.psu.edu/repos/iuc/poretools_events/poretools_events/0.6.1a1.1	Extract the raw nanopore events from each FAST5 file.
toolshed.g2.bx.psu.edu/repos/iuc/poretools_extract/poretools_extract/0.6.1a1.0	Extract sequences from fast5 files generated by the Oxford Nanopore sequencing technology.
toolshed.g2.bx.psu.edu/repos/iuc/poretools_times/poretools_times/0.6.1a1.0	Collect read size statistics from a set of FAST5 files.
toolshed.g2.bx.psu.edu/repos/bgruening/flye/flye/2.9.6+galaxy0	"Purpose
 Flye is a de novo assembler for single molecule sequencing reads, such as those produced by PacBio and Oxford Nanopore Technologies. It is designed for a wide range of datasets, from small bacterial projects to large mammalian-scale assemblies. The package represents a complete pipeline: it takes raw PacBio/ONT reads as input and outputs polished contigs. Flye also has a special mode for metagenome assembly. ---- 
Quick usage
 Input reads can be in FASTA or FASTQ format, uncompressed or compressed with gz. Currently, PacBio (raw, corrected, HiFi) and ONT reads (raw, corrected) are supported. Expected error rates are <30% for raw, <3% for corrected, and <1% for HiFi. Note that Flye was primarily developed to run on raw reads. You may specify multiple files with reads (separated by spaces). Mixing different read types is not yet supported. The 
--meta
 o ption enables the mode for metagenome/uneven coverage assembly. Genome size estimate is no longer a required option. You need to provide an estimate if using 
--asm-coverage
 option. To reduce memory consumption for large genome assemblies, you can use a subset of the longest reads for initial disjointig assembly by specifying 
--asm-coverage
 and 
--genome-size
 options. Typically, 40x coverage is enough to produce good disjointigs. ---- 
Outputs
 The main output files are: * Final assembly: contains contigs and possibly scaffolds (see below). * Final repeat graph: note that the edge sequences might be different (shorter) than contig sequences, because contigs might include multiple graph edges. * Extra information about contigs (such as length or coverage). Each contig is formed by a single unique graph edge. If possible, unique contigs are extended with the sequence from flanking unresolved repeats on the graph. Thus, a contig fully contains the corresponding graph edge (with the same id), but might be longer then this edge. This is somewhat similar to unitig-contig relation in OLC assemblers. In a rare case when a repetitive graph edge is not covered by the set of ""extended"" contigs, it will be also output in the assembly file. Sometimes it is possible to further order contigs into scaffolds based on the repeat graph structure. These ordered contigs will be output as a part of scaffold in the assembly file (with a scaffold prefix). Since it is hard to give a reliable estimate of the gap size, those gaps are represented with the default 100 Ns. assembly_info.txt file (below) contains additional information about how scaffolds were formed. Extra information about contigs/scaffolds is output into the assembly_info.txt file. It is a tab-delimited table with the columns as follows: * Contig/scaffold id * Length * Coverage * Is circular, (Y)es or (N)o * Is repetitive, (Y)es or (N)o * Multiplicity (based on coverage) * Alternative group * Graph path (graph path corresponding to this contig/scaffold). Scaffold gaps are marked with 
??
 symbols, and 
*
 symbol denotes a terminal graph node. Alternative contigs (representing alternative haplotypes) will have the same alt. group ID. Primary contigs are marked by 
*
. ---- 
Algorithm Description
 This is a brief description of the Flye algorithm. Please refer to the manuscript for more detailed information. The draft contig extension is organized as follows: * K-mer counting / erroneous k-mer pre-filtering * Solid k-mer selection (k-mers with sufficient frequency, which are unlikely to be erroneous) * Contig extension. The algorithm starts from a single read and extends it with a next overlapping read (overlaps are dynamically detected using the selected solid k-mers). Note that we do not attempt to resolve repeats at this stage, thus the reconstructed contigs might contain misassemblies. Flye then aligns the reads on these draft contigs using minimap2 and calls a consensus. Afterwards, Flye performs repeat analysis as follows: * Repeat graph is constructed from the (possibly misassembled) contigs * In this graph all repeats longer than minimum overlap are collapsed * The algorithm resolves repeats using the read information and graph structure * The unbranching paths in the graph are output as contigs If enabled, after resolving bridged repeats, Trestle module attempts to resolve simple unbridged repeats (of multiplicity 2) using the heterogeneities between repeat copies. Finally, Flye performs polishing of the resulting assembly to correct the remaining errors: * Alignment of all reads to the current assembly using minimap2 * Partition the alignment into mini-alignments (bubbles) * Error correction of each bubble using a maximum likelihood approach The polishing steps could be repeated, which might slightly increase quality for some datasets."
toolshed.g2.bx.psu.edu/repos/iuc/poretools_qualpos/poretools_qualpos/0.6.1a1.1	Produce a box-whisker plot of quality score distribution over positions in nanopore reads
toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/0.6.1a1.1	Draw a histogram of read lengths in one or more nanopore reads.
toolshed.g2.bx.psu.edu/repos/iuc/poretools_winner/poretools_winner/0.6.1a1.0	Report the longest read among a set of FAST5 files.
toolshed.g2.bx.psu.edu/repos/iuc/nanoplot/nanoplot/1.46.2+galaxy0	"What it does
 NanoPlot_ is a plotting tool for long read sequencing data and alignments written by 
Wouter De Coster
 .. _NanoPlot: https://github.com/wdecoster/NanoPlot .. 
Wouter De Coster
: https://github.com/wdecoster 
Input
 NanoPlot requires 1 or more files as input. They can either be fastq (can be generated by albacore, guppy or MinKNOW containing additional information), fasta, sorted bam, sorted cram or sequencing summary. 
Output
 NanoPlot produces different number of plots depending on the data and customizations. A detailed view can be seen on here_. Additionally a file showing the statistics is generated. .. _here: https://github.com/wdecoster/NanoPlot#plots-generated"
toolshed.g2.bx.psu.edu/repos/iuc/poretools_occupancy/poretools_occupancy/0.6.1a1.1	Plot the throughput performance of each pore on the flowcell during a given sequencing run.
toolshed.g2.bx.psu.edu/repos/iuc/poretools_squiggle/poretools_squiggle/0.6.1a1.1	Plot the observed signals for FAST5 reads.
toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/0.2.4+galaxy1	Porechop is a tool for finding and removing adapters from Oxford Nanopore reads. Adapters on the ends of reads are trimmed off, and when a read has an adapter in its middle, it is treated as chimeric and chopped into separate reads. Porechop performs thorough alignments to effectively find adapters, even at low sequence identity. Porechop also supports demultiplexing of Nanopore reads that were barcoded with the Native Barcoding Kit, PCR Barcoding Kit or Rapid Barcoding Kit.
toolshed.g2.bx.psu.edu/repos/iuc/pycoqc/pycoqc/2.5.2+galaxy0	".. class:: infomark 
What it does
 
pycoqc
 computes metrics and generates interactive QC plots for Oxford Nanopore technologies sequencing data. 
Input
 - Guppy Sequencing Summary Output (tsv) - Aligned Reads (bam) (Optional) 
Output
 - Output QC Report (HTML) - Output QC Metrics (JSON) 
References
 More information are available on the 
pycoQC website &lt;https://a-slide.github.io/pycoQC/&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/poretools_stats/poretools_stats/0.6.1a1.0	Collect read size statistics from a set of FAST5 files.
toolshed.g2.bx.psu.edu/repos/iuc/poretools_nucdist/poretools_nucdist/0.6.1a1.0	Get the nucleotide composition of a set of FAST5 files.
toolshed.g2.bx.psu.edu/repos/iuc/poretools_qualdist/poretools_qualdist/0.6.1a1.0	Get the the quality score composition of a set of FAST5 files.
toolshed.g2.bx.psu.edu/repos/iuc/medaka_consensus_pipeline/medaka_consensus_pipeline/2.1.1+galaxy0	".. class:: infomark 
What it does
 
medaka
 is a tool suite to create a consensus sequence from nanopore sequencing data. This task is performed using neural networks applied from a pileup of individual sequencing reads against a draft assembly. It outperforms graph-based methods operating on basecalled data, and can be competitive with state-of-the-art signal-based methods, whilst being much faster. The 
medaka_consensus
 pipeline performs assembly polishing via neural networks. ---- .. class:: infomark 
Input
 An 
assembly
 in .fasta format and 
basecalls
 in .fasta or .fastq format are required. ---- .. class:: infomark 
Output
 - Consensus polished assembly (FASTA) - Consensus Probabilities (H5/HDF) - Calls To Draft (BAM) - Draft To Consensus (chain, TXT) - Variants: VCF of changes (VCF) - Polished: BED file of polished regions (BED) ---- .. class:: infomark 
Models
 For best results it is important to specify the correct model, -m in the above, according to the basecaller used. Allowed values can be found by running medaka tools list_models. Medaka models are named to indicate i) the pore type, ii) the sequencing device (MinION or PromethION), iii) the basecaller variant, and iv) the basecaller version, with the format: :: {pore}
{device}
{caller variant}
{caller version} For example the model named r941_min_fast_g303 should be used with data from MinION (or GridION) R9.4.1 flowcells using the fast Guppy basecaller version 3.0.3. By contrast the model r941_prom_hac_g303 should be used with PromethION data and the high accuracy basecaller (termed ""hac"" in Guppy configuration files). Where a version of Guppy has been used without an exactly corresponding medaka model, the medaka model with the highest version equal to or less than the guppy version should be selected. ---- .. class:: infomark 
References
 More information are available in the 
github &lt;https://github.com/nanoporetech/medaka&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/medaka_consensus/medaka_consensus/2.1.1+galaxy0	".. class:: infomark 
What it does
 
medaka
 is a tool suite to create a consensus sequence from nanopore sequencing data. This task is performed using neural networks applied from a pileup of individual sequencing reads against a draft assembly. It outperforms graph-based methods operating on basecalled data, and can be competitive with state-of-the-art signal-based methods, whilst being much faster. The module 
consensus
 runs inference from a trained model and alignments. ---- .. class:: infomark 
Inputs and outputs
 Medaka requires a BAM file as input, and generates a Hierarchical Data Format (H5/HDF) datafile. ---- .. class:: infomark 
Models
 For best results it is important to specify the correct model, -m in the above, according to the basecaller used. Allowed values can be found by running medaka tools list_models. Medaka models are named to indicate i) the pore type, ii) the sequencing device (MinION or PromethION), iii) the basecaller variant, and iv) the basecaller version, with the format: :: {pore}
{device}
{caller variant}
{caller version} For example the model named r941_min_fast_g303 should be used with data from MinION (or GridION) R9.4.1 flowcells using the fast Guppy basecaller version 3.0.3. By contrast the model r941_prom_hac_g303 should be used with PromethION data and the high accuracy basecaller (termed ""hac"" in Guppy configuration files). Where a version of Guppy has been used without an exactly corresponding medaka model, the medaka model with the highest version equal to or less than the guppy version should be selected. ---- .. class:: infomark 
References
 More information are available in the 
github &lt;https://github.com/nanoporetech/medaka&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/medaka_variant_pipeline/medaka_variant_pipeline/1.4.4+galaxy1	".. class:: infomark 
What it does
 
medaka
 is a tool suite to create a consensus sequence from nanopore sequencing data. This task is performed using neural networks applied from a pileup of individual sequencing reads against a draft assembly. It outperforms graph-based methods operating on basecalled data, and can be competitive with state-of-the-art signal-based methods, whilst being much faster. The module 
medaka_variant
 performs a variant calling via neural networks. ---- .. class:: infomark 
Input
 It is unlikely that the model arguments should be changed from their defaults. - reads aligned to reference (BAM), should be aligned to the reference against which to call variants - reference (FASTA) ---- .. class:: infomark 
Output
 - round_0_hap_mixed_phased.bam - round_0_hap_mixed_phased.vcf - round_0_hap_mixed_probs.hdf - round_0_hap_mixed_unphased.vcf - round_1_hap_1_probs.hdf - round_1_hap_1.vcf - round_1_hap_2_probs.hdf - round_1_hap_2.vcf - round_1_phased.vcf - round_1_unfiltered.vcf - round_1.vcf - log ---- .. class:: infomark 
Models
 For best results it is important to specify the correct model, -m in the above, according to the basecaller used. Allowed values can be found by running medaka tools list_models. Medaka models are named to indicate i) the pore type, ii) the sequencing device (MinION or PromethION), iii) the basecaller variant, and iv) the basecaller version, with the format: :: {pore}
{device}
{caller variant}
{caller version} For example the model named r941_min_fast_g303 should be used with data from MinION (or GridION) R9.4.1 flowcells using the fast Guppy basecaller version 3.0.3. By contrast the model r941_prom_hac_g303 should be used with PromethION data and the high accuracy basecaller (termed ""hac"" in Guppy configuration files). Where a version of Guppy has been used without an exactly corresponding medaka model, the medaka model with the highest version equal to or less than the guppy version should be selected. ---- .. class:: infomark 
References
 More information are available in the 
manual &lt;https://github.com/nanoporetech/medaka/tree/master/docs&gt;
 and 
github &lt;https://github.com/nanoporetech/medaka&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/medaka_variant/medaka_variant/2.1.1+galaxy0	".. class:: infomark 
What it does
 
medaka
 is a tool suite to create a consensus sequence from nanopore sequencing data. This task is performed using neural networks applied from a pileup of individual sequencing reads against a draft assembly. It outperforms graph-based methods operating on basecalled data, and can be competitive with state-of-the-art signal-based methods, whilst being much faster. The module 
variant
 decodes probabilities. ---- .. class:: infomark 
Input
 - reference sequence (FASTA) - (several) consensus files (H5/HDF) ---- .. class:: infomark 
Output
 - decoded probabilities (VCF) ---- .. class:: infomark 
References
 More information are available in the 
github &lt;https://github.com/nanoporetech/medaka&gt;
_."
aggregate_scores_in_intervals2	".. class:: warningmark This tool currently only has cached data for genome builds hg16, hg17 and hg18. However, you may use your own data point (wiggle) data, such as those available from UCSC. If you are trying to use your own data point file and it is not appearing as an option, make sure that the builds for your history items are the same. .. class:: warningmark This tool assumes that the input dataset is in interval format and contains at least a chrom column, a start column and an end column. These 3 columns can be dispersed throughout any number of other data columns. ----- .. class:: infomark 
TIP:
 Computing summary information may throw exceptions if the data type (e.g., string, integer) in every line of the columns is not appropriate for the computation (e.g., attempting numerical calculations on strings). If an exception is thrown when computing summary information for a line, that line is skipped as invalid for the computation. The number of invalid skipped lines is documented in the resulting history item as a ""Data issue"". ----- 
Syntax
 This tool appends columns of summary information for each interval matched against a selected dataset. For each interval, the average, minimum and maximum for the data falling within the interval is computed. - Several quantitative scores are provided for the ENCODE regions. - Various Scores - Regulatory Potential - Neutral rate (Ancestral Repeats) - GC fraction - Conservation Scores - PhastCons - binCons - GERP ----- 
Example
 If your original data has the following format: +------+-----+-----+---+------+ |other1|chrom|start|end|other2| +------+-----+-----+---+------+ and you choose to aggregate phastCons scores, your output will look like this: +------+-----+-----+---+------+---+---+---+ |other1|chrom|start|end|other2|avg|min|max| +------+-----+-----+---+------+---+---+---+ where: * 
avg
 - average phastCons score for each region * 
min
 - minimum phastCons score for each region * 
max
 - maximum phastCons score for each region"
toolshed.g2.bx.psu.edu/repos/devteam/basecoverage/gops_basecoverage_1/1.0.0	".. class:: infomark 
TIP:
 If your dataset does not appear in the pulldown menu, it means that it is not in interval format. Use ""edit attributes"" to set chromosome, start, end, and strand columns. This operation counts the total bases covered by a set of intervals. Bases that are covered by more than one interval are 
not
 counted more than once towards the total. ----- 
Screencasts!
 See Galaxy Interval Operation Screencasts_ (right click to open this link in another window). .. _Screencasts: http://wiki.g2.bx.psu.edu/Learn/Interval%20Operations 
Example
 .. image:: gops_baseCoverage.gif"
toolshed.g2.bx.psu.edu/repos/devteam/cluster/gops_cluster_1/1.0.0	".. class:: infomark 
TIP:
 If your dataset does not appear in the pulldown menu, it means that it is not in interval format. Use ""edit attributes"" to set chromosome, start, end, and strand columns. ----- 
Screencasts!
 See Galaxy Interval Operation Screencasts_ (right click to open this link in another window). .. _Screencasts: http://wiki.g2.bx.psu.edu/Learn/Interval%20Operations ----- 
Syntax
 - 
Maximum distance
 is greatest distance in base pairs allowed between intervals that will be considered ""clustered"". 
Negative
 values for distance are allowed, and are useful for clustering intervals that overlap. - 
Minimum intervals per cluster
 allow a threshold to be set on the minimum number of intervals to be considered a cluster. Any area with less than this minimum will not be included in the output. - 
Merge clusters into single intervals
 outputs intervals that span the entire cluster. - 
Find cluster intervals; preserve comments and order
 filters out non-cluster intervals while maintaining the original ordering and comments in the file. - 
Find cluster intervals; output grouped by clusters
 filters out non-cluster intervals, but outputs the cluster intervals so that they are grouped together. Comments and original ordering in the file are lost. ----- 
Examples
 Find Clusters: .. image:: gops_clusterFind.gif Merge Clusters: .. image:: gops_clusterMerge.gif"
toolshed.g2.bx.psu.edu/repos/devteam/complement/gops_complement_1/1.0.0	".. class:: infomark 
TIP:
 If your dataset does not appear in the pulldown menu, it means that it is not in interval format. Use ""edit attributes"" to set chromosome, start, end, and strand columns. This operation complements the regions of a set of intervals. Regions are returned that represent the empty space in the input interval. ----- 
Screencasts!
 See Galaxy Interval Operation Screencasts_ (right click to open this link in another window). .. _Screencasts: http://wiki.g2.bx.psu.edu/Learn/Interval%20Operations ----- 
Syntax
 - 
Genome-wide complement
 will complement all chromosomes of the genome. Leaving this option unchecked will only complement chromosomes present in the dataset. ----- 
Example
 .. image:: gops_complement.gif"
toolshed.g2.bx.psu.edu/repos/devteam/concat/gops_concat_1/1.0.1	".. class:: infomark 
TIP:
 If your dataset does not appear in the pulldown menu -> it is not in interval format. Use ""edit attributes"" to set chromosome, start, end, and strand columns. ----- 
Screencasts!
 See Galaxy Interval Operation Screencasts_ (right click to open this link in another window). .. _Screencasts: https://galaxyproject.org/learn/interval-operations/ ----- 
Syntax
 - 
Both datasets are exactly the same filetype
 will preserve all extra fields in both files. Leaving this unchecked will force the second dataset to use the same column assignments for chrom, start, end and strand, but will fill extra fields with a period(.). In both cases, the output fields are truncated or padded with fields of periods to maintain a truly tabular output. ----- 
Example
 .. image:: gops_concatenate.gif"
toolshed.g2.bx.psu.edu/repos/devteam/coverage/gops_coverage_1/1.0.0	".. class:: infomark 
TIP:
 If your dataset does not appear in the pulldown menu -> it is not in interval format. Use ""edit attributes"" to set chromosome, start, end, and strand columns. Find the coverage of intervals in the first dataset on intervals in the second dataset. The coverage is added as two columns, the first being bases covered, and the second being the fraction of bases covered by that interval. ----- 
Screencasts!
 See Galaxy Interval Operation Screencasts_ (right click to open this link in another window). .. _Screencasts: http://wiki.g2.bx.psu.edu/Learn/Interval%20Operations ----- 
Example
 if 
First dataset
 are genes :: chr11 5203271 5204877 NM_000518 0 - chr11 5210634 5212434 NM_000519 0 - chr11 5226077 5227663 NM_000559 0 - chr11 5226079 5232587 BC020719 0 - chr11 5230996 5232587 NM_000184 0 - and 
Second dataset
 are repeats:: chr11 5203895 5203991 L1MA6 500 + chr11 5204163 5204239 A-rich 219 + chr11 5211034 5211167 (CATATA)n 245 + chr11 5211642 5211673 AT_rich 24 + chr11 5226551 5226606 (CA)n 303 + chr11 5228782 5228825 (TTTTTG)n 208 + chr11 5229045 5229121 L1PA11 440 + chr11 5229133 5229319 MER41A 1106 + chr11 5229374 5229485 L2 244 - chr11 5229751 5230083 MLT1A 913 - chr11 5231469 5231526 (CA)n 330 + the Result is the coverage density of repeats in the genes:: chr11 5203271 5204877 NM_000518 0 - 172 0.107098 chr11 5210634 5212434 NM_000519 0 - 164 0.091111 chr11 5226077 5227663 NM_000559 0 - 55 0.034678 chr11 5226079 5232587 BC020719 0 - 860 0.132145 chr11 5230996 5232587 NM_000184 0 - 57 0.035827 For example, the following line of output:: chr11 5203271 5204877 NM_000518 0 - 172 0.107098 implies that 172 nucleotides accounting for 10.7% of the this interval (chr11:5203271-5204877) overlap with repetitive elements."
toolshed.g2.bx.psu.edu/repos/devteam/flanking_features/flanking_features_1/4.0.1	".. class:: infomark 
What it does
 For every interval in the 
interval
 dataset, this tool fetches the 
closest non-overlapping
 upstream and / or downstream features from the 
features
 dataset. ----- .. class:: warningmark 
Note:
 Every line should contain at least 3 columns: chromosome number, start and stop coordinates. If any of these columns is missing or if start and stop coordinates are not numerical, the lines will be treated as invalid and skipped. The number of skipped lines is documented in the resulting history item as a ""data issue"". If the strand column is missing from your input interval dataset, the intervals will be considered to be on positive strand. You can add a strand column to your input dataset by using the 
Text Manipulation->Add column
 tool. For GFF files, features are added as a GTF-style attribute at the end of the line. ----- 
Example
 If the 
intervals
 are:: chr1 10 100 Query1.1 chr1 500 1000 Query1.2 chr1 1100 1250 Query1.3 and the 
features
 are:: chr1 120 180 Query2.1 chr1 140 200 Query2.2 chr1 580 1050 Query2.3 chr1 2000 2204 Query2.4 chr1 2500 3000 Query2.5 Running this tool for 
Both Upstream and Downstream
 will return:: chr1 10 100 Query1.1 chr1 120 180 Query2.1 chr1 500 1000 Query1.2 chr1 140 200 Query2.2 chr1 500 1000 Query1.2 chr1 2000 2204 Query2.4 chr1 1100 1250 Query1.3 chr1 580 1050 Query2.3 chr1 1100 1250 Query1.3 chr1 2000 2204 Query2.4"
gene2exon1	".. class:: warningmark This tool works only on a BED file that contains at least 12 fields (see 
Example
 and 
About formats
 below). The output will be empty if applied to a BED file with 3 or 6 fields. ------ 
What it does
 BED format can be used to represent a single gene in just one line, which contains the information about exons, coding sequence location (CDS), and positions of untranslated regions (UTRs). This tool 
unpacks
 this information by converting a single line describing a gene into a collection of lines representing individual exons, introns, UTRs, etc. ------- 
Example
 Extracting 
Coding Exons + UTR Exons
 from the following two BED lines:: chr7 127475281 127491632 NM_000230 0 + 127486022 127488767 0 3 29,172,3225, 0,10713,13126 chr7 127486011 127488900 D49487 0 + 127486022 127488767 0 2 155,490, 0,2399 will return:: chr7 127475281 127475310 NM_000230 0 + chr7 127485994 127486166 NM_000230 0 + chr7 127488407 127491632 NM_000230 0 + chr7 127486011 127486166 D49487 0 + chr7 127488410 127488900 D49487 0 + ------ .. class:: infomark 
About formats
 
BED format
 Browser Extensible Data format was designed at UCSC for displaying data tracks in the Genome Browser. It has three required fields and additional optional ones. In the specific case of this tool the following fields must be present:: 1. chrom - The name of the chromosome (e.g. chr1, chrY_random). 2. chromStart - The starting position in the chromosome. (The first base in a chromosome is numbered 0.) 3. chromEnd - The ending position in the chromosome, plus 1 (i.e., a half-open interval). 4. name - The name of the BED line. 5. score - A score between 0 and 1000. 6. strand - Defines the strand - either '+' or '-'. 7. thickStart - The starting position where the feature is drawn thickly at the Genome Browser. 8. thickEnd - The ending position where the feature is drawn thickly at the Genome Browser. 9. reserved - This should always be set to zero. 10. blockCount - The number of blocks (exons) in the BED line. 11. blockSizes - A comma-separated list of the block sizes. The number of items in this list should correspond to blockCount. 12. blockStarts - A comma-separated list of block starts. All of the blockStart positions should be calculated relative to chromStart. The number of items in this list should correspond to blockCount."
toolshed.g2.bx.psu.edu/repos/devteam/get_flanks/get_flanks1/1.0.0	"This tool finds the upstream and/or downstream flanking region(s) of all the selected regions in the input file. 
Note:
 Every line should contain at least 3 columns: Chromosome number, Start and Stop co-ordinates. If any of these columns is missing or if start and stop co-ordinates are not numerical, the tool may encounter exceptions and such lines are skipped as invalid. The number of invalid skipped lines is documented in the resulting history item as a ""Data issue"". ----- 
Example 1
 - For the following dataset:: chr22 1000 7000 NM_174568 0 + - running get flanks with Region: Around start, Offset: -200, Flank-length: 300 and Location: Upstream will return 
(Red: Dataset positive strand; Blue: Flanks output)
:: chr22 500 800 NM_174568 0 + .. image:: flanks_ex1.gif 
Example 2
 - For the following dataset:: chr22 1000 7000 NM_028946 0 - - running get flanks with Region: Whole, Offset: 200, Flank-length: 300 and Location: Downstream will return 
(Orange: Dataset negative strand; Magenta: Flanks output)
:: chr22 500 800 NM_028946 0 - .. image:: flanks_ex2.gif"
toolshed.g2.bx.psu.edu/repos/devteam/intersect/gops_intersect_1/1.0.0	".. class:: infomark 
TIP:
 If your dataset does not appear in the pulldown menu, it means that it is not in interval format. Use ""edit attributes"" to set chromosome, start, end, and strand columns. ----- 
Screencasts!
 See Galaxy Interval Operation Screencasts_ (right click to open this link in another window). .. _Screencasts: https://galaxyproject.org/learn/interval-operations/ ----- 
Syntax
 - 
Where overlap is at least
 sets the minimum length (in base pairs) of overlap between elements of the two datasets - 
Overlapping Intervals
 returns entire intervals from the first dataset that overlap the second dataset. The returned intervals are completely unchanged, and this option only filters out intervals that do not overlap with the second dataset. - 
Overlapping pieces of Intervals
 returns intervals that indicate the exact base pair overlap between the first dataset and the second dataset. The intervals returned are from the first dataset, and all fields besides start and end are guaranteed to remain unchanged. ----- 
Examples
 Overlapping Intervals: .. image:: gops_intersectOverlappingIntervals.gif Overlapping Pieces of Intervals: .. image:: gops_intersectOverlappingPieces.gif"
toolshed.g2.bx.psu.edu/repos/devteam/join/gops_join_1/1.0.0	".. class:: infomark 
TIP:
 If your dataset does not appear in the pulldown menu, it means that it is not in interval format. Use ""edit attributes"" to set chromosome, start, end, and strand columns. ----- 
Screencasts!
 See Galaxy Interval Operation Screencasts_ (right click to open this link in another window). .. _Screencasts: http://wiki.g2.bx.psu.edu/Learn/Interval%20Operations ----- 
Syntax
 - 
Where overlap
 specifies the minimum overlap between intervals that allows them to be joined. - 
Return only records that are joined
 returns only the records of the first dataset that join to a record in the second dataset. This is analogous to an INNER JOIN. - 
Return all records of first dataset (fill null with ""."")
 returns all intervals of the first dataset, and any intervals that do not join an interval from the second dataset are filled in with a period(.). This is analogous to a LEFT JOIN. - 
Return all records of second dataset (fill null with ""."")
 returns all intervals of the second dataset, and any intervals that do not join an interval from the first dataset are filled in with a period(.). 
Note that this may produce an invalid interval file, since a period(.) is not a valid chrom, start, end or strand.
 - 
Return all records of both datasets (fill nulls with ""."")
 returns all records from both datasets, and fills on either the right or left with periods. 
Note that this may produce an invalid interval file, since a period(.) is not a valid chrom, start, end or strand.
 ----- 
Examples
 .. image:: gops_joinRecordsList.gif Only records that are joined (inner join): .. image:: gops_joinInner.gif All records of first dataset: .. image:: gops_joinLeftOuter.gif All records of second dataset: .. image:: gops_joinRightOuter.gif All records of both datasets: .. image:: gops_joinFullOuter.gif"
toolshed.g2.bx.psu.edu/repos/devteam/merge/gops_merge_1/1.0.0	".. class:: infomark 
TIP:
 If your dataset does not appear in the pulldown menu, it means that it is not in interval format. Use ""edit attributes"" to set chromosome, start, end, and strand columns. ----- 
Screencasts!
 See Galaxy Interval Operation Screencasts_ (right click to open this link in another window). .. _Screencasts: https://galaxyproject.org/learn/interval-operations/ ----- This operation merges all overlapping intervals into single intervals. 
Example
 .. image:: gops_merge.gif"
toolshed.g2.bx.psu.edu/repos/devteam/subtract/gops_subtract_1/1.0.0	".. class:: infomark 
TIP:
 If your dataset does not appear in the pulldown menu, it means that it is not in interval format. Use ""edit attributes"" to set chromosome, start, end, and strand columns. ----- 
Screencasts!
 See Galaxy Interval Operation Screencasts_ (right click to open this link in another window). .. _Screencasts: https://galaxyproject.org/learn/interval-operations/ ----- 
Syntax
 - 
Where overlap is at least
 sets the minimum length (in base pairs) of overlap between elements of the two datasets. - 
Intervals with no overlap
 returns entire intervals from the first dataset that do not overlap the second dataset. The returned intervals are completely unchanged, and this option only filters out intervals that overlap with the second dataset. - 
Non-overlapping pieces of intervals
 returns intervals from the first dataset that have the intervals from the second dataset removed. Any overlapping base pairs are removed from the range of the interval. All fields besides start and end are guaranteed to remain unchanged. ----- 
Example
 Intervals with no overlap: .. image:: gops_subtractOverlappingIntervals.gif Non-overlapping pieces of intervals: .. image:: gops_subtractOverlappingPieces.gif"
toolshed.g2.bx.psu.edu/repos/galaxyp/translate_bed/translate_bed/0.1.0	Each region is specifed as: chr or chr:pos or chr:from-to
wiggle2simple1	"Syntax
 This tool converts wiggle data into interval type. - 
Wiggle format
: The .wig format is line-oriented. Wiggle data is preceded by a UCSC track definition line. Following the track definition line is the track data, which can be entered in three different formats described below. - 
BED format
 with no declaration line and four columns of data:: chromA chromStartA chromEndA dataValueA chromB chromStartB chromEndB dataValueB - 
variableStep
 two column data; started by a declaration line and followed with chromosome positions and data values:: variableStep chrom=chrN [span=windowSize] chromStartA dataValueA chromStartB dataValueB - 
fixedStep
 single column data; started by a declaration line and followed with data values:: fixedStep chrom=chrN start=position step=stepInterval [span=windowSize] dataValue1 dataValue2 ----- 
Example
 - input wiggle format file:: #track type=wiggle_0 name=""Bed Format"" description=""BED format"" chr19 59302000 59302300 -1.0 chr19 59302300 59302600 -0.75 chr19 59302600 59302900 -0.50 chr19 59302900 59303200 -0.25 chr19 59303200 59303500 0.0 #track type=wiggle_0 name=""variableStep"" description=""variableStep format"" variableStep chrom=chr19 span=150 59304701 10.0 59304901 12.5 59305401 15.0 59305601 17.5 #track type=wiggle_0 name=""fixedStep"" description=""fixed step"" visibility=full fixedStep chrom=chr19 start=59307401 step=300 span=200 1000 900 800 700 600 - convert the above file to interval file:: chr19 59302000 59302300 + -1.0 chr19 59302300 59302600 + -0.75 chr19 59302600 59302900 + -0.5 chr19 59302900 59303200 + -0.25 chr19 59303200 59303500 + 0.0 chr19 59304701 59304851 + 10.0 chr19 59304901 59305051 + 12.5 chr19 59305401 59305551 + 15.0 chr19 59305601 59305751 + 17.5 chr19 59307701 59307901 + 1000.0 chr19 59308001 59308201 + 900.0 chr19 59308301 59308501 + 800.0 chr19 59308601 59308801 + 700.0 chr19 59308901 59309101 + 600.0"
toolshed.g2.bx.psu.edu/repos/bgruening/pharmcat/pharmcat/1.7.0+galaxy0	PharmCAT is a tool to extract all CPIC guideline gene variants from a genetic dataset (represented as a VCF file), interpret the variant alleles, and generate a report.
hgv_beam	".. class:: infomark This tool can take a long time to run, depending on the number of SNPs, the sample size, and the number of MCMC steps specified. If you have hundreds of thousands of SNPs, it may take over a day. The main tasks that slow down this tool are searching for interactions and dynamically partitioning the SNPs into blocks. Optimization is certainly possible, but hasn't been done yet. 
If your only interest is to detect SNPs with primary effects (i.e., single-SNP associations), please use the GPASS tool instead.
 ----- 
Dataset formats
 The input dataset must be in lped_ format. The output datasets are both tabular_. (
Dataset missing?
_) .. _lped: ${static_path}/formatHelp.html#lped .. _tabular: ${static_path}/formatHelp.html#tabular .. _Dataset missing?: ${static_path}/formatHelp.html ----- 
What it does
 BEAM (Bayesian Epistasis Association Mapping) uses a Markov Chain Monte Carlo (MCMC) method to infer SNP block structures and detect both single-marker and interaction effects from case-control SNP data. This tool also partitions SNPs into blocks based on linkage disequilibrium (LD). The method utilized is Bayesian, so the outputs are posterior probabilities of association, along with block partitions. An advantage of this method is that it provides uncertainty measures for the associations and block partitions, and it scales well from small to large sample sizes. It is powerful in detecting gene-gene interactions, although slow for large datasets. ----- 
Example
 - input map file:: 1 rs0 0 738547 1 rs1 0 5597094 1 rs2 0 9424115 etc. - input ped file:: 1 1 0 0 1 1 G G A A A A A A A A A G A A G G G G A A G G G G G G A A A A A G A A G G A G A G A A G G A A G G A A G G A G A A G G A A G G A A A G A G G G A G G G G G A A A G A A G G G G G G G G A G A A A A A A A A 1 1 0 0 1 1 G G A G G G A A A A A G A A G G G G G G A A G G A G A G G G G G A G G G A G A A G G A G G G A A G G G G A G A G G G A G A A A A G G G G A G A G G G A G A A A A A G G G A G G G A G G G G G A A G G A G etc. - first output file, significance.txt:: ID chr position results rs0 chr1 738547 10 20 score= 45.101397 , df= 8 , p= 0.000431 , N=1225 - second output file, posterior.txt:: id: chr position marginal + interaction = total posterior 0: 1 738547 0.0000 + 0.0000 = 0.0000 1: 1 5597094 0.0000 + 0.0000 = 0.0000 2: 1 9424115 0.0000 + 0.0000 = 0.0000 3: 1 13879818 0.0000 + 0.0000 = 0.0000 4: 1 13934751 0.0000 + 0.0000 = 0.0000 5: 1 16803491 0.0000 + 0.0000 = 0.0000 6: 1 17236854 0.0000 + 0.0000 = 0.0000 7: 1 18445387 0.0000 + 0.0000 = 0.0000 8: 1 21222571 0.0000 + 0.0000 = 0.0000 etc. id: chr position block_boundary | allele counts in cases and controls 0: 1 738547 1.000 | 156 93 251 | 169 83 248 1: 1 5597094 1.000 | 323 19 158 | 328 16 156 2: 1 9424115 1.000 | 366 6 128 | 369 11 120 3: 1 13879818 1.000 | 252 31 217 | 278 32 190 4: 1 13934751 1.000 | 246 64 190 | 224 58 218 5: 1 16803491 1.000 | 91 160 249 | 91 174 235 6: 1 17236854 1.000 | 252 43 205 | 249 44 207 7: 1 18445387 1.000 | 205 66 229 | 217 56 227 8: 1 21222571 1.000 | 353 9 138 | 352 8 140 etc. The ""id"" field is an internally used index. ----- 
References
 Zhang Y, Liu JS. (2007) Bayesian inference of epistatic interactions in case-control studies. Nat Genet. 39(9):1167-73. Epub 2007 Aug 26. Zhang Y, Zhang J, Liu JS. (2010) Block-based bayesian epistasis association mapping with application to WTCCC type 1 diabetes data. Submitted."
hgv_david	".. class:: infomark The list is limited to 400 IDs. ----- 
Dataset formats
 The input dataset is in tabular_ format. The output dataset is html_ with a link to the DAVID website as described below. (
Dataset missing?
_) .. _tabular: ${static_path}/formatHelp.html#tab .. _html: ${static_path}/formatHelp.html#html .. _Dataset missing?: ${static_path}/formatHelp.html ----- 
What it does
 This tool creates a link to the Database for Annotation, Visualization, and Integrated Discovery (DAVID) website at NIH, sending a list of IDs from the selected column of a tabular Galaxy dataset. To follow the created link, click on the eye icon once the Galaxy tool has finished running. DAVID provides a comprehensive set of functional annotation tools to help investigators discover biological meaning behind large lists of genes. ----- 
References
 Huang DW, Sherman BT, Lempicki RA. (2009) Systematic and integrative analysis of large gene lists using DAVID bioinformatics resources. Nat Protoc. 4(1):44-57. Dennis G, Sherman BT, Hosack DA, Yang J, Gao W, Lane HC, Lempicki RA. (2003) DAVID: database for annotation, visualization, and integrated discovery. Genome Biol. 4(5):P3. Epub 2003 Apr 3."
toolshed.g2.bx.psu.edu/repos/devteam/hgv_fundo/hgv_funDo/1.0.0	"Dataset formats
 There is no input dataset. The output is in interval_ format. .. _interval: ${static_path}/formatHelp.html#interval ----- 
What it does
 This tool searches the disease-term field of the DOLite mappings used by the FunDO project and returns a set of genes that are associated with terms matching the specified pattern. (This is the reverse of what FunDO's own server does.) The search is case insensitive, and selects terms that contain any of the given words, either exactly or within a longer word (e.g. ""nemia"" selects not only ""anemia"", but also ""hyperglycinemia"", ""tyrosinemias"", and many other things). Multiple words should be separated by spaces, not commas. As a special case, entering the word ""disease"" returns all genes associated with any disease, even if that word does not actually appear in the term field. Website: http://django.nubic.northwestern.edu/fundo/ ----- 
Example
 Typing:: carcinoma results in:: 1. 2. 3. 4. 5. 6. 7. chr11 89507465 89565427 + NAALAD2 10003 Adenocarcinoma chr15 50189113 50192264 - BCL2L10 10017 Carcinoma chr7 150535855 150555250 - ABCF2 10061 Clear cell carcinoma chr7 150540508 150555250 - ABCF2 10061 Clear cell carcinoma chr10 134925911 134940397 - ADAM8 101 Adenocarcinoma chr10 134925911 134940397 - ADAM8 101 Adenocarcinoma etc. where the column contents are as follows:: 1. chromosome name 2. start position of the gene 3. end position of the gene 4. strand 4. gene name 6. Entrez Gene ID 7. disease term ----- 
References
 Du P, Feng G, Flatow J, Song J, Holko M, Kibbe WA, Lin SM. (2009) From disease ontology to disease-ontology lite: statistical methods to adapt a general-purpose ontology for the test of gene-ontology associations. Bioinformatics. 25(12):i63-8. Osborne JD, Flatow J, Holko M, Lin SM, Kibbe WA, Zhu LJ, Danila MI, Feng G, Chisholm RL. (2009) Annotating the human genome with Disease Ontology. BMC Genomics. 10 Suppl 1:S6."
hgv_gpass	"Dataset formats
 The input dataset must be in lped_ format, and the output is tabular_. (
Dataset missing?
_) .. _lped: ${static_path}/formatHelp.html#lped .. _tabular: ${static_path}/formatHelp.html#tab .. _Dataset missing?: ${static_path}/formatHelp.html ----- 
What it does
 GPASS (Genome-wide Poisson Approximation for Statistical Significance) detects significant single-SNP associations in case-control studies at a user-specified FDR. Unlike previous methods, this tool can accurately approximate the genome-wide significance and FDR of SNP associations, while adjusting for millions of multiple comparisons, within seconds or minutes. The program has two main functionalities: 1. Detect significant single-SNP associations at a user-specified false discovery rate (FDR). 
Note
: a ""typical"" definition of FDR could be FDR = E(# of false positive SNPs / # of significant SNPs) This definition however is very inappropriate for association mapping, since SNPs are highly correlated. Our FDR is defined differently to account for SNP correlations, and thus will obtain a proper FDR in terms of ""proportion of false positive loci"". 2. Approximate the significance of a list of candidate SNPs, adjusting for multiple comparisons. If you have isolated a few SNPs of interest and want to know their significance in a GWAS, you can supply the GWAS data and let the program specifically test those SNPs. 
Also note
: the number of SNPs in a study cannot be both too small and at the same time too clustered in a local region. A few hundreds of SNPs, or tens of SNPs spread in different regions, will be fine. The sample size cannot be too small either; around 100 or more individuals (case + control combined) will be fine. Otherwise use permutation. ----- 
Example
 - input map file:: 1 rs0 0 738547 1 rs1 0 5597094 1 rs2 0 9424115 etc. - input ped file:: 1 1 0 0 1 1 G G A A A A A A A A A G A A G G G G A A G G G G G G A A A A A G A A G G A G A G A A G G A A G G A A G G A G A A G G A A G G A A A G A G G G A G G G G G A A A G A A G G G G G G G G A G A A A A A A A A 1 1 0 0 1 1 G G A G G G A A A A A G A A G G G G G G A A G G A G A G G G G G A G G G A G A A G G A G G G A A G G G G A G A G G G A G A A A A G G G G A G A G G G A G A A A A A G G G A G G G A G G G G G A A G G A G etc. - output dataset, showing significant SNPs and their p-values and FDR:: #ID chr position Statistics adj-Pvalue FDR rs35 chr1 136606952 4.890849 0.991562 0.682138 rs36 chr1 137748344 4.931934 0.991562 0.795827 rs44 chr2 14423047 7.712832 0.665086 0.218776 etc. ----- 
Reference
 Zhang Y, Liu JS. (2010) Fast and accurate significance approximation for genome-wide association studies. Submitted."
toolshed.g2.bx.psu.edu/repos/devteam/hgv_hilbertvis/hgv_hilbertvis/1.0.0	"Dataset formats
 The input format is interval_, and the output is an image in PDF format. (
Dataset missing?
_) .. _interval: ${static_path}/formatHelp.html#interval .. _Dataset missing?: ${static_path}/formatHelp.html ----- 
What it does
 HilbertVis uses the Hilbert space-filling curve to visualize the structure of position-dependent data. It maps the traditional one-dimensional line visualization onto a two-dimensional square. For example, here is a diagram showing the path of a level-2 Hilbert curve. .. image:: hilbertvisDiagram.png The shade of each pixel represents the value for the corresponding bin of consecutive genomic positions, calculated according to the specified summarization mode. The pixels are arranged so that bins that are close to each other on the data vector are represented by pixels that are close to each other in the plot. In particular, adjacent bins are mapped to adjacent pixels. Hence, dark spots in a figure represent a peak; the area of the spot in the two-dimensional plot is proportional to the width of the peak in the one-dimensional data, and the darkness of the spot corresponds to the height of the peak. The input file is in interval format, and typically contains a column with scores or other numbers, such as conservation scores, SNP density, the coverage of aligned reads from ChIP-Seq data, etc. Website: http://www.ebi.ac.uk/huber-srv/hilbert/ ----- 
Examples
 Here are some examples from the HilbertVis homepage, using ChIP-Seq data. .. image:: hilbertvis1.png ----- .. image:: hilbertvis2.png ----- 
Reference
 Anders S. (2009) Visualization of genomic data with the Hilbert curve. Bioinformatics. 25(10):1231-5. Epub 2009 Mar 17."
hgv_ldtools	"Dataset formats
 The input and output datasets are tabular_. (
Dataset missing?
_) .. _tabular: ${static_path}/formatHelp.html#tab .. _Dataset missing?: ${static_path}/formatHelp.html ----- 
What it does
 This tool can be used to analyze the patterns of linkage disequilibrium (LD) between polymorphic sites in a locus. SNPs are grouped based on the threshold level of LD as measured by r\ :sup:
2
 (regardless of genomic position), and a representative ""tag SNP"" is reported for each group. The other SNPs in the group are in LD with the tag SNP, but not necessarily with each other. The underlying algorithm is the same as the one used in ldSelect (Carlson et al. 2004). However, this tool is implemented to be much faster and more efficient than ldSelect. The input is a tabular file with genotype information for each individual at each SNP site, in exactly four columns: site ID, sample ID, and the two allele nucleotides. ----- 
Example
 - input file:: rs2334386 NA20364 G T rs2334386 NA20363 G G rs2334386 NA20360 G G rs2334386 NA20359 G G rs2334386 NA20358 G G rs2334386 NA20356 G G rs2334386 NA20357 G G rs2334386 NA20350 G G rs2334386 NA20349 G G rs2334386 NA20348 G G rs2334386 NA20347 G G rs2334386 NA20346 G G rs2334386 NA20345 G G rs2334386 NA20344 G G rs2334386 NA20342 G G etc. - output file:: rs2238748 rs2793064,rs6518516,rs6518517,rs2283641,rs5993533,rs715590,rs2072123,rs2105421,rs2800954,rs1557847,rs807750,rs807753,rs5993488,rs8138035,rs2800980,rs2525079,rs5992353,rs712966,rs2525036,rs807743,rs1034727,rs807744,rs2074003 rs2871023 rs1210715,rs1210711,rs5748189,rs1210709,rs3788298,rs7284649,rs9306217,rs9604954,rs1210703,rs5748179,rs5746727,rs5748190,rs5993603,rs2238766,rs885981,rs2238763,rs5748165,rs9605996,rs9606001,rs5992398 rs7292006 rs13447232,rs5993665,rs2073733,rs1057457,rs756658,rs5992395,rs2073760,rs739369,rs9606017,rs739370,rs4493360,rs2073736 rs2518840 rs1061325,rs2283646,rs362148,rs1340958,rs361956,rs361991,rs2073754,rs2040771,rs2073740,rs2282684 rs2073775 rs10160,rs2800981,rs807751,rs5993492,rs2189490,rs5747997,rs2238743 rs5747263 rs12159924,rs2300688,rs4239846,rs3747025,rs3747024,rs3747023,rs2300691 rs433576 rs9605439,rs1109052,rs400509,rs401099,rs396012,rs410456,rs385105 rs2106145 rs5748131,rs2013516,rs1210684,rs1210685,rs2238767,rs2277837 rs2587082 rs2257083,rs2109659,rs2587081,rs5747306,rs2535704,rs2535694 rs807667 rs2800974,rs756651,rs762523,rs2800973,rs1018764 rs2518866 rs1206542,rs807467,rs807464,rs807462,rs712950 rs1110661 rs1110660,rs7286607,rs1110659,rs5992917,rs1110662 rs759076 rs5748760,rs5748755,rs5748752,rs4819925,rs933461 rs5746487 rs5992895,rs2034113,rs2075455,rs1867353 rs5748212 rs5746736,rs4141527,rs5748147,rs5748202 etc. ----- 
Reference
 Carlson CS, Eberle MA, Rieder MJ, Yi Q, Kruglyak L, Nickerson DA. (2004) Selecting a maximally informative set of single-nucleotide polymorphisms for association analyses using linkage disequilibrium. Am J Hum Genet. 74(1):106-20. Epub 2003 Dec 15."
hgv_lps	"Dataset formats
 The input and output datasets are tabular_. The columns are described below. There is a second output dataset (a log) that is in text_ format. (
Dataset missing?
_) .. _tabular: ${static_path}/formatHelp.html#tab .. _text: ${static_path}/formatHelp.html#text .. _Dataset missing?: ${static_path}/formatHelp.html ----- 
What it does
 The LASSO-Patternsearch algorithm fits your dataset to an L1-regularized logistic regression model. A benefit of using L1-regularization is that it typically yields a weight vector with relatively few non-zero coefficients. For example, say you have a dataset containing M rows (subjects) and N columns (attributes) where one of these N attributes is binary, indicating whether or not the subject has some property of interest P. In simple terms, LPS calculates a weight for each of the other attributes in your dataset. This weight indicates how ""relevant"" that attribute is for predicting whether or not a given subject has property P. The L1-regularization causes most of these weights to be equal to zero, which means LPS will find a ""small"" subset of the remaining N-1 attributes in your dataset that can be used to predict P. In other words, LPS can be used for feature selection. The input dataset is tabular, and must contain a label column which indicates whether or not a given row has property P. In the current version of this tool, P must be encoded using +1 and -1. The Lambda_fac parameter ranges from 0 to 1, and controls how sparse the weight vector will be. At the low end, when Lambda_fac = 0, there will be no regularization. At the high end, when Lambda_fac = 1, there will be ""too much"" regularization, and all of the weights will equal zero. The LPS tool creates two output datasets. The first, called the results file, is a tabular dataset containing one column of weights for each value of the regularization parameter lambda that was tried. The weight columns are in order from left to right by decreasing values of lambda. The first N-1 rows in each column are the weights for the N-1 attributes in your input dataset. The final row is a constant, the intercept. Let 
x
 be a row from your input dataset and let 
b
 be a column from the results file. To compute the probability that row 
x
 has a label value of +1: Probability(row 
x
 has label value = +1) = 1 / [1 + exp{
x
 * 
b
[1..N-1] + 
b
[N]}] where 
x
 * 
b
[1..N-1] represents matrix multiplication. The second output dataset, called the log file, is a text file which contains additional data about the fitted L1-regularized logistic regression model. These data include the number of features, the computed value of lambda_max, the actual values of lambda used, the optimal values of the log-likelihood and regularized log-likelihood functions, the number of non-zeros, and the number of iterations. Website: http://pages.cs.wisc.edu/~swright/LPS/ ----- 
Example
 - input file:: +1 1 0 0 0 0 1 0 1 1 ... +1 1 1 1 0 0 1 0 1 1 ... +1 1 0 1 0 1 0 1 0 1 ... etc. - output results file:: 0 0 0 0 0.025541 etc. - output log file:: Data set has 100 vectors with 50 features. calculateLambdaMax: n=50, m=100, m+=50, m-=50 computed value of lambda_max: 5.0000e-01 lambda=2.96e-02 solution: optimal log-likelihood function value: 6.46e-01 optimal 
regularized
 log-likelihood function value: 6.79e-01 number of nonzeros at the optimum: 5 number of iterations required: 43 etc."
master2pgSnp	"Dataset formats
 The input dataset is in the MasterVar_ format provided by the Complete Genomics analysis process (Galaxy considers this to be tabular_, but it must have the columns specified for MasterVar). The output dataset is in pgSnp_ format. (
Dataset missing?
_) .. _Dataset missing?: ./static/formatHelp.html .. _pgSnp: ./static/formatHelp.html#pgSnp .. _MasterVar: ./static/formatHelp.html#mastervar .. _tabular: ./static/formatHelp.html#tab ----- 
What it does
 This converts a Complete Genomics MasterVar file to pgSnp format, so it can be viewed in browsers or used with the phenotype association and interval operations tools. Positions homozygous for the reference are skipped. ----- 
Examples
 - input MasterVar file:: 934 2 chr1 41980 41981 hom snp A G G 76 97 dbsnp.86:rs806721 425 1 1 1 2 -170 ERVL-E-int:ERVL:47.4 2 1.17 N 935 2 chr1 41981 42198 hom ref = = = -170 1.17 N 1102 2 chr1 53205 53206 het-ref snp G C G 93 127 dbsnp.100:rs2854676 477 7 30 0 37 -127 2 1.17 N etc. - output:: chr1 41980 41981 G 1 1 76 chr1 51672 51673 C 1 1 53 chr1 52237 52238 G 1 7 63 chr1 53205 53206 C/G 2 7,30 93,127 etc."
hgv_pass	"Dataset formats
 The input is in GFF_ format, and the output is tabular_. (
Dataset missing?
_) .. _GFF: ${static_path}/formatHelp.html#gff .. _tabular: ${static_path}/formatHelp.html#tab .. _Dataset missing?: ${static_path}/formatHelp.html ----- 
What it does
 PASS (Poisson Approximation for Statistical Significance) detects significant transcription factor binding sites in the genome from ChIP data. This is probably the only peak-calling method that accurately controls the false-positive rate and FDR in ChIP data, which is important given the huge discrepancy in results obtained from different peak-calling algorithms. At the same time, this method achieves a similar or better power than previous methods. ----- 
Hints
 - ChIP-Seq data: If the data is from ChIP-Seq, you need to convert the ChIP-Seq values into z-scores before using this program. It is also recommended that you group read counts within a neighborhood together, e.g. in tiled windows of 30bp. In this way, the ChIP-Seq data will resemble ChIP-chip data in format. - Choosing window size options: The window size is related to the probe tiling density. For example, if the probes are tiled at every 100bp, then setting the smallest window = 2 and largest window = 6 is appropriate, because the DNA fragment size is around 300-500bp. ----- 
Example
 - input file:: chr7 Nimblegen ID 40307603 40307652 1.668944 . . . chr7 Nimblegen ID 40307703 40307752 0.8041307 . . . chr7 Nimblegen ID 40307808 40307865 -1.089931 . . . chr7 Nimblegen ID 40307920 40307969 1.055044 . . . chr7 Nimblegen ID 40308005 40308068 2.447853 . . . chr7 Nimblegen ID 40308125 40308174 0.1638694 . . . chr7 Nimblegen ID 40308223 40308275 -0.04796628 . . . chr7 Nimblegen ID 40308318 40308367 0.9335709 . . . chr7 Nimblegen ID 40308526 40308584 0.5143972 . . . chr7 Nimblegen ID 40308611 40308660 -1.089931 . . . etc. In GFF, a value of dot '.' is used to mean ""not applicable"". - output file:: ID Chr Start End WinSz PeakValue # of FPs FDR 1 chr7 40310931 40311266 4 1.663446 0.248817 0.248817 ----- 
References
 Zhang Y. (2008) Poisson approximation for significance in genome-wide ChIP-chip tiling arrays. Bioinformatics. 24(24):2825-31. Epub 2008 Oct 25. Chen KB, Zhang Y. (2010) A varying threshold method for ChIP peak calling using multiple sources of information. Submitted."
toolshed.g2.bx.psu.edu/repos/devteam/divide_pg_snp/dividePgSnp/1.0.0	"Dataset formats
 The input dataset is of Galaxy datatype interval_ with the columns specified for pgSnp_. Any additional columns beyond the pgSnp defined columns will be appended to the output. The output dataset is in interval_ format. (
Dataset missing?
_) .. _interval: ./static/formatHelp.html#interval .. _Dataset missing?: ./static/formatHelp.html .. _pgSnp: ./static/formatHelp.html#pgSnp 
What it does
 This separates the alleles from a pgSnp dataset into separate columns, as well as the frequencies and scores that go with the alleles. It will skip any positions with more than 2 alleles. If only a single allele is given then ""N"" will be used for the second, with a frequency and score of zero. Or, if a column with reference alleles is provided, the value in that column will be used in place of the ""N"" for single alleles. ----- 
Examples
 - input pgSnp file:: chr1 256 257 A/C 2 3,4 10,20 chr1 56100 56101 A 1 5 30 chr1 77052 77053 A/G 2 6,7 40,50 chr1 110904 110905 A 1 8 60 etc. - output:: chr1 256 257 A 3 10 C 4 20 chr1 56100 56101 A 5 30 N 0 0 chr1 77052 77053 A 6 40 G 7 50 chr1 110904 110905 A 8 60 N 0 0 etc."
toolshed.g2.bx.psu.edu/repos/devteam/vcf2pgsnp/vcf2pgSnp/1.0.0	"Dataset formats
 The input dataset is VCF_ format. The output dataset is pgSnp_. (
Dataset missing?
_) .. _Dataset missing?: ./static/formatHelp.html .. _VCF: ./static/formatHelp.html#vcf .. _pgSnp: ./static/formatHelp.html#pgSnp ----- 
What it does
 This converts a VCF dataset to pgSnp with the frequency counts being chromosome counts. If there is more than one column of SNP data it will either accumulate all columns as a population or convert the column indicated to pgSnp. ----- 
Examples
 - input:: 1 13327 rs144762171 G C 100 PASS VT=SNP;SNPSOURCE=LOWCOV GT:DS:GL 0|0:0.000:-0.03,-1.11,-5.00 0|1:1.000:-1.97,-0.01,-2.51 0|0:0.050:-0.01,-1.69,-5.00 0|0:0.100:-0.48,-0.48,-0.48 1 13980 rs151276478 T C 100 PASS VT=SNP;SNPSOURCE=LOWCOV GT:DS:GL 0|0:0.100:-0.48,-0.48,-0.48 0|1:0.950:-0.48,-0.48,-0.48 0|0:0.050:-0.48,-0.48,-0.48 0|0:0.050:-0.48,-0.48,-0.48 1 30923 rs140337953 G T 100 PASS VT=SNP;SNPSOURCE=LOWCOV GT:DS:GL 1|1:1.950:-5.00,-0.61,-0.12 0|0:0.450:-0.10,-0.69,-2.81 0|0:0.450:-0.11,-0.64,-3.49 1|1:1.500:-0.48,-0.48,-0.48 etc. - output as a population:: chr1 13326 13327 G/C 2 7,1 0,0 chr1 13979 13980 T/C 2 7,1 0,0 chr1 30922 30923 G/T 2 4,4 0,0 etc. - output for each column separately:: chr1 13326 13327 G 1 2 0 G/C 2 1,1 0,0 G 1 2 0 G 1 2 0 chr1 13979 13980 T 1 2 0 T/C 2 1,1 0,0 T 1 2 0 T 1 2 0 chr1 30922 30923 T 1 2 0 G 1 2 0 G 1 2 0 T 1 2 0 etc."
hgv_linkToGProfile	"Dataset formats
 The input dataset is tabular_ with a column of identifiers. The output dataset is html_ with a link to g:Profiler. (
Dataset missing?
_) .. _tabular: ${static_path}/formatHelp.html#tab .. _html: ${static_path}/formatHelp.html#html .. _Dataset missing?: ${static_path}/formatHelp.html ----- 
What it does
 This tool creates a link to the g:GOSt tool (Gene Group Functional Profiling), which is part of the g:Profiler site at the University of Tartu in Estonia. g:GOSt retrieves the most significant Gene Ontology (GO) terms, KEGG and REACTOME pathways, and TRANSFAC motifs for a user-specified group of genes, proteins, or microarray probes. g:GOSt also allows analysis of ranked or ordered lists of genes, visual browsing of GO graph structure, interactive visualization of retrieved results, and many other features. Multiple testing corrections are applied to extract only statistically important results. The g:GOSt form is pre-filled with gene, protein, or microarray probe IDs from the selected column of a tabular Galaxy dataset. Or you can chose to use the genomic coordinates (must be lastest build used by Ensembl). The coordinates don't have to be genes they can be for SNPs, and g:GOst will map to the gene ID. To follow the created link, click on the eye icon when the Galaxy tool has finished running. Once at the g:Profiler site, scroll down to see the g:GOSt results. You can also adjust the options in the g:GOSt form to your liking, or use the row of links between the form and the results to run other g:Profiler tools using the same list of IDs. ----- 
Reference
 Reimand J, Kull M, Peterson H, Hansen J, Vilo J. (2007) g:Profiler -- a web-based toolset for functional profiling of gene lists from large-scale experiments. Nucleic Acids Res. 35(Web Server issue):W193-200. Epub 2007 May 3."
toolshed.g2.bx.psu.edu/repos/iuc/plink/plink/1.90b6.21+galaxy1	PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner. For detailed usage notes, visit http://www.cog-genomics.org/plink/2.0/
toolshed.g2.bx.psu.edu/repos/devteam/snpfreq/hgv_snpFreq/1.0.1	"Dataset formats
 The input is tabular_, with six columns of allele counts. The output is also tabular, and includes all of the input data plus the additional columns described below. (
Dataset missing?
_) .. _tabular: ${static_path}/formatHelp.html#tab .. _Dataset missing?: ${static_path}/formatHelp.html ----- 
What it does
 This tool performs a basic analysis of bi-allelic SNPs in case-control data, using the R statistical environment and Fisher's exact test to identify SNPs with a significant difference in the allele frequencies between the two groups. R's ""qvalue"" package is used to correct for multiple testing. The input file includes counts for each allele combination (AA aa Aa) for each group at each SNP position. The assignment of codes (1 2 3) to these genotypes is arbitrary, as long as it is consistent for both groups. Any other input columns are ignored in the computation, but are copied to the output. The output appends eight additional columns, namely the minimum expected counts of the three genotypes for each group, the p-value, and the q-value. ----- 
Example
 - input file:: chr1 210 211 38 4 15 56 0 1 x chr1 228 229 55 0 2 56 0 1 x chr1 230 231 46 0 11 55 0 2 x chr1 234 235 43 0 14 55 0 2 x chr1 236 237 55 0 2 13 10 34 x chr1 437 438 55 0 2 46 0 11 x chr1 439 440 56 0 1 55 0 2 x chr1 449 450 56 0 1 13 20 24 x chr1 518 519 56 0 1 38 4 15 x Here the group 1 genotype counts are in columns 4 - 6, while those for group 2 are in columns 7 - 9. Note that the ""x"" column has no meaning. It was added to this example to show that extra columns can be included, and to make it easier to see where the new columns are appended in the output. - output file:: chr1 210 211 38 4 15 56 0 1 x 47 2 8 47 2 8 1.50219088598917e-05 6.32501425679652e-06 chr1 228 229 55 0 2 56 0 1 x 55.5 0 1.5 55.5 0 1.5 1 0.210526315789474 chr1 230 231 46 0 11 55 0 2 x 50.5 0 6.5 50.5 0 6.5 0.0155644201009862 0.00409590002657532 chr1 234 235 43 0 14 55 0 2 x 49 0 8 49 0 8 0.00210854461554067 0.000739840215979182 chr1 236 237 55 0 2 13 10 34 x 34 5 18 34 5 18 6.14613878554783e-17 4.31307984950725e-17 chr1 437 438 55 0 2 46 0 11 x 50.5 0 6.5 50.5 0 6.5 0.0155644201009862 0.00409590002657532 chr1 439 440 56 0 1 55 0 2 x 55.5 0 1.5 55.5 0 1.5 1 0.210526315789474 chr1 449 450 56 0 1 13 20 24 x 34.5 10 12.5 34.5 10 12.5 2.25757007974134e-18 2.37638955762246e-18 chr1 518 519 56 0 1 38 4 15 x 47 2 8 47 2 8 1.50219088598917e-05 6.32501425679652e-06"
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_AddCommentsToBam/3.1.1.0	".. class:: infomark 
Purpose
 Adds one or more comments (@CO) to the header of a specified BAM dataset. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: COMMENT=String C=String Comments to add to the BAM file This option may be specified 0 or more times. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_AddOrReplaceReadGroups/3.1.1.0	".. class:: infomark 
Purpose
 Add or Replace Read Groups in an input BAM or SAM file. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------- 
Read Groups are Important!
 Setting read groups correctly from the start will simplify your life greatly because you can merge multiple BAM files into one significantly reducing the number of analysis steps. Below we provide an explanation of read groups fields taken from GATK FAQ webpage: .. csv-table:: :header-rows: 1 Tag,Importance,Definition,Meaning ""ID"",""Required"",""Read group identifier. Each @RG line must have a unique ID. The value of ID is used in the RG tags of alignment records. Must be unique among all read groups in header section. Read group IDs may be modified when merging SAM files in order to handle collisions."",""Ideally, this should be a globally unique identify across all sequencing data in the world, such as the Illumina flowcell + lane name and number. Will be referenced by each read with the RG:Z field, allowing tools to determine the read group information associated with each read, including the sample from which the read came. Also, a read group is effectively treated as a separate run of the NGS instrument in tools like base quality score recalibration (a GATK component) -- all reads within a read group are assumed to come from the same instrument run and to therefore share the same error model."" ""SM"",""Sample. Use pool name where a pool is being sequenced."",""Required. As important as ID."",""The name of the sample sequenced in this read group. GATK tools treat all read groups with the same SM value as containing sequencing data for the same sample. Therefore it's critical that the SM field be correctly specified, especially when using multi-sample tools like the Unified Genotyper (a GATK component)."" ""PL"",""Platform/technology used to produce the read. Valid values: ILLUMINA, SOLID, LS454, HELICOS and PACBIO."",""Important. Not currently used in the GATK, but was in the past, and may return. The only way to known the sequencing technology used to generate the sequencing data"",""It's a good idea to use this field."" ""LB"",""DNA preparation library identify"",""Essential for MarkDuplicates"",""MarkDuplicates uses the LB field to determine which read groups might contain molecular duplicates, in case the same DNA library was sequenced on multiple lanes."" 
Example of Read Group usage
 Support we have a trio of samples: MOM, DAD, and KID. Each has two DNA libraries prepared, one with 400 bp inserts and another with 200 bp inserts. Each of these libraries is run on two lanes of an illumina hiseq, requiring 3 x 2 x 2 = 12 lanes of data. When the data come off the sequencer, we would create 12 BAM files, with the following @RG fields in the header:: Dad's data: @RG ID:FLOWCELL1.LANE1 PL:illumina LB:LIB-DAD-1 SM:DAD PI:200 @RG ID:FLOWCELL1.LANE2 PL:illumina LB:LIB-DAD-1 SM:DAD PI:200 @RG ID:FLOWCELL1.LANE3 PL:illumina LB:LIB-DAD-2 SM:DAD PI:400 @RG ID:FLOWCELL1.LANE4 PL:illumina LB:LIB-DAD-2 SM:DAD PI:400 Mom's data: @RG ID:FLOWCELL1.LANE5 PL:illumina LB:LIB-MOM-1 SM:MOM PI:200 @RG ID:FLOWCELL1.LANE6 PL:illumina LB:LIB-MOM-1 SM:MOM PI:200 @RG ID:FLOWCELL1.LANE7 PL:illumina LB:LIB-MOM-2 SM:MOM PI:400 @RG ID:FLOWCELL1.LANE8 PL:illumina LB:LIB-MOM-2 SM:MOM PI:400 Kid's data: @RG ID:FLOWCELL2.LANE1 PL:illumina LB:LIB-KID-1 SM:KID PI:200 @RG ID:FLOWCELL2.LANE2 PL:illumina LB:LIB-KID-1 SM:KID PI:200 @RG ID:FLOWCELL2.LANE3 PL:illumina LB:LIB-KID-2 SM:KID PI:400 @RG ID:FLOWCELL2.LANE4 PL:illumina LB:LIB-KID-2 SM:KID PI:400 Note the hierarchical relationship between read groups (unique for each lane) to libraries (sequenced on two lanes) and samples (across four lanes, two lanes for each library). ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: INPUT=File I=File Input file (bam or sam). Required. OUTPUT=File O=File Output file (bam or sam). Required. SORT_ORDER=SortOrder SO=SortOrder Optional sort order to output in. If not supplied OUTPUT is in the same order as INPUT. Default value: null. Possible values: {unsorted, queryname, coordinate} RGID=String ID=String Read Group ID Default value: 1. This option can be set to 'null' to clear the default value. RGLB=String LB=String Read Group Library Required. RGPL=String PL=String Read Group platform (e.g. illumina, solid) Required. RGPU=String PU=String Read Group platform unit (eg. run barcode) Required. RGSM=String SM=String Read Group sample name Required. RGCN=String CN=String Read Group sequencing center name Default value: null. RGDS=String DS=String Read Group description Default value: null. RGDT=Iso8601Date DT=Iso8601Date Read Group run date Default value: null. RGPI=Integer PI=Integer Read Group predicted insert size Default value: null. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_BedToIntervalList/3.1.1.0	".. class:: infomark 
Purpose
 Convert coordinate data (such as BED or Galaxy Interval) into Picard Interval Format. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: SEQUENCE_DICTIONARY=File SD=File The sequence dictionary. You can either use dictionary pre-cached on this instance of Galaxy, or create one on teh fly from a FASTA file uploaded to history (right pane of the interface). ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_CleanSam/3.1.1.0	".. class:: infomark 
Purpose
 Read SAM/BAM and perform various fix-ups. Currently, the only fix-ups are: 1. to soft-clip an alignment that hangs off the end of its reference sequence. 2. to set MAPQ to 0 if a read is unmapped. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_CASM/3.1.1.0	".. class:: infomark 
Purpose
 Reads a SAM or BAM file and writes a file containing summary alignment metrics. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: MAX_INSERT_SIZE=Integer Paired end reads above this insert size will be considered chimeric along with inter-chromosomal pairs. Default value: 100000. ADAPTER_SEQUENCE=String List of adapter sequences to use when processing the alignment metrics This option may be specified 0 or more times. METRIC_ACCUMULATION_LEVEL=MetricAccumulationLevel LEVEL=MetricAccumulationLevel The level(s) at which to accumulate metrics. Possible values: {ALL_READS, SAMPLE, LIBRARY, READ_GROUP} This option may be specified 0 or more times. IS_BISULFITE_SEQUENCED=Boolean BS=Boolean Whether the SAM or BAM file consists of bisulfite sequenced reads. REFERENCE_SEQUENCE=File R=File Reference sequence fasta Default value: null. ASSUME_SORTED=Boolean AS=Boolean If true (default), then the sort order in the header file will be ignored. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_CollectBaseDistributionByCycle/3.1.1.0	".. class:: infomark 
Purpose
 Program to chart the nucleotide distribution per cycle in a SAM or BAM file. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: ALIGNED_READS_ONLY=Boolean If set to true, calculate the base distribution over aligned reads only. Default value: false. This option can be set to 'null' to clear the default value. Possible values: {true, false} PF_READS_ONLY=Boolean If set to true calculate the base distribution over PF reads only. Default value: false. This option can be set to 'null' to clear the default value. Possible values: {true, false} REFERENCE_SEQUENCE=File R=File Reference sequence fasta Default value: null. ASSUME_SORTED=Boolean AS=Boolean If true (default), then the sort order in the header file will be ignored. Default ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_CollectGcBiasMetrics/3.1.1.0	".. class:: infomark 
Purpose
 Program to chart the nucleotide distribution per cycle in a SAM or BAM file. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: ALIGNED_READS_ONLY=Boolean If set to true, calculate the base distribution over aligned reads only. Default value: false. Possible values: {true, false} PF_READS_ONLY=Boolean If set to true calculate the base distribution over PF reads only. Default value: false. This option can be set to 'null' to clear the default value. Possible values: {true, false} ASSUME_SORTED=Boolean AS=Boolean If true (default), then the sort order in the header file will be ignored. Default: True ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_CollectHsMetrics/3.1.1	".. class:: infomark 
Purpose
 Computes a number of metrics that are useful for evaluating coverage and performance of datasets generated through hybrid-selection. Hybrid-selection (HS) is the most commonly used technique to capture exon-specific sequences for targeted sequencing experiments such as exome sequencing. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ----- .. class:: warningmark 
Obtaining bait and target interval files in Picard interval_list format
 This tool requires an aligned SAM or BAM file as well as bait and target interval files in Picard interval_list format. You should use the bait and interval files that correspond to the capture kit that was used to generate the capture libraries for sequencing, which can generally be obtained from the kit manufacturer. If the baits and target intervals are provided in BED format, you can convert them to the Picard interval_list format using Picard's 
BedToIntervalList
 tool, which will also add the required SAM style header. ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: MINIMUM_MAPPING_QUALITY=Integer MQ=Integer Minimum mapping quality for a read to contribute coverage. Default value: 20. MINIMUM_BASE_QUALITY=Integer Q=Integer Minimum base quality for a base to contribute coverage. Default value: 20. COVERAGE_CAP=Integer CAP=Integer Treat bases with coverage exceeding this value as if they had coverage at this value. Default value: 200. CLIP_OVERLAPPING_READS=Boolean If true, clip overlapping reads, false otherwise. Default value: true. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_CollectInsertSizeMetrics/3.1.1.0	".. class:: infomark 
Purpose
 Reads a SAM or BAM dataset and writes a file containing metrics about the statistical distribution of insert size (excluding duplicates) and generates a Histogram plot. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: DEVIATIONS=Double Generate mean, sd and plots by trimming the data down to MEDIAN + DEVIATIONS
MEDIAN_ABSOLUTE_DEVIATION. This is done because insert size data typically includes enough anomalous values from chimeras and other artifacts to make the mean and sd grossly misleading regarding the real distribution. Default value: 10.0. HISTOGRAM_WIDTH=Integer W=Integer Explicitly sets the Histogram width, overriding automatic truncation of Histogram tail. Also, when calculating mean and standard deviation, only bins <= Histogram_WIDTH will be included. Default value: not set. MINIMUM_PCT=Float M=Float When generating the Histogram, discard any data categories (out of FR, TANDEM, RF) that have fewer than this percentage of overall reads. (Range: 0 to 1). Default value: 0.05. METRIC_ACCUMULATION_LEVEL=MetricAccumulationLevel LEVEL=MetricAccumulationLevel The level(s) at which to accumulate metrics. Possible values: {ALL_READS, SAMPLE, LIBRARY, READ_GROUP} This option may be specified 0 or more times. ASSUME_SORTED=Boolean AS=Boolean If true (default), then the sort order in the header file will be ignored. Default value: true. This option can be set to 'null' to clear the default value. Possible values: {true, false} ------ 
Additional information
* Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_CollectRnaSeqMetrics/3.1.1.0	".. class:: infomark 
Purpose
 Collects metrics about the alignment of RNA to various functional classes of loci in the genome: coding, intronic, UTR, intergenic, ribosomal. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ----- .. class:: warningmark 
Obtaining gene annotations in refFlat format
 This tool requires gene annotations in refFlat_ format. These data can be obtained from UCSC table browser directly through Galaxy by following these steps: 1. Click on 
Get Data
 in the upper part of left pane of Galaxy interface 2. Click on 
UCSC Main
 link 3. Set your genome and dataset of interest. It 
must
 be the same genome build against which you have mapped the reads contained in the BAM file you are analyzing 4. In the 
output format
 field choose 
selected fields from primary and related tables
 5. Click 
get output
 button 6. In the first table presented at the top of the page select (using checkboxes) first 11 fields: name chrom strand txStart txEnd cdsStart cdsEnd exonCount exonStarts exonEnds proteinId 7. Click 
done with selection
 8. Click 
Send query to Galaxy
 9. A new dataset will appear in the current Galaxy history 10. Use this dataset as the input for 
Gene annotations in refFlat form
 dropdown of this tool .. _refFlat: https://genome.ucsc.edu/FAQ/FAQformat.html#format9 ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: REF_FLAT=File Gene annotations in refFlat form. Format described here: https://genome.ucsc.edu/FAQ/FAQformat.html#format9 Required. RIBOSOMAL_INTERVALS=File Location of rRNA sequences in genome, in interval_list format. If not specified no bases will be identified as being ribosomal. Format described here: https://samtools.github.io/htsjdk/javadoc/htsjdk/htsjdk/samtools/util/IntervalList.html and can be generated from BED datasetes using Galaxy's wrapper for picard_BedToIntervalList tool STRAND_SPECIFICITY=StrandSpecificity STRAND=StrandSpecificity For strand-specific library prep. For unpaired reads, use FIRST_READ_TRANSCRIPTION_STRAND if the reads are expected to be on the transcription strand. Required. Possible values: {NONE, FIRST_READ_TRANSCRIPTION_STRAND, SECOND_READ_TRANSCRIPTION_STRAND} MINIMUM_LENGTH=Integer When calculating coverage based values (e.g. CV of coverage) only use transcripts of this length or greater. Default value: 500. IGNORE_SEQUENCE=String If a read maps to a sequence specified with this option, all the bases in the read are counted as ignored bases. RRNA_FRAGMENT_PERCENTAGE=Double This percentage of the length of a fragment must overlap one of the ribosomal intervals for a read or read pair by this must in order to be considered rRNA. Default value: 0.8. METRIC_ACCUMULATION_LEVEL=MetricAccumulationLevel LEVEL=MetricAccumulationLevel The level(s) at which to accumulate metrics. Possible values: {ALL_READS, SAMPLE, LIBRARY, READ_GROUP} This option may be specified 0 or more times. ASSUME_SORTED=Boolean AS=Boolean If true (default), then the sort order in the header file will be ignored. Default value: true. Possible values: {true, false} ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_CollectWgsMetrics/3.1.1.0	".. class:: infomark 
Purpose
 Computes a number of metrics that are useful for evaluating coverage and performance of whole genome sequencing experiments. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: MINIMUM_MAPPING_QUALITY=Integer MQ=Integer Minimum mapping quality for a read to contribute coverage. Default value: 20. MINIMUM_BASE_QUALITY=Integer Q=Integer Minimum base quality for a base to contribute coverage. Default value: 20. COVERAGE_CAP=Integer CAP=Integer Treat bases with coverage exceeding this value as if they had coverage at this value. Default value: 250. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_DownsampleSam/3.1.1.0	".. class:: infomark 
Purpose
 Randomly down-sample a SAM or BAM file to retain a random subset of the reads. Mate-pairs are either both kept or both discarded. Reads marked as not primary alignments are all discarded. Each read is given a probability P of being retained - results with the exact same input in the same order and with the same value for RANDOM_SEED will produce the same results. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: INPUT=File I=File The input SAM or BAM file to downsample. Required. OUTPUT=File O=File The output, downsampled, SAM or BAM file to write. Required. RANDOM_SEED=Long R=Long Random seed to use if reproducibilty is desired. Setting to null will cause multiple invocations to produce different results. PROBABILITY=Double P=Double The probability of keeping any individual read, between 0 and 1. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_EstimateLibraryComplexity/3.1.1.0	"Purpose
 Attempts to estimate library complexity from sequence of read pairs alone. Does so by sorting all reads by the first N bases (5 by default) of each read and then comparing reads with the first N bases identical to each other for duplicates. Reads are considered to be duplicates if they match each other with no gaps and an overall mismatch rate less than or equal to MAX_DIFF_RATE (0.03 by default). Reads of poor quality are filtered out so as to provide a more accurate estimate. The filtering removes reads with any no-calls in the first N bases or with a mean base quality lower than MIN_MEAN_QUALITY across either the first or second read. Unpaired reads are ignored in this computation. The algorithm attempts to detect optical duplicates separately from PCR duplicates and excludes these in the calculation of library size. Also, since there is no alignment to screen out technical reads one further filter is applied on the data. After examining all reads a Histogram is built of [#reads in duplicate set -> #of duplicate sets]; all bins that contain exactly one duplicate set are then removed from the Histogram as outliers before library size is estimated. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: MIN_IDENTICAL_BASES=Integer The minimum number of bases at the starts of reads that must be identical for reads to be grouped together for duplicate detection. In effect total_reads / 4^max_id_bases reads will be compared at a time, so lower numbers will produce more accurate results but consume exponentially more memory and CPU. Default value: 5. MAX_DIFF_RATE=Double The maximum rate of differences between two reads to call them identical. Default value: 0.03. MIN_MEAN_QUALITY=Integer The minimum mean quality of the bases in a read pair for the read to be analyzed. Reads with lower average quality are filtered out and not considered in any calculations. Default value: 20. MAX_GROUP_RATIO=Integer Do not process self-similar groups that are this many times over the mean expected group size. I.e. if the input contains 10m read pairs and MIN_IDENTICAL_BASES is set to 5, then the mean expected group size would be approximately 10 reads. Default value: 500. READ_NAME_REGEX=String Regular expression that can be used to parse read names in the incoming SAM file. Read names are parsed to extract three variables: tile/region, x coordinate and y coordinate. These values are used to estimate the rate of optical duplication in order to give a more accurate estimated library size. Set this option to null to disable optical duplicate detection. The regular expression should contain three capture groups for the three variables, in order. It must match the entire read name. Note that if the default regex is specified, a regex match is not actually done, but instead the read name is split on colon character. For 5 element names, the 3rd, 4th and 5th elements are assumed to be tile, x and y values. For 7 element names (CASAVA 1.8), the 5th, 6th, and 7th elements are assumed to be tile, x and y values. Default value: [a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).
. OPTICAL_DUPLICATE_PIXEL_DISTANCE=Integer The maximum offset between two duplicte clusters in order to consider them optical duplicates. This should usually be set to some fairly small number (e.g. 5-10 pixels) unless using later versions of the Illumina pipeline that multiply pixel values by 10, in which case 50-100 is more normal. Default value: 100. ------ 
Additional information
* Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_FastqToSam/3.1.1.0	".. class:: infomark 
Purpose
 Computes a number of metrics that are useful for evaluating coverage and performance of whole genome sequencing experiments. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------- 
Read Groups are Important!
 Setting read groups correctly from the start will simplify your life greatly because you can merge multiple BAM files into one significantly reducing the number of analysis steps. Below we provide an explanation of read groups fields taken from GATK FAQ webpage: .. csv-table:: :header-rows: 1 Tag,Importance,Definition,Meaning ""ID"",""Required"",""Read group identifier. Each @RG line must have a unique ID. The value of ID is used in the RG tags of alignment records. Must be unique among all read groups in header section. Read group IDs may be modified when merging SAM files in order to handle collisions."",""Ideally, this should be a globally unique identify across all sequencing data in the world, such as the Illumina flowcell + lane name and number. Will be referenced by each read with the RG:Z field, allowing tools to determine the read group information associated with each read, including the sample from which the read came. Also, a read group is effectively treated as a separate run of the NGS instrument in tools like base quality score recalibration (a GATK component) -- all reads within a read group are assumed to come from the same instrument run and to therefore share the same error model."" ""SM"",""Sample. Use pool name where a pool is being sequenced."",""Required. As important as ID."",""The name of the sample sequenced in this read group. GATK tools treat all read groups with the same SM value as containing sequencing data for the same sample. Therefore it's critical that the SM field be correctly specified, especially when using multi-sample tools like the Unified Genotyper (a GATK component)."" ""PL"",""Platform/technology used to produce the read. Valid values: ILLUMINA, SOLID, LS454, HELICOS and PACBIO."",""Important. Not currently used in the GATK, but was in the past, and may return. The only way to known the sequencing technology used to generate the sequencing data"",""It's a good idea to use this field."" ""LB"",""DNA preparation library identify"",""Essential for MarkDuplicates"",""MarkDuplicates uses the LB field to determine which read groups might contain molecular duplicates, in case the same DNA library was sequenced on multiple lanes."" 
Example of Read Group usage
 Support we have a trio of samples: MOM, DAD, and KID. Each has two DNA libraries prepared, one with 400 bp inserts and another with 200 bp inserts. Each of these libraries is run on two lanes of an illumina hiseq, requiring 3 x 2 x 2 = 12 lanes of data. When the data come off the sequencer, we would create 12 BAM files, with the following @RG fields in the header:: Dad's data: @RG ID:FLOWCELL1.LANE1 PL:illumina LB:LIB-DAD-1 SM:DAD PI:200 @RG ID:FLOWCELL1.LANE2 PL:illumina LB:LIB-DAD-1 SM:DAD PI:200 @RG ID:FLOWCELL1.LANE3 PL:illumina LB:LIB-DAD-2 SM:DAD PI:400 @RG ID:FLOWCELL1.LANE4 PL:illumina LB:LIB-DAD-2 SM:DAD PI:400 Mom's data: @RG ID:FLOWCELL1.LANE5 PL:illumina LB:LIB-MOM-1 SM:MOM PI:200 @RG ID:FLOWCELL1.LANE6 PL:illumina LB:LIB-MOM-1 SM:MOM PI:200 @RG ID:FLOWCELL1.LANE7 PL:illumina LB:LIB-MOM-2 SM:MOM PI:400 @RG ID:FLOWCELL1.LANE8 PL:illumina LB:LIB-MOM-2 SM:MOM PI:400 Kid's data: @RG ID:FLOWCELL2.LANE1 PL:illumina LB:LIB-KID-1 SM:KID PI:200 @RG ID:FLOWCELL2.LANE2 PL:illumina LB:LIB-KID-1 SM:KID PI:200 @RG ID:FLOWCELL2.LANE3 PL:illumina LB:LIB-KID-2 SM:KID PI:400 @RG ID:FLOWCELL2.LANE4 PL:illumina LB:LIB-KID-2 SM:KID PI:400 Note the hierarchical relationship between read groups (unique for each lane) to libraries (sequenced on two lanes) and samples (across four lanes, two lanes for each library). ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: FASTQ=File F1=File Input fastq file for single end data, or first read in paired end data. Required. FASTQ2=File F2=File Input fastq file for the second read of paired end data (if used). QUALITY_FORMAT=FastqQualityFormat V=FastqQualityFormat A value describing how the quality values are encoded in the fastq. Either Solexa for pre-pipeline 1.3 style scores (solexa scaling + 66), Illumina for pipeline 1.3 and above (phred scaling + 64) or Standard for phred scaled scores with a character shift of 33. If this value is not specified, the quality format will be detected automatically. Default value: null. Possible values: {Solexa, Illumina, Standard} READ_GROUP_NAME=String RG=String Read group name Default value: A. SAMPLE_NAME=String SM=String Sample name to insert into the read group header Required. LIBRARY_NAME=String LB=String The library name to place into the LB attribute in the read group header. PLATFORM_UNIT=String PU=String The platform unit (often run_barcode.lane) to insert into the read group header. PLATFORM=String PL=String The platform type (e.g. illumina, solid) to insert into the read group header. SEQUENCING_CENTER=String CN=String The sequencing center from which the data originated. PREDICTED_INSERT_SIZE=Integer PI=Integer Predicted median insert size, to insert into the read group header. COMMENT=String CO=String Comment to include in the merged output file's header. DESCRIPTION=String DS=String Inserted into the read group header. RUN_DATE=Iso8601Date DT=Iso8601Date Date the run was produced, to insert into the read group header. MIN_Q=Integer Minimum quality allowed in the input fastq. An exception will be thrown if a quality is less than this value. Default value: 0. MAX_Q=Integer Maximum quality allowed in the input fastq. An exception will be thrown if a quality is greater than this value. Default value: 93. STRIP_UNPAIRED_MATE_NUMBER=Boolean If true and this is an unpaired fastq any occurance of '/1' will be removed from the end of a read name. Default value: false. Possible values: {true, false} ALLOW_AND_IGNORE_EMPTY_LINES=Boolean Allow (and ignore) empty lines Default value: false. Possible values: {true, false} ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_FilterSamReads/3.1.1.0	"Purpose
 Computes a number of metrics that are useful for evaluating coverage and performance of whole genome sequencing experiments. ------ .. class:: warningmark 
Warning on using this tool on BWA-MEM output
 This tool will likely fail on BAM datasets generated by BWA MEM as it generates partial read alignemnts. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: FILTER=Filter Filter. Required. Possible values: includeAligned [OUTPUT SAM/BAM will contain aligned reads only. (Note that 
both
 first and second of paired reads must be aligned to be included in the OUTPUT SAM or BAM)], excludeAligned [OUTPUT SAM/BAM will contain un-mapped reads only. (Note that 
both
 first and second of pair must be aligned to be excluded from the OUTPUT SAM or BAM)] includeReadList [OUTPUT SAM/BAM will contain reads that are supplied in the READ_LIST_FILE file] excludeReadList [OUTPUT bam will contain reads that are 
not
 supplied in the READ_LIST_FILE file]} READ_LIST_FILE=File RLF=File Read List File containing reads that will be included or excluded from the OUTPUT SAM or BAM file. Default value: null. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_FixMateInformation/3.1.1.0	"Purpose
 Ensure that all mate-pair information is in sync between each read and it's mate pair. Reads marked with the secondary alignment flag are written to the output file unchanged. ------ .. class:: warningmark 
Warning on using ASSUME_SORTED option
 Datasets imported into Galaxy are automatically coordinate sorted. So use this option (set it to True) only if you are sure that this is necessary. If you are not sure - a good rule of thumb is to assume that the BAM you are working with is coordinate sorted. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: ASSUME_SORTED=Boolean AS=Boolean If true, assume that the input file is queryname sorted, even if the header says otherwise. Default value: false. ADD_MATE_CIGAR=Boolean MC=Boolean Adds the mate CIGAR tag (MC) if true, does not if false. Default value: true. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/3.1.1.0	"Purpose
 Examines aligned records in the supplied SAM or BAM dataset to locate duplicate molecules. All records are then written to the output file with the duplicate records flagged. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: COMMENT=String CO=String Comment(s) to include in the output file's header. This option may be specified 0 or more times. REMOVE_DUPLICATES=Boolean If true do not write duplicates to the output file instead of writing them with appropriate flags set. Default value: false. READ_NAME_REGEX=String This option is only needed if your read names do not follow a standard illumina convention of colon separation but do contain tile, x, and y coordinates (unusual). A regular expression that can be used to parse read names in the incoming SAM file. Read names are parsed to extract three variables: tile/region, x coordinate and y coordinate. These values are used to estimate the rate of optical duplication in order to give a more accurate estimated library size. Set this option to null to disable optical duplicate detection. The regular expression should contain three capture groups for the three variables, in order. It must match the entire read name. Note that if the default regex is specified, a regex match is not actually done, but instead the read name is split on colon character. For 5 element names, the 3rd, 4th and 5th elements are assumed to be tile, x and y values. For 7 element names (CASAVA 1.8), the 5th, 6th, and 7th elements are assumed to be tile, x and y values. Default value: '' DUPLICATE_SCORING_STRATEGY=ScoringStrategy DS=ScoringStrategy The scoring strategy for choosing the non-duplicate among candidates. Default value: SUM_OF_BASE_QUALITIES. Possible values: {SUM_OF_BASE_QUALITIES, TOTAL_MAPPED_REFERENCE_LENGTH} OPTICAL_DUPLICATE_PIXEL_DISTANCE=Integer The maximum offset between two duplicate clusters in order to consider them optical duplicates. This should be set to 100 for (circa 2011+) read names and typical flowcells. Structured flow cells (NovaSeq, HiSeq 4000, X) should use ~2500. For older conventions, distances could be to some fairly small number (e.g. 5-10 pixels) Default value: 100. BARCODE_TAG=String Barcode SAM tag (ex. BC for 10X Genomics) Default value: null. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicatesWithMateCigar/3.1.1.0	"Purpose
 Examines aligned records in the supplied SAM or BAM dataset to locate duplicate molecules. All records are then written to the output file with the duplicate records flagged. ------ .. class:: warningmark On the difference between 
MarkDuplicates
 and 
picard_MarkDuplicatesWithMateCigar
 From Samtools Announce MailingList_: This tool can replace MarkDuplicates if the input SAM/BAM has Mate CIGAR (MC) optional tags pre-computed (see the tools RevertOriginalBaseQualitiesAndAddMateCigar and FixMateInformation). This allows the new tool to perform a streaming duplicate marking routine (i.e. a single-pass). This tool cannot be used with alignments that have large gaps or reference skips, which happens frequently in RNA-seq data. .. _MailingList: http://sourceforge.net/p/samtools/mailman/message/32910359/ ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: MINIMUM_DISTANCE=Integer The minimum distance to buffer records to account for clipping on the 5' end of the records.Set this number to -1 to use twice the first read's read length (or 100, whichever is smaller). Default value: -1. This option can be set to 'null' to clear the default value. SKIP_PAIRS_WITH_NO_MATE_CIGAR=Boolean Skip record pairs with no mate cigar and include them in the output. Default value: true. This option can be set to 'null' to clear the default value. Possible values: {true, false} COMMENT=String CO=String Comment(s) to include in the output file's header. This option may be specified 0 or more times. REMOVE_DUPLICATES=Boolean If true do not write duplicates to the output file instead of writing them with appropriate flags set. Default value: false. READ_NAME_REGEX=String Regular expression that can be used to parse read names in the incoming SAM file. Read names are parsed to extract three variables: tile/region, x coordinate and y coordinate. These values are used to estimate the rate of optical duplication in order to give a more accurate estimated library size. Set this option to null to disable optical duplicate detection. The regular expression should contain three capture groups for the three variables, in order. It must match the entire read name. Note that if the default regex is specified, a regex match is not actually done, but instead the read name is split on colon character. For 5 element names, the 3rd, 4th and 5th elements are assumed to be tile, x and y values. For 7 element names (CASAVA 1.8), the 5th, 6th, and 7th elements are assumed to be tile, x and y values. Default value: [a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).
. DUPLICATE_SCORING_STRATEGY=ScoringStrategy DS=ScoringStrategy The scoring strategy for choosing the non-duplicate among candidates. Default value: TOTAL_MAPPED_REFERENCE_LENGTH. Possible values: {SUM_OF_BASE_QUALITIES, TOTAL_MAPPED_REFERENCE_LENGTH} OPTICAL_DUPLICATE_PIXEL_DISTANCE=Integer The maximum offset between two duplicte clusters in order to consider them optical duplicates. This should usually be set to some fairly small number (e.g. 5-10 pixels) unless using later versions of the Illumina pipeline that multiply pixel values by 10, in which case 50-100 is more normal. Default value: 100. ------ 
Additional information
* Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MeanQualityByCycle/3.1.1.0	".. class:: infomark 
Purpose
 Program to chart the distribution of base qualities by cycle within reads supplied in a SAM or BAM dataset. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: ALIGNED_READS_ONLY=Boolean If set to true, calculate the base distribution over aligned reads only. Default value: false. Possible values: {true, false} PF_READS_ONLY=Boolean If set to true calculate the base distribution over PF reads only. Default value: false. This option can be set to 'null' to clear the default value. Possible values: {true, false} ASSUME_SORTED=Boolean AS=Boolean If true (default), then the sort order in the header file will be ignored. Default: True ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MergeBamAlignment/3.1.1.0	".. class:: infomark 
Purpose
 Merges alignment data from a SAM or BAM dataset with additional data stored in an unmapped BAM dataset and produces a third SAM or BAM dataset of aligned and unaligned reads. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: UNMAPPED_BAM=File UNMAPPED=File Original SAM or BAM file of unmapped reads, which must be in queryname order. Required. ALIGNED_BAM=File ALIGNED=File SAM or BAM file(s) with alignment data. This option may be specified 0 or more times. Cannot be used in conjuction with option(s) READ1_ALIGNED_BAM (R1_ALIGNED) READ2_ALIGNED_BAM (R2_ALIGNED) READ1_ALIGNED_BAM=File R1_ALIGNED=File SAM or BAM file(s) with alignment data from the first read of a pair. This option may be specified 0 or more times. Cannot be used in conjuction with option(s) ALIGNED_BAM (ALIGNED) READ2_ALIGNED_BAM=File R2_ALIGNED=File SAM or BAM file(s) with alignment data from the second read of a pair. This option may be specified 0 or more times. Cannot be used in conjuction with option(s) ALIGNED_BAM (ALIGNED) PAIRED_RUN=Boolean PE=Boolean This argument is ignored and will be removed. Required. Possible values: {true, false} JUMP_SIZE=Integer JUMP=Integer The expected jump size (required if this is a jumping library). Deprecated. Use EXPECTED_ORIENTATIONS instead Default value: null. Cannot be used in conjuction with option(s) EXPECTED_ORIENTATIONS (ORIENTATIONS) CLIP_ADAPTERS=Boolean Whether to clip adapters where identified. Default value: true. Possible values: {true, false} IS_BISULFITE_SEQUENCE=Boolean Whether the lane is bisulfite sequence (used when caculating the NM tag). Default value: false. Possible values: {true, false} ALIGNED_READS_ONLY=Boolean Whether to output only aligned reads. Default value: false. Possible values: {true, false} MAX_INSERTIONS_OR_DELETIONS=Integer MAX_GAPS=Integer The maximum number of insertions or deletions permitted for an alignment to be included. Alignments with more than this many insertions or deletions will be ignored. Set to -1 to allow any number of insertions or deletions. Default value: 1. ATTRIBUTES_TO_RETAIN=String Reserved alignment attributes (tags starting with X, Y, or Z) that should be brought over from the alignment data when merging. This option may be specified 0 or more times. ATTRIBUTES_TO_REMOVE=String Attributes from the alignment record that should be removed when merging. This overrides ATTRIBUTES_TO_RETAIN if they share common tags. This option may be specified 0 or more times. READ1_TRIM=Integer R1_TRIM=Integer The number of bases trimmed from the beginning of read 1 prior to alignment Default value: 0. READ2_TRIM=Integer R2_TRIM=Integer The number of bases trimmed from the beginning of read 2 prior to alignment Default value: 0. EXPECTED_ORIENTATIONS=PairOrientation ORIENTATIONS=PairOrientation The expected orientation of proper read pairs. Replaces JUMP_SIZE Possible values: {FR, RF, TANDEM} This option may be specified 0 or more times. Cannot be used in conjuction with option(s) JUMP_SIZE (JUMP) ALIGNER_PROPER_PAIR_FLAGS=Boolean Use the aligner's idea of what a proper pair is rather than computing in this program. Default value: false. Possible values: {true, false} SORT_ORDER=SortOrder SO=SortOrder The order in which the merged reads should be output. Default value: coordinate. Possible values: {unsorted, queryname, coordinate} PRIMARY_ALIGNMENT_STRATEGY=PrimaryAlignmentStrategy Strategy for selecting primary alignment when the aligner has provided more than one alignment for a pair or fragment, and none are marked as primary, more than one is marked as primary, or the primary alignment is filtered out for some reason. BestMapq expects that multiple alignments will be correlated with HI tag, and prefers the pair of alignments with the largest MAPQ, in the absence of a primary selected by the aligner. EarliestFragment prefers the alignment which maps the earliest base in the read. Note that EarliestFragment may not be used for paired reads. BestEndMapq is appropriate for cases in which the aligner is not pair-aware, and does not output the HI tag. It simply picks the alignment for each end with the highest MAPQ, and makes those alignments primary, regardless of whether the two alignments make sense together.MostDistant is also for a non-pair-aware aligner, and picks the alignment pair with the largest insert size. If all alignments would be chimeric, it picks the alignments for each end with the best MAPQ. For all algorithms, ties are resolved arbitrarily. Default value: BestMapq. Possible values: {BestMapq, EarliestFragment, BestEndMapq, MostDistant} CLIP_OVERLAPPING_READS=BooleanFor paired reads, soft clip the 3' end of each read if necessary so that it does not extend past the 5' end of its mate. Default value: true. Possible values: {true, false} INCLUDE_SECONDARY_ALIGNMENTS=Boolean If false, do not write secondary alignments to output. Default value: true. Possible values: {true, false} ADD_MATE_CIGAR=Boolean MC=Boolean Adds the mate CIGAR tag (MC) if true, does not if false. Possible values: {true, false} ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MergeSamFiles/3.1.1.0	"Purpose
 Merges multiple SAM/BAM datasets into one. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: ASSUME_SORTED=Boolean AS=Boolean If true, assume that the input files are in the same sort order as the requested output sort order, even if their headers say otherwise. Default value: false. This option can be set to 'null' to clear the default value. Possible values: {true, false} MERGE_SEQUENCE_DICTIONARIES=Boolean MSD=Boolean Merge the sequence dictionaries Default value: false. This option can be set to 'null' to clear the default value. Possible values: {true, false} COMMENT=String CO=String Comment(s) to include in the merged output file's header. This option may be specified 0 or more times. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_NormalizeFasta/3.1.1.0	"Purpose
 Takes any dataset that conforms to the fasta format and normalizes it so that all lines of sequence except the last line per named sequence are of the same length. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: LINE_LENGTH=Integer The line length to be used for the output fasta file. Default value: 100. TRUNCATE_SEQUENCE_NAMES_AT_WHITESPACE=Boolean Truncate sequence names at first whitespace. Default value: false. Possible values: {true, false} ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_artifact_metrics/3.1.1.0	".. class:: infomark 
Purpose
 Program to chart the distribution of potential sequencing ""single nucleotide mutation"" artifacts in a SAM or BAM file. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: ASSUME_SORTED=Boolean If true (default), then the sort order in the header file will be ignored. Default: True CONTEXT_SIZE=integer The number of context bases to include on each side of the assayed base. CONTEXT_SIZE_TO_PRINT=String If specified, only print results for these contexts in the detail metrics output. However, the summary metrics output will still take all contexts into consideration. DB_SNP=text file VCF format dbSNP file, used to exclude regions around known polymorphisms from analysis. INCLUDE_DUPLICATES=Boolean Include duplicate reads. If set to true then all reads flagged as duplicates will be included as well. INCLUDE_UNPAIRED=Boolean Include unpaired reads. If set to true then all paired reads will be included as well - MINIMUM_INSERT_SIZE and MAXIMUM_INSERT_SIZE will be ignored. MAXIMUM_INSERT_SIZE=Integer The maximum insert size for a read to be included in analysis. Set to 0 to have no maximum. Default = 600 MINIMUM_INSERT_SIZE=Integer The minimum insert size for a read to be included in analysis. Default = 60 MINIMUM_MAPPING_QUALITY The minimum mapping quality score for a base to be included in analysis. Default = 30 MINIMUM_QUALITY_SCORE The minimum base quality score for a base to be included in analysis. Default = 20 ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_QualityScoreDistribution/3.1.1.0	".. class:: infomark 
Purpose
 Program to chart quality score distributions in a SAM or BAM dataset. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: ALIGNED_READS_ONLY=Boolean If set to true, calculate the base distribution over aligned reads only. Default value: false. Possible values: {true, false} PF_READS_ONLY=Boolean If set to true calculate the base distribution over PF reads only. Default value: false. Possible values: {true, false} INCLUDE_NO_CALLS=Boolean If set to true, include quality for no-call bases in the distribution. Default value: false. Possible values: {true, false} ASSUME_SORTED=Boolean AS=Boolean If true (default), then the sort order in the header file will be ignored. Default: True ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_ReorderSam/3.1.1.0	".. class:: infomark 
Purpose
 ReorderSam reorders reads in a SAM/BAM file to match the contig ordering in a provided reference file, as determined by exact name matching of contigs. Reads mapped to contigs absent in the new reference are dropped. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ---- .. class:: warningmark Not to be confused with 
SortSam
. ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: ALLOW_INCOMPLETE_DICT_CONCORDANCE=Boolean S=Boolean If true, then allows only a partial overlap of the BAM contigs with the new reference sequence contigs. By default, this tool requires a corresponding contig in the new reference for each read contig Default value: false. Possible values: {true, false} ALLOW_CONTIG_LENGTH_DISCORDANCE=Boolean U=Boolean If true, then permits mapping from a read contig to a new reference contig with the same name but a different length. Highly dangerous, only use if you know what you are doing. Default value: false. Possible values: {true, false} ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_ReplaceSamHeader/3.1.1.0	"Purpose
 Replace the SAMFileHeader in a SAM/BAM dataset with the given header. Validation is minimal. It is up to the user to ensure that all the elements referred to in the SAMRecords are present in the new header. Sort order of the two input datasets must be the same. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: HEADER=File SAM file from which SAMFileHeader will be read. Required. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_RevertOriginalBaseQualitiesAndAddMateCigar/3.1.1.0	"Purpose
 Reverts the original base qualities and adds the mate cigar tag to SAM or BAMs. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: RESTORE_ORIGINAL_QUALITIES=Boolean OQ=Boolean True to restore original qualities from the OQ field to the QUAL field if available. Default value: true. Possible values: {true, false} MAX_RECORDS_TO_EXAMINE=IntegerThe maximum number of records to examine to determine if we can exit early and not output, given that there are a no original base qualities (if we are to restore) and mate cigars exist. Set to 0 to never skip the file. Default value: 10000. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_RevertSam/3.1.1.0	"Purpose
 Reverts SAM or BAM files to a previous state by removing certain types of information and/or substituting in the original quality scores when available. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: SORT_ORDER=SortOrder SO=SortOrder The sort order to create the reverted output file with. Default value: queryname. Possible values: {unsorted, queryname, coordinate} RESTORE_ORIGINAL_QUALITIES=Boolean OQ=Boolean True to restore original qualities from the OQ field to the QUAL field if available. Default value: true. Possible values: {true, false} REMOVE_DUPLICATE_INFORMATION=Boolean Remove duplicate read flags from all reads. Note that if this is true and REMOVE_ALIGNMENT_INFORMATION==false, the output may have the unusual but sometimes desirable trait of having unmapped reads that are marked as duplicates. Default value: true. Possible values: {true, false} REMOVE_ALIGNMENT_INFORMATION=Boolean Remove all alignment information from the file. Default value: true. TPossible values: {true, false} ATTRIBUTE_TO_CLEAR=String When removing alignment information, the set of optional tags to remove. This option may be specified 0 or more times. SANITIZE=Boolean WARNING: This option is potentially destructive. If enabled will discard reads in order to produce a consistent output BAM. Reads discarded include (but are not limited to) paired reads with missing mates, duplicated records, records with mismatches in length of bases and qualities. This option can only be enabled if the output sort order is queryname and will always cause sorting to occur. Possible values: {true, false} MAX_DISCARD_FRACTION=Double If SANITIZE=true and higher than MAX_DISCARD_FRACTION reads are discarded due to sanitization thenthe program will exit with an Exception instead of exiting cleanly. Output BAM will still be valid. Default value: 0.01. SAMPLE_ALIAS=String ALIAS=String The sample alias to use in the reverted output file. This will override the existing sample alias in the file and is used only if all the read groups in the input file have the same sample alias Default value: null. LIBRARY_NAME=String LIB=String The library name to use in the reverted output file. This will override the existing sample alias in the file and is used only if all the read groups in the input file have the same sample alias Default value: null. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_SamToFastq/3.1.1.0	"Purpose
 Extracts read sequences and qualities from the input SAM/BAM dataset and outputs them in Sanger fastq format. In the RE_REVERSE=True mode (default behavior), if the read is aligned and the alignment is to the reverse strand on the genome, the read's sequence from input SAM.BAM dataset will be reverse-complemented prior to writing it to fastq in order restore correctly the original read sequence as it was generated by the sequencer. .. class:: warningmark ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: FASTQ=File F=File Output fastq file (single-end fastq or, if paired, first end of the pair fastq). Required. SECOND_END_FASTQ=File F2=File Output fastq file (if paired, second end of the pair fastq). Default value: null. UNPAIRED_FASTQ=File FU=File Output fastq file for unpaired reads; may only be provided in paired-fastq mode Default value: null. RE_REVERSE=Boolean RC=Boolean Re-reverse bases and qualities of reads with negative strand flag set before writing them to fastq Default value: true. Possible values: {true, false} INTERLEAVE=Boolean INTER=Boolean Will generate an interleaved fastq if paired, each line will have /1 or /2 to describe which end it came from Default value: false. Possible values: {true, false} INCLUDE_NON_PF_READS=Boolean NON_PF=Boolean Include non-PF reads from the SAM file into the output FASTQ files. PF means 'passes filtering'. Reads whose 'not passing quality controls' flag is set are non-PF reads. Default value: false. Possible values: {true, false} CLIPPING_ATTRIBUTE=String CLIP_ATTR=String The attribute that stores the position at which the SAM record should be clipped Default value: null. CLIPPING_ACTION=String CLIP_ACT=String The action that should be taken with clipped reads: 'X' means the reads and qualities should be trimmed at the clipped position; 'N' means the bases should be changed to Ns in the clipped region; and any integer means that the base qualities should be set to that value in the clipped region. Default value: null. READ1_TRIM=Integer R1_TRIM=Integer The number of bases to trim from the beginning of read 1. Default value: 0. READ1_MAX_BASES_TO_WRITE=Integer R1_MAX_BASES=Integer The maximum number of bases to write from read 1 after trimming. If there are fewer than this many bases left after trimming, all will be written. If this value is null then all bases left after trimming will be written. Default value: null. READ2_TRIM=Integer R2_TRIM=Integer The number of bases to trim from the beginning of read 2. Default value: 0. READ2_MAX_BASES_TO_WRITE=Integer R2_MAX_BASES=Integer The maximum number of bases to write from read 2 after trimming. If there are fewer than this many bases left after trimming, all will be written. If this value is null then all bases left after trimming will be written. Default value: null. INCLUDE_NON_PRIMARY_ALIGNMENTS=Boolean If true, include non-primary alignments in the output. Support of non-primary alignments in SamToFastq is not comprehensive, so there may be exceptions if this is set to true and there are paired reads with non-primary alignments. Default value: false. Possible values: {true, false} ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_SortSam/3.1.1.0	".. class:: infomark 
Purpose
 Sorts the input SAM or BAM. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: SORT_ORDER=SortOrder SO=SortOrder Sort order of output file. You can either sort by queryname or by coordinate. ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_ValidateSamFile/3.1.1.0	"Purpose
 Reads a SAM/BAM dataset and report on its validity. ------ 
Dataset collections - processing large numbers of datasets at once
 This will be added shortly ------ 
Inputs, outputs, and parameters
 Either a SAM file or a BAM file must be supplied. Galaxy automatically coordinate-sorts all uploaded BAM files. From Picard documentation( http://broadinstitute.github.io/picard/):: MODE=Mode M=Mode Mode of output Default value: VERBOSE. This option can be set to 'null' to clear the default value. Possible values: {VERBOSE, SUMMARY} IGNORE=Type List of validation error types to ignore. Possible values: {INVALID_QUALITY_FORMAT, INVALID_FLAG_PROPER_PAIR, INVALID_FLAG_MATE_UNMAPPED, MISMATCH_FLAG_MATE_UNMAPPED, INVALID_FLAG_MATE_NEG_STRAND, MISMATCH_FLAG_MATE_NEG_STRAND, INVALID_FLAG_FIRST_OF_PAIR, INVALID_FLAG_SECOND_OF_PAIR, PAIRED_READ_NOT_MARKED_AS_FIRST_OR_SECOND, INVALID_FLAG_NOT_PRIM_ALIGNMENT, INVALID_FLAG_SUPPLEMENTARY_ALIGNMENT, INVALID_FLAG_READ_UNMAPPED, INVALID_INSERT_SIZE, INVALID_MAPPING_QUALITY, INVALID_CIGAR, ADJACENT_INDEL_IN_CIGAR, INVALID_MATE_REF_INDEX, MISMATCH_MATE_REF_INDEX, INVALID_REFERENCE_INDEX, INVALID_ALIGNMENT_START, MISMATCH_MATE_ALIGNMENT_START, MATE_FIELD_MISMATCH, INVALID_TAG_NM, MISSING_TAG_NM, MISSING_HEADER, MISSING_SEQUENCE_DICTIONARY, MISSING_READ_GROUP, RECORD_OUT_OF_ORDER, READ_GROUP_NOT_FOUND, RECORD_MISSING_READ_GROUP, INVALID_INDEXING_BIN, MISSING_VERSION_NUMBER, INVALID_VERSION_NUMBER, TRUNCATED_FILE, MISMATCH_READ_LENGTH_AND_QUALS_LENGTH, EMPTY_READ, CIGAR_MAPS_OFF_REFERENCE, MISMATCH_READ_LENGTH_AND_E2_LENGTH, MISMATCH_READ_LENGTH_AND_U2_LENGTH, E2_BASE_EQUALS_PRIMARY_BASE, BAM_FILE_MISSING_TERMINATOR_BLOCK, UNRECOGNIZED_HEADER_TYPE, POORLY_FORMATTED_HEADER_TAG, HEADER_TAG_MULTIPLY_DEFINED, HEADER_RECORD_MISSING_REQUIRED_TAG, INVALID_DATE_STRING, TAG_VALUE_TOO_LARGE, INVALID_INDEX_FILE_POINTER, INVALID_PREDICTED_MEDIAN_INSERT_SIZE, DUPLICATE_READ_GROUP_ID, MISSING_PLATFORM_VALUE, INVALID_PLATFORM_VALUE, DUPLICATE_PROGRAM_GROUP_ID, MATE_NOT_FOUND, MATES_ARE_SAME_END, MISMATCH_MATE_CIGAR_STRING, MATE_CIGAR_STRING_INVALID_PRESENCE} This option may be specified 0 or more times. MAX_OUTPUT=Integer MO=Integer The maximum number of lines output in verbose mode Default value: 100. This option can be set to 'null' to clear the default value. REFERENCE_SEQUENCE=File R=File Reference sequence file, the NM tag check will be skipped if this is missing Default value: null. IGNORE_WARNINGS=Boolean If true, only report errors and ignore warnings. Default value: false. This option can be set to 'null' to clear the default value. Possible values: {true, false} VALIDATE_INDEX=Boolean If true and input is a BAM file with an index file, also validates the index. Default value: true. This option can be set to 'null' to clear the default value. Possible values: {true, false} IS_BISULFITE_SEQUENCED=Boolean BISULFITE=Boolean Whether the SAM or BAM file consists of bisulfite sequenced reads. If so, C->T is not counted as an error in computing the value of the NM tag. Default value: false. This option can be set to 'null' to clear the default value. Possible values: {true, false} ------ 
Additional information
 Additional information about Picard tools is available from Picard web site at http://broadinstitute.github.io/picard/ ."
toolshed.g2.bx.psu.edu/repos/greg/plant_tribes_assembly_post_processor/plant_tribes_assembly_post_processor/1.0.4.0+galaxy0	"This tool is one of the PlantTribes collection of automated modular analysis pipelines for comparative and evolutionary analyses of genome-scale gene families and transcriptomes. This tool post-processes de novo assembled transcripts into putative coding sequences and their corresponding amino acid translations and optionally assigns transcripts to circumscribed gene families (""orthogroups"")[2]. After transcripts have been assigned to gene families, overlapping contigs can be identified and merged to reduce fragmentation in the de novo assembly. ----- 
Required options
 * 
Transcriptome assembly fasta file
 - either de novo or reference-guided transcriptome assembly fasta file selected from your history. * 
Coding regions prediction method
 - method for finding coding regions within transcripts. Available methods are TransDecoder[3] and ESTScan[4]. 
Other options
 * 
Perform targeted gene assembly?
 - selecting 'Yes' enables local assembly of one or more targeted gene families in a specific scaffold. Scaffolds are defined in PlantTribes as clusters of paralogous/orthologous sequences from a specified set of proteomes[5-7]. * 
Targeted gene families
 - select a history item containing a list of targeted orthogroup identifiers corresponding to the gene family classification from a specified scaffold. Gene family identifiers can be obtained from the function annotation table (""Orthogroup ID"" field of .summary file) of scaffold data installed into Galaxy via the PlantTribes Scaffolds Download Data Manager tool, and are also available in the PlantTribes ""annotation"" directory of the scaffold data download. * 
Gene family scaffold
 - one of the PlantTribes gene family scaffolds (installed into Galaxy by the PlantTribes Scaffolds Download Data Manager tool) whose orthogroup(s) are targeted for the localized assembly. * 
Protein clustering method
 - gene family scaffold protein clustering method. Each PlantTribes scaffold data has up to three sets of clusters - GFam[8] (clusters of consensus domain architecture), OrthoFinder[9] (broadly defined clusters) or OrthoMCL[10] (narrowly defined clusters). You can also install your own data scaffold created using a different clustering method as long as it conforms to the PlantTribes scaffold data format. * 
Trim alignments
 - trim gene family multiple sequence alignments that include scaffold backbone genes and locally assembled transcripts to remove non-conserved regions (gappy sites)[11]. The trimmed alignments are used in assigning scores to locally assembled transcripts to determine how well they compare to the backbone gene models. The default setting of 0.1 removes sites that have gaps in 90% or more of the sequences in the multiple sequence alignment. This option is restricted to the range 0.0 - 1.0. * 
Minimum alignment coverage
 - allowable sequence coverage in the orthogroup trimmed protein multiple sequence alignments. Selecting transcripts with coverage of at least the average of the backbone orthogroup gene models is recommended. Details are shown in the targeted gene family assembly statistics history item. * 
Strand-specific assembly?
 - select 'Yes' if transcriptome library sequences were strand-specific. If 'Yes"" is selected, transcripts from the minority strand (antisense) are removed. * 
Remove duplicate sequences?
 - select 'Yes' to remove duplicated and exact subsequences[12]. * 
Minimum sequence length
 - set the minimum sequence length of predicted coding regions. The default is 200 bp."
toolshed.g2.bx.psu.edu/repos/greg/plant_tribes_gene_family_aligner/plant_tribes_gene_family_aligner/1.0.4.0+galaxy0	"This tool is one of the PlantTribes collection of automated modular analysis pipelines for comparative and evolutionary analyses of genome-scale gene families and transcriptomes. This tool estimates protein and codon multiple sequence alignments of integrated orthologous gene family fasta files produced by the GeneFamilyIntegrator tool. ----- 
Required options
 * 
Integrated orthogroup fasta files
 - orthogroup fasta files produced by the GeneFamilyIntegrator tool selected from your history. Depending on how the GeneFamilyClassifier tool was executed, these could either be proteins or proteins and their corresponding coding sequences. * 
Multiple sequence alignment method
 - method for estimating orthogroup multiple sequence alignments. PlantTribes estimates alignments using either MAFFT's L-INS-i algorithm or the divide and conquer approach implemented in the PASTA pipeline for large alignments. - 
PASTA iteration limit
 - number of PASTA iterations. By default, PASTA performs 3 iterations. * 
Codon alignments
 - select 'Yes' to create codon multiple sequence alignments. This option requires both protein and their corresponding coding sequence orthogroup fasta files to be present in the GeneFamilyAligner input data that was produced by the GeneFamilyIntegrator. 
Other options
 * 
Alignment post-processing configuration
 - select 'Yes' to enable multiple sequence alignment post-processing configuration options. - 
Trimming method
 - multiple sequence alignment trimming method. PlantTribes trims alignments using two automated approaches implemented in trimAl. Gap score based trimming removes alignments sites that do not achieve a user specified gap score. For example, a setting of 0.1 removes sites that have gaps in 90% or more of the sequences in the multiple sequence alignment. The automated heuristic trimming approach determines the best automated trimAl method to trim a given alignment as described in the trimAl tutorial 
trimAl
_. - 
Gap score
 - the fraction of sequences with gap allowed in an alignment site. The score is restricted to the range 0.0 - 1.0. Zero value has no effect. - 
Remove sequences
 - select 'Yes' to remove sequences in multiple sequence alignments that do not achieve a user specified alignment coverage score. For example, a setting of 0.7 removes sequences with more than 30% gaps in the alignment. This option requires one of the trimming methods to be set. - 
Coverage score
 - minimum fraction of sites without gaps for a sequence in a multiple sequence alignment. The score is restricted to the range 0.0 - 1.0. Zero value has no effect. - 
Realignment iteration limit
 - number of iterations to perform trimming, removal of sequences, and realignment of orthogroup sequences. Zero value has no effect. * 
Output primary and intermediate alignments
 - selecting 'Yes' will produce a dataset collection of primary and intermediate alignments, the elements of which can be viewed with viaula tools, in addition to the final trimmed and/or filtered alignments dataset collection. .. _trimAl: http://trimal.cgenomics.org"
toolshed.g2.bx.psu.edu/repos/greg/plant_tribes_gene_family_classifier/plant_tribes_gene_family_classifier/1.0.3.0	"This tool is one of the PlantTribes collection of automated modular analysis pipelines for comparative and evolutionary analyses of genome-scale gene families and transcriptomes. This tool classifies gene coding sequences either produced by the AssemblyPostProcessor tool or from an external source into pre-computed orthologous gene family clusters (orthogroups) of a PlantTribes scaffold. Classified sequences are then assigned with the corresponding orthogroups’ metadata that includes gene counts of backbone taxa, super clusters (super orthogoups) at multiple stringencies, and functional annotations from sources such as Gene Ontology (GO), InterPro protein domains, TAIR, UniProtKB/TrEMBL, and UniProtKB/Swiss-Prot. Additionally, sequences belonging to single/low-copy gene families that are mainly utilized in species tree inference can be determined. ----- 
Required options
 * 
Proteins fasta file
 - proteins fasta file either produced by the AssemblyPostProcessor tool or an external source selected from your history. * 
Gene family scaffold
 - one of the PlantTribes gene family scaffolds [2-4] installed into Galaxy by the PlantTribes Scaffold Data Manager tool. * 
Protein clustering method
 - gene family scaffold protein clustering method as described in the AssemblyPostProcessor tool. * 
Protein classifier
 - classifier to assign protein sequences into a specified scaffold orthogroups. PlantTribes implements three classification approaches; blastp (faster)[5], hmmscan (slower but more sensitive assignment of divergent homologs)[6], and both blastp and hmmscan (disagreements resolved in favor of hmmscan; more exhaustive). 
Other options
 * 
Super orthogroups configuration
 - select ‘Yes’ to enable super orthogroups configuration options. Super orthogroups[7] are constructed through a second iteration of MCL clustering to connect distant, but potentially related orthogroup clusters. * 
Clustering distance measure
 - distance measure used in merging orthogroup clusters into super orthogroup clusters. PlantTribes pre-computed super orthogroups are based on the minimum and average blastp e-value between all pairs of scaffold orthogroups used as the input matrix for MCL clustering algorithm[8]. * 
Single copy orthogroups configuration
 - select ‘Yes’ to enable single/low-copy orthogroups selection configuration options. * 
Selection criterion
 - single/low-copy orthogroups selection criterion. PlantTribes provides custom and global selection criteria for selecting user-defined single/low-copy scaffold orthogoups. * 
Global selection configuration
 - the upper limit values of the following two parameters vary depending on the selected gene family scaffold, and the tool will produce an error if the value exceeds the number of species in the circumscribed scaffold. * 
Minimum single copy taxa
 - minimum number of taxa with single copy genes in the orthogroup. * 
Minimum taxa present
 - minimum number of taxa present in the orthogroup. * 
Custom selection configuration
 - select ‘Yes’ to enable selection of a single copy configuration file. Scaffold configuration templates (.singleCopy.config) of how to customize single/low-copy orthogroups selection can be found in the scaffold data installed into Galaxy via the PlantTribes Scaffolds Download Data Manager tool, and also available at the PlantTribes GitHub 
repository
_. Single/low-copy settings shown in these templates are used as defaults if ‘No’ is selected. * 
Custom selection file
 - select a single/low-copy customized configuration file from your history. * 
Orthogroups fasta configuration
 - select ‘Yes’ to create proteins orthogroups fasta files for the classified sequences. * 
Orthogroups coding sequences
 - select ‘Yes’ to create corresponding coding sequences orthogroup fasta files for the classified protein sequences. Requires coding sequences fasta file corresponding to the proteins fasta file to be selected from your history. * 
Coding sequences fasta file
 - select coding sequences fasta file corresponding to the proteins fasta file from your history. .. _repository: https://github.com/dePamphilis/PlantTribes/tree/master/config"
toolshed.g2.bx.psu.edu/repos/greg/plant_tribes_gene_family_integrator/plant_tribes_gene_family_integrator/1.0.3.0	"This tool is one of the PlantTribes collection of automated modular analysis pipelines for comparative and evolutionary analyses of genome-scale gene families and transcriptomes. This tool integrates PlantTribes scaffold orthogroup backbone gene models with gene coding sequences classified into the scaffold by the GeneFamilyClassifier tool. ----- 
Required options
 * 
Classified orthogroup fasta files
 - orthogroup fasta files produced by the GeneFamilyClassifier tool selected from your history. Depending on how the GeneFamilyClassifier tool was executed, these could either be proteins or proteins and their corresponding coding sequences. * 
Gene family scaffold
 - one of the PlantTribes gene family scaffolds installed into Galaxy by the PlantTribes Scaffold Data Manager tool. * 
Protein clustering method
 - gene family scaffold protein clustering method as described in the AssemblyPostProcessor tool."
toolshed.g2.bx.psu.edu/repos/greg/plant_tribes_gene_family_phylogeny_builder/plant_tribes_gene_family_phylogeny_builder/1.0.3.1	"This tool is one of the PlantTribes collection of automated modular analysis pipelines for comparative and evolutionary analyses of genome-scale gene families and transcriptomes. This tool performs gene family phylogenetic inference of multiple sequence alignments produced by the GeneFamilyAligner tool. ----- 
Required options
 * 
Orthogroup alignments
 - orthogroup alignment fasta files produced by the GeneFamilyAligner tool selected from your history. Depending on how the GeneFamilyAligner tool was executed, these could either be pre-processed alignments, trimmed alignments or both trimmed and filtered alignments. * 
Phylogenetic inference method
 - method for estimating orthogroup maximum likelihood (ML) phylogenetic trees. PlantTribes estimates ML phylogenetic trees using either RAxML or FastTree algorithms. - 
Gene family scaffold
 - one of the PlantTribes gene family scaffolds installed into Galaxy by the PlantTribes Scaffold Download Data Manager tool. This is used only if RAxML is selected as the phylogenetic inference method. - 
Protein clustering method
 - gene family scaffold protein clustering method as described in the AssemblyPostProcessor tool. This is used only if RAxML is selected as the phylogenetic inference method. 
Other options
 * 
Rooting order configuration
 - select 'Yes' to enable selection of a rooting order configuration file for RAxML. Scaffold configuration templates (.rootingOrder.config) of how to customize the RAxML ML tree rooting order can be found in the scaffold data installed into Galaxy via the PlantTribes Scaffolds Download Data Manager tool, and is also available at the PlantTribes GitHub 
repository
_. Phylogenetic tree rooting order settings shown in these templates are used as defaults if 'No' is selected. * 
Bootstrap replicates
 - number of bootstrap replicates for RAxML to conduct a rapid bootstrap analysis and search for the best-scoring ML tree (default = 100). * 
Maximum orthogroup size
 - maximum number of sequences allowed in orthogroup alignments (default = 100). * 
Minimum orthogroup size
 - minimum number of sequences allowed in orthogroup alignments (default = 4). .. _repository: https://github.com/dePamphilis/PlantTribes/tree/master/config"
toolshed.g2.bx.psu.edu/repos/greg/plant_tribes_kaks_analysis/plant_tribes_kaks_analysis/1.0.4.0	"This tool is one of the PlantTribes collection of automated modular analysis pipelines for comparative and evolutionary analyses of genome-scale gene families and transcriptomes. This tool estimates paralogous and orthologous pairwise synonymous (Ks) and non-synonymous (Ka) substitution rates for a set of gene coding sequences either produced by the AssemblyPostProcessor tool or from an external source. Optionally, the resulting set of estimated Ks values can be clustered into components using a mixture of multivariate normal distributions to identify significant duplication event(s) in a species or a pair of species. ----- * 
Required options
 - 
Coding sequences for the first species
 - coding sequence fasta file for the first species either produced by the AssemblyPostProcessor tool or from an external source selected from your history. - 
Protein sequences for the first species
 - corresponding protein sequence fasta files for the first species either produced by the AssemblyPostProcessor tool or from an external source selected from your history. - 
Type of sequence comparison
 - pairwise sequence comparison to determine homologous pairs. This can be either paralogous for self-species comparison or orthologous for cross-species comparison. Cross-species comparison requires input for the second species. * 
Other options
 - 
Coding sequences for the second species
 - coding sequence fasta file for the second species either produced by the AssemblyPostProcessor tool or from an external source selected from your history. This option is required only for orthologous comparison. - 
Protein sequences for the second species
 - corresponding protein sequence fasta files for the second species either produced by the AssemblyPostProcessor tool or from an external source selected from your history. This option is required only for orthologous comparison. - 
Determine for cross-species orthologs using
 - select option for blast orthology. - 
reciprocal best BLAST
 - use the default stringent reciprocal BLAST package for orthology assignment. - 
conditional reciprocal best BLAST
 - use the CRB-BLAST package for orthology assignment which increases sensitivity to orthology comparisons and determines additional cross-species orthologs that are being left out by the defaul stringent reciprocal BLAST. - 
Alignment coverage configuration
 - select 'Yes' to set the minimum allowable alignment coverage length between homologous pairs. PlantTribes uses global codon alignment match score to determine the pairwise alignment coverage. By default, the match score is set to 0.5 if 'No' is selected. - 
match score
 - number of base matches in a pairwise sequence alignment divided by the length of shorter sequence. Positions in the alignment corresponding to gaps are not considered. The score is restricted to the range 0.3 - 1.0. - 
Species rates recalibration configuration
 - select 'Yes' to recalibrate synonymous substitution rates of a species using a predetermined evolutionary rate. Recalibration evolutionary rate can be determined from a species tree inferred from a collection of conserved single copy genes from taxa of interest as described in [7]. Rate recalibration applies only to paralogous comparisons. - 
recalibration rate
 - a predetermined evolutionary recalibration rate. - 
PAML codeml configuration
 - select 'Yes' to enable selection of a PAML codeml control file to carry out maximum likelihood analysis of protein-coding DNA sequences using codon substitution models. Template file ""codeml.ctl.args"" can be found in the scaffold data installed into Galaxy via the PlantTribes Scaffolds Download Data Manager tool, and are also available at the PlantTribes GitHub 
repository
_. Default settings shown in the template are used if 'No' is selected. - 
Rates clustering configuration
 - select 'Yes' to estimate clusters of synonymous substitution rates using a mixture of multivariate normal distributions which represent putative duplication event(s). - 
Number of components
 - number of components to include in the normal mixture model. - 
Lower limit synonymous substitution rates configuration
 - select 'Yes' to set the minimum allowable synonymous substitution rate to use in the normal mixtures cluster analysis to exclude young paralogs that arise from normal gene births and deaths in a genome. - 
Minimum rate
 - minimum allowable synonymous substitution rate. - 
Upper limit synonymous substitution rates configuration
 - select 'Yes' to set the maximum allowable synonymous substitution rate to use in the normal mixtures cluster analysis to exclude likely ancient paralogs in a genome. - 
Maximum rate
 - maximum allowable synonymous substitution rate. .. _repository: https://github.com/dePamphilis/PlantTribes/blob/master/config/codeml.ctl.args"
toolshed.g2.bx.psu.edu/repos/greg/plant_tribes_ks_distribution/ks_distribution/1.0.3.0	"What it does
 This tool is one of the PlantTribes collection of automated modular analysis pipelines for comparative and evolutionary analyses of genome-scale gene families and transcriptomes. This tool uses the analysis results produced by the KaKsAnalysis tool to plot the distribution of synonymous substitution (Ks) rates and fit the estimated significant normal mixtures component(s) onto the distribution. ----- 
Options
 * 
Synonymous substitution rates
 - estimated synonymous substitution (Ks) rates output file produced by the KaKsAnalysis tool selected from your history. * 
Synonymous components
 - estimated significant component(s) output file produced by the KaKsAnalysis tool selected from your history. * 
Choose colors for significant components
 - select 'Yes' to specify component colors or 'No' for colors chosen randlomly. * 
Component colors
 - select a color from the palette for each component (colors will be chosen randomly for unspecified components)."
toolshed.g2.bx.psu.edu/repos/guerler/dbkit/dbkit_create/0.1.1	This database creation tool downloads entries from an external resource and creates a datasets with all its contents. Additionally an index file is generated indicating the start and size of every entry within the database file. The tool can also operate on collections.
toolshed.g2.bx.psu.edu/repos/guerler/ffindex_dbkit_create/ffindex_dbkit_create/0.2+galaxy0	"What it does
 This database creation tool downloads entries from an external resource and creates a datasets with all its contents. Additionally an index file is generated indicating the start and size of every entry within the database file. The tool can also operate on collections."
toolshed.g2.bx.psu.edu/repos/guerler/dbkit/dbkit_extract/0.1.1	This database creation tool merges two pairs of ffindex/ffdata entries into a single ffindex/ffdata pair.
toolshed.g2.bx.psu.edu/repos/guerler/ffindex_dbkit_extract/ffindex_dbkit_extract/0.2+galaxy0	"What it does
 This database creation tool merges two pairs of ffindex/ffdata entries into a single ffindex/ffdata pair."
toolshed.g2.bx.psu.edu/repos/guerler/dbkit/dbkit_merge/0.1.1	This database creation tool merges two pairs of ffindex/ffdata entries into a single ffindex/ffdata pair.
toolshed.g2.bx.psu.edu/repos/guerler/ffindex_dbkit_merge/ffindex_dbkit_merge/0.2+galaxy0	"What it does
 This database creation tool merges two pairs of ffindex/ffdata entries into a single ffindex/ffdata pair."
toolshed.g2.bx.psu.edu/repos/guerler/hhsearch/hhsearch/3.2.0+galaxy0	"What it does
 HHsearch aligns a profile HMM against a database of target profile HMMs. The search first aligns the query HMM with each of the target HMMs using the Viterbi dynamic programming algorithm, which finds the alignment with the maximum score. The E-value for the target HMM is calculated from the Viterbi score. Target HMMs that reach sufficient significance to be reported are realigned using the Maximum Accuracy algorithm (MAC). This algorithm maximizes the expected number of correctly aligned pairs of residues minus a penalty between 0 and 1. Values near 0 produce greedy, long, nearly global alignments, values above 0.3 result in shorter, local alignments. HHblits is an accelerated version of HHsearch that is fast enough to perform iterative searches through millions of profile HMMs, e.g. through the Uniclust profile HMM databases, generated by clustering the UniProt database into clusters of globally alignable sequences. Analogously to PSI-BLAST and HMMER3, such iterative searches can be used to build MSAs by starting from a single query sequence. Sequences from matches to profile HMMs below some E-value threshold (e.g. 10−3) are added to the query MSA for the next search iteration. Download databases from: http://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/"
toolshed.g2.bx.psu.edu/repos/guerler/spring_cross/spring_cross/0.2+galaxy0	"What it does
 Creates a 2-column cross reference between a list of input chains and all interacting chains found in the provided PDB database. This reference is required as input for the SPRING min-Z calculator."
toolshed.g2.bx.psu.edu/repos/guerler/spring_mcc/spring_mcc/0.2+galaxy0	"What it does
 This tool generates a MCC plot for a given 2-column tabular prediction file containing UniProt Accession codes. The prediction is compared to a given BioGRID database file in TAB 3.0 format. Non-interacting protein pairs are randomly sampled."
toolshed.g2.bx.psu.edu/repos/guerler/spring_map/spring_map/0.2+galaxy0	"What it does
 Creates additional columns for the SPRING cross reference containing the chain identifier of the homologues from the template library."
toolshed.g2.bx.psu.edu/repos/guerler/spring_minz/spring_minz/0.2+galaxy0	"What it does
 This tool filters HH-search/HH-blits homology results through the protein interaction cross reference generated by SPRING. Putative interactions are identified by evaluating the min-Z score. The min-Z is the smaller of the two Z-scores for a pair of sequences matching an existing protein-protein complex structure."
toolshed.g2.bx.psu.edu/repos/guerler/spring_model/spring_model/0.2+galaxy0	"What it does
 Creates protein complex model from HHsearch threading results."
toolshed.g2.bx.psu.edu/repos/guerler/spring_model_all/spring_model_all/0.2+galaxy0	"What it does
 Creates protein complex model from HHsearch threading results."
toolshed.g2.bx.psu.edu/repos/iuc/colabfold_alphafold/colabfold_alphafold/1.5.5+galaxy1	Generate run a folding step on the output of the colabfold MSA run
toolshed.g2.bx.psu.edu/repos/iuc/colabfold_msa/colabfold_msa/1.5.5+galaxy1	Generate MSAs for the alphafold step of Colabfold
toolshed.g2.bx.psu.edu/repos/galaxyp/custom_pro_db/custom_pro_db/1.22.0	"Description
 Generate custom protein FASTAs from exosome or transcriptome data. The reference protein set can be filtered by transcript expression level (RPKM calculated from a BAM file), and variant protein forms can be predicted based on variant calls (SNPs and INDELs reported in a VCF file). 
Annotations
 CustomProDB depends on a set of annotation files (in RData format) to create reference and variant protein sequences. Galaxy administrators can use the customProDB data manager to create these annotations to make them available for users."
toolshed.g2.bx.psu.edu/repos/galaxyp/openms_decoydatabase/DecoyDatabase/3.1+galaxy0	Create decoy sequence database from forward sequence database. For more information, visit https://openms.de/doxygen/release/3.1.0/html/TOPP_DecoyDatabase.html
toolshed.g2.bx.psu.edu/repos/galaxyp/encyclopedia_quantify/encyclopedia_quantify/1.12.34+galaxy0	"mzML conversion from RAW requires special options: msconvert --zlib --64 --mzML --simAsSpectra --filter ""peakPicking true 1-"" --filter ""demultiplex optimization=overlap_only"" *.raw"
toolshed.g2.bx.psu.edu/repos/galaxyp/openms_falsediscoveryrate/FalseDiscoveryRate/3.1+galaxy0	Estimates the false discovery rate on peptide and protein level using decoy searches. For more information, visit https://openms.de/doxygen/release/3.1.0/html/TOPP_FalseDiscoveryRate.html
toolshed.g2.bx.psu.edu/repos/galaxyp/peptideshaker/fasta_cli/4.0.41+galaxy1	"What it does
 Appends decoy sequences to FASTA files. Default format is adequated to be used by SearchGUI and PeptideShaker tools, ie: * Decoy flag: _REVERSED * Location: suffix * Target decoy suffix: _concatenated_target_decoy"
toolshed.g2.bx.psu.edu/repos/galaxyp/openms_featurefindermultiplex/FeatureFinderMultiplex/3.1+galaxy0	Determination of peak ratios in LC-MS data For more information, visit https://openms.de/doxygen/release/3.1.0/html/TOPP_FeatureFinderMultiplex.html
toolshed.g2.bx.psu.edu/repos/galaxyp/fragpipe/fragpipe/23.0+galaxy2	MSFragger is available freely for academic research and educational purposes only. I have read the ACADEMIC license for MSFragger software: http://msfragger-upgrader.nesvilab.org/upgrader/LICENSE-ACADEMIC.pdf. This license provides with non-exclusive, non-transferable right to use MSFragger solely for academic research, non-commercial or educational purposes. I agree to be subject to the terms and conditions of this license. I understand that to use MSFragger for other purposes requires a commercial license from the University of Michigan Office of Tech Transfer. <br/><br/> IonQuant is available freely for academic research and educational purposes only. I have read the ACADEMIC license for MSFragger software: https://msfragger-upgrader.nesvilab.org/ionquant/LICENSE-ACADEMIC.pdf <br/><br/> I agree to the terms of Thermo (c) Raw File Reader License Agreement: http://msfragger-upgrader.nesvilab.org/upgrader/RawFileRdr_License_Agreement_RevA.pdf <br/><br/> I agree to the terms of Bruker SDK library distribution conditions: http://msfragger-upgrader.nesvilab.org/upgrader/EULA%20TDF-SDK.pdf
toolshed.g2.bx.psu.edu/repos/galaxyp/fragpipe/fragpipe_manifest_generator/23.0+galaxy2	"Generates a 
manifest file
 that may be used as input for the FragPipe Galaxy tool, or headless FragPipe_. This file is analagous to an experimental design file. The tool takes as input a collection of scan files, or multiple collections using the 
Insert Additional Scan Groups
 parameter, and options for assigning experiment numbers, bioreplicates, and data types for each file. Each scan group will have values from three columns applied to it using different methods. - Assign consecutive integers: The scans will be number consecutively starting with 1. - Enter column values: The column values for each scan file are entered as a comma-delimited list in the same order as the files. - Assign to all scan files: A value supplied by the user is applied to all files. .. _FragPipe: https://fragpipe.nesvilab.org/docs/tutorial_headless.html"
toolshed.g2.bx.psu.edu/repos/galaxyp/openms_idmapper/IDMapper/3.1+galaxy0	Assigns protein/peptide identifications to features or consensus features. For more information, visit https://openms.de/doxygen/release/3.1.0/html/TOPP_IDMapper.html
toolshed.g2.bx.psu.edu/repos/galaxyp/peptideshaker/ident_params/4.0.41+galaxy1	"What it does
 Creates a parameters file (.par) which can be used independently by SearchGUI or PeptideShaker apps. A FASTA file with decoy sequences generated by FastaCLI is recommended if SearchGUI and PeptideShaker are going to use the output of this tool."
toolshed.g2.bx.psu.edu/repos/galaxyp/openms_msgfplusadapter/MSGFPlusAdapter/3.1+galaxy0	MS/MS database search using MS-GF+. For more information, visit https://openms.de/doxygen/release/3.1.0/html/TOPP_MSGFPlusAdapter.html
toolshed.g2.bx.psu.edu/repos/galaxyp/msstats/msstats/4.0.0+galaxy1	The processing tools report missing values differently. This option is for distinguish which value should be considered as missing, and further whether it is censored or at random. Skyline and OpenSWATH input should use '0'. MaxQuant input should use 'NA'
toolshed.g2.bx.psu.edu/repos/galaxyp/msstatstmt/msstatstmt/2.0.0+galaxy1	Heatmap requires more than one comparison
toolshed.g2.bx.psu.edu/repos/galaxyp/maxquant/maxquant/2.0.3.0+galaxy0	"MaxQuant is a quantitative proteomics software package designed for analyzing large mass-spectrometric data sets. This tool is a wrapper for MaxQuant v2.0.3.0. The current version of the wrapper only supports a reduced set of parameters, but another version of the tool that gets its parameters directly from a mqpar.xml file is available, too. 
Input files
 - Thermo raw, mzML, or mzXML files (in parameter group section) - The datatype of all files has to be either 'thermo.raw', 'mzml' or 'mzxml'. Make sure to specify the correct datatype either during upload to Galaxy or afterwards (edit attributes --> datatypes) - Fasta file: specify parse rules according to your fasta file (header). Some examples for different fasta headers: :: identifier parse rule description parse rule Uniprot identifier >.
\|(.
)\| >.
\|.
\|[^ ]+ (.
) OS NCBI accession >(gi\|[0-9]
) IPI accession >IPI:([^\| .]
) Everything after '>' >(.
) Up to first space >([^ ]
) Up to first tab character >([^\t]
) - Optional files: - Tabular file with experimental design template: - Currently four columns are needed: Name, Fraction, Experiment and PTM. The headers must have this exact naming. Name and Experiment are abitrary strings, Fraction is an integer or emtpy, PTM is either 'True', 'False' or empty. Consider you uploaded files named File1.mzxml, ..., File5.mzxml. Thermo RAW files are handled differently (see table below). This is a (syntactically) correct experimental design template: :: Name Fraction Experiment PTM File1 1 E1 False File2 2 E1 False ghost 234 none File3 3 E1 False File4 E2 True File5 E1 - This is the counter-example with one error per line: :: Name Fraction Experiment PTM File1 1 E1 no (wrong PTM value) File2.mzxml 1 E2 (filename with extension) File3 f3 E1 (fraction not an integer) File4 1 (missing experiment) (File5 missing) - However, “.raw” (Thermo) is considered a part of the filename: :: Name Fraction Experiment PTM File1.raw 1 E1 False File2.raw 2 E1 False File3.raw 3 E1 False File4.raw 1 E2 False File5.raw 2 E2 False File6.raw 3 E2 False 
Parameter Options
 - Quantitation methods (in section parameter groups) - label free (LFQ): Protein intensity will be reported as 'LFQ intensity' columns in the proteinGroups table - label based: quantifies MS1 labelled samples ('SILAC', 'Dimethyl', 'ICAT', 'ICPL', 'mTRAQ', '18 O') - for two channels: choose options from light and heavy sections - for three channels: choose options from light, medium and heavy sections - reporter ion ms2/ms3: quantifies conventional isobaric labelling samples. Either use the pre-defined labellings with correction factors set to 0 or specify a custom labelling - PTXQC quality control: quality control software creates an automatic quality control pdf report 
Output files
 Different output file options are available, most of them are part of the MaxQuant txt directory."
toolshed.g2.bx.psu.edu/repos/galaxyp/maxquant/maxquant_mqpar/2.0.3.0+galaxy0	"MaxQuant is a quantitative proteomics software package designed for analyzing large mass-spectrometric data sets. This tool is a wrapper for MaxQuant v2.0.3.0. It gets its search parameters from a previously created parameter file (mqpar.xml). A similiar tool that allows the specification of search parameters directly through galaxy is available as well and should be preferred, if possible. 
Input files
 - Thermo raw file or mzXML file - The datatype has to be 'thermo.raw' or 'mzxml'. Make sure to specify the correct datatype either during upload to Galaxy or afterwards (edit attributes --> datatypes) - mqpar.xml: - MaxQuant parameters will be taken from the provided mqpar.xml file. This parameter file MUST be created using the same version of MaxQuant as is used by this tool. The correct version of MaxQuant can be obtained via the bioconda channel for the conda package manager. 
Output files
 Different output file options are available, most of them are part of the MaxQuant txt folder. An additional mztab output option is implemented."
toolshed.g2.bx.psu.edu/repos/galaxyp/metanovo/metanovo/1.9.4+galaxy4	"MetaNovo
 MetaNovo searches MS/MS data against a FASTA database of known proteins. Two outputs are produced: - MetaNovo Output FASTA: the matching proteins produced by the search. - MetaNovo Output CSV: information about the job and other useful metadata. Two inputs are required: an MGF file or files and a FASTA database file. Two different input types are available for the MGF input. The correct input configuration depends on the desired use case, as outlined below: ======================================================= ============= Use case Configuration ======================================================= ============= Single input MGF file, single output FASTA file 
Single file
 input with 
Single dataset
 selected Multiple input MGF files, multiple output FASTA files* 
Single file
 input with 
Multiple datasets
 OR 
Dataset collection
 selected Multiple input MGF files, single output FASTA file 
Collection
 input ======================================================= ============= 
*
 One for each MGF file. In the second use case, a separate MetaNovo job is spawned for each input MGF. In the third use case, a single MetaNovo job runs with all MGF files in the collection as input. If the third use case fails due to memory limitations, users are recommended to use the second option. The multiple output FASTA databases may be merged to generate a reduced, compact database."
toolshed.g2.bx.psu.edu/repos/galaxyp/openms_metaprosip/MetaProSIP/3.1+galaxy0	Performs proteinSIP on peptide features for elemental flux analysis. For more information, visit https://openms.de/doxygen/release/3.1.0/html/TOPP_MetaProSIP.html
toolshed.g2.bx.psu.edu/repos/galaxyp/pep_pointer/pep_pointer/0.1.3+galaxy1	"PepPointer
 Given chromosomal locations of peptides in a BED file, PepPointer classifies them as CDS, UTR, exon, intron, or intergene."
toolshed.g2.bx.psu.edu/repos/galaxyp/pepquery/pepquery/1.6.2+galaxy1	"PepQuery
 PepQuery_ is a peptide-centric search engine for novel peptide identification and validation. Cancer genomics studies have identified a large number of genomic alterations that may lead to novel, cancer-specific protein sequences. Proteins resulted from these genomic alterations are attractive candidates for cancer biomarkers and therapeutic targets. The leading approach to proteomic validation of genomic alterations is to analyze tandem mass spectrometry (MS/MS) data using customized proteomics databases created from genomics data. Such analysis is time-consuming and requires thorough training and detailed knowledge in proteomics data analysis, leading to a gap between MS/MS data and the cancer genomics community. PepQuery does not require customized databases and allows quick and easy proteomic validation of genomic alterations. 
Inputs
 - A sequence to match, one of the following: - A peptide string or a history dataset with a list of peptides - A protein string or a history dataset with a protein fasta - A DNA string that is at least 60 base pairs in length - A mass spectrometry MGF file - A reference protein fasta database, peptides matching a reference sequence will be excluded. - An optional tags file for no-enzyme immunopeptidomics search - See: http://pepquery.org/data/PepQuery_for_immunopeptidomics_data.pdf 
Outputs
 - PSM annotation - tabular with columns: - peptide Query calc_mr observed_mz charge pepSeq m_label m_mz m_intensity mz intensity - Detail - tabular with columns: - 
report_spectrum_file
 spectrum_title peptide modification pep_mass score - PSM - tabular with columns: - peptide modification n 
report_spectrum_file
 spectrum_title charge exp_mass ppm pep_mass mz score n_db total_db n_random total_random pvalue - PSM Rank - tabular with columns: - peptide modification n 
report_spectrum_file
 spectrum_title charge exp_mass ppm pep_mass mz score n_db total_db n_random total_random pvalue rank 
n_ptm
 - An MGF with the best matching spectrums The 
report_spectrum_file
 is an optional field that can be added. The 
n_ptm
 field is added when using unrestricted modification searching (-um). .. _PepQuery: http://pepquery.org/document.html"
toolshed.g2.bx.psu.edu/repos/galaxyp/pepquery2/pepquery2/2.0.2+galaxy2	Peptide sequence file containing peptides which you want to search (no column headers). First column is am peptide sequence. Optional second column is spectrum title.
toolshed.g2.bx.psu.edu/repos/galaxyp/pepquery2_show_sets/pepquery2_show_sets/2.0.2+galaxy2	"Show available: PepQueryDB Datasets, Parameter Sets, and PTMs
 
(Post Translational Modifications)
 http://pepquery.org/document.html 
PepQueryDB Datasets
 - Shows a table of all the indexed MS/MS datasets available in PepQueryDB. - 
java -jar pepquery-2.0.2.jar -b show_full
 - These datasets can be used for the 
-b
 option in 
PepQuery
. - The parameter_set value can be used in the 
-p
 option in 
PepQuery
. - Columns: NO. dataset_name short_name parameter_set species data_type n_spectra n_ms_file data_link 
PepQuery Predefined Parameter Sets
 - Shows the predefined Parameter Set Names with the option settings - 
java -jar pepquery-2.0.2.jar -p show
 - The parameterset names can be used for the 
-p
 option in 
PepQuery
. 
PepQuery Modifications
 - Shows a table of the PTMs available - 
java -jar pepquery-2.0.2.jar -printPTM
 - The mod_id numbers can be used in the 
-fixMOD
 and 
-varMOD
 options in 
PepQuery
. - Columns: mod_id mod_name mod_mass mod_type mod_category unimod_accession"
toolshed.g2.bx.psu.edu/repos/galaxyp/pepquery2_index/pepquery2_index/2.0.2+galaxy2	"PepQuery Index
 Indexes MS/MS data for fast searching. This can significant speed up the search especially when the size of MS/MS data is very large. The output can be used as input to 
PepQuery2
 Galaxy tool in the 
MS/MS dataset to search
 
Indexed MS/MS spectrums
."
toolshed.g2.bx.psu.edu/repos/galaxyp/peptide_genomic_coordinate/peptide_genomic_coordinate/1.0.0	"Peptide Genomic Coodinate
 Gets genomic coordinate of peptides based on the information in mzsqlite and genomic mapping sqlite files. This tool is useful in a proteogenomics workflow. This program loads two sqlite databases (mzsqlite and genomic mapping sqlite files) and calculates the genomic coordinates of the peptides provided as input. This outputs bed file for peptides. Input: Peptide list file, mzsqlite sqlite DB file, and genomic mapping sqlite DB file Output: Tabular BED file with all the columns"
toolshed.g2.bx.psu.edu/repos/galaxyp/peptideshaker/peptide_shaker/2.0.33+galaxy1	"What it does
 PeptideShaker is a search engine for interpretation of proteomics identification results from multiple search engines, currently supporting X!Tandem, MS-GF+, MS Amanda, OMSSA, MyriMatch, Comet, Tide, Mascot, Andromeda and mzIdentML. http://compomics.github.io/projects/peptide-shaker.html ---- Outputs ======= 
zip
 ------ When choosing zip option, all other outputs are compressed and included into the zip file and, by default, no other file is shown in the Galaxy history. This last behaviour may be changed in order to have all other outputs also available in the history. 
psdb
 ------ psdb is the native format used by PeptideShaker. It contains all the information required by PeptideShaker to run. 
mzIdentML
 ----------- PeptideShaker can load results from virtually any identification algorithm in the mzIdentML format as long as the minimal peptide to spectrum match information is present in the file. The following is required: • Spectrum file format has to be mgf. • Each PSM has a score or e-value as a PSM score CV term. 
Follow-up analysis
 -------------------- Spectra ''''''' Exports the spectra according to the category of the PSMs: • Spectra of Non-Validated PSMs • Spectra of Non-Validated Peptides • Spectra of Non-Validated Proteins • Spectra of Validated PSMs • Spectra of Validated PSMs of Validated Peptides • Spectra of validated PSMs of Validated Peptides of Validated Proteins. Exported results are in mgf format. Proteins: Accession numbers ''''''''''''''''''''''''''' Export the protein accessions according to the category of the proteins: • Main Accession of Validated Protein Groups • All Accessions of Validated Protein Groups • Non-Validated Accessions Exported protein accessions are text format as TXT. Proteins: Sequences ''''''''''''''''''' Exports the protein details according to a category of proteins: • Main Accession of Validated Protein Groups • All Accessions of Validated Protein Groups • Non-Validated Accessions. Exported protein details are in fasta format. Proteins: Proteoforms ''''''''''''''''''''' Exports all possible proteoforms of the validated proteins. Exported results are in text format as TXT. Results file can be easily imported and used by PathwayMatcher_ tool. .. _PathwayMatcher: https://toolshed.g2.bx.psu.edu/repository?repository_id=6d75f02b86acc421 Label free quantification ''''''''''''''''''''''''' Exports the spectra from different categories of PSMs according to the export type: • Validated PSMs of Validated Peptides of Validated Proteins • Validated PSMs of Validated Peptides • Validated PSMs • Confidently localized PTMs of Validated PSMs of Validated Peptides of Validated Proteins Exported identification results are in Progenesis LC-MS compatible format as TXT. Inclusion/Exclusion list '''''''''''''''''''''''' Exports an inclusion list of validated hits. Inclusion list may be filtered according to peptide types: • Miscleaved Peptides • Reactive Peptides • Degenerated Peptides. and also according to protein inferences: • Related Proteins • Related and Unrelated Proteins • Unrelated Proteins. Finally, a retention time window may be established as a final filter. Exported results can be in Thermo, ABI, Bruker or MassLynx formats. 
Identification Features Reports
 --------------------------------- PSM Report '''''''''' ============================= ======================================== Column Description ============================= ======================================== Protein(s) Protein(s) to which the peptide can be attached Sequence Sequence of the peptide AAs Before The amino acids before the sequence AAs After The amino acids after the sequence Position Position of the peptide in the protein sequence(s). Modified Sequence The amino acids sequence annotated with variable modifications. Variable Modifications The variable modifications Fixed Modifications The fixed modifications. Spectrum File The spectrum file. Spectrum Title The title of the spectrum. Spectrum Scan Number The spectrum scan number. RT Retention time m/z Measured m/z Measured Charge The charge as given in the spectrum file. Identification Charge The charge as inferred by the search engine. Theoretical Mass The theoretical mass of the peptide. Isotope Number The isotope number targetted by the instrument. Precursor m/z Error [ppm] The precursor m/z matching error. Localization Confidence The confidence in variable PTM localization. probabilistic PTM score The probabilistic score (e.g. A-score or PhosphoRS) used for variable PTM localization. D-score D-score for variable PTM localization Confidence Confidence in percent associated to the retained PSM. Validation Indicates the validation level of the protein group. ============================= ======================================== PSM Report with non-validated matches ''''''''''''''''''''''''''''''''''''' Same columns as the original PSM report but also includes non-validated matches. PSM Phosphorylation Report '''''''''''''''''''''''''' ============================= ========================================================= Column Description ============================= ========================================================= Protein(s) Protein(s) to which the peptide can be attached Sequence Sequence of the peptide Variable Modifications The variable modifications Fixed Modifications The fixed modifications. Spectrum File The spectrum file. Spectrum Title The title of the spectrum. Spectrum Scan Number The spectrum scan number. RT Retention time m/z Measured m/z Measured Charge The charge as given in the spectrum file. Identification Charge The charge as inferred by the search engine. Theoretical Mass The theoretical mass of the peptide. Isotope Number The isotope number targetted by the instrument. Precursor m/z Error [ppm] The precursor m/z matching error. Localization Confidence The confidence in variable PTM localization. probabilistic PTM score The probabilistic score (e.g. A-score or PhosphoRS) used for variable PTM localization. D-score D-score for variable PTM localization Confident Phosphosites List of the sites where a phosphorylation was confidently localized. #Confident Phosphosites Number of confidently localized phosphorylations. Ambiguous Phosphosites List of the sites where a phosphorylation was ambiguously localized. #Ambiguous Phosphosites Number of ambiguously localized phosphorylations. Confidence [%] Confidence in percent associated to the retained PSM. Validation Indicates the validation level of the protein group. ============================= ========================================================= Extended PSM Report ''''''''''''''''''' Same as the ordinary PSM report but adds a 
Decoy
 column and lacks the 
Confidence
 column. ============================= ========================================================= Extra column Description ============================= ========================================================= Decoy Indicates whether the peptide is a decoy (1: yes, 0: no). ============================= ========================================================= Peptide Report '''''''''''''' ========================================== ===================================================== Column Description ========================================== ===================================================== Protein(s): Protein(s) to which this peptide can be attached. Protein Group(s) List of identified protein groups this peptide can map to with associated validation level. #Validated Protein Group(s) Indicates the number of protein groups this peptide maps to. Unique Group Indicates whether the peptide maps to a unique protein group. Sequence Sequence of the peptide. Modified Sequence The peptide sequence annotated with variable modifications. Position Position of the peptide in the protein sequence(s). AAs Before The amino acids before the sequence. AAs After The amino acids after the sequence. Variable Modifications The variable modifications. Fixed Modifications The fixed modifications. Localization Confidence The confidence in PTMs localization. #Validated PSMs Number of validated PSMs. #PSMs Number of PSMs. Confidence [%] Confidence in percent associated to the peptide. Validation Indicates the validation level of the peptide. ========================================== ===================================================== Peptide Report with non-validated matches ''''''''''''''''''''''''''''''''''''''''' Same columns as the original Peptide report but also includes non-validated matches. Peptide Phosphorylation Report '''''''''''''''''''''''''''''' ========================================== ===================================================== Column Description ========================================== ===================================================== Protein(s): Protein(s) to which this peptide can be attached. Protein Group(s) List of identified protein groups this peptide can map to with associated validation level. #Validated Protein Group(s) Indicates the number of protein groups this peptide maps to. Unique Group Indicates whether the peptide maps to a unique protein group. Sequence Sequence of the peptide. Modified Sequence The peptide sequence annotated with variable modifications. AAs Before The amino acids before the sequence. AAs After The amino acids after the sequence. Variable Modifications The variable modifications. Fixed Modifications The fixed modifications. Localization Confidence The confidence in PTMs localization. Confident Phosphosites List of the sites where a phosphorylation was confidently localized. #Confident Phosphosites Number of confidently localized phosphorylations. Ambiguous Phosphosites List of the sites where a phosphorylation was ambiguously localized. #Ambiguous Phosphosites Number of ambiguously localized phosphorylations. #Validated PSMs Number of validated PSMs. #PSMs Number of PSMs. Confidence [%] Confidence in percent associated to the peptide. Validation Indicates the validation level of the peptide. ========================================== ===================================================== Protein Report '''''''''''''' ========================================== ===================================================== Column Description ========================================== ===================================================== Main Accession Main accession of the protein group. Description Description of the protein designed by the main accession. Gene Name The gene names of the Ensembl gene ID associated to the main accession. Chromosome The chromosome of the Ensembl gene ID associated to the main accession. MW (kDa) Molecular Weight. Possible Coverage (%) Possible sequence coverage in percent of the protein designed by the main accession according to the search settings. Coverage (%) Sequence coverage in percent of the protein designed by the main accession. Spectrum Counting The selected spectrum counting metric. Confidently Localized Modification Sites List of the sites where a variable modification was confidently localized. # Confidently Localized Modification Sites Number of sites where a variable modification was confidently localized. Ambiguously Localized Modification Sites List of the sites where ambiguously localized variable modification could possibly be located. #Ambiguously Localized Modification Sites Number of ambiguously localized modifications. Protein Inference Protein Inference status of the protein group. Secondary Accessions Other accessions in the protein group (alphabetical order). Protein Group The complete protein group (alphabetical order). #Validated Peptides Number of validated peptides. #Peptides Total number of peptides. #Unique Total number of peptides unique to this protein group. #Validated Unique Total number of peptides unique to this protein group. #Validated PSMs Number of validated PSMs. #PSMs Number of PSMs. Confidence [%] Confidence in percent associated to the protein group. Validation Indicates the validation level of the protein group. ========================================== ===================================================== Protein Report with non-validated matches ''''''''''''''''''''''''''''''''''''''''' Same columns as the original Protein report but also includes non-validated matches. Protein Phosphorylation Report '''''''''''''''''''''''''''''' ========================================== ===================================================== Column Description ========================================== ===================================================== Main Accession Main accession of the protein group. Description Description of the protein designed by the main accession. Gene Name The gene names of the Ensembl gene ID associated to the main accession. Chromosome The chromosome of the Ensembl gene ID associated to the main accession. MW (kDa) Molecular Weight. Possible Coverage (%) Possible sequence coverage in percent of the protein designed by the main accession according to the search settings. Coverage (%) Sequence coverage in percent of the protein designed by the main accession. Spectrum Counting NSAF Raw Normalized Spectrum Abundance Factor (NSAF). Confident Phosphosites List of the sites where a phosphorylation was confidently localized. #Confident Phosphosites Number of sites where a phosphorylation was confidently localized. Ambiguous Phosphosites List of the sites where a phosphorylation was ambiguously localized. #Ambiguous Phosphosites Number of sites where a phosphorylation was ambiguously localized. Protein Inference Protein Inference status of the protein group. Secondary Accessions Other accessions in the protein group (alphabetical order). Protein Group The complete protein group (alphabetical order). #Validated Peptides Number of validated peptides. #Peptides Total number of peptides. #Unique Total number of peptides unique to this protein group. #Validated Unique Total number of peptides unique to this protein group. #Validated PSMs Number of validated PSMs. #PSMs Number of PSMs. Confidence [%] Confidence in percent associated to the protein group. Validation Indicates the validation level of the protein group. ========================================== ===================================================== Certificate of Analysis ''''''''''''''''''''''' Presents a summary of the most important statistics of the search, grouped by: * Project Details: ‣ PeptideShaker Version, Date, Experiment, Sample, Replicate Number, Identification Algorithms. * Database Search Parameters: ‣ Precursor Tolerance Unit, Precursor Ion m/z Tolerance, Fragment Ion Tolerance Unit, Fragment Ion m/z Tolerance, Cleavage, Enzyme, Missed Cleavages, Specificity, Database, Forward Ion, Rewind Ion, Fixed Modifications, Variable Modifications, Refinement Variable Modifications, Refinement Fixed Modifications. * Input Filters: ‣ Minimal Peptide Length, Maximal Peptide Length, Precursor m/z Tolerance, Precursor m/z Tolerance Unit, Unrecognized Modifications Discarded. * Validation Summary: • Proteins: ‣ #Validated, Total Possible TP, FDR Limit [%], FNR Limit [%], Confidence Limit, PEP Limit [%], Confidence Accuracy [%]. • Peptides: ‣ #Validated, Total Possible TP, FDR Limit [%], FNR Limit [%], Confidence Limit [%], PEP Limit [%], Confidence Accuracy [%]. • PSMs: ‣ #Validated PSM, Total Possible TP, FDR Limit [%]: PSMs, Confidence Limit [%], PEP Limit [%], Confidence Accuracy [%]. * PTM Scoring Settings: ‣ Probabilistic Score, Accounting for Neutral Losses, Threshold. * Spectrum Counting Parameters: ‣ Method, Validated Matches Only * Annotation Settings: ‣ Intensity Limit, Automatic Annotation, Selected Ions, Neutral Losses, Neutral Losses Sequence Dependence, Fragment Ion m/z Tolerance. Hierachical Report '''''''''''''''''' ========================================== ===================================================== Column Description ========================================== ===================================================== Main Accession Main accession of the protein group. Description Description of the protein designed by the main accession. MW (kDa) Molecular Weight. Possible Coverage (%) Possible sequence coverage in percent of the protein designed by the main accession according to the search settings. Coverage (%) Sequence coverage in percent of the protein designed by the main accession. Spectrum Counting NSAF Normalized Spectrum Abundance Factor (NSAF) Confidently Localized Modification Sites List of the sites where a variable modification was confidently localized. # Confidently Localized Modification Sites Number of sites where a variable modification was confidently localized. Ambiguously Localized Modification Sites List of the sites where ambiguously localized variable modification could possibly be located. #Ambiguously Localized Modification Sites Number of ambiguously localized modifications. Protein Inference Protein Inference status of the protein group. Secondary Accessions Other accessions in the protein group (alphabetical order). Protein Group The complete protein group (alphabetical order). #Validated Peptides Number of validated peptides. #Peptides Total number of peptides. #Unique Total number of peptides unique to this protein group. #Validated PSMs Number of validated PSMs #PSMs Number of PSMs Confidence Confidence in percent associated to the protein group. Decoy Indicates whether the protein group is a decoy (1: yes, 0: no). Validation Indicates the validation level of the protein group. ========================================== ===================================================== ------ 
Citation
 To cite the underlying tools (PeptideShaker and SearchGUI) please refer to the list of papers at http://compomics.github.io If you use this tool in Galaxy, please cite Chilton J, Ira Cooke, Bjoern Gruening et al."
toolshed.g2.bx.psu.edu/repos/galaxyp/openms_peptideindexer/PeptideIndexer/3.1+galaxy0	Refreshes the protein references for all peptide hits. For more information, visit https://openms.de/doxygen/release/3.1.0/html/TOPP_PeptideIndexer.html
toolshed.g2.bx.psu.edu/repos/paul_wolfe/reactome_cli/reactome/1.2.1	"Reactome &lt;https://reactome.org&gt;
 is a manually-curated and peer-reviewed database of pathways and reactions in human biology. Analyse Gene List ----------------- The “Analyse Gene List” tool will perform overrepresentation analysis on a list of gene/proteins provided as identifiers, one per line. The ideal identifiers to use are UniProt IDs for proteins, ChEBI IDs for small molecules, and either HGNC gene symbols or ENSEMBL IDs for DNA/RNA molecules, as these are our main external reference sources for proteins and small molecules. To use, select an input type from the dropdown list to upload a file or paste a gene list. See 
here &lt;https://zenodo.org/records/17203734&gt;
 for sample datasets. Sample input: ~~~~~~~~~~~~~ .. code-block:: text #GBM Uniprot P01023 Q99758 O15439 O43184 Q13444 P82987 Species Comparison ------------------ The Species Comparison tool allows you to compare human pathways with computationally-predicted pathways in model organisms, highlighting the elements of the pathway that are common to both species and those that may be absent in the model organism. To use, select a species from the dropdown list. Tissue Distribution ------------------- The Tissue Distribution tool allows you to classify reactions into different human tissue types, to provide an evolving picture of the reactions and pathways in different cell- and tissue-specific environments. We have imported protein expression in different cell/tissue types from the Human Protein Atlas (HPA), overlaid these proteins on Reactome data, and extracted the subset of reactions for that particular cell type. To use, select one or more tissues from the dropdown list. Output ------ Running the analysis will produce a list of pathways in .CSV or JSON format. There are also options for a detailed PDF report and HTML report. Entities that are found and not found from your submitted list. ================== ======================================== ================ ================= =============== =============== ============== ================= ================= ================ ================== ============ ======================== ======================= ========================================= Pathway identifier Pathway name #Entities found #Entities total Entities ratio Entities pValue Entities FDR #Reactions found #Reactions total Reactions ratio Species identifier Species name Submitted entities found Mapped entities Found reaction identifiers ================== ======================================== ================ ================= =============== =============== ============== ================= ================= ================ ================== ============ ======================== ======================= ========================================= R-HSA-382556 ABC-family proteins mediated transport 2 90 0.00755794 1.71E-04 0.0014858 2 27 0.00180457 9606 Homo sapiens O15439;Q99758 O15439;Q99758 R-HSA-1454916;R-HSA-5683714 R-HSA-382551 Transport of small molecules 3 725 0.06088344 2.26E-04 0.0014858 3 462 0.03087822 9606 Homo sapiens O15439;P01023;Q99758 O15439;P01023;Q99758 R-HSA-264758;R-HSA-1454916;R-HSA-5683714 R-HSA-5683678 Defective ABCA3 causes SMDP3 1 1 8.40E-05 2.52E-04 0.0014858 1 1 6.68E-05 9606 Homo sapiens Q99758 Q99758 R-HSA-5683672 R-HSA-5688399 Defective ABCA3 causes SMDP3 1 1 8.40E-05 2.52E-04 0.0014858 1 1 6.68E-05 9606 Homo sapiens Q99758 Q99758 R-HSA-5688397 R-HSA-114608 Platelet degranulation 2 128 0.01074908 3.44E-04 0.0014858 2 11 7.35E-04 9606 Homo sapiens O15439;P01023 O15439;P01023 R-HSA-429157;R-HSA-481007 ================== ======================================== ================ ================= =============== =============== ============== ================= ================= ================ ================== ============ ======================== ======================= ========================================= More information ---------------- Visit the 
Reactome User Guide &lt;https://reactome.org/userguide&gt;
 for detailed documentation about each tool. For more information: visit our Youtube channel for an 
Introduction to Reactome! &lt;https://youtu.be/cA7lQACsgZk&gt;
 Please also see additional 
publications &lt;https://reactome.org/community/publications&gt;
. Contact us ---------- If you have any feedback or questions, please contact us at the 
Reactome HelpDesk &lt;mailto:help@reactome.org&gt;
."
toolshed.g2.bx.psu.edu/repos/paul_wolfe/reactome_cli/reactome_gsa/1.2.1	"Reactome &lt;https://reactome.org&gt;
 is a manually-curated and peer-reviewed database of pathways and reactions in human biology. Analyse Gene Expression (ReactomeGSA) ------------------------------------- This “Analyse Gene Expression” or ReactomeGSA resource provides comparative pathway analyses of multi-omics datasets. It allows researchers to uncover the functional relevance of a list of genes, associated with quantitative data, in the context of biological pathways and processes. The ideal identifiers to use are: * UniProt IDs for proteins * ChEBI IDs for small molecules * HGNC gene symbols or ENSEMBL IDs for DNA/RNA molecules These are our main external reference sources for proteins and small molecules. In Reactome, we offer three gene-set enrichment analysis algorithms: PADOG: Pathway Analysis with Down-weighting of Overlapping Genes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ - It corrects for 
gene set redundancy
; some pathways share many genes, which can bias results. - Instead of treating all genes equally, PADOG down-weights genes that appear in multiple pathways, making the analysis less biased by highly represented genes. - Works well in cases where overlapping genes skew enrichment scores in traditional methods. CAMERA: Correlation Adjusted Mean Rank ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ - It adjusts for 
inter-gene
 correlation in gene sets. - Traditional enrichment approaches assume genes are independent, but in reality, co-expressed genes within a pathway tend to be correlated. - CAMERA corrects for this by adjusting the statistical testing, making it more reliable when genes within pathways have strong dependencies. - Works well for datasets where gene co-expression is expected, for example, in transcriptomic data. ssGSEA: Single-sample Gene Set Enrichment Analysis ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ - ssGSEA calculates an 
enrichment score
 for each gene set in individual samples. - It does not rely on ranking differentially expressed genes across conditions but rather assigns an enrichment score per sample based on the expression of genes in a pathway. - This makes it useful for single-sample comparisons, such as identifying pathway activity in individual patients or cell types. - Works well for single-cell or single-sample datasets. To use, select an input type from the dropdown list to upload a file. See 
here &lt;https://zenodo.org/records/17487197&gt;
 for sample datasets and associated annotation tables. More Information ---------------- Visit the 
Reactome User Guide &lt;https://reactome.org/userguide&gt;
 for detailed documentation about each tool. For more information: visit our Youtube channel for an 
Introduction to Reactome &lt;https://youtu.be/cA7lQACsgZk&gt;
! Contact Us ---------- If you have any feedback or questions, please contact us at the 
Reactome HelpDesk &lt;mailto:help@reactome.org&gt;
_."
toolshed.g2.bx.psu.edu/repos/galaxyp/peptideshaker/search_gui/4.0.41+galaxy1	Comet and Tide shouldn't both be selected since they use a similar algoritm. OMSSA might not work into isolated environments like containers. Ms Amanda may not work either when executed into isolated environments based on MacOS X (use SG 4.0.22 to solve any problem running MsAmanda). MetaMorpheus only produce results when using mzML format.
toolshed.g2.bx.psu.edu/repos/galaxyp/encyclopedia_searchtolib/encyclopedia_searchtolib/1.12.34+galaxy0	"mzML conversion from RAW requires special options: msconvert --zlib --64 --mzML --simAsSpectra --filter ""peakPicking true 1-"" --filter ""demultiplex optimization=overlap_only"" *.raw"
toolshed.g2.bx.psu.edu/repos/galaxyp/validate_fasta_database/validate_fasta_database/0.1.5	"Notes
 Takes a FASTA database and validates the headers using the Compomics (developers of SearchGUI and PeptideShaker) schema. Custom FASTA databases may be in an invalid format, which causes SearchGUI to crash. 
Output
 The main output of this tool, ""Validate FASTA: Passed Sequences"", is a FASTA database that can be run through SearchGUI without error. The failed sequences may be examined for typos and other errors. In addition, the tool will print the databases assigned by the Compomics utility (i.e., UniProt), for a quick check of the validity of the custom FASTA database. Sequences that may cause the tool to report an exception are those that are not valid examples of the following formats: * UniProt, * SwissProt (starts with "">sw|"" or "">SW|"") * NCBI (starts with "">gi|"" or "">GI|"") * Halobacterium from Max Planck (starts with ""OE"") * H Influenza, from Novartis (starts with "">hflu_"") * C Trachomatis (starts with "">C.tr_"" or ""C_trachomatis_"") * M Tuberculosis (starts with "">M. tub"") * Saccharomyces Genome Database (contains ""SGDID"") * Genome translation (ex. "">dm345_3L-sense [2343534-234353938]"") * Genome Annotation Framework for Flexible Analysis (GAFFA) (starts with "">GAFFA"") * UPS (contains ""_HUMAN_UPS"") Many sequences are reported as Generic, which may or may not allow for extraction of the accession number."
toolshed.g2.bx.psu.edu/repos/galaxyp/eggnog_mapper/eggnog_mapper/2.1.13+galaxy0	Min E-value expected when searching for seed eggNOG ortholog. Applies to phmmer/diamond searches. Queries not having a significant seed orthologs (E-value less than threshold) will not be annotated.
toolshed.g2.bx.psu.edu/repos/galaxyp/eggnog_mapper/eggnog_mapper_annotate/2.1.13+galaxy0	Min E-value expected when searching for seed eggNOG ortholog. Applies to phmmer/diamond searches. Queries not having a significant seed orthologs (E-value less than threshold) will not be annotated.
toolshed.g2.bx.psu.edu/repos/galaxyp/eggnog_mapper/eggnog_mapper_search/2.1.13+galaxy0	"eggnog-mapper ============= Overview -------- 
eggnog-mapper
 is a tool for fast functional annotation of novel sequences (genes or proteins) using precomputed eggNOG-based orthology assignments. Obvious examples include the annotation of novel genomes, transcriptomes or even metagenomic gene catalogs. The use of orthology predictions for functional annotation is considered more precise than traditional homology searches, as it avoids transferring annotations from paralogs (duplicate genes with a higher chance of being involved in functional divergence). EggNOG-mapper is also available as a public online resource: 
&lt;http://beta-eggnogdb.embl.de/#/app/emapper&gt;
_. Outputs ------- 
seed orthologs
 each line in the file provides the best match of each query within the best Orthologous Group (OG) reported in the [project].hmm_hits file, obtained running PHMMER against all sequences within the best OG. The seed ortholog is used to fetch fine-grained orthology relationships from eggNOG. If using the diamond search mode, seed orthologs are directly obtained from the best matching sequences by running DIAMOND against the whole eggNOG protein space. 
Recommentation for large input data
 EggNOG-mapper consists of two phases 1. finding seed orthologous sequences (compute intensive) 2. expanding annotations (IO intensive) by default (i.e. if 
Method to search seed orthologs
 is not 
Skip search stage...
 and 
Annotate seed orthologs
 is 
Yes
) both phases are executed within one tool run. For large input FASTA datasets in can be favourable to split this in two separate tool runs as follows: 1. Split the FASTA (e.g. 1M seqs per data set) 2. Run the search phase only (set 
Annotate seed orthologs
 to 
No
) on the separate FASTA files. 3. Run the annotation phase (set 
Method to search seed orthologs
 to 
Skip search stage...
) See 
also
 Another alternative is to use cached annotations (produced in a run with --md5 enabled)."
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__alignment__mafft/qiime2__alignment__mafft/2026.1.0+q2galaxy.2026.1.0	QIIME 2: alignment mafft ======================== De novo multiple sequence alignment with MAFFT Outputs: -------- :alignment.qza: The aligned sequences. | Description: ------------ Perform de novo multiple sequence alignment using MAFFT. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__alignment__mafft_add/qiime2__alignment__mafft_add/2026.1.0+q2galaxy.2026.1.0	QIIME 2: alignment mafft-add ============================ Add sequences to multiple sequence alignment with MAFFT. Outputs: -------- :expanded_alignment.qza: Alignment containing the provided aligned and unaligned sequences. | Description: ------------ Add new sequences to an existing alignment with MAFFT. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__alignment__mask/qiime2__alignment__mask/2026.1.0+q2galaxy.2026.1.0	QIIME 2: alignment mask ======================= Positional conservation and gap filtering. Outputs: -------- :masked_alignment.qza: The masked alignment. | Description: ------------ Mask (i.e., filter) unconserved and highly gapped columns from an alignment. Default min_conservation was chosen to reproduce the mask presented in Lane (1991). |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__composition__add_pseudocount/qiime2__composition__add_pseudocount/2026.1.0+0.g4b3aa86.dirty-q2galaxy.2026.1.0	QIIME 2: composition add-pseudocount ==================================== Add pseudocount to table. Outputs: -------- :composition_table.qza: The resulting feature table. | Description: ------------ Increment all counts in table by pseudocount. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__composition__ancom/qiime2__composition__ancom/2026.1.0+0.g4b3aa86.dirty-q2galaxy.2026.1.0	QIIME 2: composition ancom ========================== Apply ANCOM to identify features that differ in abundance. Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Apply Analysis of Composition of Microbiomes (ANCOM) to identify features that are differentially abundant across groups. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__composition__ancombc/qiime2__composition__ancombc/2026.1.0+0.g4b3aa86.dirty-q2galaxy.2026.1.0	"QIIME 2: composition ancombc ============================ Analysis of Composition of Microbiomes with Bias Correction Outputs: -------- :differentials.qza: The calculated per-feature differentials. | Description: ------------ Apply Analysis of Compositions of Microbiomes with Bias Correction (ANCOM-BC) to identify features that are differentially abundant across groups. Examples: --------- ancombc_single_formula 
*
*
*
 Using the 
qiime2 composition ancombc
 tool: #. Set 
""table""
 to 
#: table.qza
 #. For 
""metadata""
: - Perform the following steps. #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
metadata.tsv
 #. Set 
""formula""
 to 
bodysite
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 composition ancombc [...] : differentials.qza
 - 
dataloaf.qza
 ancombc_multi_formula_with_reference_levels 
*
*
*
*
*
*
 Using the 
qiime2 composition ancombc
 tool: #. Set 
""table""
 to 
#: table.qza
 #. For 
""metadata""
: - Perform the following steps. #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
metadata.tsv
 #. Set 
""formula""
 to 
bodysite + animal
 #. Expand the 
additional options
 section - For 
""reference_levels""
, use the 
+ reference_levels
 button to add the corresponding values: #. Add 
""element""
 set to 
bodysite::tongue
 #. Add 
""element""
 set to 
animal::dog
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 composition ancombc [...] : differentials.qza
 - 
dataloaf.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__composition__tabulate/qiime2__composition__tabulate/2026.1.0+0.g4b3aa86.dirty-q2galaxy.2026.1.0	QIIME 2: composition tabulate ============================= View tabular output from ANCOM-BC or ANCOM-BC2. Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Generate tabular view of ANCOM-BC or ANCOM-BC2 output, which includes per-page views for the log-fold change (lfc), standard error (se), P values, Q values, and W scores. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__cutadapt__demux_paired/qiime2__cutadapt__demux_paired/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: cutadapt demux-paired ============================== Demultiplex paired-end sequence data with barcodes in-sequence. Outputs: -------- :per_sample_sequences.qza: The resulting demultiplexed sequences. :untrimmed_sequences.qza: The sequences that were unmatched to barcodes. | Description: ------------ Demultiplex sequence data (i.e., map barcode reads to sample ids). Barcodes are expected to be located within the sequence data (versus the header, or a separate barcode file). Examples: --------- paired 
*
 Using the 
qiime2 cutadapt demux-paired
 tool: #. Set 
""seqs""
 to 
#: seqs.qza
 #. For 
""forward_barcodes""
: #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
md.tsv
 #. Set 
""Column Name""* to 
barcode-sequence
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__cutadapt__demux_single/qiime2__cutadapt__demux_single/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: cutadapt demux-single ============================== Demultiplex single-end sequence data with barcodes in-sequence. Outputs: -------- :per_sample_sequences.qza: The resulting demultiplexed sequences. :untrimmed_sequences.qza: The sequences that were unmatched to barcodes. | Description: ------------ Demultiplex sequence data (i.e., map barcode reads to sample ids). Barcodes are expected to be located within the sequence data (versus the header, or a separate barcode file). Examples: --------- demux_single 
*
*
 Using the 
qiime2 cutadapt demux-single
 tool: #. Set 
""seqs""
 to 
#: seqs.qza
 #. For 
""barcodes""
: #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
md.tsv
 #. Set 
""Column Name""
 to 
BarcodeSequence
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__cutadapt__trim_paired/qiime2__cutadapt__trim_paired/2026.1.0+q2galaxy.2026.1.0	QIIME 2: cutadapt trim-paired ============================= Find and remove adapters in demultiplexed paired-end sequences. Outputs: -------- :trimmed_sequences.qza: The resulting trimmed sequences. | Description: ------------ Search demultiplexed paired-end sequences for adapters and remove them. The parameter descriptions in this method are adapted from the official cutadapt docs - please see those docs at https://cutadapt.readthedocs.io for complete details. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__cutadapt__trim_single/qiime2__cutadapt__trim_single/2026.1.0+q2galaxy.2026.1.0	QIIME 2: cutadapt trim-single ============================= Find and remove adapters in demultiplexed single-end sequences. Outputs: -------- :trimmed_sequences.qza: The resulting trimmed sequences. | Description: ------------ Search demultiplexed single-end sequences for adapters and remove them. The parameter descriptions in this method are adapted from the official cutadapt docs - please see those docs at https://cutadapt.readthedocs.io for complete details. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__dada2__denoise_ccs/qiime2__dada2__denoise_ccs/2026.1.0+q2galaxy.2026.1.0	QIIME 2: dada2 denoise-ccs ========================== Denoise and dereplicate single-end Pacbio CCS Outputs: -------- :table.qza: The resulting feature table. :representative_sequences.qza: The resulting feature sequences. Each feature in the feature table will be represented by exactly one sequence. :denoising_stats.qza: A table listing per-sample read retention counts and percentages after each stage of the pipeline. :base_transition_stats.qza: A table listing the transition rates of each ordered pair of nucleotides at each quality score. | Description: ------------ This method denoises single-end Pacbio CCS sequences, dereplicates them, and filters chimeras. Tutorial and workflow: https://github.com/benjjneb/LRASManuscript |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__dada2__denoise_paired/qiime2__dada2__denoise_paired/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: dada2 denoise-paired ============================= Denoise and dereplicate paired-end sequences Outputs: -------- :table.qza: The resulting feature table. :representative_sequences.qza: The resulting feature sequences. Each feature in the feature table will be represented by exactly one sequence, and these sequences will be the joined paired-end sequences. :denoising_stats.qza: A table listing per-sample read retention counts and percentages after each stage of the pipeline. :base_transition_stats.qza: A table listing the transition rates of each ordered pair of nucleotides at each quality score. | Description: ------------ This method denoises paired-end sequences, dereplicates them, and filters chimeras. Examples: --------- denoise_paired 
*
*
 Using the 
qiime2 dada2 denoise-paired
 tool: #. Set 
""demultiplexed_seqs""
 to 
#: demux-paired.qza
 #. Set 
""trunc_len_f""
 to 
150
 #. Set 
""trunc_len_r""
 to 
140
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__dada2__denoise_pyro/qiime2__dada2__denoise_pyro/2026.1.0+q2galaxy.2026.1.0	QIIME 2: dada2 denoise-pyro =========================== Denoise and dereplicate single-end pyrosequences Outputs: -------- :table.qza: The resulting feature table. :representative_sequences.qza: The resulting feature sequences. Each feature in the feature table will be represented by exactly one sequence. :denoising_stats.qza: A table listing per-sample read retention counts and percentages after each stage of the pipeline. :base_transition_stats.qza: A table listing the transition rates of each ordered pair of nucleotides at each quality score. | Description: ------------ This method denoises single-end pyrosequencing sequences, dereplicates them, and filters chimeras. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__dada2__denoise_single/qiime2__dada2__denoise_single/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: dada2 denoise-single ============================= Denoise and dereplicate single-end sequences Outputs: -------- :table.qza: The resulting feature table. :representative_sequences.qza: The resulting feature sequences. Each feature in the feature table will be represented by exactly one sequence. :denoising_stats.qza: A table listing per-sample read retention counts and percentages after each stage of the pipeline. :base_transition_stats.qza: A table listing the transition rates of each ordered pair of nucleotides at each quality score. | Description: ------------ This method denoises single-end sequences, dereplicates them, and filters chimeras. Examples: --------- denoise_single 
*
*
 Using the 
qiime2 dada2 denoise-single
 tool: #. Set 
""demultiplexed_seqs""
 to 
#: demux-single.qza
 #. Set 
""trunc_len""
 to 
120
 #. Expand the 
additional options
 section - Leave 
""trim_left""
 as its default value of 
0
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__deblur__denoise_16s/qiime2__deblur__denoise_16S/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: deblur denoise-16S =========================== Deblur sequences using a 16S positive filter. Outputs: -------- :table.qza: The resulting denoised feature table. :representative_sequences.qza: The resulting feature sequences. :stats.qza: Per-sample stats if requested. | Description: ------------ Perform sequence quality control for Illumina data using the Deblur workflow with a 16S reference as a positive filter. Only forward reads are supported at this time. The specific reference used is the 88% OTUs from Greengenes 13_8. This mode of operation should only be used when data were generated from a 16S amplicon protocol on an Illumina platform. The reference is only used to assess whether each sequence is likely to be 16S by a local alignment using SortMeRNA with a permissive e-value; the reference is not used to characterize the sequences. Examples: --------- denoise_16S 
*
 Using the 
qiime2 deblur denoise-16S
 tool: #. Set 
""demultiplexed_seqs""
 to 
#: demux-filtered.qza
 #. Set 
""trim_length""
 to 
120
 #. Expand the 
additional options
 section - Set 
""sample_stats""
 to 
Yes
 #. Press the 
Execute
 button. Once completed, for each new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 deblur denoise-16S [...] : table.qza
 - 
table.qza
 * - 
#: qiime2 deblur denoise-16S [...] : representative_sequences.qza
 - 
representative-sequences.qza
 * - 
#: qiime2 deblur denoise-16S [...] : stats.qza
 - 
denoising-stats.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__deblur__denoise_other/qiime2__deblur__denoise_other/2026.1.0+q2galaxy.2026.1.0	QIIME 2: deblur denoise-other ============================= Deblur sequences using a user-specified positive filter. Outputs: -------- :table.qza: The resulting denoised feature table. :representative_sequences.qza: The resulting feature sequences. :stats.qza: Per-sample stats if requested. | Description: ------------ Perform sequence quality control for Illumina data using the Deblur workflow, including positive alignment-based filtering. Only forward reads are supported at this time. This mode of execution is particularly useful when operating on non-16S data. For example, to apply Deblur to 18S data, you would want to specify a reference composed of 18S sequences in order to filter out sequences which do not appear to be 18S. The assessment is performed by local alignment using SortMeRNA with a permissive e-value threshold. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__deblur__visualize_stats/qiime2__deblur__visualize_stats/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: deblur visualize-stats =============================== Visualize Deblur stats per sample. Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Display Deblur statistics per sample Examples: --------- visualize_stats 
*
*
 Using the 
qiime2 deblur visualize-stats
 tool: #. Set 
""deblur_stats""
 to 
#: deblur-stats.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 deblur visualize-stats [...] : visualization.qzv
 - 
deblur-stats-viz.qzv
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__demux__emp_paired/qiime2__demux__emp_paired/2026.1.0+q2galaxy.2026.1.0	QIIME 2: demux emp-paired ========================= Demultiplex paired-end sequence data generated with the EMP protocol. Outputs: -------- :per_sample_sequences.qza: The resulting demultiplexed sequences. :error_correction_details.qza: Detail about the barcode error corrections. | Description: ------------ Demultiplex paired-end sequence data (i.e., map barcode reads to sample ids) for data generated with the Earth Microbiome Project (EMP) amplicon sequencing protocol. Details about this protocol can be found at http://www.earthmicrobiome.org/protocols-and-standards/ |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__demux__emp_single/qiime2__demux__emp_single/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: demux emp-single ========================= Demultiplex sequence data generated with the EMP protocol. Outputs: -------- :per_sample_sequences.qza: The resulting demultiplexed sequences. :error_correction_details.qza: Detail about the barcode error corrections. | Description: ------------ Demultiplex sequence data (i.e., map barcode reads to sample ids) for data generated with the Earth Microbiome Project (EMP) amplicon sequencing protocol. Details about this protocol can be found at http://www.earthmicrobiome.org/protocols-and-standards/ Examples: --------- demux 
*
 Using the 
qiime2 demux emp-single
 tool: #. Set 
""seqs""
 to 
#: sequences.qza
 #. For 
""barcodes""
: #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
sample-metadata.tsv
 #. Set 
""Column Name""
 to 
barcode-sequence
 #. Press the 
Execute
 button. Once completed, for each new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 demux emp-single [...] : per_sample_sequences.qza
 - 
demux.qza
 * - 
#: qiime2 demux emp-single [...] : error_correction_details.qza
 - 
demux-details.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__demux__filter_samples/qiime2__demux__filter_samples/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: demux filter-samples ============================= Filter samples out of demultiplexed data. Outputs: -------- :filtered_demux.qza: Filtered demultiplexed data. | Description: ------------ Filter samples indicated in given metadata out of demultiplexed data or filter out empty samples. Specific samples can be further selected with the WHERE clause, and the 
exclude_ids
 parameter allows for filtering of all samples not specified. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__demux__subsample_paired/qiime2__demux__subsample_paired/2026.1.0+q2galaxy.2026.1.0	QIIME 2: demux subsample-paired =============================== Subsample paired-end sequences without replacement. Outputs: -------- :subsampled_sequences.qza: The subsampled sequences. | Description: ------------ Generate a random subsample of paired-end sequences containing approximately the fraction of input sequences specified by the fraction parameter. The number of output samples will always be equal to the number of input samples, even if some of those samples contain no sequences after subsampling. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__demux__subsample_single/qiime2__demux__subsample_single/2026.1.0+q2galaxy.2026.1.0	QIIME 2: demux subsample-single =============================== Subsample single-end sequences without replacement. Outputs: -------- :subsampled_sequences.qza: The subsampled sequences. | Description: ------------ Generate a random subsample of single-end sequences containing approximately the fraction of input sequences specified by the fraction parameter. The number of output samples will always be equal to the number of input samples, even if some of those samples contain no sequences after subsampling. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__demux__summarize/qiime2__demux__summarize/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: demux summarize ======================== Summarize counts per sample. Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Summarize counts per sample for all samples, and generate interactive positional quality plots based on 
n
 randomly selected sequences. Examples: --------- demux 
*
 Using the 
qiime2 demux summarize
 tool: #. Set 
""data""
 to 
#: demux.qza
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__adonis/qiime2__diversity__adonis/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity adonis ========================= adonis PERMANOVA test for beta group significance Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Determine whether groups of samples are significantly different from one another using the ADONIS permutation-based statistical test in vegan-R. The function partitions sums of squares of a multivariate data set, and is directly analogous to MANOVA (multivariate analysis of variance). This action differs from beta_group_significance in that it accepts R formulae to perform multi-way ADONIS tests; beta_group_signficance only performs one-way tests. For more details, consult the reference manual available on the CRAN vegan page: https://CRAN.R-project.org/package=vegan |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__alpha/qiime2__diversity__alpha/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity alpha ======================== Alpha diversity Outputs: -------- :alpha_diversity.qza: Vector containing per-sample alpha diversities. | Description: ------------ Computes a user-specified alpha diversity metric for all samples in a feature table. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__alpha_correlation/qiime2__diversity__alpha_correlation/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity alpha-correlation ==================================== Alpha diversity correlation Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Determine whether numeric sample metadata columns are correlated with alpha diversity. Examples: --------- alpha_correlation_faith_pd 
*
*
*
*
 Using the 
qiime2 diversity alpha-correlation
 tool: #. Set 
""alpha_diversity""
 to 
#: alpha-div-faith-pd.qza
 #. For 
""metadata""
: - Perform the following steps. #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
metadata.tsv
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__alpha_group_significance/qiime2__diversity__alpha_group_significance/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity alpha-group-significance =========================================== Alpha diversity comparisons Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Visually and statistically compare groups of alpha diversity values. Examples: --------- alpha_group_significance_faith_pd 
*
*
*
*
*
 Using the 
qiime2 diversity alpha-group-significance
 tool: #. Set 
""alpha_diversity""
 to 
#: alpha-div-faith-pd.qza
 #. For 
""metadata""
: - Perform the following steps. #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
metadata.tsv
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__alpha_phylogenetic/qiime2__diversity__alpha_phylogenetic/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity alpha-phylogenetic ===================================== Alpha diversity (phylogenetic) Outputs: -------- :alpha_diversity.qza: Vector containing per-sample alpha diversities. | Description: ------------ Computes a user-specified phylogenetic alpha diversity metric for all samples in a feature table. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__alpha_rarefaction/qiime2__diversity__alpha_rarefaction/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity alpha-rarefaction ==================================== Alpha rarefaction curves Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Generate interactive alpha rarefaction curves by computing rarefactions between 
min_depth
 and 
max_depth
. The number of intermediate depths to compute is controlled by the 
steps
 parameter, with n 
iterations
 being computed at each rarefaction depth. If sample metadata is provided, samples may be grouped based on distinct values within a metadata column. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__beta/qiime2__diversity__beta/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity beta ======================= Beta diversity Outputs: -------- :distance_matrix.qza: The resulting distance matrix. | Description: ------------ Computes a user-specified beta diversity metric for all pairs of samples in a feature table. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__beta_correlation/qiime2__diversity__beta_correlation/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity beta-correlation =================================== Beta diversity correlation Outputs: -------- :metadata_distance_matrix.qza: The Distance Matrix produced from the metadata column and used in the mantel test :mantel_scatter_visualization.qzv: Scatter plot rendering of the manteltest results | Description: ------------ Create a distance matrix from a numeric metadata column and apply a two-sided Mantel test to identify correlation between two distance matrices. Actions used internally: 
distance-matrix
 from q2-metadata and 
mantel
 from q2-diversity. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__beta_group_significance/qiime2__diversity__beta_group_significance/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity beta-group-significance ========================================== Beta diversity group significance Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Determine whether groups of samples are significantly different from one another using a permutation-based statistical test. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__beta_phylogenetic/qiime2__diversity__beta_phylogenetic/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity beta-phylogenetic ==================================== Beta diversity (phylogenetic) Outputs: -------- :distance_matrix.qza: The resulting distance matrix. | Description: ------------ Computes a user-specified phylogenetic beta diversity metric for all pairs of samples in a feature table. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__beta_rarefaction/qiime2__diversity__beta_rarefaction/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity beta-rarefaction =================================== Beta diversity rarefaction Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Repeatedly rarefy a feature table to compare beta diversity results within a given rarefaction depth. For a given beta diversity metric, this visualizer will provide: an Emperor jackknifed PCoA plot, samples clustered by UPGMA or neighbor joining with support calculation, and a heatmap showing the correlation between rarefaction trials of that beta diversity metric. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__bioenv/qiime2__diversity__bioenv/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity bioenv ========================= bioenv Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Find the subsets of variables in metadata whose Euclidean distances are maximally rank-correlated with distance matrix. All numeric variables in metadata will be considered, and samples which are missing data will be dropped. The output visualization will indicate how many samples were dropped due to missing data, if any were dropped. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__core_metrics/qiime2__diversity__core_metrics/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity core-metrics =============================== Core diversity metrics (non-phylogenetic) Outputs: -------- :rarefied_table.qza: The resulting rarefied feature table. :observed_features_vector.qza: Vector of Observed Features values by sample. :shannon_vector.qza: Vector of Shannon diversity values by sample. :evenness_vector.qza: Vector of Pielou's evenness values by sample. :jaccard_distance_matrix.qza: Matrix of Jaccard distances between pairs of samples. :bray_curtis_distance_matrix.qza: Matrix of Bray-Curtis distances between pairs of samples. :jaccard_pcoa_results.qza: PCoA matrix computed from Jaccard distances between samples. :bray_curtis_pcoa_results.qza: PCoA matrix computed from Bray-Curtis distances between samples. :jaccard_emperor.qzv: Emperor plot of the PCoA matrix computed from Jaccard. :bray_curtis_emperor.qzv: Emperor plot of the PCoA matrix computed from Bray-Curtis. | Description: ------------ Applies a collection of diversity metrics (non-phylogenetic) to a feature table. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__core_metrics_phylogenetic/qiime2__diversity__core_metrics_phylogenetic/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity core-metrics-phylogenetic ============================================ Core diversity metrics (phylogenetic and non-phylogenetic) Outputs: -------- :rarefied_table.qza: The resulting rarefied feature table. :faith_pd_vector.qza: Vector of Faith PD values by sample. :observed_features_vector.qza: Vector of Observed Features values by sample. :shannon_vector.qza: Vector of Shannon diversity values by sample. :evenness_vector.qza: Vector of Pielou's evenness values by sample. :unweighted_unifrac_distance_matrix.qza: Matrix of unweighted UniFrac distances between pairs of samples. :weighted_unifrac_distance_matrix.qza: Matrix of weighted UniFrac distances between pairs of samples. :jaccard_distance_matrix.qza: Matrix of Jaccard distances between pairs of samples. :bray_curtis_distance_matrix.qza: Matrix of Bray-Curtis distances between pairs of samples. :unweighted_unifrac_pcoa_results.qza: PCoA matrix computed from unweighted UniFrac distances between samples. :weighted_unifrac_pcoa_results.qza: PCoA matrix computed from weighted UniFrac distances between samples. :jaccard_pcoa_results.qza: PCoA matrix computed from Jaccard distances between samples. :bray_curtis_pcoa_results.qza: PCoA matrix computed from Bray-Curtis distances between samples. :unweighted_unifrac_emperor.qzv: Emperor plot of the PCoA matrix computed from unweighted UniFrac. :weighted_unifrac_emperor.qzv: Emperor plot of the PCoA matrix computed from weighted UniFrac. :jaccard_emperor.qzv: Emperor plot of the PCoA matrix computed from Jaccard. :bray_curtis_emperor.qzv: Emperor plot of the PCoA matrix computed from Bray-Curtis. | Description: ------------ Applies a collection of diversity metrics (both phylogenetic and non-phylogenetic) to a feature table. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__filter_distance_matrix/qiime2__diversity__filter_distance_matrix/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity filter-distance-matrix ========================================= Filter samples from a distance matrix. Outputs: -------- :filtered_distance_matrix.qza: Distance matrix filtered to include samples matching search criteria | Description: ------------ Filter samples from a distance matrix, retaining only the samples matching search criteria specified by 
metadata
 and 
where
 parameters (or retaining only the samples not matching that criteria, if 
exclude_ids
 is True). See the filtering tutorial on https://docs.qiime2.org for additional details. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__mantel/qiime2__diversity__mantel/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity mantel ========================= Apply the Mantel test to two distance matrices Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Apply a two-sided Mantel test to identify correlation between two distance matrices. Note: the directionality of the comparison has no bearing on the results. Thus, comparing distance matrix X to distance matrix Y is equivalent to comparing Y to X. Note: the order of samples within the two distance matrices does not need to be the same; the distance matrices will be reordered before applying the Mantel test. See the scikit-bio docs for more details about the Mantel test: http://scikit-bio.org/docs/latest/generated/skbio.stats.distance.mantel |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__pcoa/qiime2__diversity__pcoa/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity pcoa ======================= Principal Coordinate Analysis Outputs: -------- :pcoa.qza: The resulting PCoA matrix. | Description: ------------ Apply principal coordinate analysis. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__pcoa_biplot/qiime2__diversity__pcoa_biplot/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity pcoa-biplot ============================== Principal Coordinate Analysis Biplot Outputs: -------- :biplot.qza: The resulting PCoA matrix. | Description: ------------ Project features into a principal coordinates matrix. The features used should be the features used to compute the distance matrix. It is recommended that these variables be normalized in cases of dimensionally heterogeneous physical variables. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__procrustes_analysis/qiime2__diversity__procrustes_analysis/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity procrustes-analysis ====================================== Procrustes Analysis Outputs: -------- :transformed_reference.qza: A normalized version of the ""reference"" ordination matrix. :transformed_other.qza: A normalized and fitted version of the ""other"" ordination matrix. :disparity_results.qza: The sum of the squares of the pointwise differences between the two input datasets & its p value. | Description: ------------ Fit two ordination matrices with Procrustes analysis |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__tsne/qiime2__diversity__tsne/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity tsne ======================= t-distributed stochastic neighbor embedding Outputs: -------- :tsne.qza: The resulting t-SNE matrix. | Description: ------------ Apply t-distributed stochastic neighbor embedding. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity__umap/qiime2__diversity__umap/2026.1.0+q2galaxy.2026.1.0	QIIME 2: diversity umap ======================= Uniform Manifold Approximation and Projection Outputs: -------- :umap.qza: The resulting UMAP matrix. | Description: ------------ Apply Uniform Manifold Approximation and Projection. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity_lib__alpha_passthrough/qiime2__diversity_lib__alpha_passthrough/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity-lib alpha-passthrough ======================================== Alpha Passthrough (non-phylogenetic) Outputs: -------- :vector.qza: Vector containing per-sample values for the chosen metric. | Description: ------------ Computes a vector of values (one value for each samples in a feature table) using the scikit-bio implementation of the selected alpha diversity metric. Examples: --------- basic 
*
 Using the 
qiime2 diversity-lib alpha-passthrough
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""metric""
 to 
simpson
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib alpha-passthrough [...] : vector.qza
 - 
simpson-vector.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity_lib__beta_passthrough/qiime2__diversity_lib__beta_passthrough/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity-lib beta-passthrough ======================================= Beta Passthrough (non-phylogenetic) Outputs: -------- :distance_matrix.qza: The resulting distance matrix. | Description: ------------ Computes a distance matrix for all pairs of samples in a feature table using the scikit-bio implementation of the selected beta diversity metric. Examples: --------- run on one core (by default) 
*
*
*
*
 Using the 
qiime2 diversity-lib beta-passthrough
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""metric""
 to 
euclidean
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-passthrough [...] : distance_matrix.qza
 - 
euclidean-dm.qza
 to run on n cores, replace 1 here with your preferred integer 
*
*
*
*
*
*
*
*
*
 Using the 
qiime2 diversity-lib beta-passthrough
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""metric""
 to 
euclidean
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-passthrough [...] : distance_matrix.qza
 - 
euclidean-dm.qza
 use 'auto' to run on all of host system's available CPU cores 
*
*
*
*
*
*
*
*
 | A default pseudocount of 1 is added to feature counts. Pseudocount is ignored for non-compositional metrics. Using the 
qiime2 diversity-lib beta-passthrough
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""metric""
 to 
aitchison
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-passthrough [...] : distance_matrix.qza
 - 
aitchison-dm.qza
 use 'pseudocount' to manually set a pseudocount for compositional metrics 
*
*
*
*
*
*
*
*
*
*
 Using the 
qiime2 diversity-lib beta-passthrough
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""metric""
 to 
aitchison
 #. Expand the 
additional options
 section - Set 
""pseudocount""
 to 
5
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-passthrough [...] : distance_matrix.qza
 - 
aitchison-dm.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity_lib__beta_phylogenetic_meta_passthrough/qiime2__diversity_lib__beta_phylogenetic_meta_passthrough/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity-lib beta-phylogenetic-meta-passthrough ========================================================= Beta Phylogenetic Meta Passthrough Outputs: -------- :distance_matrix.qza: The resulting distance matrix. | Description: ------------ Computes a distance matrix for all pairs of samples in the set of feature table and phylogeny pairs, using the unifrac implementation of the selected beta diversity metric. Examples: --------- Basic meta unifrac 
*
*
* | For brevity, these examples are focused on meta-specific parameters. See the documentation for beta_phylogenetic_passthrough for additional relevant information. | NOTE: the number of trees and tables must match. Using the 
qiime2 diversity-lib beta-phylogenetic-meta-passthrough
 tool: #. For 
""tables""
, use ctrl-(or command)-click to select the following inputs: #. 
#: feature-table1.qza
 #. 
#: feature-table2.qza
 #. For 
""phylogenies""
, use ctrl-(or command)-click to select the following inputs: #. 
#: phylogeny1.qza
 #. 
#: phylogeny2.qza
 #. Set 
""metric""
 to 
weighted_normalized_unifrac
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-phylogenetic-meta-passthrough [...] : distance_matrix.qza
 - 
ft1-ft2-w-norm-unifrac-dm.qza
 meta with weights 
*
*
 | The number of weights must match the number of tables/trees. | If meaningful, it is possible to pass the same phylogeny more than once. Using the 
qiime2 diversity-lib beta-phylogenetic-meta-passthrough
 tool: #. For 
""tables""
, use ctrl-(or command)-click to select the following inputs: #. 
#: feature-table1.qza
 #. 
#: feature-table2.qza
 #. For 
""phylogenies""
, use ctrl-(or command)-click to select the following inputs: #. 
#: phylogeny.qza
 #. 
#: phylogeny.qza
 #. Set 
""metric""
 to 
weighted_normalized_unifrac
 #. Expand the 
additional options
 section - For 
""weights""
, use the 
+ weights
 button to add the corresponding values: #. Add 
""element""
 set to 
3.0
 #. Add 
""element""
 set to 
42.0
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-phylogenetic-meta-passthrough [...] : distance_matrix.qza
 - 
ft1-ft2-w-norm-unifrac-dm.qza
 changing the consolidation method 
*
*
*
*
 Using the 
qiime2 diversity-lib beta-phylogenetic-meta-passthrough
 tool: #. For 
""tables""
, use ctrl-(or command)-click to select the following inputs: #. 
#: feature-table1.qza
 #. 
#: feature-table2.qza
 #. For 
""phylogenies""
, use ctrl-(or command)-click to select the following inputs: #. 
#: phylogeny1.qza
 #. 
#: phylogeny2.qza
 #. Set 
""metric""
 to 
weighted_normalized_unifrac
 #. Expand the 
additional options
 section #. For 
""weights""
, use the 
+ weights
 button to add the corresponding values: #. Add 
""element""
 set to 
0.4
 #. Add 
""element""
 set to 
0.6
 #. Leave 
""consolidation""
 as its default value of 
skipping_missing_values
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-phylogenetic-meta-passthrough [...] : distance_matrix.qza
 - 
ft1-ft2-w-norm-unifrac-dm.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity_lib__beta_phylogenetic_passthrough/qiime2__diversity_lib__beta_phylogenetic_passthrough/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity-lib beta-phylogenetic-passthrough ==================================================== Beta Phylogenetic Passthrough Outputs: -------- :distance_matrix.qza: The resulting distance matrix. | Description: ------------ Computes a distance matrix for all pairs of samples in a feature table using the unifrac implementation of the selected beta diversity metric. Examples: --------- run on one core (by default) 
*
*
*
*
 Using the 
qiime2 diversity-lib beta-phylogenetic-passthrough
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Set 
""metric""
 to 
weighted_normalized_unifrac
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-phylogenetic-passthrough [...] : distance_matrix.qza
 - 
weighted-normalized-unifrac-dm.qza
 to run on n cores, replace 1 here with your preferred integer 
*
*
*
*
*
*
*
*
*
 Using the 
qiime2 diversity-lib beta-phylogenetic-passthrough
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Set 
""metric""
 to 
weighted_normalized_unifrac
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-phylogenetic-passthrough [...] : distance_matrix.qza
 - 
weighted-normalized-unifrac-dm.qza
 use 'auto' to run on all of host system's available CPU cores 
*
*
*
*
*
*
*
*
 Using the 
qiime2 diversity-lib beta-phylogenetic-passthrough
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Set 
""metric""
 to 
weighted_normalized_unifrac
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-phylogenetic-passthrough [...] : distance_matrix.qza
 - 
weighted-normalized-unifrac-dm.qza
 use bypass_tips to trade specificity for reduced compute time 
*
*
*
*
*
*
*
*
 | bypass_tips can be used with any threads setting, but auto may be a good choice if you're trimming run time. Using the 
qiime2 diversity-lib beta-phylogenetic-passthrough
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Set 
""metric""
 to 
weighted_normalized_unifrac
 #. Expand the 
additional options
 section - Set 
""bypass_tips""
 to 
Yes
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-phylogenetic-passthrough [...] : distance_matrix.qza
 - 
weighted-normalized-unifrac-dm.qza
 variance adjustment 
*
*
 | Chang et al's variance adjustment may be applied to any unifrac method by using this passthrough function. Using the 
qiime2 diversity-lib beta-phylogenetic-passthrough
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Set 
""metric""
 to 
weighted_unifrac
 #. Expand the 
additional options
 section - Set 
""variance_adjusted""
 to 
Yes
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-phylogenetic-passthrough [...] : distance_matrix.qza
 - 
var-adj-weighted-unifrac-dm.qza
 minimal generalized unifrac 
*
*
*
* | Generalized unifrac is passed alpha=1 by default. This is roughly equivalent to weighted normalized unifrac, which method will be used instead, because it is better optimized. Using the 
qiime2 diversity-lib beta-phylogenetic-passthrough
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Set 
""metric""
 to 
generalized_unifrac
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-phylogenetic-passthrough [...] : distance_matrix.qza
 - 
generalized-unifrac-dm.qza
 generalized unifrac 
*
*
 | passing a float between 0 and 1 to 'alpha' gives you control over the importance of sample proportions. Using the 
qiime2 diversity-lib beta-phylogenetic-passthrough
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Set 
""metric""
 to 
generalized_unifrac
 #. Expand the 
additional options
 section - Set 
""alpha""
 to 
0.75
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib beta-phylogenetic-passthrough [...] : distance_matrix.qza
 - 
generalized-unifrac-dm.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity_lib__bray_curtis/qiime2__diversity_lib__bray_curtis/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity-lib bray-curtis ================================== Bray-Curtis Dissimilarity Outputs: -------- :distance_matrix.qza: Distance matrix for Bray-Curtis dissimilarity | Description: ------------ Compute Bray-Curtis dissimilarity for each sample in a feature table. Note: Frequency and relative frequency data produce different results unless overall sample sizes are identical. Please consider the impact on your results if you use Bray-Curtis with count data that has not been adjusted (normalized). Examples: --------- run on one core (by default) 
*
*
*
*
 Using the 
qiime2 diversity-lib bray-curtis
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib bray-curtis [...] : distance_matrix.qza
 - 
bray-curtis-dm.qza
 to run on n cores, replace 1 here with your preferred integer 
*
*
*
*
*
*
*
*
*
 Using the 
qiime2 diversity-lib bray-curtis
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib bray-curtis [...] : distance_matrix.qza
 - 
bray-curtis-dm.qza
 use 'auto' to run on all of host system's available CPU cores 
*
*
*
*
*
*
*
*
 Using the 
qiime2 diversity-lib bray-curtis
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib bray-curtis [...] : distance_matrix.qza
 - 
bray-curtis-dm.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity_lib__faith_pd/qiime2__diversity_lib__faith_pd/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity-lib faith-pd =============================== Faith's Phylogenetic Diversity Outputs: -------- :vector.qza: Vector containing per-sample values for Faith's Phylogenetic Diversity. | Description: ------------ Computes Faith's Phylogenetic Diversity for all samples in a feature table. Examples: --------- basic 
*
 Using the 
qiime2 diversity-lib faith-pd
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib faith-pd [...] : vector.qza
 - 
faith-pd-vector.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity_lib__jaccard/qiime2__diversity_lib__jaccard/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity-lib jaccard ============================== Jaccard Distance Outputs: -------- :distance_matrix.qza: Distance matrix for Jaccard index | Description: ------------ Compute Jaccard distance for each sample in a feature table. Jaccard is calculated usingpresence/absence data. Data of type FeatureTable[Frequency | Relative Frequency] is reducedto presence/absence prior to calculation. Examples: --------- run on one core (by default) 
*
*
*
*
 Using the 
qiime2 diversity-lib jaccard
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib jaccard [...] : distance_matrix.qza
 - 
jaccard-dm.qza
 to run on n cores, replace 1 here with your preferred integer 
*
*
*
*
*
*
*
*
*
 Using the 
qiime2 diversity-lib jaccard
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib jaccard [...] : distance_matrix.qza
 - 
jaccard-dm.qza
 use 'auto' to run on all of host system's available CPU cores 
*
*
*
*
*
*
*
*
 Using the 
qiime2 diversity-lib jaccard
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib jaccard [...] : distance_matrix.qza
 - 
jaccard-dm.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity_lib__observed_features/qiime2__diversity_lib__observed_features/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity-lib observed-features ======================================== Observed Features Outputs: -------- :vector.qza: Vector containing per-sample counts of observed features. | Description: ------------ Compute the number of observed features for each sample in a feature table Examples: --------- basic 
*
 Using the 
qiime2 diversity-lib observed-features
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib observed-features [...] : vector.qza
 - 
obs-feat-vector.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity_lib__pielou_evenness/qiime2__diversity_lib__pielou_evenness/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity-lib pielou-evenness ====================================== Pielou's Evenness Outputs: -------- :vector.qza: Vector containing per-sample values for Pielou's Evenness. | Description: ------------ Compute Pielou's Evenness for each sample in a feature table Examples: --------- basic 
*
 Using the 
qiime2 diversity-lib pielou-evenness
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib pielou-evenness [...] : vector.qza
 - 
pielou-vector.qza
 dropping undefined samples 
*
*
*
 Using the 
qiime2 diversity-lib pielou-evenness
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Expand the 
additional options
 section - Set 
""drop_undefined_samples""
 to 
Yes
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib pielou-evenness [...] : vector.qza
 - 
pielou-vector.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity_lib__shannon_entropy/qiime2__diversity_lib__shannon_entropy/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity-lib shannon-entropy ====================================== Shannon's Entropy Outputs: -------- :vector.qza: Vector containing per-sample values for Shannon's Entropy. | Description: ------------ Compute Shannon's Entropy for each sample in a feature table Examples: --------- basic 
*
 Using the 
qiime2 diversity-lib shannon-entropy
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib shannon-entropy [...] : vector.qza
 - 
shannon-vector.qza
 base_e 
* | Set the logarithm base to e for the Shannon calculation. | This will result in values that match those produced by | vegan and scikit-bio. Using the 
qiime2 diversity-lib shannon-entropy
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Expand the 
additional options
 section - Set 
""base""
 to 
e
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib shannon-entropy [...] : vector.qza
 - 
shannon-vector.qza
 dropping undefined samples 
*
*
*
 Using the 
qiime2 diversity-lib shannon-entropy
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Expand the 
additional options
 section - Set 
""drop_undefined_samples""
 to 
Yes
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib shannon-entropy [...] : vector.qza
 - 
shannon-vector.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity_lib__unweighted_unifrac/qiime2__diversity_lib__unweighted_unifrac/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity-lib unweighted-unifrac ========================================= Unweighted Unifrac Outputs: -------- :distance_matrix.qza: Distance matrix for Unweighted Unifrac. | Description: ------------ Compute Unweighted Unifrac for each sample in a feature table Examples: --------- run on one core (by default) 
*
*
*
*
 Using the 
qiime2 diversity-lib unweighted-unifrac
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib unweighted-unifrac [...] : distance_matrix.qza
 - 
unweighted-unifrac-dm.qza
 to run on n cores, replace 1 here with your preferred integer 
*
*
*
*
*
*
*
*
*
 Using the 
qiime2 diversity-lib unweighted-unifrac
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib unweighted-unifrac [...] : distance_matrix.qza
 - 
unweighted-unifrac-dm.qza
 use 'auto' to run on all of host system's available CPU cores 
*
*
*
*
*
*
*
*
 Using the 
qiime2 diversity-lib unweighted-unifrac
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib unweighted-unifrac [...] : distance_matrix.qza
 - 
unweighted-unifrac-dm.qza
 use bypass_tips to trade specificity for reduced compute time 
*
*
*
*
*
*
*
*
 | bypass_tips can be used with any threads setting, but auto may be a good choice if you're trimming run time. Using the 
qiime2 diversity-lib unweighted-unifrac
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Expand the 
additional options
 section - Set 
""bypass_tips""
 to 
Yes
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib unweighted-unifrac [...] : distance_matrix.qza
 - 
unweighted-unifrac-dm.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__diversity_lib__weighted_unifrac/qiime2__diversity_lib__weighted_unifrac/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: diversity-lib weighted-unifrac ======================================= Weighted Unifrac Outputs: -------- :distance_matrix.qza: Distance matrix for Unweighted Unifrac. | Description: ------------ Compute Weighted Unifrac for each sample in a feature table Examples: --------- run on one core (by default) 
*
*
*
*
 Using the 
qiime2 diversity-lib weighted-unifrac
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib weighted-unifrac [...] : distance_matrix.qza
 - 
weighted-unifrac-dm.qza
 to run on n cores, replace 1 here with your preferred integer 
*
*
*
*
*
*
*
*
*
 Using the 
qiime2 diversity-lib weighted-unifrac
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib weighted-unifrac [...] : distance_matrix.qza
 - 
weighted-unifrac-dm.qza
 use 'auto' to run on all of host system's available CPU cores 
*
*
*
*
*
*
*
*
 Using the 
qiime2 diversity-lib weighted-unifrac
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib weighted-unifrac [...] : distance_matrix.qza
 - 
weighted-unifrac-dm.qza
 use bypass_tips to trade specificity for reduced compute time 
*
*
*
*
*
*
*
*
 | bypass_tips can be used with any threads setting, but auto may be a good choice if you're trimming run time. Using the 
qiime2 diversity-lib weighted-unifrac
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""phylogeny""
 to 
#: phylogeny.qza
 #. Expand the 
additional options
 section - Set 
""bypass_tips""
 to 
Yes
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 diversity-lib weighted-unifrac [...] : distance_matrix.qza
 - 
weighted-unifrac-dm.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__emperor__biplot/qiime2__emperor__biplot/2026.1.0+q2galaxy.2026.1.0	QIIME 2: emperor biplot ======================= Visualize and Interact with Principal Coordinates Analysis Biplot Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Generates an interactive ordination biplot where the user can visually integrate sample and feature metadata. Vectors representing the n most important features are then plotted in the emperor visualization (5 largest, by default). |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__emperor__plot/qiime2__emperor__plot/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: emperor plot ===================== Visualize and Interact with Principal Coordinates Analysis Plots Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Generates an interactive ordination plot where the user can visually integrate sample metadata. Examples: --------- emperor_plot 
*
*
 Using the 
qiime2 emperor plot
 tool: #. Set 
""pcoa""
 to 
#: pcoa-result.qza
 #. For 
""metadata""
: - Perform the following steps. #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
sample-metadata.tsv
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 emperor plot [...] : visualization.qzv
 - 
plot.qzv
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__emperor__procrustes_plot/qiime2__emperor__procrustes_plot/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: emperor procrustes-plot ================================ Visualize and Interact with a procrustes plot Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Plot two procrustes-fitted matrices Examples: --------- procrustes_plot 
*
*
 Using the 
qiime2 emperor procrustes-plot
 tool: #. Set 
""reference_pcoa""
 to 
#: bc-pcoa-result.qza
 #. Set 
""other_pcoa""
 to 
#: unw-pcoa-result.qza
 #. For 
""metadata""
: - Perform the following steps. #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
sample-metadata.tsv
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 emperor procrustes-plot [...] : visualization.qzv
 - 
plot.qzv
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_classifier__blast/qiime2__feature_classifier__blast/2026.1.0+q2galaxy.2026.1.0	QIIME 2: feature-classifier blast ================================= BLAST+ local alignment search. Outputs: -------- :search_results.qza: Top hits for each query. | Description: ------------ Search for top hits in a reference database via local alignment between the query sequences and reference database sequences using BLAST+. Returns a report of the top M hits for each query (where M=maxaccepts). |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_classifier__classify_consensus_blast/qiime2__feature_classifier__classify_consensus_blast/2026.1.0+q2galaxy.2026.1.0	QIIME 2: feature-classifier classify-consensus-blast ==================================================== BLAST+ consensus taxonomy classifier Outputs: -------- :classification.qza: Taxonomy classifications of query sequences. :search_results.qza: Top hits for each query. | Description: ------------ Assign taxonomy to query sequences using BLAST+. Performs BLAST+ local alignment between query and reference_reads, then assigns consensus taxonomy to each query sequence from among maxaccepts hits, min_consensus of which share that taxonomic assignment. Note that maxaccepts selects the first N hits with > perc_identity similarity to query, not the top N matches. For top N hits, use classify-consensus-vsearch. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_classifier__classify_consensus_vsearch/qiime2__feature_classifier__classify_consensus_vsearch/2026.1.0+q2galaxy.2026.1.0	QIIME 2: feature-classifier classify-consensus-vsearch ====================================================== VSEARCH-based consensus taxonomy classifier Outputs: -------- :classification.qza: Taxonomy classifications of query sequences. :search_results.qza: Top hits for each query. | Description: ------------ Assign taxonomy to query sequences using VSEARCH. Performs VSEARCH global alignment between query and reference_reads, then assigns consensus taxonomy to each query sequence from among maxaccepts top hits, min_consensus of which share that taxonomic assignment. Unlike classify-consensus-blast, this method searches the entire reference database before choosing the top N hits, not the first N hits. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_classifier__classify_hybrid_vsearch_sklearn/qiime2__feature_classifier__classify_hybrid_vsearch_sklearn/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-classifier classify-hybrid-vsearch-sklearn =========================================================== ALPHA Hybrid classifier: VSEARCH exact match + sklearn classifier Outputs: -------- :classification.qza: Taxonomy classifications of query sequences. | Description: ------------ NOTE: THIS PIPELINE IS AN ALPHA RELEASE. Please report bugs to https://forum.qiime2.org! Assign taxonomy to query sequences using hybrid classifier. First performs rough positive filter to remove artifact and low-coverage sequences (use ""prefilter"" parameter to toggle this step on or off). Second, performs VSEARCH exact match between query and reference_reads to find exact matches, followed by least common ancestor consensus taxonomy assignment from among maxaccepts top hits, min_consensus of which share that taxonomic assignment. Query sequences without an exact match are then classified with a pre-trained sklearn taxonomy classifier to predict the most likely taxonomic lineage. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_classifier__classify_sklearn/qiime2__feature_classifier__classify_sklearn/2026.1.0+q2galaxy.2026.1.0	QIIME 2: feature-classifier classify-sklearn ============================================ Pre-fitted sklearn-based taxonomy classifier Outputs: -------- :classification.qza: <no description> | Description: ------------ Classify reads by taxon using a fitted classifier. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_classifier__extract_reads/qiime2__feature_classifier__extract_reads/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-classifier extract-reads ========================================= Extract reads from reference sequences. Outputs: -------- :reads.qza: <no description> | Description: ------------ Extract simulated amplicon reads from a reference database. Performs in-silico PCR to extract simulated amplicons from reference sequences that match the input primer sequences (within the mismatch threshold specified by 
identity
). Both primer sequences must be in the 5' -> 3' orientation. Sequences that fail to match both primers will be excluded. Reads are extracted, trimmed, and filtered in the following order: 1. reads are extracted in specified orientation; 2. primers are removed; 3. reads longer than 
max_length
 are removed; 4. reads are trimmed with 
trim_right
; 5. reads are truncated to 
trunc_len
; 6. reads are trimmed with 
trim_left
; 7. reads shorter than 
min_length
 are removed. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_classifier__find_consensus_annotation/qiime2__feature_classifier__find_consensus_annotation/2026.1.0+q2galaxy.2026.1.0	QIIME 2: feature-classifier find-consensus-annotation ===================================================== Find consensus among multiple annotations. Outputs: -------- :consensus_taxonomy.qza: Consensus taxonomy and scores. | Description: ------------ Find consensus annotation for each query searched against a reference database, by finding the least common ancestor among one or more semicolon-delimited hierarchical annotations. Note that the annotation hierarchy is assumed to have an even number of ranks. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_classifier__fit_classifier_naive_bayes/qiime2__feature_classifier__fit_classifier_naive_bayes/2026.1.0+q2galaxy.2026.1.0	QIIME 2: feature-classifier fit-classifier-naive-bayes ====================================================== Train the naive_bayes classifier Outputs: -------- :classifier.qza: <no description> | Description: ------------ Create a scikit-learn naive_bayes classifier for reads |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_classifier__fit_classifier_sklearn/qiime2__feature_classifier__fit_classifier_sklearn/2026.1.0+q2galaxy.2026.1.0	QIIME 2: feature-classifier fit-classifier-sklearn ================================================== Train an almost arbitrary scikit-learn classifier Outputs: -------- :classifier.qza: <no description> | Description: ------------ Train a scikit-learn classifier to classify reads. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_classifier__vsearch_global/qiime2__feature_classifier__vsearch_global/2026.1.0+q2galaxy.2026.1.0	QIIME 2: feature-classifier vsearch-global ========================================== VSEARCH global alignment search Outputs: -------- :search_results.qza: Top hits for each query. | Description: ------------ Search for top hits in a reference database via global alignment between the query sequences and reference database sequences using VSEARCH. Returns a report of the top M hits for each query (where M=maxaccepts or maxhits). |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__core_features/qiime2__feature_table__core_features/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-table core-features ==================================== Identify core features in table Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Identify ""core"" features, which are features observed in a user-defined fraction of the samples. Since the core features are a function of the fraction of samples that the feature must be observed in to be considered core, this is computed over a range of fractions defined by the 
min_fraction
, 
max_fraction
, and 
steps
 parameters. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__filter_features/qiime2__feature_table__filter_features/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-table filter-features ====================================== Filter features from table Outputs: -------- :filtered_table.qza: The resulting feature table filtered by feature. | Description: ------------ Filter features from table based on frequency and/or metadata. Any samples with a frequency of zero after feature filtering will also be removed. Examples: --------- filter_features_min_samples 
*
*
*
*
 Using the 
qiime2 feature-table filter-features
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Expand the 
additional options
 section - Set 
""min_samples""* to 
2
 #. Press the 
Execute
 button. filter_features_sequences 
*
*
*
 | Retain only features that are represented in the collection of sequences provided as metadata. This is useful, for example, for removing sequences that are identified as chimeric. To learn about using Artifacts as Metadata, as is performed here, see https://use.qiime2.org/en/stable/how-to-guides/artifacts-as-metadata.html Using the 
qiime2 feature-table filter-features
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Expand the 
additional options
 section - For 
""metadata""
: - Press the 
+ Insert metadata
 button to set up the next steps. #. Change to 
Metadata from Artifact
 #. Set 
""Metadata Source""
 to 
sequences.qza
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__filter_features_conditionally/qiime2__feature_table__filter_features_conditionally/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-table filter-features-conditionally ==================================================== Filter features from a table based on abundance and prevalence Outputs: -------- :filtered_table.qza: The resulting feature table filtered by feature. | Description: ------------ Filter features based on the relative abundance in a certain portion of samples (i.e., features must have a relative abundance of at least 
abundance
 in at least 
prevalence
 number of samples). Any samples with a frequency of zero after feature filtering will also be removed. Examples: --------- feature_table_filter_features_conditionally 
*
*
*
*
*
*
 | Retain only features with at least 1%% abundance in at least 34%% of samples. Using the 
qiime2 feature-table filter-features-conditionally
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""abundance""
 to 
0.01
 #. Set 
""prevalence""* to 
0.34
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__filter_samples/qiime2__feature_table__filter_samples/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-table filter-samples ===================================== Filter samples from table Outputs: -------- :filtered_table.qza: The resulting feature table filtered by sample. | Description: ------------ Filter samples from table based on frequency and/or metadata. Any features with a frequency of zero after sample filtering will also be removed. Examples: --------- filter_to_subject1 
*
*
* Using the 
qiime2 feature-table filter-samples
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Expand the 
additional options
 section #. For 
""metadata""
: - Press the 
+ Insert metadata
 button to set up the next steps. #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
sample-metadata.tsv
 #. Set 
""where""* to 
[subject]='subject-1'
 #. Press the 
Execute
 button. filter_to_skin 
*
*
 Using the 
qiime2 feature-table filter-samples
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Expand the 
additional options
 section #. For 
""metadata""
: - Press the 
+ Insert metadata
 button to set up the next steps. #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
sample-metadata.tsv
 #. Set 
""where""
 to 
[body-site] IN ('left palm', 'right palm')
 #. Press the 
Execute
 button. filter_to_subject1_gut 
*
*
*
 Using the 
qiime2 feature-table filter-samples
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Expand the 
additional options
 section #. For 
""metadata""
: - Press the 
+ Insert metadata
 button to set up the next steps. #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
sample-metadata.tsv
 #. Set 
""where""* to 
[subject]='subject-1' AND [body-site]='gut'
 #. Press the 
Execute
 button. filter_to_gut_or_abx 
*
*
* Using the 
qiime2 feature-table filter-samples
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Expand the 
additional options
 section #. For 
""metadata""
: - Press the 
+ Insert metadata
 button to set up the next steps. #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
sample-metadata.tsv
 #. Set 
""where""* to 
[body-site]='gut' OR [reported-antibiotic-usage]='Yes'
 #. Press the 
Execute
 button. filter_to_subject1_not_gut 
*
*
*
 Using the 
qiime2 feature-table filter-samples
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Expand the 
additional options
 section #. For 
""metadata""
: - Press the 
+ Insert metadata
 button to set up the next steps. #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
sample-metadata.tsv
 #. Set 
""where""* to 
[subject]='subject-1' AND NOT [body-site]='gut'
 #. Press the 
Execute
 button. filter_min_features 
*
*
 Using the 
qiime2 feature-table filter-samples
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Expand the 
additional options
 section - Set 
""min_features""* to 
10
 #. Press the 
Execute
 button. filter_min_frequency 
*
*
 Using the 
qiime2 feature-table filter-samples
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Expand the 
additional options
 section - Set 
""min_frequency""
 to 
1500
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__filter_seqs/qiime2__feature_table__filter_seqs/2026.1.0+q2galaxy.2026.1.0	QIIME 2: feature-table filter-seqs ================================== Filter features from sequences Outputs: -------- :filtered_data.qza: The resulting filtered sequences. | Description: ------------ Filter features from sequences based on a feature table or metadata. This method can filter based on ids in a table or a metadata file, but not both (i.e., the table and metadata options are mutually exclusive). |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__group/qiime2__feature_table__group/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-table group ============================ Group samples or features by a metadata column Outputs: -------- :grouped_table.qza: A table that has been grouped along the given 
axis
. IDs on that axis are replaced by values in the 
metadata
 column. | Description: ------------ Group samples or features in a feature table using metadata to define the mapping of IDs to a group. Examples: --------- group_samples 
*
*
 | Combine samples from the same body-site into single sample. Feature frequencies will be the median across the samples being combined, rounded up to the nearest whole number. Using the 
qiime2 feature-table group
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Set 
""axis""
 to 
sample
 #. For 
""metadata""
: #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
sample-metadata.tsv
 #. Set 
""Column Name""
 to 
body-site
 #. Set 
""mode""
 to 
median-ceiling
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 feature-table group [...] : grouped_table.qza
 - 
body-site-table.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__heatmap/qiime2__feature_table__heatmap/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-table heatmap ============================== Generate a heatmap representation of a feature table Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Generate a heatmap representation of a feature table with optional clustering on both the sample and feature axes. Tip: To generate a heatmap containing taxonomic annotations, use 
qiime taxa collapse
 to collapse the feature table at the desired taxonomic level. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__merge/qiime2__feature_table__merge/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-table merge ============================ Combine multiple tables Outputs: -------- :merged_table.qza: The resulting merged feature table. | Description: ------------ Combines feature tables using the 
overlap_method
 provided. Examples: --------- feature_table_merge_two_tables 
*
*
*
*
 Using the 
qiime2 feature-table merge
 tool: #. For 
""tables""*, use ctrl-(or command)-click to select the following inputs: #. 
#: feature-table1.qza
 #. 
#: feature-table2.qza
 #. Press the 
Execute
 button. feature_table_merge_three_tables 
*
*
*
*
 Using the 
qiime2 feature-table merge
 tool: #. For 
""tables""
, use ctrl-(or command)-click to select the following inputs: #. 
#: feature-table1.qza
 #. 
#: feature-table2.qza
 #. 
#: feature-table3.qza
 #. Expand the 
additional options
 section - Set 
""overlap_method""* to 
sum
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__merge_seqs/qiime2__feature_table__merge_seqs/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-table merge-seqs ================================= Combine collections of feature sequences Outputs: -------- :merged_data.qza: The resulting collection of feature sequences containing all feature sequences provided. | Description: ------------ Combines feature data objects which may or may not contain data for the same features. If different feature data is present for the same feature id in the inputs, the data from the first will be propagated to the result. Examples: --------- feature_table_merge_seqs 
*
*
*
*
 Using the 
qiime2 feature-table merge-seqs
 tool: #. For 
""data""
, use ctrl-(or command)-click to select the following inputs: #. 
#: seqs1.qza
 #. 
#: seqs2.qza
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__merge_taxa/qiime2__feature_table__merge_taxa/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-table merge-taxa ================================= Combine collections of feature taxonomies Outputs: -------- :merged_data.qza: The resulting collection of feature taxonomies containing all feature taxonomies provided. | Description: ------------ Combines a pair of feature data objects which may or may not contain data for the same features. If different feature data is present for the same feature id in the inputs, the data from the first will be propagated to the result. Examples: --------- feature_table_merge_taxa 
*
*
*
*
 Using the 
qiime2 feature-table merge-taxa
 tool: #. For 
""data""
, use ctrl-(or command)-click to select the following inputs: #. 
#: tax1.qza
 #. 
#: tax2.qza
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__presence_absence/qiime2__feature_table__presence_absence/2026.1.0+q2galaxy.2026.1.0	QIIME 2: feature-table presence-absence ======================================= Convert to presence/absence Outputs: -------- :presence_absence_table.qza: The resulting presence/absence feature table. | Description: ------------ Convert frequencies to binary values indicating presence or absence of a feature in a sample. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__rarefy/qiime2__feature_table__rarefy/2026.1.0+q2galaxy.2026.1.0	QIIME 2: feature-table rarefy ============================= Rarefy table Outputs: -------- :rarefied_table.qza: The resulting rarefied feature table. | Description: ------------ Subsample frequencies from all samples so that the sum of frequencies in each sample is equal to sampling-depth. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__relative_frequency/qiime2__feature_table__relative_frequency/2026.1.0+q2galaxy.2026.1.0	QIIME 2: feature-table relative-frequency ========================================= Convert to relative frequencies Outputs: -------- :relative_frequency_table.qza: The resulting relative frequency feature table. | Description: ------------ Convert frequencies to relative frequencies by dividing each frequency in a sample by the sum of frequencies in that sample. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__rename_ids/qiime2__feature_table__rename_ids/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-table rename-ids ================================= Renames sample or feature ids in a table Outputs: -------- :renamed_table.qza: A table which has new ids, where the ids are replaced by values in the 
metadata
 column. | Description: ------------ Renames the sample or feature ids in a feature table using metadata to define the new ids. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__subsample/qiime2__feature_table__subsample/2023.5.0+q2galaxy.2023.5.0.2	QIIME 2: feature-table subsample ================================ Subsample table Outputs: -------- :sampled_table.qza: The resulting subsampled feature table. | Description: ------------ Randomly pick samples or features, without replacement, from the table. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__summarize/qiime2__feature_table__summarize/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-table summarize ================================ Summarize table Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Generate visual and tabular summaries of a feature table. Examples: --------- feature_table_summarize 
*
*
*
 Using the 
qiime2 feature-table summarize
 tool: #. Set 
""table""
 to 
#: feature-table.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 feature-table summarize [...] : visualization.qzv
 - 
table.qzv
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__tabulate_seqs/qiime2__feature_table__tabulate_seqs/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: feature-table tabulate-seqs ==================================== View sequence associated with each feature Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Generate tabular view of feature identifier to sequence mapping, including links to BLAST each sequence against the NCBI nt database. Examples: --------- feature_table_tabulate_seqs 
*
*
*
*
 Using the 
qiime2 feature-table tabulate-seqs
 tool: #. Set 
""data""
 to 
#: rep-seqs.qza
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 feature-table tabulate-seqs [...] : visualization.qzv
 - 
rep-seqs.qzv
 feature_table_tabulate_seqs_single_taxon 
*
*
*
*
*
 Using the 
qiime2 feature-table tabulate-seqs
 tool: #. Set 
""data""
 to 
#: rep-seqs-single-taxon.qza
 #. Expand the 
additional options
 section - Set 
""taxonomy""
 to 
#: ['single-taxonomy.qza']
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 feature-table tabulate-seqs [...] : visualization.qzv
 - 
rep-seqs.qzv
 feature_table_tabulate_seqs_multi_taxon 
*
*
*
*
*
 Using the 
qiime2 feature-table tabulate-seqs
 tool: #. Set 
""data""
 to 
#: rep-seqs-multi-taxon.qza
 #. Expand the 
additional options
 section - Set 
""taxonomy""
 to 
#: multi-taxonomy/
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 feature-table tabulate-seqs [...] : visualization.qzv
 - 
rep-seqs.qzv
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__feature_table__transpose/qiime2__feature_table__transpose/2026.1.0+q2galaxy.2026.1.0	QIIME 2: feature-table transpose ================================ Transpose a feature table. Outputs: -------- :transposed_feature_table.qza: The resulting transposed feature table. | Description: ------------ Transpose the rows and columns (typically samples and features) of a feature table. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__fragment_insertion__classify_otus_experimental/qiime2__fragment_insertion__classify_otus_experimental/2026.1.0+q2galaxy.2026.1.0	QIIME 2: fragment-insertion classify-otus-experimental ====================================================== Experimental: Obtain taxonomic lineages, by finding closest OTU in reference phylogeny. Outputs: -------- :classification.qza: Taxonomic lineages for inserted fragments. | Description: ------------ Experimental: Use the resulting tree from 'sepp' and find closest OTU-ID for every inserted fragment. Then, look up the reference lineage string in the reference taxonomy. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__fragment_insertion__filter_features/qiime2__fragment_insertion__filter_features/2026.1.0+q2galaxy.2026.1.0	QIIME 2: fragment-insertion filter-features =========================================== Filter fragments in tree from table. Outputs: -------- :filtered_table.qza: The input table minus those fragments that were not part of the tree. This feature-table can be used for downstream analyses like phylogenetic alpha- or beta- diversity computation. :removed_table.qza: Those fragments that got removed from the input table, because they were not part of the tree. This table is mainly used for quality control, e.g. to inspect the ratio of removed reads per sample from the input table. You can ignore this table for downstream analyses. | Description: ------------ Filters fragments not inserted into a phylogenetic tree from a feature-table. Some fragments computed by e.g. Deblur or DADA2 are too remote to get inserted by SEPP into a reference phylogeny. To be able to use the feature-table for downstream analyses like computing Faith's PD or UniFrac, the feature-table must be cleared of fragments that are not part of the phylogenetic tree, because their path length can otherwise not be determined. Typically, the number of rejected fragments is low (<= 10), but it might be worth to inspect the ratio of reads assigned to those rejected fragments. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__fragment_insertion__sepp/qiime2__fragment_insertion__sepp/2026.1.0+q2galaxy.2026.1.0	QIIME 2: fragment-insertion sepp ================================ Insert fragment sequences using SEPP into reference phylogenies. Outputs: -------- :tree.qza: The tree with inserted feature data. :placements.qza: Information about the feature placements within the reference tree. | Description: ------------ Perform fragment insertion of sequences using the SEPP algorithm. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__gneiss__assign_ids/qiime2__gneiss__assign_ids/2023.5.0+q2galaxy.2023.5.0.2	QIIME 2: gneiss assign-ids ========================== Assigns ids on internal nodes in the tree, and makes sure that they are consistent with the table columns. Outputs: -------- :output_table.qza: A table with features matching the tree tips. :output_tree.qza: A tree with uniquely identifying ids. | Description: ------------ Assigns UUIDs to uniquely identify internal nodes in the tree. Also corrects for polytomies to create strictly bifurcating trees and aligns the table columns with the tree tip names |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__gneiss__correlation_clustering/qiime2__gneiss__correlation_clustering/2023.5.0+q2galaxy.2023.5.0.2	QIIME 2: gneiss correlation-clustering ====================================== Hierarchical clustering using feature correlation. Outputs: -------- :clustering.qza: A hierarchy of feature identifiers where each tip corresponds to the feature identifiers in the table. This tree can contain tip ids that are not present in the table, but all feature ids in the table must be present in this tree. | Description: ------------ Build a bifurcating tree that represents a hierarchical clustering of features. The hiearchical clustering uses Ward hierarchical clustering based on the degree of proportionality between features. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__gneiss__dendrogram_heatmap/qiime2__gneiss__dendrogram_heatmap/2023.5.0+q2galaxy.2023.5.0.2	QIIME 2: gneiss dendrogram-heatmap ================================== Dendrogram heatmap. Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Visualize the feature table as a heatmap, with samples sorted along a specified categorical metadata column and features clustered together specified by the tree. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__gneiss__gradient_clustering/qiime2__gneiss__gradient_clustering/2023.5.0+q2galaxy.2023.5.0.2	QIIME 2: gneiss gradient-clustering =================================== Hierarchical clustering using gradient information. Outputs: -------- :clustering.qza: A hierarchy of feature identifiers where each tip corresponds to the feature identifiers in the table. This tree can contain tip ids that are not present in the table, but all feature ids in the table must be present in this tree. | Description: ------------ Build a bifurcating tree that represents a hierarchical clustering of features. The hiearchical clustering uses Ward hierarchical clustering based on the mean difference of gradients that each feature is observed in. This method is primarily used to sort the table to reveal the underlying block-like structures. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__gneiss__ilr_hierarchical/qiime2__gneiss__ilr_hierarchical/2023.5.0+q2galaxy.2023.5.0.2	QIIME 2: gneiss ilr-hierarchical ================================ Isometric Log-ratio Transform applied to a hierarchical clustering Outputs: -------- :balances.qza: The resulting balances from the ilr transform. | Description: ------------ Calculate balances given a hierarchy. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__gneiss__ilr_phylogenetic/qiime2__gneiss__ilr_phylogenetic/2023.5.0+q2galaxy.2023.5.0.2	QIIME 2: gneiss ilr-phylogenetic ================================ Isometric Log-ratio Transform applied to a phylogenetic tree Outputs: -------- :balances.qza: The resulting balances from the ilr transform. :hierarchy.qza: Hierarchy from bifurcated phylogeny | Description: ------------ Calculate balances given a rooted phylogeny. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__gneiss__ilr_phylogenetic_differential/qiime2__gneiss__ilr_phylogenetic_differential/2023.5.0+q2galaxy.2023.5.0.2	QIIME 2: gneiss ilr-phylogenetic-differential ============================================= Differentially abundant Phylogenetic Log Ratios. Outputs: -------- :ilr_differential.qza: Per clade differential abundance results. :bifurcated_tree.qza: Bifurcating phylogeny. | Description: ------------ Compute an ILR transform of differentials given a rooted phylogeny. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__gneiss__ilr_phylogenetic_ordination/qiime2__gneiss__ilr_phylogenetic_ordination/2023.5.0+q2galaxy.2023.5.0.2	QIIME 2: gneiss ilr-phylogenetic-ordination =========================================== Ordination through a phylogenetic Isometric Log Ratio transform. Outputs: -------- :ordination.qza: The resulting ordination from the ilr transform. :bifurcated_tree.qza: Bifurcating phylogeny :clade_metadata.qza: Metadata specifying clade membership. | Description: ------------ Compute an ILR ordination given a rooted phylogeny. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__longitudinal__anova/qiime2__longitudinal__anova/2026.1.0+q2galaxy.2026.1.0	QIIME 2: longitudinal anova =========================== ANOVA test Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Perform an ANOVA test on any factors present in a metadata file and/or metadata-transformable artifacts. This is followed by pairwise t-tests to examine pairwise differences between categorical sample groups. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__longitudinal__feature_volatility/qiime2__longitudinal__feature_volatility/2026.1.0+q2galaxy.2026.1.0	QIIME 2: longitudinal feature-volatility ======================================== Feature volatility analysis Outputs: -------- :filtered_table.qza: Feature table containing only important features. :feature_importance.qza: Importance of each input feature to model accuracy. :volatility_plot.qzv: Interactive volatility plot visualization. :accuracy_results.qzv: Accuracy results visualization. :sample_estimator.qza: Trained sample regressor. | Description: ------------ Identify features that are predictive of a numeric metadata column, state_column (e.g., time), and plot their relative frequencies across states using interactive feature volatility plots. A supervised learning regressor is used to identify important features and assess their ability to predict sample states. state_column will typically be a measure of time, but any numeric metadata column can be used. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__longitudinal__first_differences/qiime2__longitudinal__first_differences/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: longitudinal first-differences ======================================= Compute first differences or difference from baseline between sequential states Outputs: -------- :first_differences.qza: Series of first differences. | Description: ------------ Calculates first differences in ""metric"" between sequential states for samples collected from individual subjects sampled repeatedly at two or more states. First differences can be performed on a metadata column (including artifacts that can be input as metadata) or a feature in a feature table. Outputs a data series of first differences for each individual subject at each sequential pair of states, labeled by the SampleID of the second state (e.g., paired differences between time 0 and time 1 would be labeled by the SampleIDs at time 1). This file can be used as input to linear mixed effects models or other longitudinal or diversity methods to compare changes in first differences across time or among groups of subjects. Also supports differences from baseline (or other static comparison state) by setting the ""baseline"" parameter. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__longitudinal__first_distances/qiime2__longitudinal__first_distances/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: longitudinal first-distances ===================================== Compute first distances or distance from baseline between sequential states Outputs: -------- :first_distances.qza: Series of first distances. | Description: ------------ Calculates first distances between sequential states for samples collected from individual subjects sampled repeatedly at two or more states. This method is similar to the ""first differences"" method, except that it requires a distance matrix as input and calculates first differences as distances between successive states. Outputs a data series of first distances for each individual subject at each sequential pair of states, labeled by the SampleID of the second state (e.g., paired distances between time 0 and time 1 would be labeled by the SampleIDs at time 1). This file can be used as input to linear mixed effects models or other longitudinal or diversity methods to compare changes in first distances across time or among groups of subjects. Also supports distance from baseline (or other static comparison state) by setting the ""baseline"" parameter. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__longitudinal__linear_mixed_effects/qiime2__longitudinal__linear_mixed_effects/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: longitudinal linear-mixed-effects ========================================== Linear mixed effects modeling Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Linear mixed effects models evaluate the contribution of exogenous covariates ""group_columns"" and ""random_effects"" to a single dependent variable, ""metric"". Perform LME and plot line plots of each group column. A feature table artifact is required input, though whether ""metric"" is derived from the feature table or metadata is optional. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__longitudinal__maturity_index/qiime2__longitudinal__maturity_index/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: longitudinal maturity-index ==================================== Microbial maturity index prediction. Outputs: -------- :sample_estimator.qza: Trained sample estimator. :feature_importance.qza: Importance of each input feature to model accuracy. :predictions.qza: Predicted target values for each input sample. :model_summary.qzv: Summarized parameter and (if enabled) feature selection information for the trained estimator. :accuracy_results.qzv: Accuracy results visualization. :maz_scores.qza: Microbiota-for-age z-score predictions. :clustermap.qzv: Heatmap of important feature abundance at each time point in each group. :volatility_plots.qzv: Interactive volatility plots of MAZ and maturity scores, target (column) predictions, and the sample metadata. | Description: ------------ Calculates a ""microbial maturity"" index from a regression model trained on feature data to predict a given continuous metadata column, e.g., to predict age as a function of microbiota composition. The model is trained on a subset of control group samples, then predicts the column value for all samples. This visualization computes maturity index z-scores to compare relative ""maturity"" between each group, as described in doi:10.1038/nature13421. This method can be used to predict between-group differences in relative trajectory across any type of continuous metadata gradient, e.g., intestinal microbiome development by age, microbial succession during wine fermentation, or microbial community differences along environmental gradients, as a function of two or more different ""treatment"" groups. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__longitudinal__nmit/qiime2__longitudinal__nmit/2026.1.0+q2galaxy.2026.1.0	QIIME 2: longitudinal nmit ========================== Nonparametric microbial interdependence test Outputs: -------- :distance_matrix.qza: The resulting distance matrix. | Description: ------------ Perform nonparametric microbial interdependence test to determine longitudinal sample similarity as a function of temporal microbial composition. For more details and citation, please see doi.org/10.1002/gepi.22065 |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__longitudinal__pairwise_differences/qiime2__longitudinal__pairwise_differences/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: longitudinal pairwise-differences ========================================== Paired difference testing and boxplots Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Performs paired difference testing between samples from each subject. Sample pairs may represent a typical intervention study (e.g., samples collected pre- and post-treatment), paired samples from two different timepoints (e.g., in a longitudinal study design), or identical samples receiving different treatments. This action tests whether the change in a numeric metadata value ""metric"" differs from zero and differs between groups (e.g., groups of subjects receiving different treatments), and produces boxplots of paired difference distributions for each group. Note that ""metric"" can be derived from a feature table or metadata. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__longitudinal__pairwise_distances/qiime2__longitudinal__pairwise_distances/2026.1.0+q2galaxy.2026.1.0	QIIME 2: longitudinal pairwise-distances ======================================== Paired pairwise distance testing and boxplots Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Performs pairwise distance testing between sample pairs from each subject. Sample pairs may represent a typical intervention study, e.g., samples collected pre- and post-treatment; paired samples from two different timepoints (e.g., in a longitudinal study design), or identical samples receiving different two different treatments. This action tests whether the pairwise distance between each subject pair differs between groups (e.g., groups of subjects receiving different treatments) and produces boxplots of paired distance distributions for each group. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__longitudinal__plot_feature_volatility/qiime2__longitudinal__plot_feature_volatility/2026.1.0+q2galaxy.2026.1.0	QIIME 2: longitudinal plot-feature-volatility ============================================= Plot longitudinal feature volatility and importances Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Plots an interactive control chart of feature abundances (y-axis) in each sample across time (or state; x-axis). Feature importance scores and descriptive statistics for each feature are plotted in interactive bar charts below the control chart, facilitating exploration of longitudinal feature data. This visualization is intended for use with the feature-volatility pipeline; use that pipeline to access this visualization. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__longitudinal__volatility/qiime2__longitudinal__volatility/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: longitudinal volatility ================================ Generate interactive volatility plot Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Generate an interactive control chart depicting the longitudinal volatility of sample metadata and/or feature frequencies across time (as set using the ""state_column"" parameter). Any numeric metadata column (and metadata-transformable artifacts, e.g., alpha diversity results) can be plotted on the y-axis, and are selectable using the ""metric_column"" selector. Metric values are averaged to compare across any categorical metadata column using the ""group_column"" selector. Longitudinal volatility for individual subjects sampled over time is co-plotted as ""spaghetti"" plots if the ""individual_id_column"" parameter is used. state_column will typically be a measure of time, but any numeric metadata column can be used. Examples: --------- longitudinal_volatility 
*
*
*
 Using the 
qiime2 longitudinal volatility
 tool: #. For 
""metadata""
: - Perform the following steps. #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
metadata.tsv
 #. Set 
""state_column""
 to 
month
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 longitudinal volatility [...] : visualization.qzv
 - 
volatility-plot.qzv
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__metadata__distance_matrix/qiime2__metadata__distance_matrix/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: metadata distance-matrix ================================= Create a distance matrix from a numeric Metadata column Outputs: -------- :distance_matrix.qza: <no description> | Description: ------------ Create a distance matrix from a numeric metadata column. The Euclidean distance is computed between each pair of samples or features in the column. Tip: the distance matrix produced by this method can be used as input to the Mantel test available in 
q2-diversity
. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__metadata__shuffle_groups/qiime2__metadata__shuffle_groups/2026.1.0+q2galaxy.2026.1.0	QIIME 2: metadata shuffle-groups ================================ Shuffle values in a categorical sample metadata column. Outputs: -------- :shuffled_groups.qza: Randomized metadata columns | Description: ------------ Create one or more categorical sample metadata columns by shuffling the values in an input metadata column. To avoid confusion, the column name and values will be derived from the provided prefixes. The number of different values (or groups), and the counts of each value, will match the input metadata column but the association of values with sample ids will be random. These data will be written to an artifact that can be used as sample metadata. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__metadata__tabulate/qiime2__metadata__tabulate/2026.1.0+q2galaxy.2026.1.0	QIIME 2: metadata tabulate ========================== Interactively explore Metadata in an HTML table Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Generate a tabular view of Metadata. The output visualization supports interactive filtering, sorting, and exporting to common file formats. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__phylogeny__align_to_tree_mafft_fasttree/qiime2__phylogeny__align_to_tree_mafft_fasttree/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: phylogeny align-to-tree-mafft-fasttree =============================================== Build a phylogenetic tree using fasttree and mafft alignment Outputs: -------- :alignment.qza: The aligned sequences. :masked_alignment.qza: The masked alignment. :tree.qza: The unrooted phylogenetic tree. :rooted_tree.qza: The rooted phylogenetic tree. | Description: ------------ This pipeline will start by creating a sequence alignment using MAFFT, after which any alignment columns that are phylogenetically uninformative or ambiguously aligned will be removed (masked). The resulting masked alignment will be used to infer a phylogenetic tree and then subsequently rooted at its midpoint. Output files from each step of the pipeline will be saved. This includes both the unmasked and masked MAFFT alignment from q2-alignment methods, and both the rooted and unrooted phylogenies from q2-phylogeny methods. Examples: --------- align_to_tree_mafft_fasttree 
*
*
*
*
 Using the 
qiime2 phylogeny align-to-tree-mafft-fasttree
 tool: #. Set 
""sequences""
 to 
#: rep-seqs.qza
 #. Press the 
Execute
 button. Once completed, for each new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 phylogeny align-to-tree-mafft-fasttree [...] : alignment.qza
 - 
aligned-rep-seqs.qza
 * - 
#: qiime2 phylogeny align-to-tree-mafft-fasttree [...] : masked_alignment.qza
 - 
masked-aligned-rep-seqs.qza
 * - 
#: qiime2 phylogeny align-to-tree-mafft-fasttree [...] : tree.qza
 - 
unrooted-tree.qza
 * - 
#: qiime2 phylogeny align-to-tree-mafft-fasttree [...] : rooted_tree.qza
 - 
rooted-tree.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__phylogeny__align_to_tree_mafft_iqtree/qiime2__phylogeny__align_to_tree_mafft_iqtree/2026.1.0+q2galaxy.2026.1.0	QIIME 2: phylogeny align-to-tree-mafft-iqtree ============================================= Build a phylogenetic tree using iqtree and mafft alignment. Outputs: -------- :alignment.qza: The aligned sequences. :masked_alignment.qza: The masked alignment. :tree.qza: The unrooted phylogenetic tree. :rooted_tree.qza: The rooted phylogenetic tree. | Description: ------------ This pipeline will start by creating a sequence alignment using MAFFT, after which any alignment columns that are phylogenetically uninformative or ambiguously aligned will be removed (masked). The resulting masked alignment will be used to infer a phylogenetic tree using IQ-TREE. By default the best fit substitution model will be determined by ModelFinder prior to phylogenetic inference. The resulting tree will be subsequently rooted at its midpoint. Output files from each step of the pipeline will be saved. This includes both the unmasked and masked MAFFT alignment from q2-alignment methods, and both the rooted and unrooted phylogenies from q2-phylogeny methods. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__phylogeny__align_to_tree_mafft_raxml/qiime2__phylogeny__align_to_tree_mafft_raxml/2026.1.0+q2galaxy.2026.1.0	QIIME 2: phylogeny align-to-tree-mafft-raxml ============================================ Build a phylogenetic tree using raxml and mafft alignment. Outputs: -------- :alignment.qza: The aligned sequences. :masked_alignment.qza: The masked alignment. :tree.qza: The unrooted phylogenetic tree. :rooted_tree.qza: The rooted phylogenetic tree. | Description: ------------ This pipeline will start by creating a sequence alignment using MAFFT, after which any alignment columns that are phylogenetically uninformative or ambiguously aligned will be removed (masked). The resulting masked alignment will be used to infer a phylogenetic tree using RAxML, under the specified substitution model, and then subsequently rooted at its midpoint. Output files from each step of the pipeline will be saved. This includes both the unmasked and masked MAFFT alignment from q2-alignment methods, and both the rooted and unrooted phylogenies from q2-phylogeny methods. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__phylogeny__fasttree/qiime2__phylogeny__fasttree/2026.1.0+q2galaxy.2026.1.0	QIIME 2: phylogeny fasttree =========================== Construct a phylogenetic tree with FastTree. Outputs: -------- :tree.qza: The resulting phylogenetic tree. | Description: ------------ Construct a phylogenetic tree with FastTree. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__phylogeny__filter_table/qiime2__phylogeny__filter_table/2026.1.0+q2galaxy.2026.1.0	QIIME 2: phylogeny filter-table =============================== Remove features from table if they're not present in tree. Outputs: -------- :filtered_table.qza: The resulting feature table. | Description: ------------ Remove features from a feature table if their identifiers are not tip identifiers in tree. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__phylogeny__filter_tree/qiime2__phylogeny__filter_tree/2026.1.0+q2galaxy.2026.1.0	QIIME 2: phylogeny filter-tree ============================== Remove features from tree based on metadata Outputs: -------- :filtered_tree.qza: The resulting phylogenetic tree. | Description: ------------ Remove tips from a tree if their identifiers based on a set of provided identifiers. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__phylogeny__iqtree/qiime2__phylogeny__iqtree/2026.1.0+q2galaxy.2026.1.0	QIIME 2: phylogeny iqtree ========================= Construct a phylogenetic tree with IQ-TREE. Outputs: -------- :tree.qza: The resulting phylogenetic tree. | Description: ------------ Construct a phylogenetic tree using IQ-TREE (http://www.iqtree.org/) with automatic model selection. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__phylogeny__iqtree_ultrafast_bootstrap/qiime2__phylogeny__iqtree_ultrafast_bootstrap/2026.1.0+q2galaxy.2026.1.0	QIIME 2: phylogeny iqtree-ultrafast-bootstrap ============================================= Construct a phylogenetic tree with IQ-TREE with bootstrap supports. Outputs: -------- :tree.qza: The resulting phylogenetic tree. | Description: ------------ Construct a phylogenetic tree using IQ-TREE (http://www.iqtree.org/) with automatic model selection and bootstrap supports. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__phylogeny__midpoint_root/qiime2__phylogeny__midpoint_root/2026.1.0+q2galaxy.2026.1.0	QIIME 2: phylogeny midpoint-root ================================ Midpoint root an unrooted phylogenetic tree. Outputs: -------- :rooted_tree.qza: The rooted phylogenetic tree. | Description: ------------ Midpoint root an unrooted phylogenetic tree. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__phylogeny__raxml/qiime2__phylogeny__raxml/2026.1.0+q2galaxy.2026.1.0	QIIME 2: phylogeny raxml ======================== Construct a phylogenetic tree with RAxML. Outputs: -------- :tree.qza: The resulting phylogenetic tree. | Description: ------------ Construct a phylogenetic tree with RAxML. See: https://sco.h-its.org/exelixis/web/software/raxml/ |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__phylogeny__raxml_rapid_bootstrap/qiime2__phylogeny__raxml_rapid_bootstrap/2026.1.0+q2galaxy.2026.1.0	QIIME 2: phylogeny raxml-rapid-bootstrap ======================================== Construct a phylogenetic tree with bootstrap supports using RAxML. Outputs: -------- :tree.qza: The resulting phylogenetic tree. | Description: ------------ Construct a phylogenetic tree with RAxML with the addition of rapid bootstrapping support values. See: https://sco.h-its.org/exelixis/web/software/raxml/ |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__phylogeny__robinson_foulds/qiime2__phylogeny__robinson_foulds/2026.1.0+q2galaxy.2026.1.0	QIIME 2: phylogeny robinson-foulds ================================== Calculate Robinson-Foulds distance between phylogenetic trees. Outputs: -------- :distance_matrix.qza: The distances between trees as a symmetric matrix. | Description: ------------ Calculate the Robinson-Foulds symmetric difference metric between two or more phylogenetic trees. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__quality_control__bowtie2_build/qiime2__quality_control__bowtie2_build/2026.1.0+q2galaxy.2026.1.0	QIIME 2: quality-control bowtie2-build ====================================== Build bowtie2 index from reference sequences. Outputs: -------- :database.qza: Bowtie2 index. | Description: ------------ Build bowtie2 index from reference sequences. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__quality_control__evaluate_composition/qiime2__quality_control__evaluate_composition/2026.1.0+q2galaxy.2026.1.0	QIIME 2: quality-control evaluate-composition ============================================= Evaluate expected vs. observed taxonomic composition of samples Outputs: -------- :visualization.qzv: <no description> | Description: ------------ This visualizer compares the feature composition of pairs of observed and expected samples containing the same sample ID in two separate feature tables. Typically, feature composition will consist of taxonomy classifications or other semicolon-delimited feature annotations. Taxon accuracy rate, taxon detection rate, and linear regression scores between expected and observed observations are calculated at each semicolon-delimited rank, and plots of per-level accuracy and observation correlations are plotted. A histogram of distance between false positive observations and the nearest expected feature is also generated, where distance equals the number of rank differences between the observed feature and the nearest common lineage in the expected feature. This visualizer is most suitable for testing per-run data quality on sequencing runs that contain mock communities or other samples with known composition. Also suitable for sanity checks of bioinformatics pipeline performance. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__quality_control__evaluate_seqs/qiime2__quality_control__evaluate_seqs/2026.1.0+q2galaxy.2026.1.0	QIIME 2: quality-control evaluate-seqs ====================================== Compare query (observed) vs. reference (expected) sequences. Outputs: -------- :visualization.qzv: <no description> | Description: ------------ This action aligns a set of query (e.g., observed) sequences against a set of reference (e.g., expected) sequences to evaluate the quality of alignment. The intended use is to align observed sequences against expected sequences (e.g., from a mock community) to determine the frequency of mismatches between observed sequences and the most similar expected sequences, e.g., as a measure of sequencing/method error. However, any sequences may be provided as input to generate a report on pairwise alignment quality against a set of reference sequences. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__quality_control__evaluate_taxonomy/qiime2__quality_control__evaluate_taxonomy/2026.1.0+q2galaxy.2026.1.0	QIIME 2: quality-control evaluate-taxonomy ========================================== Evaluate expected vs. observed taxonomic assignments Outputs: -------- :visualization.qzv: <no description> | Description: ------------ This visualizer compares a pair of observed and expected taxonomic assignments to calculate precision, recall, and F-measure at each taxonomic level, up to maximum level specified by the depth parameter. These metrics are calculated at each semicolon-delimited rank. This action is useful for comparing the accuracy of taxonomic assignment, e.g., between different taxonomy classifiers or other bioinformatics methods. Expected taxonomies should be derived from simulated or mock community sequences that have known taxonomic affiliations. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__quality_control__exclude_seqs/qiime2__quality_control__exclude_seqs/2026.1.0+q2galaxy.2026.1.0	QIIME 2: quality-control exclude-seqs ===================================== Exclude sequences by alignment Outputs: -------- :sequence_hits.qza: Subset of feature sequences that align to reference sequences :sequence_misses.qza: Subset of feature sequences that do not align to reference sequences | Description: ------------ This method aligns feature sequences to a set of reference sequences to identify sequences that hit/miss the reference within a specified perc_identity, evalue, and perc_query_aligned. This method could be used to define a positive filter, e.g., extract only feature sequences that align to a certain clade of bacteria; or to define a negative filter, e.g., identify sequences that align to contaminant or human DNA sequences that should be excluded from subsequent analyses. Note that filtering is performed based on the perc_identity, perc_query_aligned, and evalue thresholds (the latter only if method==BLAST and an evalue is set). Set perc_identity==0 and/or perc_query_aligned==0 to disable these filtering thresholds as necessary. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__quality_control__filter_reads/qiime2__quality_control__filter_reads/2026.1.0+q2galaxy.2026.1.0	QIIME 2: quality-control filter-reads ===================================== Filter demultiplexed sequences by alignment to reference database. Outputs: -------- :filtered_sequences.qza: The resulting filtered sequences. | Description: ------------ Filter out (or keep) demultiplexed single- or paired-end sequences that align to a reference database, using bowtie2 and samtools. This method can be used to filter out human DNA sequences and other contaminants in any FASTQ sequence data (e.g., shotgun genome or amplicon sequence data), or alternatively (when exclude_seqs is False) to only keep sequences that do align to the reference. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__quality_filter__q_score/qiime2__quality_filter__q_score/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: quality-filter q-score =============================== Quality filter based on sequence quality scores. Outputs: -------- :filtered_sequences.qza: The resulting quality-filtered sequences. :filter_stats.qza: Summary statistics of the filtering process. | Description: ------------ This method filters sequence based on quality scores and the presence of ambiguous base calls. Examples: --------- q_score 
*
 Using the 
qiime2 quality-filter q-score
 tool: #. Set 
""demux""
 to 
#: demuxed-seqs.qza
 #. Press the 
Execute
 button. Once completed, for each new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 quality-filter q-score [...] : filtered_sequences.qza
 - 
dumux-filtered.qza
 * - 
#: qiime2 quality-filter q-score [...] : filter_stats.qza
 - 
demux-filter-stats.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__classify_samples/qiime2__sample_classifier__classify_samples/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier classify-samples =========================================== Train and test a cross-validated supervised learning classifier. Outputs: -------- :sample_estimator.qza: Trained sample estimator. :feature_importance.qza: Importance of each input feature to model accuracy. :predictions.qza: Predicted target values for each input sample. :model_summary.qzv: Summarized parameter and (if enabled) feature selection information for the trained estimator. :accuracy_results.qzv: Accuracy results visualization. :probabilities.qza: Predicted class probabilities for each input sample. :heatmap.qzv: A heatmap of the top 50 most important features from the table. :training_targets.qza: Series containing true target values of train samples :test_targets.qza: Series containing true target values of test samples | Description: ------------ Predicts a categorical sample metadata column using a supervised learning classifier. Splits input data into training and test sets. The training set is used to train and test the estimator using a stratified k-fold cross-validation scheme. This includes optional steps for automated feature extraction and hyperparameter optimization. The test set validates classification accuracy of the optimized estimator. Outputs classification results for test set. For more details on the learning algorithm, see http://scikit-learn.org/stable/supervised_learning.html |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__classify_samples_from_dist/qiime2__sample_classifier__classify_samples_from_dist/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier classify-samples-from-dist ===================================================== Run k-nearest-neighbors on a labeled distance matrix. Outputs: -------- :predictions.qza: leave one out predictions for each sample :accuracy_results.qzv: Accuracy results visualization. | Description: ------------ Run k-nearest-neighbors on a labeled distance matrix. Return cross-validated (leave one out) predictions and accuracy. k = 1 by default |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__classify_samples_ncv/qiime2__sample_classifier__classify_samples_ncv/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier classify-samples-ncv =============================================== Nested cross-validated supervised learning classifier. Outputs: -------- :predictions.qza: Predicted target values for each input sample. :feature_importance.qza: Importance of each input feature to model accuracy. :probabilities.qza: Predicted class probabilities for each input sample. | Description: ------------ Predicts a categorical sample metadata column using a supervised learning classifier. Uses nested stratified k-fold cross validation for automated hyperparameter optimization and sample prediction. Outputs predicted values for each input sample, and relative importance of each feature for model accuracy. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__confusion_matrix/qiime2__sample_classifier__confusion_matrix/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier confusion-matrix =========================================== Make a confusion matrix from sample classifier predictions. Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Make a confusion matrix and calculate accuracy of predicted vs. true values for a set of samples classified using a sample classifier. If per-sample class probabilities are provided, will also generate Receiver Operating Characteristic curves and calculate area under the curve for each class. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__fit_classifier/qiime2__sample_classifier__fit_classifier/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier fit-classifier ========================================= Fit a supervised learning classifier. Outputs: -------- :sample_estimator.qza: Trained sample classifier. :feature_importance.qza: Importance of each input feature to model accuracy. | Description: ------------ Fit a supervised learning classifier. Outputs the fit estimator (for prediction of test samples and/or unknown samples) and the relative importance of each feature for model accuracy. Optionally use k-fold cross-validation for automatic recursive feature elimination and hyperparameter tuning. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__fit_regressor/qiime2__sample_classifier__fit_regressor/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier fit-regressor ======================================== Fit a supervised learning regressor. Outputs: -------- :sample_estimator.qza: <no description> :feature_importance.qza: Importance of each input feature to model accuracy. | Description: ------------ Fit a supervised learning regressor. Outputs the fit estimator (for prediction of test samples and/or unknown samples) and the relative importance of each feature for model accuracy. Optionally use k-fold cross-validation for automatic recursive feature elimination and hyperparameter tuning. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__heatmap/qiime2__sample_classifier__heatmap/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier heatmap ================================== Generate heatmap of important features. Outputs: -------- :heatmap.qzv: Heatmap of important features. :filtered_table.qza: Filtered feature table containing data displayed in heatmap. | Description: ------------ Generate a heatmap of important features. Features are filtered based on importance scores; samples are optionally grouped by sample metadata; and a heatmap is generated that displays (normalized) feature abundances per sample. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__metatable/qiime2__sample_classifier__metatable/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: sample-classifier metatable ==================================== Convert (and merge) positive numeric metadata (in)to feature table. Outputs: -------- :converted_table.qza: Converted feature table | Description: ------------ Convert numeric sample metadata from TSV file into a feature table. Optionally merge with an existing feature table. Only numeric metadata will be converted; categorical columns will be silently dropped. By default, if a table is used as input only samples found in both the table and metadata (intersection) are merged, and others are silently dropped. Set missing_samples=""error"" to raise an error if samples found in the table are missing from the metadata file. The metadata file can always contain a superset of samples. Note that columns will be dropped if they are non-numeric, contain no unique values (zero variance), contain only empty cells, or contain negative values. This method currently only converts postive numeric metadata into feature data. Tip: convert categorical columns to dummy variables to include them in the output feature table. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__predict_classification/qiime2__sample_classifier__predict_classification/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier predict-classification ================================================= Use trained classifier to predict target values for new samples. Outputs: -------- :predictions.qza: Predicted target values for each input sample. :probabilities.qza: Predicted class probabilities for each input sample. | Description: ------------ Use trained estimator to predict target values for new samples. These will typically be unseen samples, e.g., test data (derived manually or from split_table) or samples with unknown values, but can theoretically be any samples present in a feature table that contain overlapping features with the feature table used to train the estimator. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__predict_regression/qiime2__sample_classifier__predict_regression/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier predict-regression ============================================= Use trained regressor to predict target values for new samples. Outputs: -------- :predictions.qza: Predicted target values for each input sample. | Description: ------------ Use trained estimator to predict target values for new samples. These will typically be unseen samples, e.g., test data (derived manually or from split_table) or samples with unknown values, but can theoretically be any samples present in a feature table that contain overlapping features with the feature table used to train the estimator. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__regress_samples/qiime2__sample_classifier__regress_samples/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier regress-samples ========================================== Train and test a cross-validated supervised learning regressor. Outputs: -------- :sample_estimator.qza: Trained sample estimator. :feature_importance.qza: Importance of each input feature to model accuracy. :predictions.qza: Predicted target values for each input sample. :model_summary.qzv: Summarized parameter and (if enabled) feature selection information for the trained estimator. :accuracy_results.qzv: Accuracy results visualization. | Description: ------------ Predicts a continuous sample metadata column using a supervised learning regressor. Splits input data into training and test sets. The training set is used to train and test the estimator using a stratified k-fold cross-validation scheme. This includes optional steps for automated feature extraction and hyperparameter optimization. The test set validates classification accuracy of the optimized estimator. Outputs classification results for test set. For more details on the learning algorithm, see http://scikit-learn.org/stable/supervised_learning.html |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__regress_samples_ncv/qiime2__sample_classifier__regress_samples_ncv/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier regress-samples-ncv ============================================== Nested cross-validated supervised learning regressor. Outputs: -------- :predictions.qza: Predicted target values for each input sample. :feature_importance.qza: Importance of each input feature to model accuracy. | Description: ------------ Predicts a continuous sample metadata column using a supervised learning regressor. Uses nested stratified k-fold cross validation for automated hyperparameter optimization and sample prediction. Outputs predicted values for each input sample, and relative importance of each feature for model accuracy. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__scatterplot/qiime2__sample_classifier__scatterplot/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier scatterplot ====================================== Make 2D scatterplot and linear regression of regressor predictions. Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Make a 2D scatterplot and linear regression of predicted vs. true values for a set of samples predicted using a sample regressor. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__split_table/qiime2__sample_classifier__split_table/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier split-table ====================================== Split a feature table into training and testing sets. Outputs: -------- :training_table.qza: Feature table containing training samples :test_table.qza: Feature table containing test samples :training_targets.qza: Series containing true target values of train samples :test_targets.qza: Series containing true target values of test samples | Description: ------------ Split a feature table into training and testing sets. By default stratifies training and test sets on a metadata column, such that values in that column are evenly represented across training and test sets. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__sample_classifier__summarize/qiime2__sample_classifier__summarize/2026.1.0+q2galaxy.2026.1.0	QIIME 2: sample-classifier summarize ==================================== Summarize parameter and feature extraction information for a trained estimator. Outputs: -------- :visualization.qzv: <no description> | Description: ------------ Summarize parameter and feature extraction information for a trained estimator. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__taxa__barplot/qiime2__taxa__barplot/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: taxa barplot ===================== Visualize taxonomy with an interactive bar plot Outputs: -------- :visualization.qzv: <no description> | Description: ------------ This visualizer produces an interactive barplot visualization of taxonomies. Interactive features include multi-level sorting, plot recoloring, sample relabeling, and SVG figure export. This visualizer is planned to be replaced with the current 
barplot2
 visualizer in this plugin. We are interested in user feedback on 
barplot2
, so please consider trying it out and letting us know how it works for you. Examples: --------- barplot 
*
 Using the 
qiime2 taxa barplot
 tool: #. Set 
""table""
 to 
#: table.qza
 #. Expand the 
additional options
 section #. Set 
""taxonomy""
 to 
#: taxonomy.qza
 #. For 
""metadata""
: - Press the 
+ Insert metadata
 button to set up the next steps. #. Leave as 
Metadata from TSV
 #. Set 
""Metadata Source""
 to 
sample-metadata.tsv
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""
 to set (be sure to press 
Save
) * - 
#: qiime2 taxa barplot [...] : visualization.qzv
 - 
taxa-bar-plots.qzv
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__taxa__collapse/qiime2__taxa__collapse/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: taxa collapse ====================== Collapse features by their taxonomy at the specified level Outputs: -------- :collapsed_table.qza: The resulting feature table, where all features are now taxonomic annotations with the user-specified number of levels. | Description: ------------ Collapse groups of features that have the same taxonomic assignment through the specified level. The frequencies of all features will be summed when they are collapsed. Examples: --------- collapse 
*
 Using the 
qiime2 taxa collapse
 tool: #. Set 
""table""
 to 
#: table.qza
 #. Set 
""taxonomy""
 to 
#: taxonomy.qza
 #. Set 
""level""
 to 
6
 #. Press the 
Execute
 button. Once completed, for the new entry in your history, use the 
Edit
 button to set the name as follows: (Renaming is optional, but it will make any subsequent steps easier to complete.) .. list-table:: :align: left :header-rows: 1 * - History Name - 
""Name""* to set (be sure to press 
Save
) * - 
#: qiime2 taxa collapse [...] : collapsed_table.qza
 - 
collapsed-table-l6.qza
 |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__taxa__filter_seqs/qiime2__taxa__filter_seqs/2026.1.0+q2galaxy.2026.1.0	QIIME 2: taxa filter-seqs ========================= Taxonomy-based feature sequence filter. Outputs: -------- :filtered_sequences.qza: The taxonomy-filtered feature sequences. | Description: ------------ This method filters sequences based on their taxonomic annotations. Features can be retained in the result by specifying one or more include search terms, and can be filtered out of the result by specifying one or more exclude search terms. If both include and exclude are provided, the inclusion critera will be applied before the exclusion critera. Either include or exclude terms (or both) must be provided. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__taxa__filter_table/qiime2__taxa__filter_table/2026.1.0+q2galaxy.2026.1.0	QIIME 2: taxa filter-table ========================== Taxonomy-based feature table filter. Outputs: -------- :filtered_table.qza: The taxonomy-filtered feature table. | Description: ------------ This method filters features from a table based on their taxonomic annotations. Features can be retained in the resulting table by specifying one or more include search terms, and can be filtered out of the resulting table by specifying one or more exclude search terms. If both include and exclude are provided, the inclusion critera will be applied before the exclusion critera. Either include or exclude terms (or both) must be provided. Any samples that have a total frequency of zero after filtering will be removed from the resulting table. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2_core__tools__export/qiime2_core__tools__export/2026.1.0+dist.h02a552c2	"QIIME 2: tools export ===================== Export a QIIME 2 artifact to different formats Instructions ------------ 1. Select the QZA you would like to export. Once selected, two fields will update indicating the type and format of this QZA. 2. If you wish to change the output format, first provide the same type as the QZA (which is shown above). This will filter the avaiable formats. 3. Select the format you desire. Some limited documentation is available on these formats below. 
IMPORTANT:
 if you select the wrong type when exporting, you will recieve an error suggesting ""No transformation from <X> to <Y>"". Formats: -------- These formats have documentation available. TSVTaxonomyFormat 
*
*
 Format for a 2+ column TSV file with an expected minimal header. The only header recognized by this format is: Feature ID<tab>Taxon Optionally followed by other arbitrary columns. This format supports blank lines. The expected header must be the first non-blank line. In addition to the header, there must be at least one line of data. SequenceCharacteristicsFormat 
*
*
*
*
 Format for a TSV file with information about sequences like length of a feature. The first column contains feature identifiers and is followed by other optional columns. The file cannot be empty and must have at least two columns. Validation for additional columns can be added with a semantic validator tied to a property. For example the ""validate_seq_char_len"" validator for ""FeatureData[SequenceCharacteristics % Properties(""length"")]"" adds validation for a numerical column called ""length"". NCBIAccessionIDsFormat 
*
*
*
* This is a format used to store a list of SRA accession IDs (run, study, BioProject, sample and experiment IDs), which can be converted to QIIME's metadata. Artifacts containing of run, study and BioProject IDs can be input into any fondue action. ANCOMBC2OutputDirFmt 
*
*
* Stores the model statistics and optionally the structural zeros table= output by the ANCOMBC2 method. The slices are: - lfc: log-fold change - se: standard error - W: lfc / se (the test statistic) - p: p-value - q: adjusted p-value - diff: differentially abundant boolean (i.e. q < alpha) - passed_ss: whether sensitivity analysis was passed QIIME1DemuxFormat 
*
*
 QIIME 1 demultiplexed FASTA format. The QIIME 1 demultiplexed FASTA format is the default output format of 
split_libraries.py
 and 
split_libraries_fastq.py
. The file output by QIIME 1 is named 
seqs.fna
; this filename is sometimes associated with the file format itself due to its widespread usage in QIIME 1. The format is documented here: http://qiime.org/documentation/file_formats.html#demultiplexed-sequences Format details: - FASTA file with exactly two lines per record: header and sequence. Each sequence must span exactly one line and cannot be split across multiple lines. - The ID in each header must follow the format 
&lt;sample-id&gt;_&lt;seq-id&gt;
. 
&lt;sample-id&gt;
 is the identifier of the sample the sequence belongs to, and 
&lt;seq-id&gt;
 is an identifier for the sequence 
within
 its sample. In QIIME 1, 
&lt;seq-id&gt;
 is typically an incrementing integer starting from zero, but any non-empty value can be used here, as long as the header IDs remain unique throughout the file. Note: 
&lt;sample-id&gt;
 may contain sample IDs that contain underscores; the rightmost underscore will used to delimit sample and sequence IDs. - Descriptions in headers are permitted and ignored. - Header IDs must be unique within the file. - Each sequence must be DNA and cannot be empty. PressedProfileHmmsDirectoryFmt 
*
*
*
*
 The <hmmfile>.h3m file contains the profile HMMs and their annotation in a binary format. The <hmmfile>.h3i file is an SSI index for the <hmmfile>.h3m file. The <hmmfile>.h3f file contains precomputed data structures for the fast heuristic filter (the MSV filter). The <hmmfile>.h3p file contains precomputed data structures for the rest of each profile. SRAFailedIDsFormat 
*
*
 This is a ""fake"" format only used to store a list of failed SRA IDs, which can be converted to QIIME's metadata and input into any fondue action. FastqGzFormat 
*
* A gzipped fastq file. Additional formats without documentation: 
*
*
*
*
*
*** - ArtificialGroupingFormat - AlignedRNAFASTAFormat - SILVATaxonomyFormat - RNAMultipleProfileHmmDirectoryFmt - SeppReferenceDirFmt - NewickFormat - ContigSequencesDirFmt - ProteinFASTAFormat - QualityFilterStatsFmt - AlignedRNASequencesDirectoryFormat - SILVATaxonomyDirectoryFormat - CasavaOneEightSingleLanePerSampleDirFmt - SequenceCharacteristicsDirectoryFormat - OrdinationFormat - FirstDifferencesFormat - PredictionsFormat - TableJSONLFileFormat - Kraken2ReportDirectoryFormat - BIOMV210DirFmt - UchimeStatsDirFmt - NCBIAccessionIDsDirFmt - GenesDirectoryFormat - MAGSequencesDirFmt - ProbabilitiesFormat - SILVATaxidMapFormat - EggnogProteinSequencesDirFmt - DNAFASTAFormat - ImmutableMetadataFormat - BooleanSeriesDirectoryFormat - ErrorCorrectionDetailsFmt - BrackenDBDirectoryFormat - FeatureMapFormat - DADA2StatsDirFmt - OrdinationDirectoryFormat - BooleanSeriesFormat - QIIME1DemuxDirFmt - DataLoafPackageDirFmt - ProteinSingleProfileHmmDirectoryFmt - AlphaDiversityFormat - PairedDNASequencesDirectoryFormat - TableJSONLDirFmt - ImportanceDirectoryFormat - Kraken2DBReportFormat - BAMDirFmt - AlignedProteinFASTAFormat - ProteinsDirectoryFormat - ProcrustesStatisticsFmt - ErrorCorrectionDetailsDirFmt - FeatureMapDirFmt - MAGtoContigsFormat - QualityFilterStatsDirFmt - ProteinMultipleProfileHmmDirectoryFmt - ArtificialGroupingDirectoryFormat - SRAMetadataFormat - PairedRNASequencesDirectoryFormat - Kraken2DBReportDirectoryFormat - MAGtoContigsDirFmt - BIOMV210Format - MultiBAMDirFmt - AlignedProteinSequencesDirectoryFormat - ImportanceFormat - DifferentialFormat - DNASequencesDirectoryFormat - ProcrustesStatisticsDirFmt - FirstDifferencesDirectoryFormat - KaijuDBDirectoryFormat - DNASingleProfileHmmDirectoryFmt - SILVATaxidMapDirectoryFormat - AlignedDNAFASTAFormat - DecontamScoreFormat - LociDirectoryFormat - DifferentialDirectoryFormat - DecontamScoreDirFmt - ImmutableMetadataDirectoryFormat - SingleLanePerSamplePairedEndFastqDirFmt - EMPSingleEndDirFmt - AlignedDNASequencesDirectoryFormat - Kraken2OutputDirectoryFormat - AlphaDiversityDirectoryFormat - GenomeSequencesDirectoryFormat - LSMatFormat - MultiFASTADirectoryFormat - DiamondDatabaseFileFmt - NewickDirectoryFormat - MultiplexedSingleEndBarcodeInSequenceDirFmt - DeblurStatsDirFmt - BLASTDBDirFmtV5 - EMPPairedEndDirFmt - DADA2BaseTransitionStatsFormat - DNAMultipleProfileHmmDirectoryFmt - DiamondDatabaseDirFmt - TaxonomicClassiferTemporaryPickleDirFmt - BLAST6Format - EggnogRefDirFmt - UchimeStatsFmt - SeedOrthologDirFmt - DistanceMatrixDirectoryFormat - RNAFASTAFormat - SRAFailedIDsDirFmt - ProbabilitiesDirectoryFormat - TrueTargetsDirectoryFormat - PlacementsDirFmt - MultiMAGSequencesDirFmt - Bowtie2IndexDirFmt - MultiplexedPairedEndBarcodeInSequenceDirFmt - NCBITaxonomyDirFmt - TSVTaxonomyDirectoryFormat - PredictionsDirectoryFormat - Kraken2DBDirectoryFormat - DADA2BaseTransitionStatsDirFmt - SRAMetadataDirFmt - RNASingleProfileHmmDirectoryFmt - BLAST6DirectoryFormat - DADA2StatsFormat - PlacementsFormat - OrthologAnnotationDirFmt - SingleLanePerSampleSingleEndFastqDirFmt - RNASequencesDirectoryFormat - MultiBowtie2IndexDirFmt - ProteinSequencesDirectoryFormat - DeblurStatsFmt - SampleEstimatorDirFmt"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2_core__tools__import/qiime2_core__tools__import/2026.1.0+dist.h02a552c2	"QIIME 2: tools import ===================== Import data as a QIIME 2 artifact Instructions ------------ 1. Select the type you wish to import. If you are uncertain, consider what your next action would be and identify what type it requires. 2. Identify which format will best suite the data you have available. Many types will have only a single format available. There is some documentation available below on the different formats, however there may not be very much documentation available for your format. 3. For each part of the format, you will need to associate some data. a. If it is a simple format, you may just select the history dataset. b. If it is a more complex format, you will need to provide either a filename and history dataset, or a collection. c. For collections, they can be constructed via matching a regex against the names of the items in that collection. (You may need to append an extension if your collection's element IDs lack one.) Or you can provide individual history datasets with a filename as in the simpler cases. Formats: -------- These formats have documentation available. TSVTaxonomyFormat 
*
*
 Format for a 2+ column TSV file with an expected minimal header. The only header recognized by this format is: Feature ID<tab>Taxon Optionally followed by other arbitrary columns. This format supports blank lines. The expected header must be the first non-blank line. In addition to the header, there must be at least one line of data. SequenceCharacteristicsFormat 
*
*
*
*
 Format for a TSV file with information about sequences like length of a feature. The first column contains feature identifiers and is followed by other optional columns. The file cannot be empty and must have at least two columns. Validation for additional columns can be added with a semantic validator tied to a property. For example the ""validate_seq_char_len"" validator for ""FeatureData[SequenceCharacteristics % Properties(""length"")]"" adds validation for a numerical column called ""length"". NCBIAccessionIDsFormat 
*
*
*
* This is a format used to store a list of SRA accession IDs (run, study, BioProject, sample and experiment IDs), which can be converted to QIIME's metadata. Artifacts containing of run, study and BioProject IDs can be input into any fondue action. ANCOMBC2OutputDirFmt 
*
*
* Stores the model statistics and optionally the structural zeros table= output by the ANCOMBC2 method. The slices are: - lfc: log-fold change - se: standard error - W: lfc / se (the test statistic) - p: p-value - q: adjusted p-value - diff: differentially abundant boolean (i.e. q < alpha) - passed_ss: whether sensitivity analysis was passed PressedProfileHmmsDirectoryFmt 
*
*
*
*
* The <hmmfile>.h3m file contains the profile HMMs and their annotation in a binary format. The <hmmfile>.h3i file is an SSI index for the <hmmfile>.h3m file. The <hmmfile>.h3f file contains precomputed data structures for the fast heuristic filter (the MSV filter). The <hmmfile>.h3p file contains precomputed data structures for the rest of each profile. SRAFailedIDsFormat 
*
*
 This is a ""fake"" format only used to store a list of failed SRA IDs, which can be converted to QIIME's metadata and input into any fondue action. HeaderlessTSVTaxonomyFormat 
*
*
*
* Format for a 2+ column TSV file without a header. This format supports comment lines starting with #, and blank lines. FastqGzFormat 
*
 A gzipped fastq file. SampleIdIndexedSingleEndPerSampleDirFmt 
*
*
*
*
*
 Single-end reads in fastq.gz files where base filename is the sample id The full file name, minus the extension (
.fastq.gz
) is the sample id. For example, the sample id for the file: * 
sample-1.fastq.gz
 is 
sample-1
 * 
xyz.fastq.gz
 is 
xyz
 * 
sample-42_S1_L001_R1_001.fastq.gz
 is 
sample-42_S1_L001_R1_001
 QIIME1DemuxFormat 
*
*
 QIIME 1 demultiplexed FASTA format. The QIIME 1 demultiplexed FASTA format is the default output format of 
split_libraries.py
 and 
split_libraries_fastq.py
. The file output by QIIME 1 is named 
seqs.fna
; this filename is sometimes associated with the file format itself due to its widespread usage in QIIME 1. The format is documented here: http://qiime.org/documentation/file_formats.html#demultiplexed-sequences Format details: - FASTA file with exactly two lines per record: header and sequence. Each sequence must span exactly one line and cannot be split across multiple lines. - The ID in each header must follow the format 
&lt;sample-id&gt;_&lt;seq-id&gt;
. 
&lt;sample-id&gt;
 is the identifier of the sample the sequence belongs to, and 
&lt;seq-id&gt;
 is an identifier for the sequence 
within* its sample. In QIIME 1, 
&lt;seq-id&gt;
 is typically an incrementing integer starting from zero, but any non-empty value can be used here, as long as the header IDs remain unique throughout the file. Note: 
&lt;sample-id&gt;
 may contain sample IDs that contain underscores; the rightmost underscore will used to delimit sample and sequence IDs. - Descriptions in headers are permitted and ignored. - Header IDs must be unique within the file. - Each sequence must be DNA and cannot be empty. Additional formats without documentation: 
*
*
*
*
*
*** - FASTAFormat - ArtificialGroupingFormat - EMPPairedEndCasavaDirFmt - AlignedRNAFASTAFormat - SILVATaxonomyFormat - RNAMultipleProfileHmmDirectoryFmt - SeppReferenceDirFmt - NewickFormat - MixedCaseRNAFASTAFormat - ContigSequencesDirFmt - ProteinFASTAFormat - QualityFilterStatsFmt - CasavaOneEightSingleLanePerSampleDirFmt - OrdinationFormat - FirstDifferencesFormat - PredictionsFormat - TableJSONLFileFormat - Kraken2ReportDirectoryFormat - GenesDirectoryFormat - MAGSequencesDirFmt - ProbabilitiesFormat - MixedCaseAlignedRNAFASTAFormat - EggnogProteinSequencesDirFmt - SILVATaxidMapFormat - DNAFASTAFormat - SingleEndFastqManifestPhred33V2 - ImmutableMetadataFormat - ErrorCorrectionDetailsFmt - BrackenDBDirectoryFormat - FeatureMapFormat - BooleanSeriesFormat - DataLoafPackageDirFmt - ProteinSingleProfileHmmDirectoryFmt - AlphaDiversityFormat - PairedDNASequencesDirectoryFormat - Kraken2DBReportFormat - BAMDirFmt - BIOMV100Format - AlignedProteinFASTAFormat - ProteinsDirectoryFormat - ProcrustesStatisticsFmt - SingleEndFastqManifestPhred64V2 - MAGtoContigsFormat - ProteinMultipleProfileHmmDirectoryFmt - SRAMetadataFormat - PairedRNASequencesDirectoryFormat - BIOMV210Format - MultiBAMDirFmt - ImportanceFormat - DifferentialFormat - KaijuDBDirectoryFormat - DNASingleProfileHmmDirectoryFmt - AlignedDNAFASTAFormat - SingleEndFastqManifestPhred33 - DecontamScoreFormat - LociDirectoryFormat - MultiplexedFastaQualDirFmt - MixedCaseDNAFASTAFormat - PairedEndFastqManifestPhred33V2 - SingleLanePerSamplePairedEndFastqDirFmt - EMPSingleEndDirFmt - SingleEndFastqManifestPhred64 - Kraken2OutputDirectoryFormat - GenomeSequencesDirectoryFormat - LSMatFormat - PairedEndFastqManifestPhred64V2 - MultiFASTADirectoryFormat - DiamondDatabaseFileFmt - CasavaOneEightLanelessPerSampleDirFmt - BLASTDBDirFmtV5 - EMPPairedEndDirFmt - MixedCaseAlignedDNAFASTAFormat - DADA2BaseTransitionStatsFormat - DNAMultipleProfileHmmDirectoryFmt - PairedEndFastqManifestPhred33 - TaxonomicClassiferTemporaryPickleDirFmt - BLAST6Format - EggnogRefDirFmt - UchimeStatsFmt - SeedOrthologDirFmt - RNAFASTAFormat - MultiMAGSequencesDirFmt - Bowtie2IndexDirFmt - MultiplexedPairedEndBarcodeInSequenceDirFmt - NCBITaxonomyDirFmt - EMPSingleEndCasavaDirFmt - PairedEndFastqManifestPhred64 - Kraken2DBDirectoryFormat - RNASingleProfileHmmDirectoryFmt - DADA2StatsFormat - PlacementsFormat - OrthologAnnotationDirFmt - SingleLanePerSampleSingleEndFastqDirFmt - MultiBowtie2IndexDirFmt - DeblurStatsFmt - SampleEstimatorDirFmt"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__vsearch__cluster_features_closed_reference/qiime2__vsearch__cluster_features_closed_reference/2026.1.0+q2galaxy.2026.1.0	QIIME 2: vsearch cluster-features-closed-reference ================================================== Closed-reference clustering of features. Outputs: -------- :clustered_table.qza: The table following clustering of features. :clustered_sequences.qza: The sequences representing clustered features, relabeled by the reference IDs. :unmatched_sequences.qza: The sequences which failed to match any reference sequences. This output maps to vsearch's --notmatched parameter. | Description: ------------ Given a feature table and the associated feature sequences, cluster the features against a reference database based on user-specified percent identity threshold of their sequences. This is not a general-purpose closed-reference clustering method, but rather is intended to be used for clustering the results of quality-filtering/dereplication methods, such as DADA2, or for re-clustering a FeatureTable at a lower percent identity than it was originally clustered at. When a group of features in the input table are clustered into a single feature, the frequency of that single feature in a given sample is the sum of the frequencies of the features that were clustered in that sample. Feature identifiers will be inherited from the centroid feature of each cluster. See the vsearch documentation for details on how sequence clustering is performed. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__vsearch__cluster_features_de_novo/qiime2__vsearch__cluster_features_de_novo/2026.1.0+q2galaxy.2026.1.0	"QIIME 2: vsearch cluster-features-de-novo ========================================= De novo clustering of features. Outputs: -------- :clustered_table.qza: The table following clustering of features. :clustered_sequences.qza: Sequences representing clustered features. | Description: ------------ Given a feature table and the associated feature sequences, cluster the features based on user-specified percent identity threshold of their sequences. This is not a general-purpose de novo clustering method, but rather is intended to be used for clustering the results of quality-filtering/dereplication methods, such as DADA2, or for re-clustering a FeatureTable at a lower percent identity than it was originally clustered at. When a group of features in the input table are clustered into a single feature, the frequency of that single feature in a given sample is the sum of the frequencies of the features that were clustered in that sample. Feature identifiers and sequences will be inherited from the centroid feature of each cluster. See the vsearch documentation for details on how sequence clustering is performed. Examples: --------- cluster_features_de_novo 
*
*
*
 Using the 
qiime2 vsearch cluster-features-de-novo
 tool: #. Set 
""sequences""
 to 
#: seqs1.qza
 #. Set 
""table""
 to 
#: table1.qza
 #. Set 
""perc_identity""
 to 
0.97
 #. Expand the 
additional options
 section - Leave 
""strand""* as its default value of 
plus
 #. Press the 
Execute
 button. |"
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__vsearch__cluster_features_open_reference/qiime2__vsearch__cluster_features_open_reference/2026.1.0+q2galaxy.2026.1.0	QIIME 2: vsearch cluster-features-open-reference ================================================ Open-reference clustering of features. Outputs: -------- :clustered_table.qza: The table following clustering of features. :clustered_sequences.qza: Sequences representing clustered features. :new_reference_sequences.qza: The new reference sequences. This can be used for subsequent runs of open-reference clustering for consistent definitions of features across open-reference feature tables. | Description: ------------ Given a feature table and the associated feature sequences, cluster the features against a reference database based on user-specified percent identity threshold of their sequences. Any sequences that don't match are then clustered de novo. This is not a general-purpose clustering method, but rather is intended to be used for clustering the results of quality-filtering/dereplication methods, such as DADA2, or for re-clustering a FeatureTable at a lower percent identity than it was originally clustered at. When a group of features in the input table are clustered into a single feature, the frequency of that single feature in a given sample is the sum of the frequencies of the features that were clustered in that sample. Feature identifiers will be inherited from the centroid feature of each cluster. For features that match a reference sequence, the centroid feature is that reference sequence, so its identifier will become the feature identifier. The clustered_sequences result will contain feature representative sequences that are derived from the sequences input for all features in clustered_table. This will always be the most abundant sequence in the cluster. The new_reference_sequences result will contain the entire reference database, plus feature representative sequences for any de novo features. This is intended to be used as a reference database in subsequent iterations of cluster_features_open_reference, if applicable. See the vsearch documentation for details on how sequence clustering is performed. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__vsearch__dereplicate_sequences/qiime2__vsearch__dereplicate_sequences/2026.1.0+q2galaxy.2026.1.0	QIIME 2: vsearch dereplicate-sequences ====================================== Dereplicate sequences. Outputs: -------- :dereplicated_table.qza: The table of dereplicated sequences. :dereplicated_sequences.qza: The dereplicated sequences. | Description: ------------ Dereplicate sequence data and create a feature table and feature representative sequences. Feature identifiers in the resulting artifacts will be the sha1 hash of the sequence defining each feature. If clustering of features into OTUs is desired, the resulting artifacts can be passed to the cluster_features_* methods in this plugin. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__vsearch__fastq_stats/qiime2__vsearch__fastq_stats/2026.1.0+q2galaxy.2026.1.0	QIIME 2: vsearch fastq-stats ============================ Fastq stats with vsearch. Outputs: -------- :visualization.qzv: <no description> | Description: ------------ A fastq overview via vsearch's fastq_stats, fastq_eestats and fastq_eestats2 utilities. Please see https://github.com/torognes/vsearch for detailed documentation of these tools. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__vsearch__merge_pairs/qiime2__vsearch__merge_pairs/2026.1.0+q2galaxy.2026.1.0	QIIME 2: vsearch merge-pairs ============================ Merge paired-end reads. Outputs: -------- :merged_sequences.qza: The merged sequences. :unmerged_sequences.qza: The unmerged paired-end reads. | Description: ------------ Merge paired-end sequence reads using vsearch's merge_pairs function. See the vsearch documentation for details on how paired-end merging is performed, and for more information on the parameters to this method. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__vsearch__uchime_denovo/qiime2__vsearch__uchime_denovo/2026.1.0+q2galaxy.2026.1.0	QIIME 2: vsearch uchime-denovo ============================== De novo chimera filtering. Outputs: -------- :chimeras.qza: The chimeric sequences. :nonchimeras.qza: The non-chimeric sequences. :stats.qza: Summary statistics from chimera checking. | Description: ------------ Apply one of the vsearch uchime*_denovo methods to identify chimeric feature sequences. The results of these methods can be used to filter chimeric features from the corresponding feature table. For more details, please refer to the vsearch manual. |
toolshed.g2.bx.psu.edu/repos/q2d2/qiime2__vsearch__uchime_ref/qiime2__vsearch__uchime_ref/2026.1.0+q2galaxy.2026.1.0	QIIME 2: vsearch uchime-ref =========================== Reference-based chimera filtering. Outputs: -------- :chimeras.qza: The chimeric sequences. :nonchimeras.qza: The non-chimeric sequences. :stats.qza: Summary statistics from chimera checking. | Description: ------------ Apply the vsearch uchime_ref method to identify chimeric feature sequences. The results of this method can be used to filter chimeric features from the corresponding feature table. For additional details, please refer to the vsearch documentation. |
toolshed.g2.bx.psu.edu/repos/devteam/cuffcompare/cuffcompare/2.2.1.3	"Cuffcompare Overview
 Cuffcompare is part of Cufflinks_. Cuffcompare helps you: (a) compare your assembled transcripts to a reference annotation and (b) track Cufflinks transcripts across multiple experiments (e.g. across a time course). Please cite: Trapnell C, Williams BA, Pertea G, Mortazavi AM, Kwan G, van Baren MJ, Salzberg SL, Wold B, Pachter L. Transcript assembly and abundance estimation from RNA-Seq reveals thousands of new transcripts and switching among isoforms. Nature Biotechnology doi:10.1038/nbt.1621 .. 
Cufflinks: http://cole-trapnell-lab.github.io/cufflinks/ ------ 
Know what you are doing
 .. class:: warningmark There is no such thing (yet) as an automated gearshift in expression analysis. It is all like stick-shift driving in San Francisco. In other words, running this tool with default parameters will probably not give you meaningful results. A way to deal with this is to 
understand
 the parameters by carefully reading the 
documentation
__ and experimenting. Fortunately, Galaxy makes experimenting easy. .. __: http://cole-trapnell-lab.github.io/cufflinks/cuffcompare/ ------ 
Input format
 Cuffcompare takes Cufflinks' GTF output as input, and optionally can take a ""reference"" annotation (such as from Ensembl
) .. _Ensembl: http://www.ensembl.org ------ 
Outputs
 Cuffcompare produces the following output files: Transcripts Accuracy File: Cuffcompare reports various statistics related to the ""accuracy"" of the transcripts in each sample when compared to the reference annotation data. The typical gene finding measures of ""sensitivity"" and ""specificity"" (as defined in Burset, M., Guigó, R. : Evaluation of gene structure prediction programs (1996) Genomics, 34 (3), pp. 353-367. doi: 10.1006/geno.1996.0298) are calculated at various levels (nucleotide, exon, intron, transcript, gene) for each input file and reported in this file. The Sn and Sp columns show specificity and sensitivity values at each level, while the fSn and fSp columns are ""fuzzy"" variants of these same accuracy calculations, allowing for a very small variation in exon boundaries to still be counted as a ""match"". Transcripts Combined File: Cuffcompare reports a GTF file containing the ""union"" of all transfrags in each sample. If a transfrag is present in both samples, it is thus reported once in the combined gtf. Transcripts Tracking File: This file matches transcripts up between samples. Each row contains a transcript structure that is present in one or more input GTF files. Because the transcripts will generally have different IDs (unless you assembled your RNA-Seq reads against a reference transcriptome), cuffcompare examines the structure of each the transcripts, matching transcripts that agree on the coordinates and order of all of their introns, as well as strand. Matching transcripts are allowed to differ on the length of the first and last exons, since these lengths will naturally vary from sample to sample due to the random nature of sequencing. If you ran cuffcompare with the -r option, the first and second columns contain the closest matching reference transcript to the one described by each row. Here's an example of a line from the tracking file:: TCONS_00000045 XLOC_000023 Tcea|uc007afj.1 j \ q1:exp.115|exp.115.0|100|3.061355|0.350242|0.350207 \ q2:60hr.292|60hr.292.0|100|4.094084|0.000000|0.000000 In this example, a transcript present in the two input files, called exp.115.0 in the first and 60hr.292.0 in the second, doesn't match any reference transcript exactly, but shares exons with uc007afj.1, an isoform of the gene Tcea, as indicated by the class code j. The first three columns are as follows:: Column number Column name Example Description ----------------------------------------------------------------------- 1 Cufflinks transfrag id TCONS_00000045 A unique internal id for the transfrag 2 Cufflinks locus id XLOC_000023 A unique internal id for the locus 3 Reference gene id Tcea The gene_name attribute of the reference GTF record for this transcript, or '-' if no reference transcript overlaps this Cufflinks transcript 4 Reference transcript id uc007afj.1 The transcript_id attribute of the reference GTF record for this transcript, or '-' if no reference transcript overlaps this Cufflinks transcript 5 Class code c The type of match between the Cufflinks transcripts in column 6 and the reference transcript. See class codes Each of the columns after the fifth have the following format: qJ:gene_id|transcript_id|FMI|FPKM|conf_lo|conf_hi A transcript need be present in all samples to be reported in the tracking file. A sample not containing a transcript will have a ""-"" in its entry in the row for that transcript. Class Codes If you ran cuffcompare with the -r option, tracking rows will contain the following values. If you did not use -r, the rows will all contain ""-"" in their class code column:: Priority Code Description --------------------------------- 1 = Match 2 c Contained 3 j New isoform 4 e A single exon transcript overlapping a reference exon and at least 10 bp of a reference intron, indicating a possible pre-mRNA fragment. 5 i A single exon transcript falling entirely with a reference intron 6 o Generic exonic overlap with a reference transcript 7 p Possible polymerase run-on fragment 8 r Repeat. Currently determined by looking at the soft-masked reference sequence and applied to transcripts where at least 50% of the bases are lower case 9 u Unknown, intergenic transcript 10 x Exonic overlap with reference on the opposite strand 11 s An intron of the transfrag overlaps a reference intron on the opposite strand (likely due to read mapping errors) 12 . (.tracking file only, indicates multiple classifications) ------- 
Settings
 All of the options have a default value. You can change any of them. Most of the options in Cuffcompare have been implemented here. ------ 
Cuffcompare parameter list
 This is a list of implemented Cuffcompare options:: -r An optional ""reference"" annotation GTF. Each sample is matched against this file, and sample isoforms are tagged as overlapping, matching, or novel where appropriate. See the refmap and tmap output file descriptions below. -R If -r was specified, this option causes cuffcompare to ignore reference transcripts that are not overlapped by any transcript in one of cuff1.gtf,...,cuffN.gtf. Useful for ignoring annotated transcripts that are not present in your RNA-Seq samples and thus adjusting the ""sensitivity"" calculation in the accuracy report written in the transcripts_accuracy file"
toolshed.g2.bx.psu.edu/repos/devteam/cuffdiff/cuffdiff/2.2.1.7	"Cuffdiff Overview
 Cuffdiff is part of Cufflinks_. Cuffdiff find significant changes in transcript expression, splicing, and promoter use. Please cite: Trapnell C, Williams BA, Pertea G, Mortazavi AM, Kwan G, van Baren MJ, Salzberg SL, Wold B, Pachter L. Transcript assembly and abundance estimation from RNA-Seq reveals thousands of new transcripts and switching among isoforms. Nature Biotechnology doi:10.1038/nbt.1621 .. _Cufflinks: http://cole-trapnell-lab.github.io/cufflinks/ ------ 
Know what you are doing
 .. class:: warningmark There is no such thing (yet) as an automated gearshift in expression analysis. It is all like stick-shift driving in San Francisco. In other words, running this tool with default parameters will probably not give you meaningful results. A way to deal with this is to 
understand
 the parameters by carefully reading the 
documentation
 and experimenting. Fortunately, Galaxy makes experimenting easy. .. 
: http://cole-trapnell-lab.github.io/cufflinks/cuffdiff/ ------ 
Input format
 Cuffdiff takes Cufflinks or Cuffcompare GTF files as input along with two SAM files containing the fragment alignments for two or more samples. ------ 
Outputs
 Cuffdiff produces many output files: 1. Transcript FPKM (+count) expression tracking. 2. Gene FPKM (+count) expression tracking; tracks the summed FPKM of transcripts sharing each gene_id 3. Primary transcript FPKM (+count) tracking; tracks the summed FPKM of transcripts sharing each tss_id 4. Coding sequence FPKM (+count) tracking; tracks the summed FPKM of transcripts sharing each p_id, independent of tss_id 5. Transcript differential FPKM. 6. Gene differential FPKM. Tests difference sin the summed FPKM of transcripts sharing each gene_id 7. Primary transcript differential FPKM. Tests difference sin the summed FPKM of transcripts sharing each tss_id 8. Coding sequence differential FPKM. Tests difference sin the summed FPKM of transcripts sharing each p_id independent of tss_id 9. Differential splicing tests: this tab delimited file lists, for each primary transcript, the amount of overloading detected among its isoforms, i.e. how much differential splicing exists between isoforms processed from a single primary transcript. Only primary transcripts from which two or more isoforms are spliced are listed in this file. 10. Differential promoter tests: this tab delimited file lists, for each gene, the amount of overloading detected among its primary transcripts, i.e. how much differential promoter use exists between samples. Only genes producing two or more distinct primary transcripts (i.e. multi-promoter genes) are listed here. 11. Differential CDS tests: this tab delimited file lists, for each gene, the amount of overloading detected among its coding sequences, i.e. how much differential CDS output exists between samples. Only genes producing two or more distinct CDS (i.e. multi-protein genes) are listed here. ------- 
Settings
 All of the options have a default value. You can change any of them. Most of the options in Cuffdiff have been implemented here. ------ 
Cuffdiff parameter list
 This is a list of implemented Cuffdiff options:: -m INT Average fragment length (SE reads); default 200 -s INT Fragment legnth standard deviation (SE reads); default 80 -c INT The minimum number of alignments in a locus for needed to conduct significance testing on changes in that locus observed between samples. If no testing is performed, changes in the locus are deemed not significant, and the locus' observed changes don't contribute to correction for multiple testing. The default is 1,000 fragment alignments (up to 2,000 paired reads). --FDR FLOAT The allowed false discovery rate. The default is 0.05. --max-mle-iterations INT Sets the number of iterations allowed during maximum likelihood estimation of abundances. Default: 5000 --library-norm-method Library Normalization method : Geometric (default), classic-fpkm, quartile --dispersion-method Dispersion estimation method : Pooled (default), per-condition, blind, poisson -u Multi read correction tells Cufflinks to do an initial estimation procedure to more accurately weight reads mapping to multiple locations in the genome. -b ref.fasta bias correction. Bias detection and correction can significantly improve accuracy of transcript abundance estimates. --no-effective-length-correction Use standard length correction --no-length-correction Disable all length correction. --library-type ff-firststrand,ff-secondstrand,ff-unstranded,fr-firstrand,fr-secondstrand,fr-unstranded,transfrags --mask-file (gff3/gtf) Ignore all alignment within transcripts in this file --time-series Treat provided sam files as time series --compatible-hits-norm With this option, Cufflinks counts only those fragments compatible with some reference transcript towards the number of mapped fragments used in the FPKM denominator. Using this mode is generally recommended in Cuffdiff to reduce certain types of bias caused by differential amounts of ribosomal reads which can create the impression of falsely differentially expressed genes. --total-hits-norm With this option, Cufflinks counts all fragments, including those not compatible with any reference transcript, towards the number of mapped fragments used in the FPKM denominator --max-bundle-frags Sets the maximum number of fragments a locus may have before being skipped. Skipped loci are listed in skipped.gtf. --num-frag-count-draws Cuffdiff will make this many draws from each transcript's predicted negative binomial random numbder generator. Each draw is a number of fragments that will be probabilistically assigned to the transcripts in the transcriptome. Used to estimate the variance-covariance matrix on assigned fragment counts. --num-frag-assign-draws For each fragment drawn from a transcript, Cuffdiff will assign it this many times (probabilistically), thus estimating the assignment uncertainty for each transcript. Used to estimate the variance-covariance matrix on assigned fragment counts. --min-reps-for-js-test Cuffdiff won't test genes for differential regulation unless the conditions in question have at least this many replicates."
toolshed.g2.bx.psu.edu/repos/devteam/cufflinks/cufflinks/2.2.1.4	"Cufflinks Overview
 Cufflinks_ assembles transcripts, estimates their abundances, and tests for differential expression and regulation in RNA-Seq samples. It accepts aligned RNA-Seq reads and assembles the alignments into a parsimonious set of transcripts. Cufflinks then estimates the relative abundances of these transcripts based on how many reads support each one. Please cite: Trapnell C, Williams BA, Pertea G, Mortazavi AM, Kwan G, van Baren MJ, Salzberg SL, Wold B, Pachter L. Transcript assembly and abundance estimation from RNA-Seq reveals thousands of new transcripts and switching among isoforms. Nature Biotechnology doi:10.1038/nbt.1621 .. _Cufflinks: http://cole-trapnell-lab.github.io/cufflinks/ ------ 
Know what you are doing
 .. class:: warningmark There is no such thing (yet) as an automated gearshift in expression analysis. It is all like stick-shift driving in San Francisco. In other words, running this tool with default parameters will probably not give you meaningful results. A way to deal with this is to 
understand
 the parameters by carefully reading the 
documentation
 and experimenting. Fortunately, Galaxy makes experimenting easy. .. 
: http://cole-trapnell-lab.github.io/cufflinks/cufflinks/ ------ 
Input formats
 Cufflinks takes a text file of SAM alignments as input. The RNA-Seq read mapper TopHat produces output in this format, and is recommended for use with Cufflinks. However Cufflinks will accept SAM alignments generated by any read mapper. Here's an example of an alignment Cufflinks will accept:: s6.25mer.txt-913508 16 chr1 4482736 255 14M431N11M * 0 0 \ CAAGATGCTAGGCAAGTCTTGGAAG IIIIIIIIIIIIIIIIIIIIIIIII NM:i:0 XS:A:- Note the use of the custom tag XS. This attribute, which must have a value of ""+"" or ""-"", indicates which strand the RNA that produced this read came from. While this tag can be applied to any alignment, including unspliced ones, it must be present for all spliced alignment records (those with a 'N' operation in the CIGAR string). The SAM file supplied to Cufflinks must be sorted by reference position. If you aligned your reads with TopHat, your alignments will be properly sorted already. If you used another tool, you may want to make sure they are properly sorted as follows:: sort -k 3,3 -k 4,4n hits.sam > hits.sam.sorted NOTE: Cufflinks currently only supports SAM alignments with the CIGAR match ('M') and reference skip ('N') operations. Support for the other operations, such as insertions, deletions, and clipping, will be added in the future. ------ 
Outputs
 Cufflinks produces three output files: Transcripts and Genes: This GTF file contains Cufflinks' assembled isoforms. The first 7 columns are standard GTF, and the last column contains attributes, some of which are also standardized (e.g. gene_id, transcript_id). There one GTF record per row, and each record represents either a transcript or an exon within a transcript. The columns are defined as follows:: Column number Column name Example Description ----------------------------------------------------- 1 seqname chrX Chromosome or contig name 2 source Cufflinks The name of the program that generated this file (always 'Cufflinks') 3 feature exon The type of record (always either ""transcript"" or ""exon""). 4 start 77696957 The leftmost coordinate of this record (where 0 is the leftmost possible coordinate) 5 end 77712009 The rightmost coordinate of this record, inclusive. 6 score 77712009 The most abundant isoform for each gene is assigned a score of 1000. Minor isoforms are scored by the ratio (minor FPKM/major FPKM) 7 strand + Cufflinks' guess for which strand the isoform came from. Always one of '+', '-' '.' 7 frame . Cufflinks does not predict where the start and stop codons (if any) are located within each transcript, so this field is not used. 8 attributes See below Each GTF record is decorated with the following attributes:: Attribute Example Description ----------------------------------------- gene_id CUFF.1 Cufflinks gene id transcript_id CUFF.1.1 Cufflinks transcript id FPKM 101.267 Isoform-level relative abundance in Reads Per Kilobase of exon model per Million mapped reads frac 0.7647 Reserved. Please ignore, as this attribute may be deprecated in the future conf_lo 0.07 Lower bound of the 95% confidence interval of the abundance of this isoform, as a fraction of the isoform abundance. That is, lower bound = FPKM * (1.0 - conf_lo) conf_hi 0.1102 Upper bound of the 95% confidence interval of the abundance of this isoform, as a fraction of the isoform abundance. That is, upper bound = FPKM * (1.0 + conf_lo) cov 100.765 Estimate for the absolute depth of read coverage across the whole transcript Transcripts only: This file is simply a tab delimited file containing one row per transcript and with columns containing the attributes above. There are a few additional attributes not in the table above, but these are reserved for debugging, and may change or disappear in the future. Genes only: This file contains gene-level coordinates and expression values. ------- 
Cufflinks settings
 All of the options have a default value. You can change any of them. Most of the options in Cufflinks have been implemented here. ------ 
Cufflinks parameter list
 This is a list of implemented Cufflinks options:: -m INT This is the expected (mean) inner distance between mate pairs. For, example, for paired end runs with fragments selected at 300bp, where each end is 50bp, you should set -r to be 200. The default is 45bp. -s INT The standard deviation for the distribution on inner distances between mate pairs. The default is 20bp. -I INT The minimum intron length. Cufflinks will not report transcripts with introns longer than this, and will ignore SAM alignments with REF_SKIP CIGAR operations longer than this. The default is 300,000. -F After calculating isoform abundance for a gene, Cufflinks filters out transcripts that it believes are very low abundance, because isoforms expressed at extremely low levels often cannot reliably be assembled, and may even be artifacts of incompletely spliced precursors of processed transcripts. This parameter is also used to filter out introns that have far fewer spliced alignments supporting them. The default is 0.05, or 5% of the most abundant isoform (the major isoform) of the gene. -j Some RNA-Seq protocols produce a significant amount of reads that originate from incompletely spliced transcripts, and these reads can confound the assembly of fully spliced mRNAs. Cufflinks uses this parameter to filter out alignments that lie within the intronic intervals implied by the spliced alignments. The minimum depth of coverage in the intronic region covered by the alignment is divided by the number of spliced reads, and if the result is lower than this parameter value, the intronic alignments are ignored. The default is 5%. -G Tells Cufflinks to use the supplied reference annotation to estimate isoform expression. It will not assemble novel transcripts, and the program will ignore alignments not structurally compatible with any reference transcript. -N With this option, Cufflinks excludes the contribution of the top 25 percent most highly expressed genes from the number of mapped fragments used in the FPKM denominator. This can improve robustness of differential expression calls for less abundant genes and transcripts."
toolshed.g2.bx.psu.edu/repos/devteam/cuffmerge/cuffmerge/2.2.1.5	"Cuffmerge Overview
 Cuffmerge is part of Cufflinks_. Please cite: Trapnell C, Williams BA, Pertea G, Mortazavi AM, Kwan G, van Baren MJ, Salzberg SL, Wold B, Pachter L. Transcript assembly and abundance estimation from RNA-Seq reveals thousands of new transcripts and switching among isoforms. Nature Biotechnology doi:10.1038/nbt.1621 .. 
Cufflinks: http://cole-trapnell-lab.github.io/cufflinks/ ------ 
Know what you are doing
 .. class:: warningmark There is no such thing (yet) as an automated gearshift in expression analysis. It is all like stick-shift driving in San Francisco. In other words, running this tool with default parameters will probably not give you meaningful results. A way to deal with this is to 
understand
 the parameters by carefully reading the 
documentation
__ and experimenting. Fortunately, Galaxy makes experimenting easy. .. __: http://cole-trapnell-lab.github.io/cufflinks/cuffmerge/ ------ 
Input format
 Cuffmerge takes Cufflinks' GTF output as input, and optionally can take a ""reference"" annotation (such as from Ensembl
) .. _Ensembl: http://www.ensembl.org ------ 
Outputs
 Cuffmerge produces the following output files: Merged transcripts file: Cuffmerge produces a GTF file that contains an assembly that merges together the input assemblies."
toolshed.g2.bx.psu.edu/repos/devteam/cuffnorm/cuffnorm/2.2.1.4	"Cuffnorm Overview
 Cuffnorm is part of Cufflinks_. Running Cuffnorm is very similar to running Cuffdiff. Cuffnorm takes a GTF2/GFF3 file of transcripts as input, along with two or more SAM, BAM, or CXB files for two or more samples. It produces a number of output files that contain expression levels and normalized fragment counts at the level of transcripts, primary transcripts, and genes. It also tracks changes in the relative abundance of transcripts sharing a common transcription start site, and in the relative abundances of the primary transcripts of each gene. Tracking the former allows one to see changes in splicing, and the latter lets one see changes in relative promoter use within a gene.. Please cite: Trapnell C, Williams BA, Pertea G, Mortazavi AM, Kwan G, van Baren MJ, Salzberg SL, Wold B, Pachter L. Transcript assembly and abundance estimation from RNA-Seq reveals thousands of new transcripts and switching among isoforms. Nature Biotechnology doi:10.1038/nbt.1621 .. _Cufflinks: http://cole-trapnell-lab.github.io/cufflinks/ ------ 
Know what you are doing
 .. class:: warningmark There is no such thing (yet) as an automated gearshift in expression analysis. It is all like stick-shift driving in San Francisco. In other words, running this tool with default parameters will probably not give you meaningful results. A way to deal with this is to 
understand
 the parameters by carefully reading the 
documentation
 and experimenting. Fortunately, Galaxy makes experimenting easy. .. 
: http://cole-trapnell-lab.github.io/cufflinks/cuffnorm/ ------ 
Input format
 Cuffnorm takes Cufflinks or Cuffcompare GTF files as input along with two SAM files containing the fragment alignments for two or more samples. ------ 
Outputs
 Cuffnorm outputs a set of files containing normalized expression levels for each gene, transcript, TSS group, and CDS group in the experiment. It does not perform differential expression analysis. To assess the significance of changes in expression for genes and transcripts between conditions, use Cuffdiff. Cuffnorm's output files are useful when you have many samples and you simply want to cluster them or plot expression levels of genes important in your study. By default, Cuffnorm reports expression levels in the ""simple-table"" tab-delimted text files. The program also reports information about your samples and about the genes, transcripts, TSS groups, and CDS groups as tab delimited text files. Note that these files have a different format than the files used by Cuffdiff. However, you can direct Cuffnorm to report its output in the same format as used by Cuffdiff if you wish. Simply supply the option --output-format cuffdiff at the command line. Cuffnorm will report both FPKM values and normalized, estimates for the number of fragments that originate from each gene, transcript, TSS group, and CDS group. Note that because these counts are already normalized to account for differences in library size, they should not be used with downstream differential expression tools that require raw counts as input. To see the details of the simple table format used by Cuffnorm, refer to the simple table expression format, simple table sample attribute format, and simple table feature (e.g. gene) attribute format sections below. ------- 
Parameter list
 This is a list of implemented Cuffnorm options:: --library-norm-method Library Normalization method : Geometric (default), classic-fpkm, quartile --library-type ff-firststrand,ff-secondstrand,ff-unstranded,fr-firstrand,fr-secondstrand,fr-unstranded,transfrags --compatible-hits-norm With this option, Cufflinks counts only those fragments compatible with some reference transcript towards the number of mapped fragments used in the FPKM denominator. Using this mode is generally recommended in Cuffdiff to reduce certain types of bias caused by differential amounts of ribosomal reads which can create the impression of falsely differentially expressed genes. --total-hits-norm With this option, Cufflinks counts all fragments, including those not compatible with any reference transcript, towards the number of mapped fragments used in the FPKM denominator"
toolshed.g2.bx.psu.edu/repos/devteam/cuffquant/cuffquant/2.2.1.2	"Cuffquant Overview
 Cuffquant is part of Cufflinks_. Cuffquant provides pre-calculation of gene expression levels. The resulting file can be provided to cuffdiff or cuffnorm for further processing. Please cite: Trapnell C, Williams BA, Pertea G, Mortazavi AM, Kwan G, van Baren MJ, Salzberg SL, Wold B, Pachter L. Transcript assembly and abundance estimation from RNA-Seq reveals thousands of new transcripts and switching among isoforms. Nature Biotechnology doi:10.1038/nbt.1621 .. _Cufflinks: http://cole-trapnell-lab.github.io/cufflinks/ ------ 
Know what you are doing
 .. class:: warningmark There is no such thing (yet) as an automated gearshift in expression analysis. It is all like stick-shift driving in San Francisco. In other words, running this tool with default parameters will probably not give you meaningful results. A way to deal with this is to 
understand
 the parameters by carefully reading the 
documentation
 and experimenting. Fortunately, Galaxy makes experimenting easy. .. 
: http://cole-trapnell-lab.github.io/cufflinks/cuffquant/ ------ 
Input format
 Cuffquant takes Cufflinks or Cuffcompare GTF files as input along with two or more SAM files containing the fragment alignments for two or more samples. ------ 
Outputs
 Cuffquant produces one output file: 1. Transcript expression values in binary format. ------- 
Settings
 All of the options have a default value. You can change any of them. Most of the options in Cuffdiff have been implemented here. ------ 
Cuffdiff parameter list
 This is a list of implemented Cuffdiff options:: -m INT Average fragment length (SE reads); default 200 -s INT Fragment legnth standard deviation (SE reads); default 80 --max-mle-iterations INT Sets the number of iterations allowed during maximum likelihood estimation of abundances. Default: 5000 -u Multi read correction tells Cufflinks to do an initial estimation procedure to more accurately weight reads mapping to multiple locations in the genome. -b ref.fasta bias correction. Bias detection and correction can significantly improve accuracy of transcript abundance estimates. --no-effective-length-correction Use standard length correction --no-length-correction Disable all length correction. --library-type ff-firststrand,ff-secondstrand,ff-unstranded,fr-firstrand,fr-secondstrand,fr-unstranded,transfrags --mask-file (gff3/gtf) Ignore all alignment within transcripts in this file --max-bundle-frags Sets the maximum number of fragments a locus may have before being skipped. Skipped loci are listed in skipped.gtf."
toolshed.g2.bx.psu.edu/repos/devteam/cummerbund_to_tabular/cummerbund_to_cuffdiff/1.0.1	This tool extracts cuffdiff tabular files from a cummeRbund SQLite database.
toolshed.g2.bx.psu.edu/repos/devteam/tophat2/tophat2/2.1.1	"TopHat Overview
 TopHat_ is a fast splice junction mapper for RNA-Seq reads. It aligns RNA-Seq reads to mammalian-sized genomes using the ultra high-throughput short read aligner Bowtie(2), and then analyzes the mapping results to identify splice junctions between exons. .. 
TopHat: http://ccb.jhu.edu/software/tophat/ ------ 
Know what you are doing
 .. class:: warningmark There is no such thing (yet) as an automated gearshift in splice junction identification. It is all like stick-shift driving in San Francisco. In other words, running this tool with default parameters will probably not give you meaningful results. A way to deal with this is to 
understand
 the parameters by carefully reading the 
documentation
__ and experimenting. Fortunately, Galaxy makes experimenting easy. .. __: http://ccb.jhu.edu/software/tophat/manual.shtml ------ 
Input formats
 TopHat accepts files in Sanger FASTQ format. Use the FASTQ Groomer to prepare your files. ------ 
Outputs
 TopHat produces two output files: - junctions -- A UCSC BED
 track of junctions reported by TopHat. Each junction consists of two connected BED blocks, where each block is as long as the maximal overhang of any read spanning the junction. The score is the number of alignments spanning the junction. - accepted_hits -- A list of read alignments in BAM_ format. .. _BED: http://genome.ucsc.edu/FAQ/FAQformat.html#format1 .. _BAM: http://samtools.sourceforge.net/ Two other possible outputs, depending on the options you choose, are insertions and deletions, both of which are in BED format. ------- 
TopHat settings
 All of the options have a default value. You can change any of them. Some of the options in TopHat have been implemented here. ------ 
TopHat parameter list
 This is a list of implemented TopHat options:: -r This is the expected (mean) inner distance between mate pairs. For, example, for paired end runs with fragments selected at 300bp, where each end is 50bp, you should set -r to be 200. There is no default, and this parameter is required for paired end runs. --mate-std-dev INT The standard deviation for the distribution on inner distances between mate pairs. The default is 20bp. -a/--min-anchor-length INT The ""anchor length"". TopHat will report junctions spanned by reads with at least this many bases on each side of the junction. Note that individual spliced alignments may span a junction with fewer than this many bases on one side. However, every junction involved in spliced alignments is supported by at least one read with this many bases on each side. This must be at least 3 and the default is 8. -m/--splice-mismatches INT The maximum number of mismatches that may appear in the ""anchor"" region of a spliced alignment. The default is 0. -i/--min-intron-length INT The minimum intron length. TopHat will ignore donor/acceptor pairs closer than this many bases apart. The default is 70. -I/--max-intron-length INT The maximum intron length. When searching for junctions ab initio, TopHat will ignore donor/acceptor pairs farther than this many bases apart, except when such a pair is supported by a split segment alignment of a long read. The default is 500000. -g/--max-multihits INT Instructs TopHat to allow up to this many alignments to the reference for a given read, and suppresses all alignments for reads with more than this many alignments. The default is 40. -G/--GTF [GTF 2.2 file] Supply TopHat with a list of gene model annotations. TopHat will use the exon records in this file to build a set of known splice junctions for each gene, and will attempt to align reads to these junctions even if they would not normally be covered by the initial mapping. -j/--raw-juncs [juncs file] Supply TopHat with a list of raw junctions. Junctions are specified one per line, in a tab-delimited format. Records look like: [chrom] [left] [right] [+/-], left and right are zero-based coordinates, and specify the last character of the left sequenced to be spliced to the first character of the right sequence, inclusive. -no-novel-juncs Only look for junctions indicated in the supplied GFF file. (ignored without -G) --no-coverage-search Disables the coverage based search for junctions. --coverage-search Enables the coverage based search for junctions. Use when coverage search is disabled by default (such as for reads 75bp or longer), for maximum sensitivity. --microexon-search With this option, the pipeline will attempt to find alignments incident to microexons. Works only for reads 50bp or longer. --segment-mismatches Read segments are mapped independently, allowing up to this many mismatches in each segment alignment. The default is 2. --segment-length Each read is cut up into segments, each at least this long. These segments are mapped independently. The default is 25. --min-coverage-intron The minimum intron length that may be found during coverage search. The default is 50. --max-coverage-intron The maximum intron length that may be found during coverage search. The default is 20000. --min-segment-intron The minimum intron length that may be found during split-segment search. The default is 50. --max-segment-intron The maximum intron length that may be found during split-segment search. The default is 500000."
toolshed.g2.bx.psu.edu/repos/iuc/trinity_align_and_estimate_abundance/trinity_align_and_estimate_abundance/2.15.1+galaxy0	"Trinity_ assembles transcript sequences from Illumina RNA-Seq data. This tool estimates the abundance of isoforms and genes of a transcriptome assembled with Trinity, using FastQ of a specific sample. 
Inputs
 It takes as input a transcriptome assembled with Trinity and the reads from a RNASeq sample. You have to choose between several counting methods. If you dont align on a Trinity assembly, you need to provide a file of the following (tabular) format to map gene ids to transcript ids: =========== ================ gene1 transcript1 ----------- ---------------- gene2 transcript2 =========== ================ 
Output
 This tool will produce 2 tabular files, with counts for isoforms and genes respectively. More details on this page: .. _Trinity manual: https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Transcript-Quantification .. _Trinity: https://github.com/trinityrnaseq/trinityrnaseq/wiki"
toolshed.g2.bx.psu.edu/repos/iuc/slamdunk/alleyoop/0.4.3+galaxy1	"SLAMseq ======= SLAMseq is a novel sequencing protocol that directly uncovers 4-thiouridine incorporation events in RNA by high-throughput sequencing. When combined with metabolic labeling protocols, SLAM-seq allows to study the intracellular RNA dynamics, from transcription, RNA processing to RNA stability. Original publication: 
Herzog et al., Nature Methods, 2017; doi:10.1038/nmeth.4435 &lt;https://www.nature.com/nmeth/journal/vaop/ncurrent/full/nmeth.4435.html&gt;
 Alleyoop ======== Alleyoop (Additional sLamdunk heLpEr tools for anY diagnOstics Or Plots) is a collection of tools for post-processing and running diagnostics on Slamdunk analyses. This tool works on the output of the 
Slamdunk
 tool and requires all the inputs listed in the table below. =============== ========================================================================================================================================================== Parameter Description =============== ========================================================================================================================================================== 
Genome
 The reference fasta file (Genome assembly). 
Reference
 BED-file containing coordinates for 3' UTRs. 
Reads
 Slamdunk Filtered BAM files. 
Counts
 Slamdunk Count TSV files. 
Variants
 Slandunk VCF files. 
Read length
 Maximum length of reads (usually 50, 100, 150). =============== ========================================================================================================================================================== This tool runs the 
Alleyoop
 
summary
, 
rates
, 
utrrates
, 
tcperreadpos
 and 
tcperutrpos
 modules and outputs: * Tab-separated 
summary
 files from the summary module with mapping and PCA statistics * Tab-separated 
stats
 files from the rates, utrrates, tcperreadpos and tcperutrpos modules Optionally, the 
read-separator
 module can be run to output BAM files of separated T>C and non T>C reads. The summary and stats files can be summarised and visualised with MultiQC. An example MultiQC report can be seen here
. For information on these modules see the 
Alleyoop documentation
. .. 
Alleyoop documentation
: http://t-neumann.github.io/slamdunk/docs.html#document-Alleyoop .. _here: http://t-neumann.github.io/slamdunk/multiqc_example/multiqc_report.html"
toolshed.g2.bx.psu.edu/repos/iuc/deg_annotate/deg_annotate/1.1.0+galaxy1	"What it does
 This tool appends the output table of DESeq2/edgeR/limma/DEXSeq with gene symbols, biotypes, positions etc. The information you want to add is configurable. This information should present in the input GTF/GFF file as attributes of feature you choose. DEXSeq-Count tool is used to prepare the DEXSeq compatible annotation (flattened GTF file) from input GTF/GFF. In this process, the exons that appear multiple times, once for each transcript are collapsed to so called 
exon counting bins
. Counting bins for parts of exons arise when an exonic region appears with different boundaries in different transcripts. The resulting flattened GTF file contains pseudo exon ids per gene instead of per transcript. This tool maps the DEXSeq couting bins back to the original exon ids. This mapping is only possible if the input GTF/GFF file contains transcript identifier attribute for the chosen features type. 
Inputs
 
Differential gene expression tables
 At the moment, this tool supports DESeq2 and DEXSeq tool outputs. 
Annotation
 Annotation file ne GTF or GFF3 format that was used for counting. 
Outputs
 Input tabular file and with chosen attributes appended as additional columns."
toolshed.g2.bx.psu.edu/repos/iuc/brew3r_r/brew3r_r/1.0.2+galaxy1	".. class:: infomark 
What it does
 This tool extend the annotations existing in an input GTF file in the 3' end using annotations from another input GTF. During the process, it makes sure that there will not be new overlaps between different genes. Usage ..... 
Input
 2 GTF files: - First one to extend usually comes from a public resource. - Second one that is used as template may come from a public resource or from StringTie. 
Output
 1 GTF file with all exons from the input GTF where some of them have been extended (the exon_id ends with '.ext') and potentially new exons (the exon_id contains BREW3R)."
toolshed.g2.bx.psu.edu/repos/rnateam/blockclust/blockclust/1.1.1	".. class:: infomark 
What it does
 BlockClust is an efficient approach to detect transcripts with similar processing patterns. We propose a novel way to encode expression profiles in compact discrete structures, which can then be processed using fast graph-kernel techniques. BlockClust allows both clustering and classification of small non-coding RNAs. BlockClust runs in three operating modes: 1) Pre-processing - converts given mapped reads (BAM) into BED file of tags 2) Clustering and classification - of given input blockgroups (output of blockbuster tool) as explained in the original paper. 3) Post-processing - plots for overview of predicted clusters. For a thorough analysis of your data, we suggest you to use complete blockclust workflow, which contains all three modes of operation. 
Inputs
 BlockClust input files are dependent on the mode of operation: 1. Pre-processing mode: * Binary Sequence Alignment Map (BAM) file 2. Clustering and classification: * A blockgroups file generated by blockbuster tool * Select reference genome 3. Post-processing: * Output of cmsearch, searched clusters generated by BlockClust against Rfam * BED file containing clusters generated by BlockClust * Pairwise similarities of blockgroups generated by BlockClust 
Outputs
 1. Pre-processing mode: * BED file of tags with expressions 2. Clustering and classification: * Hierarchical clustering plot of all input blockgroups by their similarity * Pairwise similarities of all input blockgroups * BED file containing predicted clusters * BED file containing prediction of blockgroups by pre-compiled SVM binary classification model. 3. Post-processing: * Plot of distribution of ncRNA families per predicted cluster (overview of cluster precissions). The annotation of ncRNA families are retrieved by searching cluster instances against Rfam database. * Hierarchical clustering made out of centroids of each BlockClust predicted cluster"
toolshed.g2.bx.psu.edu/repos/iuc/trinity_abundance_estimates_to_matrix/trinity_abundance_estimates_to_matrix/2.15.1+galaxy0	"Trinity_ assembles transcript sequences from Illumina RNA-Seq data. This tool will combine abundance estimations (produced by 'Align reads and estimate abundance on a de novo assembly of RNA-Seq data' tool) from multiple samples into a single tabular file. This matrix can then be used by 'RNASeq samples quality check for transcript quantification' and 'Differential Expression Analysis using a Trinity assembly' tools. 
Inputs
 It takes as input multiple results from 'Align reads and estimate abundance on a de novo assembly of RNA-Seq data' tool/ Each sample must have a name, that should be used in subsequent tools. 
Output
 This tool will produce a single matrix file. More details on this page: .. _Trinity manual: https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Transcript-Quantification .. _Trinity: https://github.com/trinityrnaseq/trinityrnaseq/wiki"
toolshed.g2.bx.psu.edu/repos/iuc/cemitool/cemitool/1.30.0+galaxy0	".. class:: infomark 
Purpose
 The CEMiTool R package provides users with an easy-to-use method to automatically implement gene co-expression network analyses, obtain key information about the discovered gene modules using additional downstream analyses and retrieve publication-ready results via a high-quality interactive report. .. class:: infomark 
Purpose"
toolshed.g2.bx.psu.edu/repos/iuc/heinz/heinz_scoring/1.0	"This tool is used to calculate a Heinz score for each node (e.g., a gene). This score further serves as the input of the tool ""Identify optimal scoring subnetwork"". The input ""Node file with p-values"" should be like this: ========= =================== K10970 0.00278208628672861 --------- ------------------- K10780 0.0029566591795884 --------- ------------------- K01484 0.0157152504694443 --------- ------------------- K09055 0.0188894478579773 ========= =================== The first column is ""node identifier"" (e.g., a gene name); the second column, ""p-value""; The columns are delimited by a tab; no headers are needed. The section ""Choose your input type for BUM parameters"" requires two alternative types of input: 1.""The output of the BUM model"" (The tool ""Fit a BUM model""): the first line of the file is lambda; the second, alpha. 2.""Manually type the parameters of BUM model"": lambda and alpha."
toolshed.g2.bx.psu.edu/repos/iuc/chira_collapse/chira_collapse/1.4.31	".. class:: infomark 
What it does
 This tool deduplicates the reads from the FASTQ file and writes into a fasta each read once with it's read count. 
Inputs
 * Quality and adapter trimmed FASTQ file 
Outputs
 * FASTA file with unique sequences. The headers of the sequence are in the following format: >sequence_id|UMI|read_count"
toolshed.g2.bx.psu.edu/repos/iuc/chira_extract/chira_extract/1.4.31	".. class:: infomark 
What it does
 This tool extracts the best chimeric alignments for each read. User can optionally hybridize the loci where the chimeric arms are mapping to. 
Inputs
 * Tabular file containing CRLs information * Annotation GTF file * Reference fasta files. Provide both in case of split reference. * If your alignments are merged at genomic level in previous step (chira merge), then provide a reference genomic fasta fille. 
Output
 * Tabular file containing chimeras information"
toolshed.g2.bx.psu.edu/repos/iuc/chira_map/chira_map/1.4.30	".. class:: infomark 
What it does
 This tool handles the mapping of the reads to reference transcriptome. User can choose between the bwa-mem and CLAN alignment tools. 
Inputs
 * A fasta file containing reads * A reference fasta file containing transcript sequences * An optional second reference fasta file, incase if you split your reference into two 
Output
 * BED file containing the alignments * unmapped FASTA file (only for aligner BWA-MEM)"
toolshed.g2.bx.psu.edu/repos/iuc/chira_merge/chira_merge/1.4.30	".. class:: infomark 
What it does
 This tool merges the overlapping aligned positions to define the read concentrated loci. If an annotation GTF file produced, the transcriptomic alignment positions are first converted to their corresponding genomic positions. 
Inputs
 * Alignments in BED format * An annotation GTF file contaning reference genomic positions. 
Output
 * BED file containing the alignments with reads categorized into segments depending on which part of the read is aligned. * Tabular file containing merged alignments. 4th column contains all the alignments merged into that location."
toolshed.g2.bx.psu.edu/repos/iuc/chira_quantify/chira_quantify/1.4.30	".. class:: infomark 
What it does
 This tool first creates CRLS from merged BED file and quantifies them based on the mapped reads. 
Inputs
 * A BED file containing alignment segments * A BED file containing merged alignments 
Output
 * Tabular file containing the reads and their CRLs with TPM values."
toolshed.g2.bx.psu.edu/repos/iuc/trinity_contig_exn50_statistic/trinity_contig_exn50_statistic/2.15.1+galaxy0	"Trinity_ assembles transcript sequences from Illumina RNA-Seq data. This tool computes the N50 statistic limited to the top most highly expressed transcripts that represent x% of the total normalized expression data. This requires that you have first performed transcript abundance estimation with 'Align reads and estimate abundance for a de novo assembly of RNA-Seq data by Trinity' tool and that you have built the expression matrix with 'Build expression matrix for a de novo assembly of RNA-Seq data by Trinity' tool. 
Inputs
 It takes as input a transcriptome assembled with Trinity and the matrix of normalized expression values produced by 'Build expression matrix for a de novo assembly of RNA-Seq data by Trinity' tool. .. _Trinity: https://github.com/trinityrnaseq/trinityrnaseq/wiki"
toolshed.g2.bx.psu.edu/repos/iuc/crosscontamination_barcode_filter/crosscontamination_barcode_filter/0.3	"Cross-contamination Filter Plot ################################### For a set of barcodes and an experimental setup that uses a subset of these barcodes for each batch, this tool compares each batch against the full range of barcodes in order to determine whether any cross-contamination between batches has occured. 
Note -- Do not reuse batch numbering across plates!
 If a significant number of transcripts are shown in a batch for cell barcodes that were not designed for that batch, then this tool will show that. In the below plot, we can see that there is no significant cross-contamination taking place (pre-filter), and so we can filter out the false barcodes (post-filter). .. image:: $PATH_TO_IMAGES/crosscontam_pretopost.png :scale: 50 % Example ~~~~~~~~ Consider the following experimental setup, with a list of 100 possible barcodes, used over 3 sequencing plates, with each plate containing 4 unique batches, and each plate using a specific subset of the 100 barcodes. :: Barcodes 1 - 10 | AAA AAC AAT AAG ACA AGA ATA CAC GAG TAT 11 - 20 | CCC CCA CCT CCG CTC CGC TCT GCG TCT CGT . . 91 -100 | TTT TAT TCT TGT TTA TTC TTG TCC TGG TAA Plate 1 +-------+-------+-------+-------+ | B1 | B2 | B3 | B4 | +-------+-------+-------+-------+ 1-50 51-100 51-100 1-50 Plate 2 +-------+-------+-------+-------+ | B5 | B6 | B7 | B8 | +-------+-------+-------+-------+ 1-40 41-80 1-40 41-80 Plate 3 +-------+-------+-------+-------+ | B9 | B10 | B11 | B12 | +-------+-------+-------+-------+ 1-40 41-80 1-40 41-80 
 The above plate and barcoding setup can be more textually represented by specifying barcode ranges and plate numbers, with each denoting which batch numbers they describe as outlined below: :: 
Barcodes → Batches
 1- 50: B1, B4 51-100: B2, B3 1- 40: B5, B7, B9 , B11 41- 80: B6, B8, B10, B12 
Plates → Batches
 1: B1, B2 , B3 , B4 2: B5, B6 , B7 , B8 3: B9, B10, B11, B12"
toolshed.g2.bx.psu.edu/repos/iuc/deseq2/deseq2/2.11.40.8+galaxy2	".. class:: infomark 
What it does
 Uses DESeq2 version 1.40.2 to estimate variance-mean dependence in count data from high-throughput sequencing assays and test for differential expression based on a model using the negative binomial distribution. ----- 
Inputs
 
Count Files
 DESeq2_ takes count tables generated from 
featureCounts
, 
HTSeq-count
 or 
StringTie
 as input. Count tables must be generated for each sample individually. One header row is assumed, but files with no header (e.g from HTSeq) can be input with the 
Files have header?
 option set to No. DESeq2 is capable of handling multiple factors that affect your experiment. The first factor you input is considered as the primary factor that affects gene expressions. Optionally, you can input one or more secondary factors that might influence your experiment. But the final output will be changes in genes due to primary factor in presence of secondary factors. Each factor has two levels/states. You need to select appropriate count table from your history for each factor level. The following table gives some examples of factors and their levels: ========= ============== =============== Factor Factor level 1 Factor level 2 --------- -------------- --------------- Treatment Treated Untreated --------- -------------- --------------- Condition Knockdown Wildtype --------- -------------- --------------- TimePoint Day4 Day1 --------- -------------- --------------- SeqType SingleEnd PairedEnd --------- -------------- --------------- Gender Female Male ========= ============== =============== 
Note
: Output log2 fold changes are based on primary factor level 1 vs. factor level2. Here the order of factor levels is important. For example, for the factor 'Treatment' given in above table, DESeq2 computes fold changes of 'Treated' samples against 'Untreated', i.e. the values correspond to up or down regulations of genes in Treated samples. DESeq2_ can also take transcript-level counts from quantification tools such as, 
kallisto
, 
Salmon
 and 
Sailfish
, and this Galaxy wrapper incorporates the Bioconductor tximport_ package to process the transcript counts for DESeq2. 
Salmon or Sailfish Files
 Salmon or Sailfish 
quant.sf
 files can be imported by setting type to 
Salmon
 or 
Sailfish
 respectively above. Note: for previous version of Salmon or Sailfish, in which the quant.sf files start with comment lines you will need to remove the comment lines before inputting here. An example of the format is shown below. Example: ============ ========== =============== =========== =========== Name Length EffectiveLength TPM NumReads ------------ ---------- --------------- ----------- ----------- NR_001526 164 20.4518 0 0 NR_001526_1 164 20.4518 0 0 NR_001526_2 164 20.4518 0 0 NM_130786 1764 1956.04 2.47415 109.165 NR_015380 2129 2139.53 1.77331 85.5821 NM_001198818 9360 7796.58 2.38616e-07 4.19648e-05 NM_001198819 9527 7964.62 0 0 NM_001198820 9410 7855.78 0 0 NM_014576 9267 7714.88 0.0481114 8.37255 ============ ========== =============== =========== =========== 
kallisto Files
 kallisto 
abundance.tsv
 files can be imported by setting type to 
kallisto
 above. An example of the format is shown below. Example: ============ ========== =============== =========== =========== target_id length eff_length est_counts tpm ------------ ---------- --------------- ----------- ----------- NR_001526 164 20.4518 0 0 NR_001526_1 164 20.4518 0 0 NR_001526_2 164 20.4518 0 0 NM_130786 1764 1956.04 109.165 2.47415 NR_015380 2129 2139.53 85.5821 1.77331 NM_001198818 9360 7796.58 4.19648e-05 2.38616e-07 NM_001198819 9527 7964.62 0 0 NM_001198820 9410 7855.78 0 0 NM_014576 9267 7714.88 8.37255 0.0481114 ============ ========== =============== =========== =========== ----- 
Output
 DESeq2_ generates a tabular file containing the different columns and optional visualized results as PDF. ====== ========================================================== Column Description ------ ---------------------------------------------------------- 1 Gene Identifiers 2 mean normalised counts, averaged over all samples from both conditions 3 the logarithm (to basis 2) of the fold change (See the note in inputs section) 4 standard error estimate for the log2 fold change estimate 5 Wald statistic 6 p value for the statistical significance of this change 7 p value adjusted for multiple testing with the Benjamini-Hochberg procedure which controls false discovery rate (FDR) ====== ========================================================== By selecting 
Output sample size factors
 in the ""Output options"" selection box, the size factors used to normalize the samples can also be output as a tabular file. .. _DESeq2: http://master.bioconductor.org/packages/release/bioc/html/DESeq2.html .. _tximport: https://bioconductor.org/packages/devel/bioc/vignettes/tximport/inst/doc/tximport.html"
toolshed.g2.bx.psu.edu/repos/iuc/dexseq/dexseq/1.48.0+galaxy1	".. class:: infomark 
What it does
 Inference of differential exon usage in RNA-Seq. 
Inputs
 DEXSeq_ takes count tables generated from the dexseq_count as input. Count tables must be generated for each sample individually. DEXSeq_ is capable of handling multiple factors that affect your experiment. The first factor you input is considered to be the primary factor that affects gene expressions. You can also input several secondary factors that might influence your experiment but the final output will be changes in genes due to primary factor in the presence of secondary factors. Each factor has two levels/states. You need to select an appropriate count table from your history for each factor level. The following table gives some examples of factors and their levels: ========= ============== =============== Factor Factor level 1 Factor level 2 --------- -------------- --------------- condition Knockdown Wildtype --------- -------------- --------------- treatment Treated Untreated --------- -------------- --------------- timePoint Day4 Day1 --------- -------------- --------------- SeqType SingleEnd PairedEnd --------- -------------- --------------- Gender Female Male ========= ============== =============== 
Note
: Output log2 fold changes are based on primary factor level 1 vs. factor level 2. Here the order of factor levels is important. For example, for the factor 'condition' given in the above table, DEXSeq computes fold changes of 'Knockdown' samples against 'Wildtype', i.e. the values correspond to up or down regulations of genes in Knockdown samples. 
Output
 DEXSeq_ generates a tabular file containing the different columns and an optional html report. It can also ouput the DEXSeqResults R object that can be used with the plotDEXSeq tool to visualise individual genes. ====== ========================================================== Column Description ------ ---------------------------------------------------------- 1 Gene and exon Identifiers 2 group/gene identifier 3 feature/exon identifier 4 mean of the counts across samples in each feature/exon 5 exon dispersion estimate 6 LRT statistic 7 LRT p-value 8 BH adjusted p-values 9 exon usage coefficient factorLevel 2 10 exon usage coefficient factorLevel 1 11 relative exon usage fold changes 12 GRanges object of the coordinates of the exon/feature 13 matrix of integer counts, of each column containing a sample 14 list of transcripts overlapping with the exon ====== ========================================================== .. _DEXSeq: http://master.bioconductor.org/packages/release/bioc/html/DEXSeq.html"
toolshed.g2.bx.psu.edu/repos/iuc/dexseq/dexseq_count/1.48.0+galaxy1	".. class:: infomark 
What it does
 The main goal of this tool is to count the number of reads/fragments per exon of each gene in RNA-seq samples. In addition, it also prepares your annotation GTF file, making it compatible for counting. 
Inputs
 Mode-preprare: Takes a normal GTF file as input. For example from Ensembl database. Mode-count: Inputs are flattened GTF file and BAM file. The flattened GTF file can be generated from 'prepare' mode of this tool. 
Output
 Mode-prepare: Flattened GTF file that contains only exons with corresponding gene ids from the input GTF file. Sometimes two or more genes sharing an exon will be merged into an 'aggregate gene' if the aggregate option was used. Mode-count: Two column tab-delimited file with exon ids and their read counts. .. _DEXSeq: http://master.bioconductor.org/packages/release/bioc/html/DEXSeq.html"
toolshed.g2.bx.psu.edu/repos/bgruening/diamond/bg_diamond/2.0.8.0	"What it does
 DIAMOND_ is a new alignment tool for aligning short DNA sequencing reads to a protein reference database such as NCBI-NR. On Illumina reads of length 100-150bp, in fast mode, DIAMOND is about 20,000 times faster than BLASTX, while reporting about 80-90% of all matches that BLASTX finds, with an e-value of at most 1e-5. In sensitive mode, DIAMOND ist about 2,500 times faster than BLASTX, finding more than 94% of all matches. The DIAMOND algorithm is designed for the alignment of large datasets. The algorithm is not efficient for a small number of query sequences or only a single one of them, and speed will be low. BLAST is recommended for small datasets. .. _DIAMOND: http://ab.inf.uni-tuebingen.de/software/diamond/ 
Input
 Input data is a large protein or nucleotide sequence file. 
Output
 Diamond gives you a tabular output file with 12 columns: Column Description 1 Query Seq-id (ID of your sequence) 2 Subject Seq-id (ID of the database hit) 3 Percentage of identical matches 4 Alignment length 5 Number of mismatches 6 Number of gap openings 7 Start of alignment in query 8 End of alignment in query 9 Start of alignment in subject (database hit) 10 End of alignment in subject (database hit) 11 Expectation value (E-value) 12 Bit score Supported values for gap open and gap extend parameters depending on the selected scoring matrix. ======== ============================================ Matrix Supported values for (gap open)/(gap extend) ======== ============================================ BLOSUM45 (10-13)/3; (12-16)/2; (16-19)/1 BLOSUM50 (9-13)/3; (12-16)/2; (15-19)/1 BLOSUM62 (6-11)/2; (9-13)/1 BLOSUM80 (6-9)/2; 13/2; 25/2; (9-11)/1 BLOSUM90 (6-9)/2; (9-11)/1 PAM250 (11-15)/3; (13-17)/2; (17-21)/1 PAM70 (6-8)/2; (9-11)/1 PAM30 (5-7)/2; (8-10)/1 ======== ============================================"
toolshed.g2.bx.psu.edu/repos/bgruening/diamond/bg_diamond_makedb/2.0.8	".. class:: infomark 
What it does
 DIAMOND_ is a new alignment tool for aligning short DNA sequencing reads to a protein reference database such as NCBI-NR. On Illumina reads of length 100-150bp, in fast mode, DIAMOND is about 20,000 times faster than BLASTX, while reporting about 80-90% of all matches that BLASTX finds, with an e-value of at most 1e-5. In sensitive mode, DIAMOND is about 2,500 times faster than BLASTX, finding more than 94% of all matches. .. _DIAMOND: http://ab.inf.uni-tuebingen.de/software/diamond/ - taxonmap: Path to mapping file that maps NCBI protein accession numbers to taxon ids (gzip compressed). This parameter is optional and needs to be supplied in order to provide taxonomy features. The file can be downloaded from NCBI: ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz - taxonnames: Path to the names.dmp file from the NCBI taxonomy. This parameter is optional and needs to be supplied in order to provide taxonomy features. The file is contained within this archive downloadable at NCBI: ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip - taxonnodes: Path to the nodes.dmp file from the NCBI taxonomy. This parameter is optional and needs to be supplied in order to provide taxonomy features. The file is contained within this archive downloadable at NCBI: ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip"
toolshed.g2.bx.psu.edu/repos/bgruening/diamond/bg_diamond_view/2.0.8	"What it does
 Converts diamond daa files to multiple other formats. 
Input
 Input data is a daa file. 
Output
 Alignment results in BLAST format (pairwise/tabular), xml, sam, taxonomic (Note the latter does not work with the current diamond version. ) BLAST tables contain the following columns. Column Description 1 Query Seq-id (ID of your sequence) 2 Subject Seq-id (ID of the database hit) 3 Percentage of identical matches 4 Alignment length 5 Number of mismatches 6 Number of gap openings 7 Start of alignment in query 8 End of alignment in query 9 Start of alignment in subject (database hit) 10 End of alignment in subject (database hit) 11 Expectation value (E-value) 12 Bit score"
toolshed.g2.bx.psu.edu/repos/iuc/trinity_run_de_analysis/trinity_run_de_analysis/2.15.1+galaxy0	"Trinity_ assembles transcript sequences from Illumina RNA-Seq data. This tool performs differential expression analyses on a transcriptome assembled with Trinity. 
Inputs
 This tool uses the matrix produced by 'Build expression matrix for a de novo assembly of RNA-Seq data by Trinity' tool. You must describe your samples and replicates with a tabular file looking like this: =========== ================ ConditionA CondA_replicate1 ----------- ---------------- ConditionA CondA_replicate2 ----------- ---------------- ConditionB CondB_replicate1 ----------- ---------------- ConditionB CondB_replicate2 ----------- ---------------- ConditionC CondC_replicate1 ----------- ---------------- ConditionC CondC_replicate2 ----------- ---------------- ConditionC CondC_replicate3 =========== ================ This file can be generated with the 'Describe samples and replicates' tool. It will probably be the same file as used in the tool 'RNASeq samples quality check for transcript quantification'. The names in column 2 must match the names given in the tool 'Build expression matrix for a de novo assembly of RNA-Seq data by Trinity'. .. _Trinity: https://github.com/trinityrnaseq/trinityrnaseq/wiki"
toolshed.g2.bx.psu.edu/repos/iuc/egsea/egsea/1.20.0	".. class:: infomark 
What it does
 EGSEA_, an acronym for 
Ensemble of Gene Set Enrichment Analyses
, is a 
Bioconductor package
 that utilizes the analysis results of eleven prominent GSE algorithms from the literature to calculate collective significance scores for gene sets. These methods are currently: 
ora, globaltest, plage, safe, zscore, gage, ssgsea, roast, fry, padog, camera, gsva
. The ora, gage, camera and gsva methods depend on a competitive null hypothesis while the remaining seven methods are based on a self-contained hypothesis. EGSEA’s gene set database, the 
EGSEAdata
 Bioconductor package, contains around 25,000 gene sets from 16 collections from MSigDB
, KEGG_ and GeneSetDB_. Supported organisms are human, mouse and rat, however MSigDB is only available for human and mouse. An example 
EGSEA workflow
 is available at the Bioconductor workflows website. Currently the 
egsea.cnt
 function is implemented in this tool. This function takes a raw RNA-Seq count matrix and uses 
limma-voom
 with TMM normalization to convert the RNA-seq counts into expression values for EGSEA analysis. EGSEA returns a HTML report of detailed analysis results for each contrast of interest and comparative analysis results. The heatmap view at both the gene set and summary level and the summary level bar plots can be useful summaries to include in publications to highlight the gene set testing results. .. class:: warningmark 
WARNING: This tool is only available for non-commercial use.
 The 
GAGE
 and 
Pathview
 packages used by EGSEA make use of KEGG data and Non-academic uses may require a KEGG license agreement. Before using, be sure to review, agree, and comply with the relevant licenses for KEGG and MSigDB. * 
KEGG Licence
 * 
MSigDB Licence
 .. _KEGG Licence: http://www.kegg.jp/kegg/legal.html .. _MSigDB Licence: http://software.broadinstitute.org/gsea/license_terms_list.jsp ----- 
Inputs
 
Counts Data
 This tool requires a counts matrix (counts table) containing the raw RNA-seq read counts. The counts data can either be input as separate counts files (one sample per file) or a single count matrix (one sample per column). The rows correspond to genes, and columns correspond to the counts for the samples. Values must be tab separated, with the first row containing the sample/column labels. The first column must contain Entrez Gene IDs that are unique (not repeated) within the counts file. Entrez IDs can be obtained from the 
annotateMyIDs
 Galaxy tool. Genes with low counts should be removed, such as in the filtered counts matrix that can be output from the 
limma
 tool. Example - 
Separate Count Files
: =============== ========== EntrezID 
WT1
 =============== ========== 1 71 1000 3 10000 2310 100009605 3 100009613 9 =============== ========== Example - 
Single Count Matrix
: =============== ========== ========== ========== ========= ========= ========= EntrezID 
WT1
 
WT2
 
WT3
 
Mut1
 
Mut2
 
Mut3
 =============== ========== ========== ========== ========= ========= ========= 1 71 73 69 36 22 28 1000 3 4 2 4 0 1 10000 2310 2142 2683 1683 2068 2172 100009605 3 1 2 1 5 3 100009613 9 11 4 13 6 10 =============== ========== ========== ========== ========= ========= ========= 
Factor Information
 Enter factor names and groups in the tool form, or provide a tab-separated file that has the names of the samples in the first column and one header row. The sample names must be the same as the names in the columns of the count matrix. The second column should contain the primary factor levels (e.g. WT, Mut) with optional additional columns for any secondary factors e.g Batch. Example: ========== ============ ========= 
Sample
 
Genotype
 
Batch
 ---------- ------------ --------- WT1 WT b1 WT2 WT b2 WT3 WT b3 Mut1 Mut b1 Mut2 Mut b2 Mut3 Mut b3 ========== ============ ========= 
Factor Name:
 The name of the experimental factor being investigated e.g. Genotype, Treatment. One factor must be entered and spaces must not be used. Optionally, additional factors can be included, these are variables that might influence your experiment e.g. Batch, Gender, Subject. If additional factors are entered, edgeR will fit an additive linear model. 
Groups:
 The names of the groups for the factor. Spaces must not be used and if entered into the tool form above, the values should be separated by commas. 
Symbols Mapping file
 A file containing the Gene Symbol for each Entrez Gene ID. The first column must be the Entrez Gene IDs and the second column must be the Gene Symbols. It is used for the heatmap visualization. The number of rows should match that of the Counts Matrix. Example: ========= ========= EntrezID Symbols ========= ========= 1 A1BG 1000 CDH2 10000 AKT3 100009605 TRNAF1 100009613 ANO1-AS2 ========= ========= ----- 
Outputs
 The EGSEA report is an interactive HTML report that is generated to enable a swift navigation through the results of an EGSEA analysis. The pages below are generated for each gene set collection and contrast/comparison. 
Stats Table page
 The Stats Table page shows the detailed statistics of the EGSEA analysis for the top gene sets. It shows the EGSEA scores, individual rankings and additional annotation for each gene set. Hyperlinks to the source of each gene set can be seen in this table when they are available. The ""Direction"" column shows the regulation direction of a gene set which is calculated based on the logFC, which is either calculated from the limma differential expression analysis or provided by the user. The logFC cutoff and FDR cutoff are applied for this calculation. The calculations of the EGSEA scores can be seen in the references section. The method topSets can be used to generate custom Stats Table. 
Heatmaps page
 The Heatmaps page shows the heatmaps of the gene fold changes for the gene sets that are presented in the Stats Table page. Red indicates up-regulation while blue indicates down-regulation. Only genes that appear in the input expression/count matrix are visualized in the heat map. Gene names are coloured based on their statistical significance in the limma differential expression analysis. The ""Interpret Results"" link below each heat map allows the user to download the original heat map values along with additional statistics from limma DE analysis ( if available) so that they can be used to perform further analysis in R, e.g., customizing the heat map visualization. 
Summary Plots page
 The Summary Plots page shows the methods ranking plot along with the summary plots of EGSEA analysis. The method plot uses multidimensional scaling (MDS) to visualize the ranking of individual methods on a given gene set collection. The summary plots are bubble plots that visualize the distribution of gene sets based on the EGSEA Significance Score and another EGSEA score (default, p-value). Two summary plots are generated: ranking and directional plots. Each gene set is reprersented with a bubble which is coloured based on the EGSEA ranking (in ranking plots ) or gene set regulation direction (in directional plots) and sized based on the gene set cardinality (in ranking plots) or EGSEA Significance score (in directional plots). Since the EGSEA ""Significance Score"" is proportional to the p-value and the absolute fold changes, it could be useful to highlight gene sets that have high Significance scores. The blue labels on the summary plot indicate gene sets that do not appear in the top 10 list of gene sets based on the ""sort.by"" argument (black labels) yet they appear in the top 5 list of gene sets based on the EGSEA ""Significance Score"". If two contrasts are provided, the rank is calculated based on the ""comparison"" analysis results and the ""Significance Score"" is calculated as the mean. 
Pathways page
 The Pathways page shows the KEGG pathways for the gene sets that are presented in the Stats Table of a KEGG gene set collection. The gene fold changes are overlaid on the pathway maps and coloured based on the gene regulation direction: blue for down-regulation and red for up-regulation. Note that this page only appears if a KEGG gene set collection is used in the EGSEA analysis. 
GO Graphs page
 The GO Graphs page shows the Gene Ontology graphs for top 5 GO terms in each of three GO categories: Biological Processes (BP), Molecular Functions (MF), and Cellular Components (CC). Nodes are coloured based on the default sort.by score where red indicates high significance and yellow indicates low significance. Note that this page only appears if a Gene Ontology gene set collection is used, i.e., for the c5 collection from MSigDB or the gsdbgo collection from GeneSetDB. 
Interpret Results link
 The Interpret Results hyperlink in the EGSEA report allows the user to download the fold changes and limma analysis results and thus improve the interpretation of the results. .. class:: warningmark Note that the running time of this tool depends on a number of things, including the number of samples and contrasts provided as input, and also the number of gene set testing methods and gene set collections chosen. For example, the 
egsea.cnt example
 in the EGSEA vignette was conducted with 8 samples and 2 contrasts, using the KEGG Signaling and Disease pathways, and 7 of the 12 gene set testing methods, on a MacBook Pro machine that had a 2.8 GHz Intel Core i7 CPU and 16 GB of RAM. The execution time took 145.5 seconds using 16 threads. .. 
egsea.cnt example: https://bioconductor.org/packages/release/bioc/vignettes/EGSEA/inst/doc/EGSEA.pdf ----- 
More Information
 
MSigDB Gene Set Colletions
 The MSigDB
 gene sets are divided into 8 major collections: * 
H: hallmark gene sets
 are coherently expressed signatures derived by aggregating many MSigDB gene sets to represent well-defined biological states or processes. * 
C1: positional gene sets
 for each human chromosome and cytogenetic band. * 
C2: curated gene sets
 are from online pathway databases, publications in PubMed, and knowledge of domain experts. * 
C3: motif gene sets
 are based on conserved cis-regulatory motifs from a comparative analysis of the human, mouse, rat, and dog genomes. * 
C4: computational gene sets
 are defined by mining large collections of cancer-oriented microarray data. * 
C5: GO gene sets
 consist of genes annotated by the same GO terms. * 
C6: oncogenic gene sets
 are defined directly from microarray gene expression data from cancer gene perturbations. * 
C7: immunologic gene sets
 are defined directly from microarray gene expression data from immunologic studies. ----- 
GeneSetDB Gene Set Colletions
 GeneSetDB_ gene sets were obtained from 
multiple source databases
 (shown below) and were classified into five subclasses based on the database content: Pathway, Disease/Phenotype, Drug/Chemical, Genes Regulation and Gene Ontology. 
Pathway
 * Biocarta * EHMN (Edinburgh Human Metabolic Network) * HumnCyc * INOH (Integrating Network Objects with Hierarchies) * NetPath * PID (Pathway Interaction Database) * Reactome * Wikipathways 
Disease/Phenotype
 * CancerGenes * KEGG Disease * HPO (Human Phenotype Ontology) * MethCancerDB * MethyCancer * MPO (Mammalian Phenotype Ontology) * SIDER (SIDe Effect Resource) 
Drug/Chemical
 * CTD (Comparative Toxicogenomics Database) * DrugBank * MATADOR (Manually Annotated Targets and Drugs Online Resource) * SMPDB (Small Molecular Pathway DataBase) * STITCH (Search Tool for Interactions of Chemicals) * T3DB (Toxin and Toxin Target Database) 
Gene Regulation
 * MicroCosm Targets * miRTarBase * TFactS * Rel/NF-kappaB target genes 
Gene Ontology
 * Gene Ontology ----- 
KEGG Pathways
 Obtained by EGSEAdata from the GAGE
 Bioconductor package using the gage function kegg.gsets(). The Pathview_ Bioconductor package is used to visualize the expression data mapped onto the KEGG pathway graphs. Pathview has a GPLv3 licence which means users are required to formally cite the original 
Pathview paper
 (not just mention it) in publications or products. GAGE/Pathview divide the KEGG pathways into 3 categories: Signaling, Metabolism and Disease, listed in this file at the 
Pathview website here
. You can choose if you want to download the most recent KEGG pathways by selecting the 
Download KEGG pathways
 option in the tool form above. Note that downloading the most recent pathways may affect reproducibility as you can't choose what versions of pathways to use. 
Signaling
 * Genetic Information Processing * Environmental Information Processing * Cellular Processes * Organismal Systems 
Metabolism
 * Metabolism 
Disease
 * Human Diseases ----- Please cite EGSEA_, MSigDB_, KEGG_ and GeneSetDB_ appropriately if you use them. .. _EGSEA: https://www.ncbi.nlm.nih.gov/pubmed/27694195 .. _Bioconductor package: https://bioconductor.org/packages/release/bioc/html/EGSEA.html .. _MSigDB: http://software.broadinstitute.org/gsea/msigdb .. _KEGG: http://www.genome.jp/kegg/ .. _GeneSetDB: http://genesetdb.auckland.ac.nz/haeremai.html .. _EGSEA workflow: https://www.bioconductor.org/help/workflows/EGSEA123/ .. _multiple source databases: http://genesetdb.auckland.ac.nz/sourcedb.html .. _GAGE: https://bioconductor.org/packages/release/bioc/html/gage.html .. _Pathview: https://bioconductor.org/packages/release/bioc/html/pathview.html .. _Pathview paper: https://www.ncbi.nlm.nih.gov/pubmed/23740750 .. _Pathview website here: https://pathview.uncc.edu/data/khier.tsv"
toolshed.g2.bx.psu.edu/repos/iuc/feelnc/feelnc/0.2.1+galaxy0	"What it does
 FEELnc pipeline is used to annotate long non-coding RNAs (lncRNAs) based on reconstructed transcripts from RNA-seq data (either with or without a reference genome). -------- 
Project links:
 https://github.com/tderrien/FEELnc"
toolshed.g2.bx.psu.edu/repos/devteam/filter_transcripts_via_tracking/filter_combined_via_tracking/0.1	"What it does
 Uses a tracking file (produced by cuffcompare) to filter a GTF file of transcripts (usually the transcripts produced by cufflinks). Filtering is done by extracting transcript IDs from tracking file and then filtering the GTF so that the output GTF contains only transcript found in the tracking file. Because a tracking file has multiple samples, a sample number is used to filter transcripts for a particular sample."
toolshed.g2.bx.psu.edu/repos/iuc/trinity_filter_low_expr_transcripts/trinity_filter_low_expr_transcripts/2.15.1+galaxy0	"Trinity_ assembles transcript sequences from Illumina RNA-Seq data. This tool filters a Trinity assembly using an expression matrix built with ""Build expression matrix for a de novo assembly of RNA-Seq data by Trinity"" tool. It discards transcripts/isoforms having a low expression level. .. _Trinity: https://github.com/trinityrnaseq/trinityrnaseq/wiki"
toolshed.g2.bx.psu.edu/repos/iuc/heinz/heinz_bum/1.0	"BUM is the abbreviation for ""Beta-Uniform Mixture (BUM) distribution."" This tool (part of the package ""Bionet"") is used for fitting Beta-Uniform mixture model to a P-value distribution, the output of which is two model parameters: lambda and alpha, kept in a text file, the first line is lambda; the second, alpha."
toolshed.g2.bx.psu.edu/repos/iuc/trinity_super_transcripts/trinity_super_transcripts/2.15.1+galaxy0	"SuperTranscripts provide a gene-like view of the transcriptional complexity of a gene. SuperTranscripts were originally defined by Nadia Davidson, Anthony Hawkins, and Alicia Oshlack as described in their publication ""SuperTranscripts: a data driven reference for analysis and visualisation of transcriptomes"" Genome Biology, 2017. SuperTranscripts are useful in the context of genome-free de novo transcriptome assembly in that they provide a genome-like reference for studying aspects of the gene including differential transcript usage (aka. differential exon usage) and as a substrate for mapping reads and identifying allelic polymorphisms. A SuperTranscript is constructed by collapsing unique and common sequence regions among splicing isoforms into a single linear sequence. .. _Trinity: https://github.com/trinityrnaseq/trinityrnaseq/wiki"
toolshed.g2.bx.psu.edu/repos/iuc/trinity_gene_to_trans_map/trinity_gene_to_trans_map/2.15.1+galaxy0	"Trinity_ assembles transcript sequences from Illumina RNA-Seq data. This tool produces a file containing correspondance between gene ids and transcript ids based on the name of transcripts assembled by Trinity. The output file is intended to be used by the ""Align reads and estimate abundance"" tool. The same file is automatically generated when running Trinity, this tool is only intended to be used when you don't (or no longer) have access to the one produced by Trinity. .. _Trinity: https://github.com/trinityrnaseq/trinityrnaseq/wiki"
toolshed.g2.bx.psu.edu/repos/tyty/structurefold/get_read_pipeline/1.0	"Function
 Get RT Stop Counts derives the RT stop counts on each nucleotide of each transcript in the reference transcriptome based on a mapped file (.bam), typically the output from the Iterative Mapping module. ----- 
Input
: * 1. A mapped (.bam) file from Bowtie (or any other mapping program) * 2. Reference library sequences (fasta) used to map the reads to ----- 
Output
: A text file with reverse transcription stop counts mapped to each nucleotide (RTSC file)"
toolshed.g2.bx.psu.edu/repos/iuc/gffcompare/gffcompare/0.12.10+galaxy0	".. class:: infomark 
GffCompare Overview
 GffCompare is designed to systematically compare one or more sets of transcript predictions to a reference annotation at different levels of granularity (base level, exon level, transcript level etc.), and in the process to provide a way to ""annotate"" such transcript predictions based on their overlaps or proximity to reference annotation transcripts. When multiple transcript files (samples) are provided, GffCompare generates a non-redundant combined set of transcripts, tracking structurally equivalent transcripts across multiple samples and classifying them according to their relationship to reference transcripts. GffCompare has the following main functions: - Merge structurally equivalent transcripts and transcript fragments (transfrags) across multiple samples - Assess the accuracy of the assembled transcripts from an RNA-Seq sample by comparing it to known annotation - Track, annotate, and report all structurally distinct transfrags across multiple samples The last two purposes require the user to provide a known reference annotation file that GffCompare then uses to classify all the transcripts in the input samples according to the reference transcript that they most closely overlap. To assess the accuracy of transcriptome assemblies, GffCompare reports several accuracy metrics previously employed for gene prediction evaluation. These metrics include sensitivity and precision as well as the number of novel or missed features, and the metrics are computed at various levels (base, exon, intron chain, transcript, or locus). ---- .. class:: infomark 
Annotation mode
 When a single query GTF/GFF file is given as input for analysis, along with a reference annotation (-r option), GffCompare switches into annotation mode and it generates a 
annotated transcripts
 file, allowing annotate transcripts by using a reference annotation. It should be noted that this file is not generated when options to remove ""duplicated""/redundant transfrags are given (-D, -S, -C, -A, -X). ---- .. class:: infomark 
Merging structually equivalent transcripts
 When multiple input GTF/GFF files are provided, GffCompare reports a GTF file named 
combined transcripts
 that containing the union of all transfrags in each sample. If a transfrag with the same exact intron chain is present in both samples, it is thus reported only once in the output file. 
The ""super-locus"" concept
 A super-locus is a region of the genome where predicted transcripts and reference transcripts get clustered together by exon overlaps. When multiple GFF files are provided as input to GffCompare, this clustering is performed across all the input files. Due to the transitive nature of this clustering, these super-loci can occasionally get very large, sometimes merging a few distinct reference gene regions together, especially if there is a lot of transcription or alignment noise around the individual gene regions. For each super-locus, GffCompare assigns a unique identifier with the XLOC prefix. ---- .. class:: infomark 
Transcript accuracy estimation
 GffCompare can be used to assess the accuracy of transcriptome assemblies produced by programs like StringTie 19 with respect to a known reference annotation. To this end, GffCompare reports various statistics related to the accuracy of the input transcripts compared to the reference annotation in the 
accuracy stats
 file. Among these statistics are sensitivity and precision values computed at various levels (base, exon, intron chain, transcript, locus), which are calculated as: * Sensitivity = TP/(TP+FN) * Precision = TP/(TP+FP) where TP stands for ""true positives"", or query features (bases, exons, introns, transcripts, etc.) that agree with the corresponding reference annotation features; FN means ""false negatives"", i.e. features that are found in the reference annotation but are not present in the input data; FP (“false positives”) are features present in the input data but not confirmed by any reference annotation data. Notice that FP+ TP amounts to the whole input set of query features in the input file. If multiple query GTF/GFF files are given as input, these metrics are computed separately for each sample. Sensitivity and Precision values are estimated at various levels, which are largely an increasingly stringent way of evaluating the accuracy/correctness of a set of predicted transcripts (transfrags), when compared to the reference annotation. The six different levels that GffCompare uses are described below: * 
Base level
: At the base level, TP represents the number of exon bases that are reported at the same coordinate on both the query transcripts and any reference transcript, FN is the number of bases in reference data exons that are not covered at all by any of the query exons, and FP is the number of bases which are covered by predicted transcripts' exons but not covered by any reference transcript exons. * 
Exon level
: We define the TP, FN, and FP values at the exon level similar to the base level, but now the unit of comparison is the exon interval on the genome, i.e. if an exon of the predicted transcript overlaps and matches the boundaries of a reference transcript exon, then it is counted as a TP. * 
Intron Level
: Intron intervals are the units that are matched at the intron level, therefore each intron of the predicted transcript is checked against any introns of the reference transcripts in the same region and if there is one with the same exact start-end coordinates, it is counted as a TP. * 
Intron chain level
: At this level we count as a TP any query transcript for which all of its introns can be found, with the same exact intron coordinates as in a reference transcript that has the same number of introns. Matching all the introns at this level implies that all the internal exons also match, but this might not be true for the external boundaries of the terminal exons. * 
Transcript level
: Note that intron chain level values are calculated only by looking at multi-exon transcripts, so it completely ignores the single-exon transcripts, which can be quite numerous in a RNA-Seq experiment (possibly due to a lot of transcriptional and alignment noise). The transcript level considers single-exons as well. A TP at this level is defined as a full exon chain match between the predicted transcript and a reference transcript, where all internal exons match and the outer boundaries of the terminal query exons can only slightly differ from the reference exons (with at most 100 bases by default). Also GffCompare considers single-exon transcripts as matching an overlapping single-exon reference transcript if there is a significant overlap between the two (more than 80% of the longer transcript by default). * 
Locus level
: At this level GffCompare considers that an observed locus, defined as a cluster of exon-overlapping transcripts, matches a similarly built reference locus if at least one predicted transcript has a transcript level match with a reference transcript in the corresponding reference locus. ---- .. class:: infomark 
Tracking transcripts
 GffCompare can also be used to track all transcripts that are structurally equivalent among the different input files. GffCompare considers transcripts matching (or structurally equivalent) if all their introns are identical. Note that matching transcripts are allowed to differ on the length of the first and last exons, since these lengths can usually vary across samples for the same biological transcript. A list of all matching transcripts is reported in a file called 
tracking file
 in which each row represents a transcript. The first column in this file represents a unique id assigned to that transcripts. The second file represents the super-locus that contains that transcript. If a reference annotation is provided, the 3 rd and 4 th columns contain the reference annotation transcript that was found to be closest to the transcript and the classification code that specifies the relationship between these two transcripts, respectively. The rest of the columns show the corresponding transcript from each input file in order. 
RefMap and TMAP files
 In order to quickly see which reference transcripts match which transcripts from a sample file, two other files, called 
RefMap
 and 
TMAP
 are also created for each query. The RefMap file is a tab-delimited file that has a row for each reference transcript that either fully or partially matches a transcript from the given input file. Conversely, the TMAP file has a row for each input transcript, while the columns in this file describe the most closely matching reference transcript for that transcript."
toolshed.g2.bx.psu.edu/repos/iuc/hisat2/hisat2/2.2.1+galaxy1	"Introduction ============ What is HISAT? -------------- 
HISAT &lt;http://ccb.jhu.edu/software/hisat&gt;
 is a fast and sensitive spliced alignment program. As part of HISAT, we have developed a new indexing scheme based on the Burrows-Wheeler transform (
BWT &lt;http://en.wikipedia.org/wiki/Burrows-Wheeler_transform&gt;
) and the 
FM index &lt;http://en.wikipedia.org/wiki/FM-index&gt;
, called hierarchical indexing, that employs two types of indexes: (1) one global FM index representing the whole genome, and (2) many separate local FM indexes for small regions collectively covering the genome. Our hierarchical index for the human genome (about 3 billion bp) includes ~48,000 local FM indexes, each representing a genomic region of ~64,000bp. As the basis for non-gapped alignment, the FM index is extremely fast with a low memory footprint, as demonstrated by 
Bowtie &lt;http://bowtie-bio.sf.net&gt;
. In addition, HISAT provides several alignment strategies specifically designed for mapping different types of RNA-seq reads. All these together, HISAT enables extremely fast and sensitive alignment of reads, in particular those spanning two exons or more. As a result, HISAT is much faster >50 times than 
TopHat2 &lt;http://ccb.jhu.edu/software/tophat&gt;
 with better alignment quality. Although it uses a large number of indexes, the memory requirement of HISAT is still modest, approximately 4.3 GB for human. HISAT uses the 
Bowtie2 &lt;http://bowtie-bio.sf.net/bowtie2&gt;
 implementation to handle most of the operations on the FM index. In addition to spliced alignment, HISAT handles reads involving indels and supports a paired-end alignment mode. Multiple processors can be used simultaneously to achieve greater alignment speed. HISAT outputs alignments in 
SAM &lt;http://samtools.sourceforge.net/SAM1.pdf&gt;
 format, enabling interoperation with a large number of other tools that use SAM. HISAT is distributed under the 
GPLv3 license &lt;http://www.gnu.org/licenses/gpl-3.0.html&gt;
, and it runs on the command line under Linux, Mac OS X and Windows. Running HISAT ============= Reporting --------- The reporting mode governs how many alignments HISAT looks for, and how to report them. In general, when we say that a read has an alignment, we mean that it has a 
valid alignment &lt;#valid-alignments-meet-or-exceed-the-minimum-score-threshold&gt;
. When we say that a read has multiple alignments, we mean that it has multiple alignments that are valid and distinct from one another. Distinct alignments map a read to different places ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Two alignments for the same individual read are ""distinct"" if they map the same read to different places. Specifically, we say that two alignments are distinct if there are no alignment positions where a particular read offset is aligned opposite a particular reference offset in both alignments with the same orientation. E.g. if the first alignment is in the forward orientation and aligns the read character at read offset 10 to the reference character at chromosome 3, offset 3,445,245, and the second alignment is also in the forward orientation and also aligns the read character at read offset 10 to the reference character at chromosome 3, offset 3,445,245, they are not distinct alignments. Two alignments for the same pair are distinct if either the mate 1s in the two paired-end alignments are distinct or the mate 2s in the two alignments are distinct or both. Default mode: search for one or more alignments, report each ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HISAT searches for up to N distinct, primary alignments for each read, where N equals the integer specified with the 
-k
 parameter. Primary alignments mean alignments whose alignment score is equal or higher than any other alignments. It is possible that multiple distinct alignments whave the same score. That is, if 
-k 2
 is specified, HISAT will search for at most 2 distinct alignments. The alignment score for a paired-end alignment equals the sum of the alignment scores of the individual mates. Each reported read or pair alignment beyond the first has the SAM 'secondary' bit (which equals 256) set in its FLAGS field. See the 
SAM specification &lt;http://samtools.sourceforge.net/SAM1.pdf&gt;
 for details. HISAT does not ""find"" alignments in any specific order, so for reads that have more than N distinct, valid alignments, HISAT does not gaurantee that the N alignments reported are the best possible in terms of alignment score. Still, this mode can be effective and fast in situations where the user cares more about whether a read aligns (or aligns a certain number of times) than where exactly it originated. Alignment summmary ------------------ When HISAT finishes running, it prints messages summarizing what happened. These messages are printed to the ""standard error"" (""stderr"") filehandle and can be optionally printed to a file. Choose 
--new-summary
 under 
Summary Options
 for compatibility with 
MultiQC &lt;http://multiqc.info/docs/#hisat2&gt;
_. For datasets consisting of unpaired reads, the summary might look like this: :: 20000 reads; of these: 20000 (100.00%) were unpaired; of these: 1247 (6.24%) aligned 0 times 18739 (93.69%) aligned exactly 1 time 14 (0.07%) aligned >1 times 93.77% overall alignment rate For datasets consisting of pairs, the summary might look like this: :: 10000 reads; of these: 10000 (100.00%) were paired; of these: 650 (6.50%) aligned concordantly 0 times 8823 (88.23%) aligned concordantly exactly 1 time 527 (5.27%) aligned concordantly >1 times ---- 650 pairs aligned concordantly 0 times; of these: 34 (5.23%) aligned discordantly 1 time ---- 616 pairs aligned 0 times concordantly or discordantly; of these: 1232 mates make up the pairs; of these: 660 (53.57%) aligned 0 times 571 (46.35%) aligned exactly 1 time 1 (0.08%) aligned >1 times 96.70% overall alignment rate The indentation indicates how subtotals relate to totals. .. class:: infomark 
HISAT2 options
 Galaxy wrapper for HISAT2 implements most, but not all, options available through the command line. Supported options are described below. ----- 
Inputs
 HISAT2 accepts files in FASTQ or FASTA format (single-end or paired-end). Note that if your reads are from a stranded library, you need to choose the appropriate setting under 
Specify strand information
 above. For single-end reads, use F or R. 'F' means a read corresponds to a transcript. 'R' means a read corresponds to the reverse complemented counterpart of a transcript. For paired-end reads, use either FR or RF. With this option being used, every read alignment will have an XS attribute tag: '+' means a read belongs to a transcript on '+' strand of genome. '-' means a read belongs to a transcript on '-' strand of genome. (TopHat has a similar option, --library-type option, where fr-firststrand corresponds to R and RF; fr-secondstrand corresponds to F and FR.) ------ 
Input options
:: -s/--skip <int> Skip (i.e. do not align) the first 
&lt;int&gt;
 reads or pairs in the input. -u/--qupto <int> Align the first 
&lt;int&gt;
 reads or read pairs from the input (after the 
-s
/
--skip
 reads or pairs have been skipped), then stop. Default: no limit. -5/--trim5 <int> Trim 
&lt;int&gt;
 bases from 5' (left) end of each read before alignment (default: 0). -3/--trim3 <int> Trim 
&lt;int&gt;
 bases from 3' (right) end of each read before alignment (default: 0). --phred33 Input qualities are ASCII chars equal to the Phred quality plus 33. This is also called the ""Phred+33"" encoding, which is used by the very latest Illumina pipelines. --phred64 Input qualities are ASCII chars equal to the Phred quality plus 64. This is also called the ""Phred+64"" encoding. --solexa-quals Convert input qualities from Solexa Phred quality (which can be negative) to Phred Phred quality (which can't). This scheme was used in older Illumina GA Pipeline versions (prior to 1.3). Default: off. --int-quals Quality values are represented in the read input file as space-separated ASCII integers, e.g., 
40 40 30 40
..., rather than ASCII characters, e.g., 
II?I
.... Integers are treated as being on the Phred quality scale unless 
--solexa-quals
 is also specified. Default: off. ------ 
Alignment options
:: --n-ceil <func> Sets a function governing the maximum number of ambiguous characters (usually 
N
s and/or 
.
s) allowed in a read as a function of read length. For instance, specifying 
-L,0,0.15
 sets the N-ceiling function 
f
 to 
f(x) = 0 + 0.15 * x
, where x is the read length. Reads exceeding this ceiling are filtered out. Default: 
L,0,0.15
. --ignore-quals When calculating a mismatch penalty, always consider the quality value at the mismatched position to be the highest possible, regardless of the actual value. I.e. input is treated as though all quality values are high. This is also the default behavior when the input doesn't specify quality values (e.g. in 
-f
, 
-r
, or 
-c
 modes). --nofw/--norc If 
--nofw
 is specified, 
hisat2
 will not attempt to align unpaired reads to the forward (Watson) reference strand. If 
--norc
 is specified, 
hisat2
 will not attempt to align unpaired reads against the reverse-complement (Crick) reference strand. In paired-end mode, 
--nofw
 and 
--norc
 pertain to the fragments; i.e. specifying 
--nofw
 causes 
hisat2
 to explore only those paired-end configurations corresponding to fragments from the reverse-complement (Crick) strand. Default: both strands enabled. ----- 
Scoring options
:: --mp MX,MN Sets the maximum (
MX
) and minimum (
MN
) mismatch penalties, both integers. A number less than or equal to 
MX
 and greater than or equal to 
MN
 is subtracted from the alignment score for each position where a read character aligns to a reference character, the characters do not match, and neither is an 
N
. If 
--ignore-quals
 is specified, the number subtracted quals 
MX
. Otherwise, the number subtracted is 
MN + floor( (MX-MN)(MIN(Q, 40.0)/40.0) )
 where Q is the Phred quality value. Default: 
MX
 = 6, 
MN
 = 2. --sp MX,MN Sets the maximum (
MX
) and minimum (
MN
) penalties for soft-clipping per base, both integers. A number less than or equal to 
MX
 and greater than or equal to 
MN
 is subtracted from the alignment score for each position. The number subtracted is 
MN + floor( (MX-MN)(MIN(Q, 40.0)/40.0) )
 where Q is the Phred quality value. Default: 
MX
 = 2, 
MN
 = 1. --no-softclip Disallow soft-clipping. --np <int> Sets penalty for positions where the read, reference, or both, contain an ambiguous character such as 
N
. Default: 1. --rdg <int1>,<int2> Sets the read gap open (
&lt;int1&gt;
) and extend (
&lt;int2&gt;
) penalties. A read gap of length N gets a penalty of 
&lt;int1&gt;
 + N * 
&lt;int2&gt;
. Default: 5, 3. --rfg <int1>,<int2> Sets the reference gap open (
&lt;int1&gt;
) and extend (
&lt;int2&gt;
) penalties. A reference gap of length N gets a penalty of 
&lt;int1&gt;
 + N * 
&lt;int2&gt;
. Default: 5, 3. --score-min <func> Sets a function governing the minimum alignment score needed for an alignment to be considered ""valid"" (i.e. good enough to report). This is a function of read length. For instance, specifying 
L,0,-0.6
 sets the minimum-score function 
f
 to 
f(x) = 0 + -0.6 * x
, where 
x
 is the read length. The default is 
L,0,-0.2
. ----- 
Spliced alignment options
:: --pen-cansplice <int> Sets the penalty for each pair of canonical splice sites (e.g. GT/AG). Default: 0. --pen-noncansplice <int> Sets the penalty for each pair of non-canonical splice sites (e.g. non-GT/AG). Default: 12. --pen-canintronlen <func> Sets the penalty for long introns with canonical splice sites so that alignments with shorter introns are preferred to those with longer ones. Default: G,-8,1 --pen-noncanintronlen <func> Sets the penalty for long introns with noncanonical splice sites so that alignments with shorter introns are preferred to those with longer ones. Default: G,-8,1 --min-intronlen <int> Sets minimum intron length. Default: 20 --max-intronlen <int> Sets maximum intron length. Default: 500000 --no-spliced-alignment Disable spliced alignment. -I/--minins <int> The minimum fragment length for valid paired-end alignments.This option is valid only with 
--no-spliced-alignment
. E.g. if 
-I 60
 is specified and a paired-end alignment consists of two 20-bp alignments in the appropriate orientation with a 20-bp gap between them, that alignment is considered valid (as long as 
-X
 is also satisfied). A 19-bp gap would not be valid in that case. If trimming options 
-3
 or 
-5
 are also used, the 
-I
 constraint is applied with respect to the untrimmed mates. The larger the difference between 
-I
 and 
-X
, the slower HISAT2 will run. This is because larger differences between 
-I
 and 
-X
 require that HISAT2 scan a larger window to determine if a concordant alignment exists. For typical fragment length ranges (200 to 400 nucleotides), HISAT2 is very efficient. Default: 0 (essentially imposing no minimum) -X/--maxins <int> The maximum fragment length for valid paired-end alignments. This option is valid only with 
--no-spliced-alignment
. E.g. if 
-X 100
 is specified and a paired-end alignment consists of two 20-bp alignments in the proper orientation with a 60-bp gap between them, that alignment is considered valid (as long as 
-I
 is also satisfied). A 61-bp gap would not be valid in that case. If trimming options 
-3
 or 
-5
 are also used, the -X constraint is applied with respect to the untrimmed mates, not the trimmed mates. The larger the difference between 
-I
 and 
-X
, the slower HISAT2 will run. This is because larger differences between 
-I
 and 
-X
 require that HISAT2 scan a larger window to determine if a concordant alignment exists. For typical fragment length ranges (200 to 400 nucleotides), HISAT2 is very efficient. Default: 500. --known-splicesite-infile <path> With this mode, you can provide a list of known splice sites, which HISAT2 makes use of to align reads with small anchors. You can create such a list using python hisat2_extract_splice_sites.py genes.gtf > splicesites.txt, where hisat2_extract_splice_sites.py is included in the HISAT2 package, genes.gtf is a gene annotation file, and splicesites.txt is a list of splice sites with which you provide HISAT2 in this mode. Note that it is better to use indexes built using annotated transcripts (such as genome_tran or genome_snp_tran), which works better than using this option. It has no effect to provide splice sites that are already included in the indexes. --tmo/--transcriptome-mapping-only Report only those alignments within known transcripts. --dta/--downstream-transcriptome-assembly Report alignments tailored for transcript assemblers including StringTie. With this option, HISAT2 requires longer anchor lengths for de novo discovery of splice sites. This leads to fewer alignments with short-anchors, which helps transcript assemblers improve significantly in computation and memory usage. --dta-cufflinks Report alignments tailored specifically for Cufflinks. In addition to what HISAT2 does with the above option (--dta), With this option, HISAT2 looks for novel splice sites with three signals (GT/AG, GC/AG, AT/AC), but all user-provided splice sites are used irrespective of their signals. HISAT2 produces an optional field, XS:A:[+-], for every spliced alignment. --no-templatelen-adjustment Disables template length adjustment for RNA-seq reads. --novel-splicesite-outfile In this mode, HISAT2 reports a list of splice sites in the file : chromosome name <tab> genomic position of the flanking base on the left side of an intron <tab> genomic position of the flanking base on the right <tab> strand (+, -, and .) '.' indicates an unknown strand for non-canonical splice sites. ----- 
Reporting options
:: -k <int> It searches for at most 
&lt;int&gt;
 distinct, primary alignments for each read. Primary alignments mean alignments whose alignment score is equal or higher than any other alignments. The search terminates when it can't find more distinct valid alignments, or when it finds 
&lt;int&gt;
, whichever happens first. The alignment score for a paired-end alignment equals the sum of the alignment scores of the individual mates. Each reported read or pair alignment beyond the first has the SAM 'secondary' bit (which equals 256) set in its FLAGS field. For reads that have more than 
&lt;int&gt;
 distinct, valid alignments, hisat2 does not guarantee that the 
&lt;int&gt;
 alignments reported are the best possible in terms of alignment score. Default: 5 (HFM) or 10 (HGFM) Note: HISAT2 is not designed with large values for 
-k
 in mind, and when aligning reads to long, repetitive genomes large 
-k
 can be very, very slow. ----- 
Paired-end options
:: --fr/--rf/--ff The upstream/downstream mate orientations for a valid paired-end alignment against the forward reference strand. E.g., if 
--fr
 is specified and there is a candidate paired-end alignment where mate 1 appears upstream of the reverse complement of mate 2 and the fragment length constraints (
-I
 and 
-X
) are met, that alignment is valid. Also, if mate 2 appears upstream of the reverse complement of mate 1 and all other constraints are met, that too is valid. 
--rf
 likewise requires that an upstream mate1 be reverse-complemented and a downstream mate2 be forward-oriented. 
--ff
 requires both an upstream mate 1 and a downstream mate 2 to be forward-oriented. Default: 
--fr
 (appropriate for Illumina's Paired-end Sequencing Assay). --no-mixed By default, when 
hisat2
 cannot find a concordant or discordant alignment for a pair, it then tries to find alignments for the individual mates. This option disables that behavior. --no-discordant By default, 
hisat2
 looks for discordant alignments if it cannot find any concordant alignments. A discordant alignment is an alignment where both mates align uniquely, but that does not satisfy the paired-end constraints (
--fr
/
--rf
/
--ff
, 
-I
, 
-X
). This option disables that behavior. ----- ** SAM options --no-unal Suppress SAM records for reads that failed to align. --rg-id <text> Set the read group ID to <text>. This causes the SAM @RG header line to be printed, with <text> as the value associated with the ID: tag. It also causes the RG:Z: extra field to be attached to each SAM output record, with value set to <text>. --rg <text> Add <text> (usually of the form TAG:VAL, e.g. SM:Pool1) as a field on the @RG header line. Note: in order for the @RG line to appear, --rg-id must also be specified. This is because the ID tag is required by the SAM Spec. Specify --rg multiple times to set multiple fields. See the SAM Spec for details about what fields are legal. --remove-chrname Remove ‘chr’ from reference names in alignment (e.g., chr18 to 18) --add-chrname Add ‘chr’ to reference names in alignment (e.g., 18 to chr18) --omit-sec-seq When printing secondary alignments, HISAT2 by default will write out the SEQ and QUAL strings. Specifying this option causes HISAT2 to print an asterisk in those fields instead. ----- 
Output options
:: --un/--un-gz/--un-bz2 Write unpaired reads that fail to align to file at 
&lt;path&gt;
. These reads correspond to the SAM records with the FLAGS 
0x4
 bit set and neither the 
0x40
 nor 
0x80
 bits set. If 
--un-gz
 is specified, output will be gzip compressed. If 
--un-bz2
 is specified, output will be bzip2 compressed. Reads written in this way will appear exactly as they did in the input file, without any modification (same sequence, same name, same quality string, same quality encoding). Reads will not necessarily appear in the same order as they did in the input. --al/--al-gz/--al-bz2 Write unpaired reads that align at least once to file at 
&lt;path&gt;
. These reads correspond to the SAM records with the FLAGS 
0x4
, 
0x40
, and 
0x80
 bits unset. If 
--al-gz
 is specified, output will be gzip compressed. If 
--al-bz2
 is specified, output will be bzip2 compressed. Reads written in this way will appear exactly as they did in the input file, without any modification (same sequence, same name, same quality string, same quality encoding). Reads will not necessarily appear in the same order as they did in the input. --un-conc/--un-conc-gz/--un-conc-bz2 Write paired-end reads that fail to align concordantly to file(s) at 
&lt;path&gt;
. These reads correspond to the SAM records with the FLAGS 
0x4
 bit set and either the 
0x40
 or 
0x80
 bit set (depending on whether it's mate #1 or #2). .1 and .2 strings are added to the filename to distinguish which file contains mate #1 and mate #2. If a percent symbol, %, is used in <path>, the percent symbol is replaced with 1 or 2 to make the per-mate filenames. Otherwise, .1 or .2 are added before the final dot in <path> to make the per-mate filenames. Reads written in this way will appear exactly as they did in the input files, without any modification (same sequence, same name, same quality string, same quality encoding). Reads will not necessarily appear in the same order as they did in the inputs. --al-conc/--al-conc-gz/--al-conc-bz2 Write paired-end reads that align concordantly at least once to file(s) at 
&lt;path&gt;
. These reads correspond to the SAM records with the FLAGS 
0x4
 bit unset and either the 
0x40
 or 
0x80
 bit set (depending on whether it's mate #1 or #2). .1 and .2 strings are added to the filename to distinguish which file contains mate #1 and mate #2. If a percent symbol, %, is used in <path>, the percent symbol is replaced with 1 or 2 to make the per-mate filenames. Otherwise, .1 or .2 are added before the final dot in 
&lt;path&gt;
 to make the per-mate filenames. Reads written in this way will appear exactly as they did in the input files, without any modification (same sequence, same name, same quality string, same quality encoding). Reads will not necessarily appear in the same order as they did in the inputs. 
Other options
:: --seed <int> Use 
&lt;int&gt;
 as the seed for pseudo-random number generator. Default: 0. --non-deterministic Normally, HISAT2 re-initializes its pseudo-random generator for each read. It seeds the generator with a number derived from (a) the read name, (b) the nucleotide sequence, (c) the quality sequence, (d) the value of the 
--seed
 option. This means that if two reads are identical (same name, same nucleotides, same qualities) HISAT2 will find and report the same alignment(s) for both, even if there was ambiguity. When 
--non-deterministic
 is specified, HISAT2 re-initializes its pseudo-random generator for each read using the current time. This means that HISAT2 will not necessarily report the same alignment for two identical reads. This is counter-intuitive for some users, but might be more appropriate in situations where the input consists of many identical reads."
toolshed.g2.bx.psu.edu/repos/iuc/idr/idr/2.0.3	"The 
IDR &lt;https://github.com/nboley/idr/blob/master/README.md&gt;
__ (Irreproducible Discovery Rate) framework is a unified approach to measure the reproducibility of findings identified from replicate experiments and provide highly stable thresholds based on reproducibility. Unlike the usual scalar measures of reproducibility, the IDR approach creates a curve, which quantitatively assesses when the findings are no longer consistent across replicates."
toolshed.g2.bx.psu.edu/repos/iuc/heinz/heinz/1.0	"Note
: You are currently using a version of Heinz based on IBM CPLEX Community version, which is limiting the capacity of Heinz in handling big networks. For an unlimited version of Heinz, you need to get a license of IBM CPLEX and compile Heinz from scratch from here https://github.com/ls-cwi/heinz. We are currently looking for an alternative to IBM CPLEX in Heinz to make an unlimited version of Heinz available to the public as soon as possible. Score file --- the output file of the tool ""Calculate Heinz scores"": two columns delimited by a tab without headers, the first column is node identifier (e.g., genes, KEGG ORTHOLOGY (KO)); the second, Heinz score. ========= =================== BRCA2 -6.991782933819368 --------- ------------------- BRCA1 -5.206139799106934 --------- ------------------- AACS -0.9299868303078357 --------- ------------------- ABCC11 -5.845009850430119 ========= =================== Edge file: the background network Heinz uses in the form of a list of edges; each line is made up of two node identifiers (e.g., genes, KEGG ORTHOLOGY (KO)) delimited by a tab. In practice, we could prepare this file using different pathway databases, such as Reactome, STRING and KEGG. Which database to choose depends on the question to solve. ========= ========= ACTR1B ACVR2B --------- --------- ZSWIM9 FOXP3 --------- --------- LGALS4 PRKX --------- --------- NPTX1 CIAO1 ========= ========="
toolshed.g2.bx.psu.edu/repos/iuc/isoformswitchanalyzer/isoformswitchanalyzer/1.20.0+galaxy6	".. class:: infomark 
Purpose
 IsoformSwitchAnalyzeR is an easy-to use-R package that enables statistical identification of isoform switching from RNA-seq derived quantification of novel and/or annotated full-length isoforms. IsoformSwitchAnalyzeR facilitates integration of many sources of (predicted) annotation such as Open Reading Frame (ORF/CDS), protein domains (via Pfam), signal peptides (via SignalP), Intrinsically Disordered Regions (IDR, via NetSurfP-2 or IUPred2A), coding potential (via CPAT or CPC2) and sensitivity to Non-sense Mediated Decay (NMD) and more. The combination of identified isoform switches and their annotation enables IsoformSwitchAnalyzeR to predict potential functional consequences of the identified isoform switches — such as loss of protein domains — thereby identifying isoform switches of particular interest. Lastly, IsoformSwitchAnalyzeR provides article-ready visualization methods for isoform switches for individual genes as well as both summary statistics and visualization of the genome-wide changes/consequences of isoform switches, their consequences and the associated alternative splicing. ----- .. class:: infomark 
Differential isoform expression (DIE) and differential isoform usage (DIU)
 Differential isoform expression (DIE) and differential isoform usage (DIU) are related but distinct concepts. DIE assesses the difference of absolute expression in isoform level. In contrast, DIU assesses the difference of relative expression in isoform level. For example, if the expression of two isoforms of one gene are 10 and 20 in control and 50 and 100 in case, then there is DIE but no DIU because the relative expression of the first isoform is 1/3 in both case and control. ----- .. class:: infomark 
ORF identification methods (novel isoform analysis)
 - 
Longest
: Identifies the longest ORF in the transcript (after filtering via minORFlength). This approach is similar to what the CPAT tool uses in it is analysis of coding potential. - 
LongestAnnotated
: Identifies the longest ORF (after filtering via minORFlength) downstream of an annotated translation start site (which are supplied via the cds argument). - 
Longest.AnnotatedWhenPossible
: A merge between ""longestAnnotated"" and ""longest"". For all isoforms where CDS start positions from known isoform overlap, only these CDS starts are considered and the longest ORF is annotated (similar to ""longestAnnotated""). All isoforms without any overlapping CDS start sites they will be analysed with the ""longest"" approach. - 
MostUpstream
: Identifies the most upstream ORF in the transcript (after filtering via minORFlength). - 
MostUpstreamAnnoated
: Identifies the ORF (after filtering via minORFlength) downstream of the most upstream overlapping annotated translation start site (supplied via the cds argument)."
toolshed.g2.bx.psu.edu/repos/tyty/structurefold/iterative_map_pipeline/1.0	"Overview of StructureFold
 * StructureFold is a series of software packages that automates the process of predicting RNA secondary structure for a transcript or an entire transcriptome, with or without the inclusion of constraints on the structure(s) provided by wet bench experimentation. The process consists of mapping the raw reads of RNA structural data on every transcript in the dataset to the transcriptome, getting RT stop counts on each nucleotide, calculating structural reactivities on the nucleotides, and predicting the RNA structures. Please cite: Tang, Y, Bouvier, E, Kwok CK, Ding Y, Nekrutenko, A, Bevilacqua PC, Assmann SM, StructureFold: Genome-wide RNA secondary structure mapping and reconstruction in vivo, Bioinformatics, In press. RNA structure is predicted using the RNAstructure algorithm (http://rna.urmc.rochester.edu/RNAstructure.html) or ViennaRNA package (http://www.tbi.univie.ac.at/RNA/). ----- 
Function
 * Iterative Mapping maps the raw reads of RNA structural data to the reference transcriptome using Bowtie (v0.12.8). It allows users to trim each read from either end to iteratively map the read to the reference transcriptome. ----- 
Input
: * 1. Sequence file type (FASTA/FASTQ) * 2. Sequence file (fasta/fastq format) * 3. Reference file (fasta) used to map the reads to * 4. “Shift” (The length of the sequence that will be trimmed at the 3’end of the reads before each round of mapping) * 5. “Length” (The minimum length of the reads for mapping after trimming) * [Optional] * 1. Bowtie mapping flags (options) [Default: -v 0 -a --best --strata] (-v flag indicates the number of allowed mismatches. Use -5/-3 flag to trim the nucleotides from 5'/3' end of the reads) ----- 
Output
: A sorted .bam file with all of the reads that are mapped"
toolshed.g2.bx.psu.edu/repos/iuc/kallisto_pseudo/kallisto_pseudo/0.48.0+galaxy1	"kallisto &lt;https://pachterlab.github.io/kallisto/manual&gt;
__ is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. It is based on the novel idea of pseudoalignment for rapidly determining the compatibility of reads with targets, without the need for alignment."
toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1	kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. It is based on the novel idea of pseudoalignment for rapidly determining the compatibility of reads with targets, without the need for alignment. On benchmarks with standard RNA-Seq data, kallisto can quantify 30 million human reads in less than 3 minutes on a Mac desktop computer using only the read sequences and a transcriptome index that itself takes less than 10 minutes to build. Pseudoalignment of reads preserves the key information needed for quantification, and kallisto is therefore not only fast, but also as accurate as existing quantification tools. In fact, because the pseudoalignment procedure is robust to errors in the reads, in many benchmarks kallisto significantly outperforms existing tools.
toolshed.g2.bx.psu.edu/repos/rnateam/mirdeep2/rbc_mirdeep2/2.0.1.2+galaxy0	Reads in fasta format. The identifier should contain a prefix, a running number and a '_x' to indicate the number of reads that have this sequence. There should be no redundancy in the sequences.
toolshed.g2.bx.psu.edu/repos/rnateam/mirdeep2_mapper/rbc_mirdeep2_mapper/2.0.0.8.1	"What it does
 The MiRDeep2 Mapper module is designed as a tool to process deep sequencing reads and/or map them to the reference genome. The module works in sequence space, and can process or map data that is in sequence FASTA format. A number of the functions of the mapper module are implemented specifically with Solexa/Illumina data in mind. 
Input
 Default input is a file in FASTA format, seq.txt or qseq.txt format. More input can be given depending on the options used. 
Output
 The output depends on the options used. Either a FASTA file with processed reads or an arf file with with mapped reads, or both, are output. Arf format: Is a proprietary file format generated and processed by miRDeep2. It contains information of reads mapped to a reference genome. Each line in such a file contains 13 columns: 1. read identifier 2. length of read sequence 3. start position in read sequence that is mapped 4. end position in read sequence that is mapped 5. read sequence 6. identifier of the genome-part to which a read is mapped to. This is either a scaffold id or a chromosome name 7. length of the genome sequence a read is mapped to 8. start position in the genome where a read is mapped to 9. end position in the genome where a read is mapped to 10. genome sequence to which a read is mapped 11. genome strand information. Plus means the read is aligned to the sense-strand of the genome. Minus means it is aligned to the antisense-strand of the genome. 12. Number of mismatches in the read mapping 13. Edit string that indicates matches by lowercase 'm' and mismatches by uppercase 'M'"
toolshed.g2.bx.psu.edu/repos/rnateam/mirdeep2_quantifier/rbc_mirdeep2_quantifier/2.0.0	"What it does
 The module maps the deep sequencing reads to predefined miRNA precursors and determines by that the expression of the corresponding miRNAs. First, the predefined mature miRNA sequences are mapped to the predefined precursors. Optionally, predefined star sequences can be mapped to the precursors too. By that the mature and star sequence in the precursors are determined. Second, the deep sequencing reads are mapped to the precursors. The number of reads falling into an interval 2nt upstream and 5nt downstream of the mature/star sequence is determined. 
Input
 A FASTA file with precursor sequences, a FASTA file with mature miRNA sequences, a FASTA file with deep sequencing reads and optionally a FASTA file with star sequences and the 3 letter code of the species of interest. 
Output
 A tab separated file with miRNA identifiers and its read count, a signature file, a html file that gives an overview of all miRNAs the input data and a pdfs that contains for each miRNA a pdf file showing its signature and structure."
toolshed.g2.bx.psu.edu/repos/iuc/qualimap_counts/qualimap_counts/2.3+galaxy0	"What it does
 In RNA-seq experiments, the reads are usually first mapped to a reference genome. It is assumed that if the number of reads mapping to a certain biological feature of interest (gene, transcript, exon, ...) is sufficient, it can be used as an estimation of the abundance of that feature in the sample and interpreted as the quantification of the expression level of the corresponding region. These count data can be utilized for example to assess differential expression between two or more experimental conditions. Before assessing differential expression analysis, researchers should be aware of some potential limitations of RNA-seq data, as for example: - Has saturation been reached, or could more features be detected by increasing the sequencing depth? - Which type of features are being detected in the experiment? - How good is the quantification of expression in the sample? All of these questions can be answered by interpreting the plots generated by 
Qualimap Counts QC
. Input ===== The tool accepts tabular input of type 
tsv
. It expects gene identifiers in the first and the counts for different samples in the following column(s). The first line of the input needs to be a header line starting with 
#
, immediately followed by the name of the gene identifier column, then the sample names separated by tabs. So, for example:: #GeneID Sample1 Sample2 would be a valid header line. .. class:: infomark The 
Counts
 output of featureCounts represents nearly valid input for the tool, but you will have to 
replace
 the header line to add a leading 
#
 and to provide more telling sample names. You can 
join
 the outputs of several featureCounts runs to obtain multi-sample counts data. Analysis/Report types --------------------- 
Report overview stats for all samples
 - Generates overview plots of the counts data across all samples. 
Report feature count stats of a single sample
 - Generates plots with detailed information about a single sample. 
Compare two groups of samples
 - Lets you compare groups of samples representing different conditions. This version of Qualimap requires all samples to belong to one of two groups. Parameters ---------- 
Additional information about genes
 (optional) Qualimap requires gene annotation data to generate plots (see 
Output
 section below) of - counts across classes of features - feature length and GC content bias in the counts data , which are available for the single-sample and group comparison reports. You can provide the annotation data in the form of: - Built-in gene information for supported species For convenience, Qualimap provides the Ensembl annotations for certain species (currently Human and Mouse). In order to use these annotations, Ensembl Gene IDs should be used as the feature IDs on the count files (e.g. ENSG00000251282). - Custom gene information A tabular dataset holding annotations of the features in the counts dataset is required. It must be in a four-column tab-delimited (
tsv
) format, with the feature names or IDs in the first column, the group (
e.g.
 the biotype from Ensembl database) in the second column, feature length in the third and feature GC-content in the last column (see this 
example &lt;http://kokonech.github.io/qualimap/samples/human.ens68.txt&gt;
). 
Make sure to use the same feature IDs in the annotation and in the counts dataset!
 To generate a Qualimap-compatible info file based on an arbitrary GTF annotation and a genome FASTA file, the developers of Qualimap offer a 
Python script for the command line &lt;https://bitbucket.org/kokonech/qualimap/src/master/util/createQualimapInfoFile.py?at=master&gt;
. 
Counts threshold
 In order to remove the influence of spurious reads, a feature is considered as detected if its corresponding number of counts is greater than this threshold. By default, the threshold value is set to 5 counts, meaning that features having less than 5 counts will not be taken into account. Output ====== Many of the plots that this tool can produce are created using the NOISeq package. The 
NOISeq vignette &lt;http://www.bioconductor.org/packages/release/bioc/vignettes/NOISeq/inst/doc/NOISeq.pdf&gt;
__ contains a lot of useful information about the plots and how to interpret them. Here we provide a short explanation: Plots of overview stats for all samples --------------------------------------- 
Counts Density
 This plot shows density of counts computed from the histogram of log-transformed counts. In order to avoid infinite values in case of zero counts the transformation 
log2(expr + 0.5)
 is applied, where 
expr
 is the number of read counts for a given feature. Only log-transformed counts having value greater than 1 are plotted. 
Scatterplot Matrix
 The panel shows a scatterplot along with smoothed line (lower panel) and Pearson correlation coefficients (upper panel) for each pair of samples. Plots are generated using log-transformed counts. 
Saturation
 This plot provides information about the level of saturation in the samples, so it helps the user to decide if more sequencing is needed and more features could be detected when increasing the number of reads. Sequencing depth of each sample (on the x-axis) is plotted against the number of detected features (on the y-axis). Here, “detected features” refers to features with more than k counts, where k is the 
Counts threshold
 selected by the user. The highlighted value is the real sequencing depth of the sample(s). The expected results at other sequencing depths are simulated based on random sampling of the original data. 
Counts Distribution
 This box plot shows the overall counts distribution of each sample. 
Features With Low Counts
 This plot shows the proportion of features with low counts in each sample. Such features are usually less reliable and could be filtered out. In this plot, the bars show the percentage of features within each sample having more than 0 counts per million (CPM), or more than 1, 2, 5 and 10 CPM. Plots of single-sample count statistics --------------------------------------- .. class:: infomark Note that most single-sample plots require built-in or custom 
additional information about genes
 to be generated. The 
Saturation
 plot is the only exception. 
Saturation
 Similar to the same plot in the overview of all samples. The single-sample plot, however, has an additional y-axis (on the right) showing the number of features expected to be newly detected when increasing the sequencing depth by one million reads from each indicated sequencing depth value. 
Bio Detection
 This barplot allows the user to know which 
kind
 of features are being detected in the chosen sample. The x-axis shows all feature categories listed in the annotations file. The gray bars are the percentage of features of each group within the reference genome (or transcriptome, etc.). The striped color bars are the percentages of features of each group detected in the sample with regard to the genome. The solid color bars are the percentages that each group represents in the total detected features in the sample. 
Counts Per Biotype
 This boxplot shows the distribution of counts of features from each detected feature clas. 
Length Bias
 The plot describes the relationship between the length of the features and their expression values. Feature lengths are divided into bins, and mean expression of features falling into a particular length interval is computed and plotted. A cubic spline regression model is fitted to explain the relation between length and expression. Coefficient of determination (
R^2
) and p-value are shown together with the regression curve. 
GC Bias
 The plot describes the relantionship between the GC-content of the features and the expression values. The data for the plot is generated similar to the 
Length Bias
 plot. The GC content is divided into bins and the mean expression of features falling into any given GC interval is computed. The relation between GC-content and expression is investigated using a cubic spline regression model. Plots for comparing two groups of samples ----------------------------------------- This mode can generate side-by-side plots of - 
Counts Distribution
, - 
Features With Low Counts
 - 
Bio Detection
, - 
Length Bias
 and - 
GC Bias
 for two groups of samples. .. class:: infomark Note that the 
Bio Detection
, 
Length Bias
 and 
GC Bias
 plots can only be generated when built-in or custom 
additional information about genes
 is available."
toolshed.g2.bx.psu.edu/repos/iuc/qualimap_rnaseq/qualimap_rnaseq/2.3+galaxy0	"What it does
 
Qualimap RNA-Seq QC
 reports quality control metrics and bias estimations which are specific for whole transcriptome sequencing, including reads genomic origin, junction analysis, transcript coverage and 5’-3’ bias computation. As such, the tool complements the more general analysis with QualiMap BamQC, and its (optional) gene counts output can be analyzed further with QualiMap Counts QC. Input ===== 
Mapped reads input dataset
 The dataset holding the mapped reads to carry out the analysis with. Typically, this will have been produced by a splicing-aware aligner like 
HISAT2
 or 
RNA STAR
. 
Genome annotation data
 A GTF dataset of genomic features that mapped reads should be counted for. Parameters ---------- 
Counting mode
 Determines whether reads should be counted individually, or whether multiple reads originating from the same sequencing template (
i.e.
, the read and its mate in paired-end sequencing) should be counted as one. You will usually want to choose 
Count fragments
 for paired-end data. For single-end data, choose 
Count reads
. 
Keep the per-gene counts data?
 Controls whether the optional Counts output dataset should be produced, or not. If you choose to produce this dataset, you can use: 
Name to use for the counts column
 to specify the name of the second column in that output. Using, for example, the name of the analyzed sample here can help you keep track of your data, especially when joining several counts datasets into a count matrix later on. In addition, 
Qualimap Counts QC
 will reuse the names of counts columns as sample names. 
Read selection for counting
 section 
Strandedness
 Choose here the option that fits the strand-specificity of your sequencing library. The Galaxy Training Material has an excellent discussion of sequencing data strandedness included in the 
Reference-based RNA-Seq data analysis &lt;https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html#count-the-number-of-reads-per-annotated-gene&gt;
__ tutorial. 
Multimapping reads
 Choose here how to treat reads that are mapped ambiguously to several genome locations. - 
Count uniquely mapped reads only
 excludes multi-mapping reads - 
Count also multimapping reads
 activates 
proportional
 counting of multi-mapping reads. In this mode, each read is weighted according to the number of mapped locations. For example, a read mapped to 4 different locations will add 0.25 to the ""counts"" of each of the locations it maps to. The final calculated counts per feature will be converted to integer numbers. Note: Detection of multi-mapping reads by the tool relies on the 
NH
 tag of reads in the BAM input, so make sure the aligner used to produce the dataset is configured to write this tag. Outputs ======= HTML Report ----------- 
Summary Section
 
Reads alignment
 Summarizes the mapping characteristics of the reads in the input: - total number of mapped reads reported as left/right read mates in case of paired-end reads; excludes secondary alignments If you accidentally selected 
Count fragments
 as the 
Counting mode
 for single-end data these and the following count of 
Number of aligned pairs
 will be zero. - total number of alignments reports all alignment records found, including secondary alignments - number of secondary alignments - number of non-unique alignments reports the number of alignment records with an 
NH
 tag greater than one; corresponds to the number of alignments that will have been skipped during counting when 
Count uniquely mapped reads only
 is selected - number of reads aligned to genes - number of ambiguous alignments This is the number of mapped reads that span multiple annotated genes. Such reads are always skipped during counting. - no feature assigned reports the number of alignments that are not overlapping any annotated feature; these may represent alignments to introns or intergenic regions, or, if the number is really high, may indicate a problem with your genome annotations - not aligned number of reads not mapped by the aligner (but included in the BAM input) - strand specificity estimation (fwd/rev) computed if 
Count reads/fragments independent of strandedness
 is selected; estimate of the proportion of alignments in line with forward- and reverse- strand-specificitiy of the sequencing library Balanced proportions (
i.e.
 ~ 0.5 forward- and ~ 0.5 reverse-strand support) can be interpreted as likely non-strand-specificity of the sequencing library, while a strand-specific library would manifest itself in a large fraction of reads supporting that specific strand-specificity. 
Reads genomic origin
 Lists how many alignments (absolute number/fraction) fall into - exonic, - intronic, - intergenic regions, or are at least - overlapping an exon. 
Transcript coverage profile
 The profile provides ratios between mean coverage of 5’ regions, 3’ regions and whole transcripts. - 5’ bias the ratio of coverage median of 5’ regions (defined as the first 100 nts) to whole transcripts - 3' bias the ratio of coverage median of 3’ regions (defined as the last 100 nts) to whole transcripts - 5’-3’ bias the ratio of 5' bias to 3' bias. 
Junction analysis
 Lists the total number of reads with splice junctions and the relative frequency of the (up to) 10 most frequent junction sequences. 
Plots
 
Reads Genomic Origin
 A pie chart showing how many read alignments fall into exonic, intronic and intergenic regions. 
Coverage Profile Along Genes (Total)
 This plot shows the mean coverage profile of all genes with non-zero overall coverage. 
Coverage Profile Along Genes (Low)
 The plot shows the mean coverage profile of the 500 genes with the lowest, but non-zero overall coverage. 
Coverage Profile Along Genes (High)
 The plot shows the mean coverage profile of the 500 genes with the highest overall coverage. 
Coverage Histogram (0-50x)
 Coverage of genes from 0 to 50x. Genes with >50x coverage are added to the 50x bin. 
Junction Analysis
 This pie chart shows an analysis of the splice junctions observed in the alignments. It consists of three categories: - Known observed splice junctions both sides of which are in line with the genome annotation data - Partly known observed splice junctions for which only one junction side can be deduced from the genome annotation data - Novel observed splice junctions not predicted on either side by the genome annotation data Raw data -------- This is a 
Collection
 of 4 individual datasets. Of these, the 
rnaseq_qc_results
 dataset provides a plain-text version of the 
HTML report
 
Summary
 section. The other 3 datasets hold the tabular raw data underlying the three coverage profile plots in the 
HTML Report
. Counts data ----------- Optional. This is a 2-column tabular dataset of read or fragment counts (depending on the chosen 
Counting mode
) per annotated gene. The first column lists the gene identifiers found in the 
Genome annotation data
, the second the associated counts. This dataset represents valid (single-sample) input for the QualiMap Counts QC tool."
toolshed.g2.bx.psu.edu/repos/iuc/rgrnastar/rna_star/2.7.11b+galaxy0	This is a reimplementation of the original WASP algorithm by Bryce van de Geijn, Graham McVicker, Yoav Gilad and Jonathan K Pritchard. https://doi.org/10.1038/nmeth.3582. This option will add the vW tag to the SAM output. vW:i:1 means alignment passed WASP filtering, and all other values mean it did not:<br/> - vW:i:2 = multi-mapping read<br/> - vW:i:3 = variant base in the read is N (non-ACGT)<br/> - vW:i:4 = remapped read did not map <br/> - vW:i:5 = remapped read multi-maps <br/> - vW:i:6 = remapped read maps to a different locus <br/> - vW:i:7 = read overlaps too many variants <br/>
toolshed.g2.bx.psu.edu/repos/tyty/structurefold/predict_pipeline/1.0	"Function
 RNA Structure Prediction uses the RNAstructure program (V5.6) and ViennaRNA package (V2.1.9) to predict RNA structures without restraints (in silico) or with restraints from structural reactivities, as provided by the Reactivity Calculation module. Users can designate the temperature under which to predict the RNA structures. ----- 
Input
: * 1. A file with transcript Ids (Max num. 100), (each ID one line) * 2. Reference file (fasta) used to map the reads to * 3. Temperature for RNA structure prediction * [Optional]: * 1. A reactivity file with structural reactivity for each nucleotide on the sequence provided * /RNAstructure prediction mode/ * 2. Slope used with structural restraints (default 1.8) * 3. Intercept used with structural restraints (default -0.6) * /ViennaRNA package prediction mode/ * 2. Flag that determines whether to incoorporate G-Quadruplex prediction * 3. High reactivity threshold (Any nucleotide with structural reactivity that is higher than it will be constrainted as single stranded) (default 0.6) * 4. Low reactivity threshold (Any nucleotide with structural reactivity that is lower than it will be constrainted as double stranded) (default 0.3) ----- 
Output
: * 1. Dot bracket files with predicted RNA structures [transciptID.dbn] * 2. .ps files which depict the predicted RNA structures [transciptID.ps] * [Optional] * 3. .tif files that shows the distribution of the reactivity of each nucleotide on the transcripts of interest. [transciptID.tif] ----- 
Attention
 Make sure that none of the transcript Ids contains a ""|"" or a space! ----- 
Backend program
: * 1. This module uses RNAstructure (http://rna.urmc.rochester.edu/RNAstructure.html) or ViennaRNA package (http://www.tbi.univie.ac.at/RNA/) as the backend programs to predict RNA structures. * 2. Default parameters are used for RNAstructure and ViennaRNA package except -T (Temperature), -sm (slope used with SHAPE restraints [RNAstructure prediction mode]), -si (intercept used with SHAPE restraints [RNAstructure prediction mode]) and thresholds for high and low reactivity [ViennaRNA package prediciton mode], for which users can specify the value"
toolshed.g2.bx.psu.edu/repos/iuc/trinity_samples_qccheck/trinity_samples_qccheck/2.15.1+galaxy0	"Trinity_ assembles transcript sequences from Illumina RNA-Seq data. This tool performs some Quality Checks on a RNASeq experiment, analysing the abundance estimation for different samples using a transcriptome assembled with Trinity. 
Inputs
 This tool uses the matrix produced by 'Build expression matrix for a de novo assembly of RNA-Seq data by Trinity' tool. You must describe your samples and replicates with a tabular file looking like this: =========== ================ ConditionA CondA_replicate1 ----------- ---------------- ConditionA CondA_replicate2 ----------- ---------------- ConditionB CondB_replicate1 ----------- ---------------- ConditionB CondB_replicate2 ----------- ---------------- ConditionC CondC_replicate1 ----------- ---------------- ConditionC CondC_replicate2 ----------- ---------------- ConditionC CondC_replicate3 =========== ================ This file can be generated with the 'Describe samples and replicates' tool. The names in column 2 must match the names given in the tool 'Build expression matrix for a de novo assembly of RNA-Seq data by Trinity'. 
Output
 This tool will produce several PDF files, see the following page for more information: .. _Trinity manual: https://github.com/trinityrnaseq/trinityrnaseq/wiki/QC-Samples-and-Replicates .. _Trinity: https://github.com/trinityrnaseq/trinityrnaseq/wiki"
toolshed.g2.bx.psu.edu/repos/tyty/structurefold/react_cal_pipeline/1.0	"Function
 * Reactivity Calculation calculates the structural reactivity on each nucleotide based on an RT stop count file containing the RT stop count on each nucleotide, typically the output from the Get RT Stop Counts module. ----- 
Input
: * 1. RTSC files (Output of Get RT Stop Counts) for (+) and (-) library * 2. Reference file (fasta) used to map the reads to * 3. Nucleotide Specificity (Type of nucleotides to have reactivity, e.g. AC for DMS and ACTG for SHAPE) * [Optional]: * 1. A threshold to cap the structural reactivities. {Default: 7} * 2. Flag that determines whether to perform 2%-8% normalization {Default: Yes} ----- 
Output
: A text file with structural reactivity for each nucleotide (Reactivity file)"
toolshed.g2.bx.psu.edu/repos/iuc/ruvseq/ruvseq/1.26.0+galaxy1	".. class:: infomark 
What it does
 RUVSeq normalizes RNA-seq data using factor analysis of control genes or samples. RUVSeq has been designed for detecting unwanted variation using replicate sample information. The current RUVSeq Galaxy tool only implements estimating unwanted variation for primary factors. RUVSeq implements 3 different methods for the estimation of unwanted variation: RUVg estimates the factors of unwanted variation using control genes RUVs estimates the factors of unwanted variation using replicate samples RUVr estimating the factors of unwanted variation using residuals. This tool runs all RUV methods and outputs diagnostic plots and tables with covariates that may be used for differential expression analsys. ----- 
Inputs
 
Count Files
 RUVSeq_ takes count tables generated from 
featureCounts
, 
HTSeq-count
 or 
StringTie
 as input. Count tables must be generated for each sample individually. One header row is assumed, but files with no header (e.g from HTSeq) can be input with the 
Files have header?
 option set to No. RUVSeq_ can also take transcript-level counts from quantification tools such as, 
kallisto
, 
Salmon
 and 
Sailfish
, and this Galaxy wrapper incorporates the Bioconductor tximport_ package to process the transcript counts for DESeq2. 
Salmon or Sailfish Files
 Salmon or Sailfish 
quant.sf
 files can be imported by setting type to 
Salmon
 or 
Sailfish
 respectively above. Note: for previous version of Salmon or Sailfish, in which the quant.sf files start with comment lines you will need to remove the comment lines before inputting here. An example of the format is shown below. Example: ============ ========== =============== =========== =========== Name Length EffectiveLength TPM NumReads ------------ ---------- --------------- ----------- ----------- NR_001526 164 20.4518 0 0 NR_001526_1 164 20.4518 0 0 NR_001526_2 164 20.4518 0 0 NM_130786 1764 1956.04 2.47415 109.165 NR_015380 2129 2139.53 1.77331 85.5821 NM_001198818 9360 7796.58 2.38616e-07 4.19648e-05 NM_001198819 9527 7964.62 0 0 NM_001198820 9410 7855.78 0 0 NM_014576 9267 7714.88 0.0481114 8.37255 ============ ========== =============== =========== =========== 
kallisto Files
 kallisto 
abundance.tsv
 files can be imported by setting type to 
kallisto
 above. An example of the format is shown below. Example: ============ ========== =============== =========== =========== target_id length eff_length est_counts tpm ------------ ---------- --------------- ----------- ----------- NR_001526 164 20.4518 0 0 NR_001526_1 164 20.4518 0 0 NR_001526_2 164 20.4518 0 0 NM_130786 1764 1956.04 109.165 2.47415 NR_015380 2129 2139.53 85.5821 1.77331 NM_001198818 9360 7796.58 4.19648e-05 2.38616e-07 NM_001198819 9527 7964.62 0 0 NM_001198820 9410 7855.78 0 0 NM_014576 9267 7714.88 8.37255 0.0481114 ============ ========== =============== =========== =========== ----- 
Output
 RUVSeq_ generates a tabular file for each method and each k of variation as well as a summary PDF. RUVSeq can also generate RUVSeq normalized count tables. However, 
these counts should be used only for exploration. It is important that subsequent DE analysis be done on the original counts, as removing the unwanted factors from the counts can also remove part of a factor of interest
. .. _RUVSeq: http://master.bioconductor.org/packages/release/bioc/html/RUVSeq.html .. _tximport: https://bioconductor.org/packages/devel/bioc/vignettes/tximport/inst/doc/tximport.html"
toolshed.g2.bx.psu.edu/repos/bgruening/sailfish/sailfish/0.10.1.1	"When generating the gene-level estimates, use the provided key for aggregating transcripts. The default is the ""gene_id"" field, but other fields (e.g. ""gene_name"") might be useful depending on the specifics of the annotation being used. Note: this option only affects aggregation when using a GTF annotation; not an annotation in ""simple"" format."
toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/1.10.1+galaxy4	Salmon is a lightweight method for quantifying transcript abundance from RNA–seq reads, combining a dual-phase parallel inference algorithm and feature-rich bias models with an ultra-fast read mapping procedure. The salmon package contains 4 tools: * Index: creates a salmon index * Quant: quantifies a sample (Reads or mapping-based) * Alevin: Single-cell analysis * Quantmerge: Merges multiple quantifications into a single file Galaxy divides these four into three separate tools in the IUC toolshed: * Salmon quant * Salmon quantmerge * Alevin
toolshed.g2.bx.psu.edu/repos/iuc/seurat/seurat/4.3.0.1+galaxy1	".. class:: infomark 
What it does
 Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
 at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. See the 
Seurat Guided Clustering tutorial
 for more information. ----- 
Inputs
 * Gene count matrix in TAB-separated format or * RNA and Protein count matrices in TAB-separated formats ----- 
Outputs
 * HTML of plots Optionally you can choose to output * R commands used to generate plots printed alongside figures .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ .. _Seurat Guided Clustering tutorial: https://satijalab.org/seurat/pbmc3k_tutorial.html"
toolshed.g2.bx.psu.edu/repos/iuc/slamdunk/slamdunk/0.4.3+galaxy1	"SLAM-seq ======== SLAM-seq is a novel sequencing protocol that directly uncovers 4-thiouridine incorporation events in RNA by high-throughput sequencing. When combined with metabolic labeling protocols, SLAM-seq allows to study the intracellular RNA dynamics, from transcription, RNA processing to RNA stability. Original publication: 
Herzog et al., Nature Methods, 2017; doi:10.1038/nmeth.4435 &lt;https://www.nature.com/nmeth/journal/vaop/ncurrent/full/nmeth.4435.html&gt;
 Slamdunk ======== To analyze a given SLAM-seq dataset with 
slamdunk
 without recovering multimappers, you only need to provide the following files and keep everything else to the default parameters. =============== ========================================================================================================================================================== Parameter Description =============== ========================================================================================================================================================== 
Genome
 The reference fasta file (Genome assembly). 
Reference
 BED-file containing coordinates for 3' UTRs. 
Reads
 Sample FASTQ(gz) files. 
Read length
 Maximum length of reads (usually 50, 100, 150). =============== ========================================================================================================================================================== This will run the entire 
slamdunk
 analysis (
slamdunk all
) with the most relevant output files being: * Tab-separated 
tcount
 file (Count TSV) containing the SLAM-seq statistics per UTR * BAM-file with the final filtered mapped reads * VCF file of variants called on the final filtered alignments These files can be input to the 
Alleyoop
 tool for visualization and further processing. See the 
Slamdunk documentation
 for more information. ------------------------------------------------------ Multimapper recovery -------------------- To utilize multimapper recovery, modify the following parameters. You must either choose a separate 3' UTR file or activate filtering on the supplied reference file. Will only yield different results than a unique-mapping run by specifying a number > 1 as maximum number of multimapper aligments to consider. =================================================== ========================================================= Parameter Description =================================================== ========================================================= 
Maximum number of alignments to report per read
 The maximum number of multimapper alignments to consider. 
Use separate 3' UTR bed to filter multimappers.
 3' UTR bed file to filter. 
Use reference bed file to resolve multimappers.
 Use reference as 3' UTR bed file to filter. =================================================== ========================================================= ------------------------------------------------------ T>C conversions --------------- Depending on the use case, more stringent or more lenient measures of T>C conversion and T>C reads are required such as 2 T>C by 
Muhar et al., Science, 2018; http://doi.org/10.1126/science.aao2793 &lt;http://science.sciencemag.org/content/early/2018/04/04/science.aao2793&gt;
 This can be controlled by the following parameter: ============================ ================================================================================ Parameter Description ============================ ================================================================================ 
T>C conversion threshold
 Minimum number of T>C conversions to consider a read as T>C read. ============================ ================================================================================ .. 
Slamdunk documentation
: http://t-neumann.github.io/slamdunk/docs.html"
toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0	".. class:: infomark 
What it does
 StringTie_ is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts. It uses a novel network flow algorithm as well as an optional 
de novo
 assembly step to assemble and quantitate full-length transcripts representing multiple splice variants for each gene locus. Its input can include not only the alignments of raw reads used by other transcript assemblers, but also alignments of longer sequences that have been assembled from those reads. In order to identify differentially expressed genes between experiments, StringTie's output can be processed by specialized software like Ballgown_, Cuffdiff_ or other programs (DESeq2_, edgeR_, limma_ etc.). ----- 
Inputs
 StringTie takes as input a BAM (or SAM) file of paired-end RNA-seq reads, which must be sorted by genomic location (coordinate position). This file contains spliced read alignments and can be produced directly by programs such as HISAT2_. We recommend using HISAT2 as it is a fast and accurate alignment program. Every spliced read alignment (i.e. an alignment across at least one junction) in the input BAM file must contain the tag XS to indicate the genomic strand that produced the RNA from which the read was sequenced. Alignments produced by HISAT2 (when run with --dta option) already include this tag, but if you use a different read mapper you should check that this XS tag is included for spliced alignments. 
NOTE: be sure to run HISAT2 with the --dta option for alignment (under Spliced alignment options), or your results will suffer.
 Also note that if your reads are from a stranded library, you need to choose the appropriate setting under 
Specify strand information
 above. As, if Forward (FR) is selected, StringTie will assume the reads are from a --fr library, while if Reverse (RF) is selected, StringTie will assume the reads are from a --rf library, otherwise it is assumed that the reads are from an unstranded library (The widely-used, although now deprecated, TopHat had a similar --library-type option, where fr-firststrand corresponded to RF; fr-secondstrand corresponded to FR). If you don't know whether your reads are from are a stranded library or not, you could use the tool 
RSeQC Infer Experiment
 to try to determine. As an option, a reference annotation file in 
GTF/GFF3
 format can be provided to StringTie. In this case, StringTie will prefer to use these ""known"" genes from the annotation file, and for the ones that are expressed it will compute coverage, TPM and FPKM values. It will also produce additional transcripts to account for RNA-seq data that aren't covered by (or explained by) the annotation. Note that if option -e is not used the reference transcripts need to be fully covered by reads in order to be included in StringTie's output. In that case, other transcripts assembled from the data by StringTie and not present in the reference file will be printed as well. 
NOTE: we highly recommend that you provide annotation if you are analyzing a genome that is well-annotated, such as human, mouse, or other model organisms.
 ----- 
Outputs
 StringTie's primary output is * a GTF file containing the 
Assembled transcripts
 Optionally, it can output * a TSV (tab-delimited) file of 
Gene abundances
 If a reference GTF/GFF3 file is used as a guide, StringTie can also output: * a GTF file containing all 
fully-covered reference transcripts
 in the provided reference file that are covered end-to-end by reads * Files (tables) for 
Ballgown
 and/or 
DESeq2/edgeR/limma-voom
, which can use them to estimate differential expression 
StringTie's primary GTF output
 The primary output of StringTie is a Gene Transfer Format (GTF) file that contains details of the transcripts that StringTie assembles from RNA-Seq data. GTF is an extension of GFF (Gene Finding Format, also called General Feature Format), and is very similar to GFF2 and GFF3. The field definitions for the 9 columns of GTF output can be found at the 
Ensembl site here
. The following is an example of a transcript assembled by StringTie as shown in a GTF file: =========== ========== =========== ========= ======= ========= ========== ========= =========================================================================================== 
seqname
 
source
 
feature
 
start
 
end
 
score
 
strand
 
frame
 
attributes
 ----------- ---------- ----------- --------- ------- --------- ---------- --------- ------------------------------------------------------------------------------------------- chrX StringTie transcript 281394 303355 1000 + . gene_id ""ERR188044.1""; transcript_id ""ERR188044.1.1""; reference_id ""NM_018390""; ref_gene_id ""NM_018390""; ref_gene_name ""PLCXD1""; cov ""101.256691""; FPKM ""530.078918""; TPM ""705.667908""; chrX StringTie exon 281394 281684 1000 + . gene_id ""ERR188044.1""; transcript_id ""ERR188044.1.1""; exon_number ""1""; reference_id ""NM_018390""; ref_gene_id ""NM_018390""; ref_gene_name ""PLCXD1""; cov ""116.270836""; =========== ========== =========== ========= ======= ========= ========== ========= =========================================================================================== * 
seqname
: Denotes the chromosome, contig, or scaffold for this transcript. Here the assembled transcript is on chromosome X. * 
source
: The source of the GTF file. Since this example was produced by StringTie, this column simply shows 'StringTie'. * 
feature
: Feature type (e.g., exon, transcript, mRNA, 5'UTR). * 
start
: Start position of the feature (exon, transcript, etc), using a 1-based index. * 
end
: End position of the feature, using a 1-based index. * 
score
: A confidence score for the assembled transcript. Currently this field is not used, and StringTie reports a constant value of 1000 if the transcript has a connection to a read alignment bundle. * 
strand
: If the transcript resides on the forward strand, '+'. If the transcript resides on the reverse strand, '-'. * 
frame
: Frame or phase of CDS features. StringTie does not use this field and simply records a ""."". * 
attributes
: A semicolon-separated list of tag-value pairs, providing additional information about each feature. Depending on whether an instance is a transcript or an exon and on whether the transcript matches the reference annotation file provided by the user, the content of the attributes field will differ. The following list describes the possible attributes shown in this column: * 
gene_id
: A unique identifier for a single gene and its child transcript and exons based on the alignments' file name. * 
transcript_id
: A unique identifier for a single transcript and its child exons based on the alignments' file name. * 
exon_number
: A unique identifier for a single exon, starting from 1, within a given transcript. * 
reference_id
: The transcript_id in the reference annotation (optional) that the instance matched. * 
ref_gene_id
: The gene_id in the reference annotation (optional) that the instance matched. * 
ref_gene_name
: The gene_name in the reference annotation (optional) that the instance matched. * 
cov
: The average per-base coverage for the transcript or exon. * 
FPKM
: Fragments per kilobase of transcript per million read pairs. This is the number of pairs of reads aligning to this feature, normalized by the total number of fragments sequenced (in millions) and the length of the transcript (in kilobases). * 
TPM
: Transcripts per million. This is the number of transcripts from this particular gene normalized first by gene length, and then by sequencing depth (in millions) in the sample. A detailed explanation and a comparison of TPM and FPKM can be found here_, and TPM was defined 
by B. Li and C. Dewey here
. 
Gene abundances in tab-delimited format
 If StringTie is run with the -A option, it returns a file containing gene abundances. The tab-delimited gene abundances output file has nine fields; below is an example of a gene abundance file produced by StringTie using reference annotation: =========== ============= ============= ========== ========= ======= ============ ======== ======== 
Gene ID
 
Gene Name
 
Reference
 
Strand
 
Start
 
End
 
Coverage
 
FPKM
 
TPM
 ----------- ------------- ------------- ---------- --------- ------- ------------ -------- -------- NM_000451 SHOX chrX + 624344 646823 0.000000 0.000000 0.000000 NM_006883 SHOX chrX + 624344 659411 0.000000 0.000000 0.000000 =========== ============= ============= ========== ========= ======= ============ ======== ======== * 
Gene ID
: The gene identifier comes from the reference annotation provided with the -G option. If no reference is provided this field is replaced with the name prefix for output transcripts (-l). * 
Gene Name
: This field contains the gene name in the reference annotation provided with the -G option. If no reference is provided this field is populated with '-'. * 
Reference
: Name of the reference sequence that was used in the alignment of the reads. Equivalent to the 3rd column in the .SAM alignment. * 
Strand
: '+' denotes that the gene is on the forward strand, '-' for the reverse strand. * 
Start
: Start position of the gene (1-based index). * 
End
: End position of the gene (1-based index). * 
Coverage
: Per-base coverage of the gene. * 
FPKM
: normalized expression level in FPKM units (see previous section). * 
TPM
: normalized expression level in RPM units (see previous section). 
Fully covered transcripts matching the reference annotation transcripts (in GTF format)
 If StringTie is run with the use reference guide option (-G), it will also return a file with all the transcripts in the reference annotation that are fully covered, end to end, by reads. The output format is a GTF file as described above. Each line of the GTF is corresponds to a gene or transcript in the reference annotation. 
Ballgown Input Table Files
 An option to output files for Ballgown can be selected under 
Output files for differential expression?
 above. If selected, StringTie will return Ballgown input table files containing coverage data for the reference transcripts given with the -G option. These tables have these specific names: (1) e2t.ctab, (2) e_data.ctab, (3) i2t.ctab, (4) i_data.ctab, and (5) t_data.ctab. A detailed description of each of these five required inputs to Ballgown can be found at 
this link
. With this option StringTie can be used as a direct replacement of the tablemaker program included with the Ballgown distribution. 
DESeq2/edgeR/limma-voom Input Table Files
 DESeq2
, edgeR_ and limma_ are three popular Bioconductor_ packages for analyzing differential expression, which take as input a matrix of read counts mapped to particular genomic features (e.g., genes). This read count information can be extracted directly from the files generated by StringTie (run with the -e parameter) by selecting DESeq2/edgeR/limma-voom under 
Output files for differential expression?
 above. This uses the StringTie helper script 
prepDE.py
 to convert the GTF output from StringTie into two tab-delimited (TSV) files, containing the count matrices for genes and transcripts, using the coverage values found in the output of StringTie -e. ----- 
More Information
 
Evaluating transcript assemblies:
 A simple way of getting more information about the transcripts assembled by StringTie (summary of gene and transcript counts, novel vs. known etc.), or even performing basic tracking of assembled isoforms across multiple RNA-Seq experiments, is to use the 
gffcompare
 program. Basic usage information for this program can be found on the 
GFF utilities page
. 
Differential expression analysis:
 Together with HISAT and Ballgown (or DESeq2/edgeR/limma-voom), StringTie can be used for estimating differential expression across multiple RNA-Seq samples and generating plots and differential expression tables as described in our 
protocol paper
 and shown in a diagram in the 
StringTie manual here
. Our recommended workflow includes the following steps: 1. For each RNA-Seq sample, map the reads to the genome with HISAT2 using the --dta option. It is highly recommended to use the reference annotation information when mapping the reads, which can be either embedded in the genome index (built with the --ss and --exon options, see HISAT2 manual), or provided separately at run time (using the --known-splicesite-infile option of HISAT2). The SAM output of each HISAT2 run must be sorted and converted to BAM using samtools as explained above. 2. For each RNA-Seq sample, use this StringTie tool to assemble the read alignments obtained in the previous step; it is recommended to run StringTie with the -G option if the reference annotation is available. 3. Run the separate 
StringTie merge
 tool in order to generate a non-redundant set of transcripts observed in all the RNA-Seq samples assembled previously. 
StringTie merge
 takes as input a list of all the assembled transcripts files (in GTF format) previously obtained for each sample, as well as a reference annotation file (-G option) if available. 4. For each RNA-Seq sample, run this StringTie tool selecting to output files for Ballgown (or DESeq2/edgeR/limma-voom), which will generate tables of transcript and gene estimated abundances (count files). The option -e (
Use Reference transcripts only
) is not required but is recommended for this run in order to produce more accurate abundance estimations of the input transcripts. Each StringTie run in this step will take as input the sorted read alignments (BAM file) obtained in step 1 for the corresponding sample and the -G option with the merged transcripts (GTF file) generated by 
stringtie merge
 in step 3. Please note that this is the only case where the -G option is not used with a reference annotation, but with the global, merged set of transcripts as observed across all samples. (This step is the equivalent of the 
Tablemaker
 step described in the original Ballgown pipeline.) 5. Ballgown (or DESeq2/edgeR/limma-voom) can now be used to load the coverage tables generated in the previous step and perform various statistical analyses for differential expression, generate plots etc. An alternate, faster differential expression analysis workflow can be pursued if there is no interest in novel isoforms (i.e. assembled transcripts present in the samples but missing from the reference annotation), or if only a well known set of transcripts of interest are targeted by the analysis. This simplified protocol has only 3 steps (depicted in the 
StringTie manual here
) as it bypasses the individual assembly of each RNA-Seq sample and the ""transcript merge"" step. This simplified workflow attempts to directly estimate and analyze the expression of a known set of transcripts as given in the reference annotation file. .. 
StringTie: http://ccb.jhu.edu/software/stringtie/ .. _Ballgown: https://www.biorxiv.org/content/early/2014/09/05/003665 .. _Cuffdiff: http://cole-trapnell-lab.github.io/cufflinks/cuffdiff/ .. _DESeq2: https://bioconductor.org/packages/release/bioc/html/DESeq2.html .. _edgeR: https://bioconductor.org/packages/release/bioc/html/edgeR.html .. _limma: https://bioconductor.org/packages/release/bioc/html/limma.html .. _Bioconductor: https://www.bioconductor.org/ .. _SAM: http://samtools.github.io/hts-specs/SAMv1.pdf .. _HISAT2: http://ccb.jhu.edu/software/hisat2 .. 
GTF/GFF3
: http://ccb.jhu.edu/software/stringtie/gff.shtml .. 
this link
: https://github.com/alyssafrazee/ballgown#ballgown-readable-expression-output .. 
Ensembl site here
: http://useast.ensembl.org/info/website/upload/gff.html .. 
here: http://www.rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/ .. 
by B. Li and C. Dewey here
: http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-323 .. 
GFF utilities page
: http://ccb.jhu.edu/software/stringtie/gff.shtml#gffcompare .. 
protocol paper
: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5032908/ .. _
StringTie manual here
: http://ccb.jhu.edu/software/stringtie/index.shtml?t=manual"
toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie_merge/2.2.3+galaxy0	"What it does?
 This is a special usage mode of StringTie_, distinct from the assembly usage mode. In the merge mode, StringTie takes as input a list of GTF/GFF files and merges/assembles these transcripts into a non-redundant set of transcripts. This mode is used in the new differential analysis pipeline to generate a global, unified set of transcripts (isoforms) across multiple RNA-Seq samples. If a reference annotation is provided, StringTie will assemble the transfrags from the input GTF files with the reference transcripts. .. _StringTie: http://ccb.jhu.edu/software/stringtie/"
toolshed.g2.bx.psu.edu/repos/rnateam/targetfinder/targetfinder/1.7.0+galaxy1	".. class:: infomark 
What it does
 TargetFinder will computationally predict small RNA binding sites on target transcripts from a sequence database. This is done by aligning the input small RNA sequence against all transcripts, followed by site scoring using a position-weighted scoring matrix. ---- .. class:: infomark 
Input
 This tool requires two fasta files: :: -f Input small RNA sequences file (FASTA-format). -d Target sequence database file (FASTA-format) ---- .. class:: infomark 
Original TargetFinder Output
 Each predicted target site is printed out separately. The output consists of two parts. The first is a description line and the second is a base-pairing diagram of the target and small RNA (query) sequence. The description line contains the query name, the description line from the target sequence database, and the target prediction score. :: query=miR399a, target=AT2G33770.1 | Symbol: None | ubiquitin-conjugating enzyme family protein, low similarity to u, score=1.5 The base-pairing diagram has the target site sequence on top in 5'-3' orientation and the query sequence on the bottom in 3'-5' orientation. Between the target site sequece and the query sequence are base pair symbols. A ':' (colon) symbol represents an ordinary Watson-Crick base pair, a '.' (period) represents a G:U base pair, and a ' ' (space) represents a mismatch. :: target 5' UAGGGCAAAUCUUCUUUGGCA 3' .:::::::::::.:::::::: query 3' GUCCCGUUUAGAGGAAACCGU 5' If a small RNA is predicted to target a sequence more than once, each target site will be output as separate output. ---- .. class:: infomark 
Additional output formats
 In addition to the output described above ('classic' output), three new output format options were added to TargetFinder. Generic Feature Format (GFF3): :: AT2G33770.1 | Symbols: UBC24 | phosphate 2 | chr2:14277558-14283040 REVERSE LEN targetfinder rna_target 607 627 1.5 + . smallRNA=miR399a;target_seq=UAGGGCAAAUCUUCUUUGGCA;base_pairs=.:::::::::::.::::::::;miR_seq=GUCCCGUUUAGAGGAAACCGU AT2G33770.1 | Symbols: UBC24 | phosphate 2 | chr2:14277558-14283040 REVERSE LEN targetfinder rna_target 740 760 1.5 + . smallRNA=miR399a;target_seq=UAGGGCAUAUCUCCUUUGGCA;base_pairs=.:::::: :::::::::::::;miR_seq=GUCCCGUUUAGAGGAAACCGU AT2G33770.1 | Symbols: UBC24 | phosphate 2 | chr2:14277558-14283040 REVERSE LEN targetfinder rna_target 829 849 1.5 + . smallRNA=miR399a;target_seq=UUGGGCAAAUCUCCUUUGGCA;base_pairs=. :::::::::::::::::::;miR_seq=GUCCCGUUUAGAGGAAACCGU Tab-deliminated Format: :: miR399a AT2G33770.1 | Symbols: UBC24 | phosphate 2 | chr2:14277558-14283040 REVERSE LEN 607 627 + 1.5 UAGGGCAAAUCUUCUUUGGCA .:::::::::::.:::::::: GUCCCGUUUAGAGGAAACCGU miR399a AT2G33770.1 | Symbols: UBC24 | phosphate 2 | chr2:14277558-14283040 REVERSE LEN 740 760 + 1.5 UAGGGCAUAUCUCCUUUGGCA .:::::: ::::::::::::: GUCCCGUUUAGAGGAAACCGU miR399a AT2G33770.1 | Symbols: UBC24 | phosphate 2 | chr2:14277558-14283040 REVERSE LEN 829 849 + 1.5 UUGGGCAAAUCUCCUUUGGCA . ::::::::::::::::::: GUCCCGUUUAGAGGAAACCGU JavaScript Object Notation Format (JSON): :: { 'miR399a': { 'hits' : [ { 'Target accession': 'AT2G33770.1 | Symbols: UBC24, ATUBC24, PHO2 | phosphate 2 | chr2:14277558-14283040 REVERSE LEN', 'Score': '1.5', 'Coordinates': '607-627', 'Strand': '+', 'Target sequence': 'UAGGGCAAAUCUUCUUUGGCA', 'Base pairing': '.:::::::::::.::::::::', 'amiRNA sequence': 'GUCCCGUUUAGAGGAAACCGU' } ] } } ---- .. class:: infomark 
Method
 TargetFinder searches for potential miRNA target sites in a FASTA-formated sequence database using three main steps. 1. The small RNA query sequence is aligned to every sequence in the FASTA-formated sequence database using 
Smith-Waterman (SW) alignments &lt;https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm&gt;
_ implemented in the FASTA package (ssearch35_t). 2. The SW alignments are converted into RNA duplexes. 3. Each duplex is scored using a position-dependent scoring matrix. SW alignments are used to identify the best complementary regions between the small RNA query sequence and every sequence in the FASTA-formated sequence database."
toolshed.g2.bx.psu.edu/repos/iuc/transdecoder/transdecoder/5.5.0+galaxy2	".. class:: infomark 
What it does
 TransDecoder identifies candidate coding regions within transcript sequences such as those generated by de novo RNA-Seq transcript assembly using Trinity or constructed based on RNA-Seq alignments to the genome using Tophat and Cufflinks. TransDecoder identifies likely coding sequences based on the following criteria: - a minimum length open reading frame (ORF) is found in a transcript sequence. - a log-likelihood score similar to what is computed by the GeneID software is > 0. - the above coding score is greatest when the ORF is scored in the 1st reading frame as compared to scores in the other 5 reading frames. - if a candidate ORF is found fully encapsulated by the coordinates of another candidate ORF, the longer one is reported. However, a single transcript can report multiple ORFs (allowing for operons, chimeras, etc). - a PSSM is built/trained/used to refine the start codon prediction. - optional the putative peptide has a match to a Pfam domain above the noise cutoff score. 
Step 1
: Extract long open reading frames By default, TransDecoder.LongOrfs will identify ORFs that are at least 100 amino acids long. You can lower this via the '-m' parameter, but know that the rate of false positive ORF predictions increases drastically with shorter minimum length criteria. 
Step 2
: (optional and not part of this wrapper) The result ""longest ORFs (PEP)"" can be used to identify ORFs with homology to known proteins via BlastP or Pfam searches (
details &lt;https://github.com/TransDecoder/TransDecoder/wiki#including-homology-searches-as-orf-retention-criteria&gt;
). 
Step 3
: Predict the likely coding regions Optionally apply results of homology searches in this step and re-run the whole analysis. 
Input
 - FASTA file with transcripts - (optional) gene-to-transcript identifier mapping file - (optional) BLAST or Pfam database file (
details &lt;https://github.com/TransDecoder/TransDecoder/wiki#including-homology-searches-as-orf-retention-criteria&gt;
) 
Output
 
LongOrfs
 - longest ORFs (PEP/FASTA): all ORFs meeting the minimum length criteria, regardless of coding potential - longest ORFs (GFF3): positions of all ORFs as found in the target transcripts - longest ORFs (CDS/FASTA): the nucleotide coding sequence for all detected ORFs 
Predict
 - Results (PEP/FASTA): peptide sequences for the final candidate ORFs; all shorter candidates within longer ORFs were removed - Results (CDS/FASTA): nucleotide sequences for coding regions of the final candidate ORFs - Results (GFF3): positions within the target transcripts of the final selected ORFs - Results (BED): BED-formatted file describing ORF positions, best for viewing using GenomeView or IGV - Plots: sequence logos and scores (compressed PDF) 
Other
 - Log file .. class:: infomark 
References
 More information are available on 
GitHub &lt;https://github.com/TransDecoder/TransDecoder&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.15.1+galaxy1	Trinity_ assembles transcript sequences from Illumina RNA-Seq data. .. _Trinity: https://github.com/trinityrnaseq/trinityrnaseq/wiki
toolshed.g2.bx.psu.edu/repos/iuc/trinotate/trinotate/3.2.2+galaxy0	".. class:: infomark 
What it does
 Trinotate_ is a comprehensive annotation suite designed for automatic functional annotation of transcriptomes, particularly de novo assembled transcriptomes, from model or non-model organisms. Trinotate makes use of a number of different well referenced methods for functional annotation including homology search to known sequence data (BLAST+/SwissProt), protein domain identification (HMMER/PFAM), protein signal peptide and transmembrane domain prediction (signalP/tmHMM), and leveraging various annotation databases (eggNOG/GO/Kegg databases). All functional annotation data derived from the analysis of transcripts is integrated into a SQLite database which allows fast efficient searching for terms with specific qualities related to a desired scientific hypothesis or a means to create a whole annotation report for a transcriptome. .. 
Trinotate: https://trinotate.github.io/ -------- 
Suggested upstream Galaxy tools
 Transcripts - Trinity: iuc/trinity
 .. 
trinity: https://toolshed.g2.bx.psu.edu/repository?repository_id=faf6028922d9220a Peptides - TransDecoder: iuc/transdecoder
 .. 
transdecoder: https://toolshed.g2.bx.psu.edu/repository?repository_id=7a2a8151a50f8099 Genes to transcripts map - Generate gene to transcript map for Trinity assembly: iuc/trinity_gene_to_trans_map
 .. 
trinity_gene_to_trans_map: https://toolshed.g2.bx.psu.edu/repository?repository_id=faf6028922d9220a BLASTP: Peptides vs Uniprot.SwissProt - NCBI BLAST+ blastp: devteam/ncbi_blast_plus
 .. 
ncbi_blast_plus: https://toolshed.g2.bx.psu.edu/repository?repository_id=1d92ebdf7e8d466c BLASTX: Transcripts vs Uniprot.SwissProt - NCBI BLAST+ blastx: devteam/ncbi_blast_plus
 .. 
ncbi_blast_plus: https://toolshed.g2.bx.psu.edu/repository?repository_id=1d92ebdf7e8d466c HMMER hmmscan: Peptides vs PFAM - HMMER hmmscan: iuc/hmmer_hmmscan
 .. 
hmmer_hmmscan: https://toolshed.g2.bx.psu.edu/repository?repository_id=a2cc4683090b1800 TMHMM on Peptides - TMHMM 2.0: peterjc/tmhmm_and_signalp
 .. 
tmhmm_and_signalp: https://toolshed.g2.bx.psu.edu/repository?repository_id=292389a45f1a238a SignalP on Peptides - SignalP 3.0: peterjc/tmhmm_and_signalp
 .. _tmhmm_and_signalp: https://toolshed.g2.bx.psu.edu/repository?repository_id=292389a45f1a238a -------- 
Output
 The output has the following column headers: ====== ======================= Column Description ------ ----------------------- 0 #gene_id 1 transcript_id 2 sprot_Top_BLASTX_hit 3 RNAMMER 4 prot_id 5 prot_coords 6 sprot_Top_BLASTP_hit 7 custom_pombe_pep_BLASTX 8 custom_pombe_pep_BLASTP 9 Pfam 10 SignalP 11 TmHMM 12 eggnog 13 Kegg 14 gene_ontology_blast 15 gene_ontology_pfam 16 transcript 17 peptide ====== ======================="
toolshed.g2.bx.psu.edu/repos/iuc/heinz/heinz_visualization/0.1.1	"This tool provides a simple visualisation of the raw output of the Heinz Galaxy tool and saves the output as PDF. Heinz output file: output file of the tool ""Identify optimal scoring subnetwork""."
toolshed.g2.bx.psu.edu/repos/rnateam/blockbuster/blockbuster/0.1.2	"What it does
 Blockbuster_ detects blocks of overlapping reads using a gaussian-distribution approach. Once short read sequences are mapped to a reference genome, one will face the problem of dividing consecutive reads into blocks to detect specific expression patterns. Due to biological variability and sequencing inaccuracies, the read arrangement does not always show exact block boundaries. The blockbuster tool automatically assigns reads to blocks and gives a unique chance to actually see the different origins where the short reads come from. .. _Blockbuster: http://hoffmann.bioinf.uni-leipzig.de/LIFE/blockbuster.html 
Input
 Input file can be a BED file or an Segemehl output file."
toolshed.g2.bx.psu.edu/repos/devteam/cummerbund/cummeRbund/2.16.0+galaxy1	"This tool allows for persistent storage, access, exploration, and manipulation of Cufflinks high-throughput sequencing data. In addition, provides numerous plotting functions for commonly used visualizations. ------ Based on the 
cummeRbund wrapper &lt;https://toolshed.g2.bx.psu.edu/view/jjohnson/cummerbund&gt;
_ written by James E. Johnson of the Minnesota Supercomputing Institute."
toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy7	".. class:: infomark 
What it does
 Given a counts matrix, or a set of counts files, for example from 
featureCounts
, and optional information about the genes, this tool produces plots and tables useful in the analysis of differential gene expression. This tool uses the 
edgeR
 quasi-likelihood pipeline (edgeR-quasi) for differential expression analysis. This statistical methodology uses negative binomial generalized linear models, but with F-tests instead of likelihood ratio tests. This method provides stricter error rate control than other negative binomial based pipelines, including the traditional edgeR pipelines or DESeq2. While the limma pipelines are recommended for large-scale datasets, because of their speed and flexibility, the edgeR-quasi pipeline gives better performance in low-count situations. For the data analyzed in this 
edgeR workflow article
 ,the edgeR-quasi, limma-voom and limma-trend pipelines are all equally suitable and give similar results. .. 
edgeR: http://www.bioconductor.org/packages/release/bioc/html/edgeR.html .. _edgeR workflow article: https://f1000research.com/articles/5-1438 ----- 
Inputs
 
Counts Data:
 The counts data can either be input as separate counts files (one sample per file) or a single count matrix (one sample per column). The rows correspond to genes, and columns correspond to the counts for the samples. Values must be tab separated, with the first row containing the sample/column labels and the first column containing the row/gene labels. The sample labels must start with a letter. Gene identifiers can be of any type but must be unique and not repeated within a counts file. Example - 
Separate Count Files
: ========== ======= 
GeneID
 
WT1
 ---------- ------- 11287 1699 11298 1905 11302 6 11303 2099 11304 356 11305 2528 ========== ======= Example - 
Single Count Matrix
: ========== ======= ======= ======= ======== ======== ======== 
GeneID
 
WT1
 
WT2
 
WT3
 
Mut1
 
Mut2
 
Mut3
 ---------- ------- ------- ------- -------- -------- -------- 11287 1699 1528 1601 1463 1441 1495 11298 1905 1744 1834 1345 1291 1346 11302 6 8 7 5 6 5 11303 2099 1974 2100 1574 1519 1654 11304 356 312 337 361 397 346 11305 2528 2438 2493 1762 1942 2027 ========== ======= ======= ======= ======== ======== ======== 
Gene Annotations:
 Optional input for gene annotations, this can contain more information about the genes than just an ID number. The annotations will be available in the differential expression results table and the optional normalised counts table. The file must contain a header row and have the gene IDs in the first column. The number of rows should match that of the counts files, add NA for any gene IDs with no annotation. The Galaxy tool 
annotateMyIDs
 can be used to obtain annotations for human, mouse, fly and zebrafish. Example: ========== ========== =================================================== 
GeneID
 
Symbol
 
GeneName
 ---------- ---------- --------------------------------------------------- 11287 Pzp pregnancy zone protein 11298 Aanat arylalkylamine N-acetyltransferase 11302 Aatk apoptosis-associated tyrosine kinase 11303 Abca1 ATP-binding cassette, sub-family A (ABC1), member 1 11304 Abca4 ATP-binding cassette, sub-family A (ABC1), member 4 11305 Abca2 ATP-binding cassette, sub-family A (ABC1), member 2 ========== ========== =================================================== 
Factor Information:
 Enter factor names and groups in the tool form, or provide a tab-separated file that has the names of the samples in the first column and one header row. The sample names must be the same as the names in the columns of the count matrix. The second column should contain the primary factor levels (e.g. WT, Mut) with optional additional columns for any secondary factors. Example: ========== ============ ========= 
Sample
 
Genotype
 
Batch
 ---------- ------------ --------- WT1 WT b1 WT2 WT b2 WT3 WT b3 Mut1 Mut b1 Mut2 Mut b2 Mut3 Mut b3 ========== ============ ========= 
Factor Name:
 The name of the experimental factor being investigated e.g. Genotype, Treatment. One factor must be entered, the name should start with a letter and spaces must not be used. Optionally, additional factors can be included, these are variables that might influence your experiment e.g. Batch, Gender, Subject. If additional factors are entered, an additive linear model will be used. 
Groups:
 The names of the groups for the factor. The names should start with a letter, and only contain letters, numbers and underscores, other characters such as spaces and hyphens must not be used. If entered into the tool form above, the order must be the same as the samples (to which the groups correspond) are listed in the columns of the counts matrix, with the values separated by commas. 
Formula:
 By default the tool will construct a formula for modelling counts based on the contents of the factors files or the factors given. This can be overriden by directly providing the EdgeR formula in section named Formula. 
Contrasts of Interest:
 The contrasts you wish to make between levels. A common contrast would be a simple difference between two levels: ""Mut-WT"" represents the difference between the mutant and wild type genotypes. Multiple contrasts must be entered separately using the Insert Contrast button, spaces must not be used. Alternatively, you can specify a file with contrasts. The file must contain a header (it's value is irrelevant) and one contrast per line on the first column (other columns are ignored). If using this option, make sure to remove any contrast section from the manual part, or the tool will fail. 
Working with Interactions:
 When you have multiple factors, you may want to test for interaction effects between them. For example, with factors Genotype (Mut, WT) and Batch (b1, b2, b3), you might want to test whether the effect of genotype differs between batches. 
Interaction Formula:
 To include interaction terms, use the Formula field with one of these patterns: - 
~ Genotype * Batch
 - Full factorial model (main effects + interactions) - 
~ Genotype + Batch + Genotype:Batch
 - Equivalent to above - 
~ 0 + Genotype:Batch
 - Cell-means model (each group combination separately) The cell-means model (
~ 0 + Genotype:Batch
) is often preferred for testing specific group comparisons as it creates individual parameters for each combination (e.g., Mut.b1, WT.b1, etc.). 
Interaction Contrasts:
 When using interactions, contrast names will contain colons that get converted to dots automatically. For example: 
Cell-means model contrasts
 (with 
~ 0 + Genotype:Batch
): - Input contrast: 
Mut:b1-WT:b1
 (compare Mut vs WT within batch b1) - Input contrast: 
Mut:b2-Mut:b3
 (compare batch b2 vs b3 within Mut genotype) - Output file names: 
edgeR_Mut.b1-WT.b1.tsv
, 
edgeR_Mut.b2-Mut.b3.tsv
 
Full factorial model contrasts
 (with 
~ Genotype * Batch
): - Input contrast: 
WT:b2
 (interaction effect: WT×b2 vs baseline) - Input contrast: 
WT:b3-WT:b2
 (compare interaction effects: WT×b3 vs WT×b2) - Output file names: 
edgeR_WT.b2.tsv
, 
edgeR_WT.b3-WT.b2.tsv
 
Using Contrasts Files with Interactions:
 When using a contrasts file instead of manual entry, the same interaction contrast syntax applies. Create a tab-separated file with a header and one contrast per line: Example contrasts file for cell-means model:: Contrast Mut:b1-WT:b1 Mut:b2-Mut:b3 WT:b1-WT:b2 Example contrasts file for full factorial model:: Contrast WT:b2 WT:b3-WT:b2 WT The colons in contrast names will be automatically converted to dots in output file names, regardless of whether contrasts are entered manually or via file. Note: Ensure you have sufficient replicates for interaction models. With n factors each having k levels, you need more than n×k samples to estimate interactions properly. 
Filter Low Counts:
 Genes with very low counts across all libraries provide little evidence for differential expression. In the biological point of view, a gene must be expressed at some minimal level before it is likely to be translated into a protein or to be biologically important. In addition, the pronounced discreteness of these counts interferes with some of the statistical approximations that are used later in the pipeline. These genes should be filtered out prior to further analysis. As a rule of thumb, genes are dropped if they can’t possibly be expressed in all the samples for any of the conditions. Users can set their own definition of genes being expressed. Usually a gene is required to have a count of 5-10 in a library to be considered expressed in that library. Users should also filter with count-per-million (CPM) rather than filtering on the counts directly, as the latter does not account for differences in library sizes between samples. Option to ignore the genes that do not show significant levels of expression, this filtering is dependent on two criteria: CPM/count and number of samples. You can specify to filter on CPM (Minimum CPM) or count (Minimum Count) values: * 
Minimum CPM:
 This is the minimum count per million that a gene must have in at least the number of samples specified under Minimum Samples. * 
Minimum Count:
 This is the minimum count that a gene must have. It can be combined with either Filter on Total Count or Minimum Samples. * 
Filter on Total Count:
 This can be used with the Minimum Count filter to keep genes with a minimum total read count. * 
Minimum Samples:
 This is the number of samples in which the Minimum CPM/Count requirement must be met in order for that gene to be kept. If the Minimum Samples filter is applied, only genes that exhibit a CPM/count greater than the required amount in at least the number of samples specified will be used for analysis. Care should be taken to ensure that the sample requirement is appropriate. In the case of an experiment with two experimental groups each with two members, if there is a change from insignificant CPM/count to significant CPM/count but the sample requirement is set to 3, then this will cause that gene to fail the criteria. When in doubt simply do not filter or consult the 
edgeR workflow article
 for filtering recommendations. 
Advanced Options:
 By default error rate for multiple testing is controlled using Benjamini and Hochberg's false discovery rate control at a threshold value of 0.05. However there are options to change this to custom values. * 
Minimum log2-fold-change Required:
 In addition to meeting the requirement for the adjusted statistic for multiple testing, the observation must have an absolute log2-fold-change greater than this threshold to be considered significant, thus highlighted in the MD plot. * 
Adjusted Threshold:
 Set the threshold for the resulting value of the multiple testing control method. Only observations whose statistic falls below this value is considered significant, thus highlighted in the MD plot. * 
P-Value Adjustment Method:
 Change the multiple testing control method, the options are BH(1995) and BY(2001) which are both false discovery rate controls. There is also Holm(1979) which is a method for family-wise error rate control. 
Normalisation Method:
 The most obvious technical factor that affects the read counts, other than gene expression levels, is the sequencing depth of each RNA sample. edgeR adjusts any differential expression analysis for varying sequencing depths as represented by differing library sizes. This is part of the basic modeling procedure and flows automatically into fold-change or p-value calculations. It is always present, and doesn’t require any user intervention. The second most important technical influence on differential expression is one that is less obvious. RNA-seq provides a measure of the relative abundance of each gene in each RNA sample, but does not provide any measure of the total RNA output on a per-cell basis. This commonly becomes important when a small number of genes are very highly expressed in one sample, but not in another. The highly expressed genes can consume a substantial proportion of the total library size, causing the remaining genes to be under-sampled in that sample. Unless this RNA composition effect is adjusted for, the remaining genes may falsely appear to be down-regulated in that sample . The edgeR 
calcNormFactors
 function normalizes for RNA composition by finding a set of scaling factors for the library sizes that minimize the log-fold changes between the samples for most genes. The default method for computing these scale factors uses a trimmed mean of M values (TMM) between each pair of samples. We call the product of the original library size and the scaling factor the 
effective library size
. The effective library size replaces the original library size in all downsteam analyses. TMM is the recommended method for most RNA-Seq data where the majority (more than half) of the genes are believed not differentially expressed between any pair of the samples. You can change the normalisation method under 
Advanced Options
 above. For more information, see the 
calcNormFactors
 section in the 
edgeR User's Guide
. 
Robust Settings
 Option to use robust settings. Using robust settings (robust=TRUE) with the edgeR estimateDisp and glmQLFit functions is usually recommended to protect against outlier genes. This is turned on by default. Note that it is only used with the quasi-likelihood F test method. For more information, see the 
edgeR workflow article
. 
Test Method
 Option to use the likelihood ratio test instead of the quasi-likelihood F test. For more information, see the 
edgeR workflow article
_. .. _edgeR User's Guide: http://www.bioconductor.org/packages/release/bioc/html/edgeR.html ----- 
Outputs
 This tool outputs * a table of differentially expressed genes for each contrast of interest * a HTML report with plots and additional information Optionally, under 
Output Options
 you can choose to output * a normalised counts table * the R script used by this tool * an RData file ----- 
Citations
 Please try to cite the appropriate articles when you publish results obtained using software, as such citation is the main means by which the authors receive credit for their work. For the edgeR method itself, please cite Robinson et al., 2010, and for this tool (which was developed from the Galaxy limma-voom tool) please cite Liu et al., 2015."
toolshed.g2.bx.psu.edu/repos/iuc/featurecounts/featurecounts/2.1.1+galaxy0	"featureCounts ############# Overview -------- FeatureCounts is a light-weight read counting program written entirely in the C programming language. It can be used to count both gDNA-seq and RNA-seq reads for genomic features in in SAM/BAM files. FeatureCounts is part of the Subread_ package. Input formats ------------- Alignments should be provided in either: - SAM format, http://samtools.sourceforge.net/samtools.shtml#5 - BAM format Annotations for gene regions should be provided in the GFF/GTF format: - http://genome.ucsc.edu/FAQ/FAQformat.html#format3 - http://www.ensembl.org/info/website/upload/gff.html Alternatively, the featureCounts built-in annotations for genomes hg38, hg19, mm10 and mm9 can be used through selecting the built-in option above. These annotation files are in simplified annotation format (SAF) as shown below. The GeneID column contains Entrez gene identifiers and each entry (row) is taken as a feature (e.g. an exon). Example - 
Built-in annotation format
: ====== ==== ======= ======= ====== GeneID Chr Start End Strand ====== ==== ======= ======= ====== 497097 chr1 3204563 3207049 - 497098 chr1 3411783 3411982 - 497099 chr1 3660633 3661579 - ====== ==== ======= ======= ====== These annotation files can be found in the 
Subread package
. You can see the version of Subread used by this wrapper in the tool form above under 
Options &gt; Requirements
. To create the files, the annotations were downloaded from NCBI RefSeq database and then adapted by merging overlapping exons from the same gene to form a set of disjoint exons for each gene. Genes with the same Entrez gene identifiers were also merged into one gene. See the 
Subread User's Guide
 for more information. Gene names can be obtained for these Entrez identifiers with the Galaxy 
annotateMyIDs
 tool. Output format ------------- FeatureCounts produces a table containing counted reads, per gene, per row. Optionally the last column can be set to be the effective gene-length. These tables are compatible with the DESeq2, edgeR and limma-voom Galaxy wrappers by IUC. .. _Subread: http://subread.sourceforge.net/ .. 
Subread User's Guide
: https://bioconductor.org/packages/release/bioc/vignettes/Rsubread/inst/doc/SubreadUsersGuide.pdf"
toolshed.g2.bx.psu.edu/repos/iuc/goseq/goseq/1.44.0+galaxy0	".. class:: infomark 
What it does
 
Gene Ontology
 (GO) analysis is widely used to reduce complexity and highlight biological processes in genome-wide expression studies, but standard methods give biased results on RNA-seq data due to over-detection of differential expression for long and highly expressed transcripts. This tool provides methods for performing GO analysis of RNA-seq data, taking length bias into account. The methods and software used by goseq are equally applicable to other category based tests of RNA-seq data, such as KEGG
 pathway analysis. Options map closely to the excellent goseq manual_. ----- 
Inputs
 
Differentially expressed genes file
 goseq needs a tabular file containing information on differentially expressed genes. This should contain all genes assayed in the RNA-seq experiment. The file should have two columns with an optional header row. The first column should contain the Gene IDs, which must be unique within the file and not repeated. The second column should contain True or False. True means the gene should count as differentially expressed, False means it is not differentially expressed. You can use the ""Compute an expression on every row"" tool to create a True / False column for your dataset. Example: =============== ===== ENSG00000236824 False ENSG00000162526 False ENSG00000090402 True ENSG00000169188 False ENSG00000124103 False =============== ===== 
Gene lengths file
 goseq needs information about the length of a gene to correct for potential length bias in differentially expressed genes using a Probability Weight Function (PWF). The PWF can be thought of, as a function which gives the probability that a gene will be differentially expressed, based on its length alone. The gene length file should have two columns with an optional header row. The first column should contain the Gene IDs, and the second column should contain the gene length in bp. If length data is unavailable for some genes, that entry should be set to NA. The goseq authors recommend using the gene lengths obtained from upstream summarization programs, such as 
featureCounts
, if provided. Alternatively, the 
Gene length and GC content
 tool can produce such a file. Example: =============== ===== ENSG00000236824 13458 ENSG00000162526 2191 ENSG00000090402 6138 ENSG00000169188 3245 ENSG00000124103 1137 =============== ===== 
Gene categories file
 This tool can get GO and KEGG categories for some genomes. The three GO categories are GO:MF (Molecular Function - molecular activities of gene products), GO:CC (Cellular Component - where gene products are active), GO:BP (Biological Process - pathways and larger processes made up of the activities of multiple gene products). If your genome is not available, you will also need a file describing the membership of genes in categories. The category file should have two columns with an optional header row. with Gene ID in the first column and category identifier in the second column. As the mapping between categories and genes is usually many-to-many, this table will usually have multiple rows with the same Gene ID and category identifier. Example: =============== =========== ENSG00000162526 GO\:0000003 ENSG00000198648 GO\:0000278 ENSG00000112312 GO\:0000278 ENSG00000174442 GO\:0000278 ENSG00000108953 GO\:0000278 =============== =========== ----- 
Outputs
 This tool outputs a tabular file containing a ranked list of gene categories, similar to below. The default output is the Wallenius method table. If the Sampling and/or Hypergeometric methods are also selected, additional tables are produced. Example: =========== =============== ================ ============ ========== ======================================== ========== =================== ==================== 
category
 
over_rep_pval
 
under_rep_pval
 
numDEInCat
 
numInCat
 
term
 
ontology
 
p_adjust_over_rep
 
p_adjust_under_rep
 ----------- --------------- ---------------- ------------ ---------- ---------------------------------------- ---------- ------------------- -------------------- GO\:0005576 0.000054 0.999975 56 142 extracellular region CC 0.394825 1 GO\:0005840 0.000143 0.999988 9 12 ribosome CC 0.394825 1 GO\:0044763 0.000252 0.999858 148 473 single-organism cellular process BP 0.394825 1 GO\:0044699 0.000279 0.999844 158 513 single-organism process BP 0.394825 1 GO\:0065010 0.000428 0.999808 43 108 extracellular membrane-bounded organelle CC 0.394825 1 GO\:0070062 0.000428 0.999808 43 108 extracellular exosome CC 0.394825 1 =========== =============== ================ ============ ========== ======================================== ========== =================== ==================== Optionally, this tool can also output: * a plot of the top 10 over-represented GO categories * some diagnostic plots * a tabular with the differentially expressed genes in categories (GO/KEGG terms) * an RData file ----- 
Method options
 3 methods, 
Wallenius
, 
Sampling
 and 
Hypergeometric
, can be used to calculate the p-values as follows. 
Wallenius
 approximates the true distribution of numbers of members of a category amongst DE genes by the Wallenius non-central hypergeometric distribution. This distribution assumes that within a category all genes have the same probability of being chosen. Therefore, this approximation works best when the range in probabilities obtained by the probability weighting function is small. This is the method used by default. 
Sampling
 uses random sampling to approximate the true distribution and uses it to calculate the p-values for over (and under) representation of categories. Although this is the most accurate method given a high enough value of sampling number, its use quickly becomes computationally prohibitive. It may sometimes be desirable to use random sampling to generate the null distribution for category membership. For example, to check consistency against results from the Wallenius approximation. This is easily accomplished by using the method option to additionally specify sampling and the number of samples to generate. 
Hypergeometric
 assumes there is no bias in power to detect differential expression at all and calculates the p-values using a standard hypergeometric distribution (no length bias correction is performed). Useful if you wish to test the effect of length bias on your results. Caution: Hypergeometric should NEVER be used for producing results for biological interpretation of RNA-seq data. If length bias is truly not present in your data, goseq will produce a nearly flat PWF plot, no length bias correction will be applied to your data, and all methods will produce the same results. ----- 
More Information
 In order to account for the length bias inherent to RNA-seq data when performing a GO analysis (or other category based tests), one cannot simply use the hypergeometric distribution as the null distribution for category membership, which is appropriate for data without DE length bias, such as microarray data. GO analysis of RNA-seq data requires the use of random sampling in order to generate a suitable null distribution for GO category membership and calculate each categories significance for over representation amongst DE genes. However, this random sampling is computationally expensive. In most cases, the Wallenius distribution can be used to approximate the true null distribution, without any significant loss in accuracy. The goseq package implements this approximation as its default option. The option to generate the null distribution using random sampling is also included as an option, but users should be aware that the default number of samples generated will not be enough to accurately call enrichment when there are a large number of go terms. Having established a null distribution, each category is then tested for over and under representation amongst the set of differentially expressed genes and the null is used to calculate a p-value for under and over representation. Having performed a GO analysis, you may now wish to interpret the results. If you wish to identify categories significantly enriched/unenriched below some p-value cutoff, it is necessary to first apply some kind of multiple hypothesis testing correction. For example, you can identify GO categories over enriched using a 0.05 FDR (p.adjust) cutoff [Benjamini and Hochberg, 1995]. Unless you are a machine, GO and KEGG category identifiers are probably not very meaningful to you. Information about each identifier can be obtained from the 
Gene Ontology
 and KEGG
 websites. .. _manual: https://bioconductor.org/packages/release/bioc/vignettes/goseq/inst/doc/goseq.pdf .. _Gene Ontology: http://www.geneontology.org .. _KEGG: http://www.genome.jp/kegg"
toolshed.g2.bx.psu.edu/repos/lparsons/htseq_count/htseq_count/2.0.9+galaxy0	Mode to handle reads overlapping more than one feature.
toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0	".. class:: infomark 
What it does
 Given a matrix of counts (e.g. from featureCounts) and optional information about the genes, this tool performs differential expression (DE) using the limma_ Bioconductor package and produces plots and tables useful in DE analysis. Interactive Glimma_ plots and tables can also be generated and links to the Glimma plots will be provided in the report. See an example workflow here_. In the 
limma approach
 to RNA-seq, read counts are converted to log2-counts-per-million (logCPM) and the mean-variance relationship is modelled either with precision weights or with an empirical Bayes prior trend. The precision weights approach is called “voom” and the prior trend approach is called “limma-trend”. For more information, see the Help section below. ----- 
Inputs
 
Differential Expression Method:
 Option to use the limma-voom or limma-trend approach for differential expression. The default is limma-voom. If the sequencing depth is reasonably consistent across the RNA samples, then the simplest and most robust approach to differential expression is to use limma-trend. This approach will usually work well if the ratio of the largest library size to the smallest is not more than about 3-fold. When the library sizes are quite variable between samples, then the voom approach is theoretically more powerful than limma-trend. For more information see the excellent 
limma User's Guide
. 
Counts Data:
 The counts data can either be input as separate counts files (one sample per file) or a single count matrix (one sample per column). The rows correspond to genes, and columns correspond to the counts for the samples. Values must be tab separated, with the first row containing the sample/column labels and the first column containing the row/gene labels. The sample labels must start with a letter. Gene identifiers can be of any type but must be unique and not repeated within a counts file. Example - 
Separate Count Files
: ========== ======= 
GeneID
 
WT1
 ---------- ------- 11287 1699 11298 1905 11302 6 11303 2099 11304 356 11305 2528 ========== ======= Example - 
Single Count Matrix
: ========== ======= ======= ======= ======== ======== ======== 
GeneID
 
WT1
 
WT2
 
WT3
 
Mut1
 
Mut2
 
Mut3
 ---------- ------- ------- ------- -------- -------- -------- 11287 1699 1528 1601 1463 1441 1495 11298 1905 1744 1834 1345 1291 1346 11302 6 8 7 5 6 5 11303 2099 1974 2100 1574 1519 1654 11304 356 312 337 361 397 346 11305 2528 2438 2493 1762 1942 2027 ========== ======= ======= ======= ======== ======== ======== 
Gene Annotations:
 Optional input for gene annotations, this can contain more information about the genes than just an ID number. The annotations will be available in the differential expression results table and the optional normalised counts table. They will also be used to generate interactive Glimma_ Volcano, MD plots and tables of differential expression. The input annotation file must contain a header row and have the gene IDs in the first column. The second column will be used to label the genes in the Volcano plot and interactive Glimma plots, additional columns will be available in the Glimma interactive table. The number of rows should match that of the counts files, add NA for any gene IDs with no annotation. The Galaxy tool 
annotateMyIDs
 can be used to obtain annotations for human, mouse, fly and zebrafish. Example: ========== ========== =================================================== 
GeneID
 
Symbol
 
GeneName
 ---------- ---------- --------------------------------------------------- 11287 Pzp pregnancy zone protein 11298 Aanat arylalkylamine N-acetyltransferase 11302 Aatk apoptosis-associated tyrosine kinase 11303 Abca1 ATP-binding cassette, sub-family A (ABC1), member 1 11304 Abca4 ATP-binding cassette, sub-family A (ABC1), member 4 11305 Abca2 ATP-binding cassette, sub-family A (ABC1), member 2 ========== ========== =================================================== 
Factor Information:
 Enter factor names and groups in the tool form, or provide a tab-separated file that has the names of the samples in the first column and one header row. The sample names must be the same as the names in the columns of the count matrix. The second column should contain the primary factor levels (e.g. WT, Mut) with optional additional columns for any secondary factors. Example: ========== ============ ========= 
Sample
 
Genotype
 
Batch
 ---------- ------------ --------- WT1 WT b1 WT2 WT b2 WT3 WT b3 Mut1 Mut b1 Mut2 Mut b2 Mut3 Mut b3 ========== ============ ========= 
Factor Name:
 The name of the experimental factor being investigated e.g. Genotype, Treatment. One factor must be entered and spaces must not be used. Optionally, additional factors can be included, these are variables that might influence your experiment e.g. Batch, Gender, Subject. If additional factors are entered, an additive linear model will be used. 
Groups:
 The names of the groups for the factor. The names should only contain letters, numbers and underscores, other characters such as spaces and hyphens MUST not be used. If entered into the tool form above, the order must be the same as the samples (to which the groups correspond) are listed in the columns of the counts matrix, with the values separated by commas. If the group names begin with a number an X will be added as a prefix. 
Contrasts of Interest:
 The contrasts you wish to make between levels. A common contrast would be a simple difference between two levels: ""Mut-WT"" represents the difference between the mutant and wild type genotypes. Multiple contrasts must be entered separately using the Insert Contrast button, spaces must not be used. Alternatively, a tab-separated file can be input that has the names of the comparisons in the first column and one header row, as shown below. Example: ============= = 
Contrasts
 ------------- - Mut-WT WT-Mut ============= = 
Filter Low Counts:
 Genes with very low counts across all libraries provide little evidence for differential expression. In the biological point of view, a gene must be expressed at some minimal level before it is likely to be translated into a protein or to be biologically important. In addition, the pronounced discreteness of these counts interferes with some of the statistical approximations that are used later in the pipeline. These genes should be filtered out prior to further analysis. As a rule of thumb, genes are dropped if they can’t possibly be expressed in all the samples for any of the conditions. Users can set their own definition of genes being expressed. Usually a gene is required to have a count of 5-10 in a library to be considered expressed in that library. Users should also filter with count-per-million (CPM) rather than filtering on the counts directly, as the latter does not account for differences in library sizes between samples. Option to ignore the genes that do not show significant levels of expression, this filtering is dependent on two criteria: CPM/count and number of samples. You can specify to filter on CPM (Minimum CPM) or count (Minimum Count) values: * 
Minimum CPM:
 This is the minimum count per million that a gene must have in at least the number of samples specified under Minimum Samples. * 
Minimum Count:
 This is the minimum count that a gene must have. It can be combined with either Filter on Total Count or Minimum Samples. * 
Filter on Total Count:
 This can be used with the Minimum Count filter to keep genes with a minimum total read count. * 
Minimum Samples:
 This is the number of samples in which the Minimum CPM/Count requirement must be met in order for that gene to be kept. If the Minimum Samples filter is applied, only genes that exhibit a CPM/count greater than the required amount in at least the number of samples specified will be used for analysis. Care should be taken to ensure that the sample requirement is appropriate. In the case of an experiment with two experimental groups each with two members, if there is a change from insignificant CPM/count to significant CPM/count but the sample requirement is set to 3, then this will cause that gene to fail the criteria. When in doubt simply do not filter or consult the 
limma User's Guide
 for filtering recommendations. 
Advanced Options:
 By default error rate for multiple testing is controlled using Benjamini and Hochberg's false discovery rate control at a threshold value of 0.05. However there are options to change this to custom values. * 
Minimum log2-fold-change Required:
 In addition to meeting the requirement for the adjusted statistic for multiple testing, the observation must have an absolute log2-fold-change greater than this threshold to be considered significant, thus highlighted in the MD plot. * 
Adjusted Threshold:
 Set the threshold for the resulting value of the multiple testing control method. Only observations whose statistic falls below this value is considered significant, thus highlighted in the MD plot. * 
P-Value Adjustment Method:
 Change the multiple testing control method, the options are BH(1995) and BY(2001) which are both false discovery rate controls. There is also Holm(1979) which is a method for family-wise error rate control. 
Testing relative to a threshold (TREAT):
 If there are a lot of differentially expressed genes, a fold change threshold can be applied in addition to the P-value threshold to select genes that are more likely to be biologically significant. However, ranking by P-value and discarding genes with small logFCs can increase the false discovery rate. Using the limma TREAT function performs this analysis correctly (
McCarthy and Smyth, 2009
). 
Normalisation Method:
 The most obvious technical factor that affects the read counts, other than gene expression levels, is the sequencing depth of each RNA sample. edgeR adjusts any differential expression analysis for varying sequencing depths as represented by differing library sizes. This is part of the basic modeling procedure and flows automatically into fold-change or p-value calculations. It is always present, and doesn’t require any user intervention. The second most important technical influence on differential expression is one that is less obvious. RNA-seq provides a measure of the relative abundance of each gene in each RNA sample, but does not provide any measure of the total RNA output on a per-cell basis. This commonly becomes important when a small number of genes are very highly expressed in one sample, but not in another. The highly expressed genes can consume a substantial proportion of the total library size, causing the remaining genes to be under-sampled in that sample. Unless this RNA composition effect is adjusted for, the remaining genes may falsely appear to be down-regulated in that sample . The edgeR 
calcNormFactors
 function normalizes for RNA composition by finding a set of scaling factors for the library sizes that minimize the log-fold changes between the samples for most genes. The default method for computing these scale factors uses a trimmed mean of M values (TMM) between each pair of samples. We call the product of the original library size and the scaling factor the 
effective library size
. The effective library size replaces the original library size in all downsteam analyses. TMM is the recommended method for most RNA-Seq data where the majority (more than half) of the genes are believed not differentially expressed between any pair of the samples. You can change the normalisation method under 
Advanced Options
 above. For more information, see the 
calcNormFactors
 section in the 
edgeR User's Guide
. 
Robust Settings
 Option to use robust settings with eBayes or TREAT, used by both limma-voom and limma-trend. Using robust settings is usually recommended to protect against outlier genes, for more information see the 
limma User's Guide
 and 
Phipson et al. 2016
. This is turned on by default. 
Prior Count:
 If the limma-trend method is used, a count (
prior.count
) is added to all counts to avoid taking a log of zero, and damp down the variances of logarithms of low counts. A default of 3 is used, as recommended in the 
limma User's Guide
. 
Apply Sample Weights:
 If the limma-voom method is used, an option is available to downweight outlier samples, such that their information is still used in the statistical analysis but their impact is reduced. Use this whenever significant outliers are present. The MDS plotting tool in this package is useful for identifying outliers. For more information on this option see Liu et al. (2015). 
Outputs
 This tool outputs * a table of differentially expressed genes for each contrast of interest * a HTML report with plots and additional information Optionally, under 
Output Options
 you can choose to output * interactive Glimma plots and tables: MDS plot, and (if annotation file is input) Volcano plot and MD plot (default: Yes) * additional plots in the report and as PDFs * a normalised counts table * a library size information file * the R script used by this tool * an RData file ----- 
Citations:
 Please try to cite the appropriate articles when you publish results obtained using software, as such citation is the main means by which the authors receive credit for their work. limma Please cite the paper below for the limma software itself. Please also try to cite the appropriate methodology articles that describe the statistical methods implemented in limma, depending on which limma functions you are using. The methodology articles are listed in Section 2.1 of the 
limma User's Guide
. * Smyth GK (2005). Limma: linear models for microarray data. In: 'Bioinformatics and Computational Biology Solutions using R and Bioconductor'. R. Gentleman, V. Carey, S. Dudoit, R. Irizarry, W. Huber (eds), Springer, New York, pages 397-420. * Law CW, Chen Y, Shi W, and Smyth GK (2014). Voom: precision weights unlock linear model analysis tools for RNA-seq read counts. Genome Biology 15, R29. * Liu R, Holik AZ, Su S, Jansz N, Chen K, Leong HS, Blewitt ME, Asselin-Labat ML, Smyth GK, Ritchie ME (2015). Why weight? Modelling sample and observational level variability improves power in RNA-seq analyses. Nucleic Acids Research, 43(15), e97. * Ritchie, M. E., Diyagama, D., Neilson, J., van Laar, R., Dobrovic, A., Holloway, A., and Smyth, G. K. (2006). Empirical array quality weights for microarray data. BMC Bioinformatics 7, Article 261. edgeR Please cite the first paper for the software itself and the other papers for the various original statistical methods implemented in edgeR. See Section 1.2 in the 
edgeR User's Guide
 for more detail. * Robinson MD, McCarthy DJ and Smyth GK (2010). edgeR: a Bioconductor package for differential expression analysis of digital gene expression data. Bioinformatics 26, 139-140 * Robinson MD and Smyth GK (2007). Moderated statistical tests for assessing differences in tag abundance. Bioinformatics 23, 2881-2887 * Robinson MD and Smyth GK (2008). Small-sample estimation of negative binomial dispersion, with applications to SAGE data. Biostatistics, 9, 321-332 * McCarthy DJ, Chen Y and Smyth GK (2012). Differential expression analysis of multifactor RNA-Seq experiments with respect to biological variation. Nucleic Acids Research 40, 4288-4297 Please report problems or suggestions to: su.s@wehi.edu.au .. _limma: http://www.bioconductor.org/packages/release/bioc/html/limma.html .. _Glimma: https://bioconductor.org/packages/release/bioc/html/Glimma.html .. _here: https://f1000research.com/articles/5-1408/v3 .. _limma approach: https://www.ncbi.nlm.nih.gov/pubmed/25605792 .. _limma User's Guide: http://bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf .. _edgeR: http://www.bioconductor.org/packages/release/bioc/html/edgeR.html .. _edgeR User's Guide: https://bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf .. _McCarthy and Smyth, 2009: https://www.ncbi.nlm.nih.gov/pubmed/19176553 .. _Phipson et al. 2016: https://www.ncbi.nlm.nih.gov/pubmed/28367255"
toolshed.g2.bx.psu.edu/repos/iuc/masigpro/masigpro/1.49.3.1+galaxy1	".. class:: infomark 
What it does
 maSigPro_ is a regression based approach to find genes for which there are significant gene expression profile differences between experimental groups in time course microarray and RNA-Seq experiments. 
Inputs
 The maSigPro wrapper has two options for input data: - directly through two seperate text files containing the experiment design (edesign) and the data or - count tables generated from HTSeq-count. Count tables must be generated for each sample individually. To set up an experimental design from seperate count files you first have to select which files belong to a certain time point. Likewise you can specify which files are replicates. In a third step you have to create the experimental groups and select the related files. For a more comfortable setup in future analysis you have the option to output the generated edesign and data files. 
Output
 maSigPro_ generates a summary file containing the list of significant genes. Additionally you can obtain a PDF file containing plots of profiles and groups that visualize the clustering analysis. .. _maSigPro: https://bioconductor.org/packages/release/bioc/html/maSigPro.html"
toolshed.g2.bx.psu.edu/repos/iuc/ngsderive_strandedness/ngsderive_strandedness/4.0.0+galaxy0	"What it does
 ngsderive strandedness infers the strandedness protocol used to generate RNA-seq data by analyzing read alignments against a gene model. It can determine whether your data was generated using a Stranded-Forward, Stranded-Reverse, or Unstranded protocol. This tool is useful when you have RNA-seq data but are unsure about the library preparation protocol used. Knowing the correct strandedness is essential for accurate gene expression quantification. 
How it works
 The tool randomly samples genes from the provided gene model and examines how reads align to those genes. Based on the proportion of reads mapping in the forward vs reverse orientation, it classifies the library as: - 
Unstranded
: ~40-60% forward reads - 
Stranded-Forward
: ≥80% forward reads - 
Stranded-Reverse
: ≥80% reverse reads - 
Inconclusive
: Results don't clearly indicate a strandedness type 
Inputs
 - 
Alignment file
: Paired-end RNA-seq alignments in BAM format - 
Gene annotation
: GTF file with gene models (gzipped GTF supported) 
Output
 A tabular file with the following columns: - 
File
: Name of the input BAM file - 
ReadGroup
: Read group identifier (or ""overall"" for combined results) - 
TotalReads
: Number of reads used in the analysis - 
ForwardPct
: Percentage of reads supporting forward strandedness - 
ReversePct
: Percentage of reads supporting reverse strandedness - 
Predicted
: The inferred strandedness (Stranded-Forward, Stranded-Reverse, Unstranded, or Inconclusive) 
Notes
 - Only paired-end reads are currently supported - For best results, ensure your BAM file has sufficient read depth For more information, see the 
ngsderive documentation &lt;https://stjudecloud.github.io/ngsderive/subcommands/strandedness/&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/pizzly/pizzly/0.37.3.1	".. class:: infomark 
What it does
 pizzly_ is a program for detecting gene fusions from RNA-Seq data of cancer samples. pizzly introduces a novel approach to fusion detection, it builds on the pseudoalignment idea (that simplifies and accelerates transcript quantification) by inspecting paired reads that cannot be pseudoaligned due to conflicting matches. pizzly filters false positives, assembles new transcripts from the fusion reads, and reports candidate fusions. With pizzly, fusion detection from RNA-Seq reads can be performed 
in a matter of minutes
. .. _pizzly: https://github.com/pmelsted/pizzly ----- 
Inputs
 - one or more tabular files from kallisto quant (run with the --fusion parameter on paired-end fastqs) - a FASTA file containing the reference transcriptome. This must be the same fasta that was used to build the kallisto index. - a GTF file describing the transcriptome. Ensembl transcriptomes recommended. pizzly has been tested on Ensembl (versions 75+) and Gencode (version 19+) annotations. The latest Ensembl annotations (version 87 GTF, FASTA) are recommended for running with pizzly. ----- 
Outputs
 Two output files are created containing the filtered fusion calls - a tabular file - a FASTA file ----- 
More Information
 Fusion junctions are detected by pizzly using a two-stage method. The first stage is implemented in kallisto and detects individual reads or read pairs, whose constituent parts pseudoalign to a reference transcriptome but that in combination fail to pseudoalign. pizzly takes the output of the potential fusion junctions found by kallisto and performs a detailed analysis of the associated reads by aligning them across the putative junctions. Additionally, pizzly is annotation aware, i.e. it uses information about the genomic coordinates and gene identities of each isoform to identify possible false positives arising from repetitive sequences across the genome."
toolshed.g2.bx.psu.edu/repos/iuc/dexseq/plotdexseq/1.48.0+galaxy1	".. class:: infomark 
What it does
 This tool enables visualization of DEXSeq results for individual genes. The input is a DEXSeqResults rds file, which can be output from the DEXSeq tool, and a single gene ID or list of IDs to plot. .. _DEXSeq: http://master.bioconductor.org/packages/release/bioc/html/DEXSeq.html"
toolshed.g2.bx.psu.edu/repos/iuc/scpipe/scpipe/1.0.0+galaxy2	".. class:: infomark 
What it does
 scPipe_ is an 
R/Bioconductor package
 that integrates barcode demultiplexing, read alignment, UMI-aware gene-level quantification and quality control of raw sequencing data generated by multiple protocols that include CEL-seq, MARS-seq, Chromium 10X, Drop-seq and Smart-seq. scPipe produces a count matrix that is essential for downstream analysis along with QC metrics and a HTML report that summarises data quality. These results can be used as input for downstream analyses including normalization, visualization and statistical testing. The scPipe workflow is described in this vignette
 and examples of the report output can be found here_. Note that outlier cells are detected and removed by default but they can be kept if ""Keep outliers?"" is selected. ----- 
Inputs
 Either * Reference genome in FASTA format * Paired-end FASTQ.GZ reads * Cell barcodes TAB-separated file (Optional) OR * BAM file * Cell barcodes TAB-separated file AND * Exon annotation in ENSEMBL GFF3 format 
Read Structure
 The default read structure represents CEL-seq paired-ended reads, with one cell barcode in Read 2 Start from 6bp and UMI sequence in Read 2 Start from the first bp. So the read structure will be : 
bs1=-1, bl1=0, bs2=6, bl2=8, us=0, ul=6
. 
bs1=-1, bl1=0
 means we don't have index in Read 1 so we set a negative value to start position and zero to the length. 
bs2=6, bl2=8
 means we have index in Read 2 which starts at 6bp with 8bp length. 
us=0, ul=6
 means we have UMI from the start of Read 2 and the length is 6bp. NOTE: the zero based index is used so the index of the sequence starts from zero. For a typical Drop-seq experiment the setting will be 
bs1=-1, bl1=0, bs2=0, bl2=12, us=12, ul=8
, which means Read 1 only contains transcript and the first 12bp in Read 2 are index, followed by 8bp UMIs. ----- 
Outputs
 * Count matrix of genes in Tabular format Optionally you can choose to output * PDF of QC Plots (default is Yes) * QC metrics matrix * HTML report (if FASTQs are input) * Rscript * RData .. _scPipe: http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006361 .. _R/Bioconductor package: https://bioconductor.org/packages/release/bioc/html/scPipe.html .. _vignette: https://bioconductor.org/packages/release/bioc/vignettes/scPipe/inst/doc/scPipe_tutorial.html .. _here: http://bioinf.wehi.edu.au/scPipe/"
toolshed.g2.bx.psu.edu/repos/iuc/tximport/tximport/1.30.0	.. class:: infomark Current version only works in 'merge' mode: A single table of gene summarizations is generated with one column for each sample file. Take into account that DEseq2 package in Galaxy requires one table per sample.
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_bam2wig/5.0.3+galaxy0	"bam2wig.py ++++++++++ Visualization is the most straightforward and effective way to QC your RNA-seq data. For example, change of expression or new splicing can be easily checked by visually comparing two RNA-seq tracks using genome browser such as UCSC_, IGB_ and IGV_. 
bam2wig.py
 converts all types of RNA-seq data from BAM_ format into wiggle_ format in one-stop. wiggle_ files can then be easily converted into bigwig_. Bigwig is indexed, binary format of wiggle file, and it's particular useful to display large, continuous dataset on genome browser. Inputs ++++++++++++++ Input BAM file Alignment file in BAM format (SAM is not supported). BAM file will be sorted and indexed using samTools. Chromosome size file Tab or space separated text file with 2 columns: first column is chromosome name, second column is size of the chromosome. Chromosome names (such as ""chr1"") should be consistent between this file and BAM file. Specified wigsum (default=none) Specified wigsum. Wigsum of 100000000 equals to coverage achieved by 1 million 100nt reads. Ignore this option to disable normalization. Skip multiple Hit reads skips multiple hit reads or only use uniquely mapped reads Strand-specific (default=none) How read(s) were stranded during sequencing. If you are not sure about the strand rule, run infer_experiment.py Outputs ++++++++++++++ If RNA-seq is not strand specific, one wig file will be generated, if RNA-seq is strand specific, two wig files corresponding to Forward and Reverse will be generated. ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/ .. _UCSC: http://genome.ucsc.edu/index.html .. _IGB: http://bioviz.org/igb/ .. _IGV: http://software.broadinstitute.org/software/igv/ .. _BAM: http://genome.ucsc.edu/goldenPath/help/bam.html .. _wiggle: http://genome.ucsc.edu/goldenPath/help/wiggle.html .. _bigwig: http://genome.ucsc.edu/FAQ/FAQformat.html#format6.1"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_bam_stat/5.0.3+galaxy0	"bam_stat.py +++++++++++ This program is used to calculate reads mapping statistics from provided BAM file. This script determines ""uniquely mapped reads"" from 
mapping quality &lt;http://genome.sph.umich.edu/wiki/Mapping_Quality_Scores&gt;
, which quality the probability that a read is misplaced (Do NOT confused with sequence quality, sequence quality measures the probability that a base-calling was wrong) . Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Minimum mapping quality Minimum mapping quality for an alignment to be called ""uniquely mapped"" (default=30) Output ++++++++++++++ - Total Reads (Total records) = {Multiple mapped reads} + {Uniquely mapped} - Uniquely mapped Reads = {read-1} + {read-2} (if paired end) - Uniquely mapped Reads = {Reads map to '+'} + {Reads map to '-'} - Uniquely mapped Reads = {Splice reads} + {Non-splice reads} ----- About RSeQC +++++++++++ The RSeQC
 package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_clipping_profile/5.0.3+galaxy0	"clipping_profile.py +++++++++++++++++++ This program is used to estimate clipping profile of RNA-seq reads from BAM or SAM file. Note that to use this funciton, CIGAR strings within SAM/BAM file should have 'S' operation (This means your reads aligner should support clipped mapping). Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Minimum mapping quality Minimum mapping quality for an alignment to be considered as ""uniquely mapped"". default=30 Sequencing layout Denotes whether the sequecing was single-end (SE) or paired-end (PE). Sample Output ++++++++++++++ .. image:: $PATH_TO_IMAGES/clipping_good.png :height: 600 px :width: 600 px :scale: 80 % ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_deletion_profile/5.0.3+galaxy0	"deletion_profile.py +++++++++++++++++++ Calculate the distributions of deleted nucleotides across reads. Inputs ++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Alignment length of read It is usually set to the orignial read length. For example, all these cigar strings (""101M"", ""68M140N33M"", ""53M1D48M"") suggest the read alignment length is 101. [required] Number of aligned reads used Number of aligned reads with deletions used to calculate the deletion profile. default=1000000 Minimum mapping quality Minimum mapping quality for an alignment to be considered as ""uniquely mapped"". default=30 Sample Output ++++++++++++++ .. image:: $PATH_TO_IMAGES/out.deletion_profile.png :height: 600 px :width: 600 px :scale: 80 % ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_FPKM_count/5.0.3+galaxy0	"FPKM_count.py +++++++++++++ Given a BAM file and reference gene model, this program will calculate the raw read count, FPM (fragments per million), and FPKM (fragments per million mapped reads per kilobase exon) for each gene in a BED file. For strand specific RNA-seq data, program will assign read to its parental gene according to strand rule, if you don't know the strand rule, run infer_experiment.py. Please note that chromosome ID, genome cooridinates should be concordant between BAM and BED files. Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Reference gene model Gene model in BED format. Strand sequencing type (default=none) See Infer Experiment tool if uncertain. Options ++++++++++++++ Skip Multiple Hit Reads Use Multiple hit reads or use only uniquely mapped reads. Minimum mapping quality Minimum mapping quality (phred scaled) for an alignment to be called ""uniquely mapped"". default=30 Only use exonic reads Renders program only used exonic (UTR exons and CDS exons) reads, otherwise use all reads. Single Reads How to count read-pairs that only have one end mapped. 0: ignore it. 0.5: treat it as half fragment. 1: treat it as whole fragment. default=1 Sample Output ++++++++++++++ ====== ========= ========= ========= ========= =========== ========== ============ ============ #chrom st end accession mRNA_size gene_strand Frag_count FPM FPKM ====== ========= ========= ========= ========= =========== ========== ============ ============ chr1 100652477 100715409 NM_001918 10815.0 ‘-‘ 5498.0 191.73788949 17.728884835 chr1 175913961 176176380 NM_022457 2789.0 ‘-‘ 923.0 32.188809021 11.541344217 chr1 150980972 151008189 NM_021222 2977.0 ‘+’ 687.0 23.958517657 8.0478729115 chr1 6281252 6296044 NM_012405 4815.0 ‘-‘ 1396.0 48.684265866 10.11095864 chr1 20959947 20978004 NM_032409 2660.0 ‘+’ 509.0 17.750925018 6.6732800821 chr1 32479294 32509482 NM_006559 2891.0 ‘+’ 2151.0 75.014223408 25.947500314 ====== ========= ========= ========= ========= =========== ========== ============ ============ ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_geneBody_coverage/5.0.3+galaxy0	"geneBody_coverage.py Read coverage over gene body. This module is used to check if read coverage is uniform and if there is any 5\'/3\' bias. This module scales all transcripts to 100 nt and calculates the number of reads covering each nucleotide position. Finally, it generates plots illustrating the coverage profile along the gene body. If 3 or more BAM files were provided. This program generates a lineGraph and a heatmap. If fewer than 3 BAM files were provided, only lineGraph is generated. See below for examples. When heatmap is generated, samples are ranked by the ""skewness"" of the coverage: Sample with best (worst) coverage will be displayed at the top (bottom) of the heatmap. Coverage skewness was measured by 
Pearson’s skewness coefficients &lt;http://en.wikipedia.org/wiki/Skewness#Pearson.27s_skewness_coefficients&gt;
 .. image:: $PATH_TO_IMAGES/geneBody_workflow.png :width: 800 px :scale: 80 % ## Inputs Input BAM/SAM file Alignment file in BAM/SAM format. Reference gene model Gene Model in BED format. Minimum mRNA length Minimum mRNA length (bp). mRNA that are shorter than this value will be skipped (default is 100). ## Outputs Text Table that includes the data used to generate the plots R Script R script file that reads the data and generates the plot PDF The final plot, in PDF format Example plots: .. image:: $PATH_TO_IMAGES/Aug_26.geneBodyCoverage.curves.png :height: 600 px :width: 600 px :scale: 80 % .. image:: $PATH_TO_IMAGES/Aug_26.geneBodyCoverage.heatMap.png :height: 600 px :width: 600 px :scale: 80 % ----- About RSeQC +++++++++++ The RSeQC
 package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_geneBody_coverage2/5.0.3+galaxy0	"geneBody_coverage2.py +++++++++++++++++++++ Similar to geneBody_coverage.py. This module takes bigwig instead of BAM as input, and thus requires much less memory. The BigWig file could be arbitrarily large. Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Reference gene model Gene Model in BED format. Outputs ++++++++++++++ Read coverage over gene body. This module is used to check if reads coverage is uniform and if there is any 5’/3’ bias. This module scales all transcripts to 100 nt and calculates the number of reads covering each nucleotide position. Finally, it generates a plot illustrating the coverage profile along the gene body. NOTE: this module requires lots of memory for large BAM files, because it load the entire BAM file into memory. We add another script ""geneBody_coverage2.py"" into v2.3.1 which takes bigwig (instead of BAM) as input. It only use 200M RAM, but users need to convert BAM into WIG, and then WIG into BigWig. Example output: .. image:: $PATH_TO_IMAGES/geneBody_coverage.png :height: 600 px :width: 600 px :scale: 80 % ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_read_hexamer/5.0.3+galaxy0	"read_hexamer.py +++++++++++++++++++++ Calculate hexamer (6mer) frequency. If ‘-r’ was specified, hexamer frequency is also calculated for the reference genome. If ‘-g’ was provided, hexamer frequency is also calculated for the mRNA sequences. Inputs ++++++++++++++ Input reads file Read sequences in fasta or fastq format. Reference Genome Reference genome sequence in fasta format. Reference Gene Reference mRNA sequences in fasta format. Outputs ++++++++++++++ Tabular file of hexamer frequences in for each input. ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_infer_experiment/5.0.3+galaxy0	"infer_experiment.py +++++++++++++++++++ This program is used to speculate how RNA-seq sequencing were configured, especially how reads were stranded for strand-specific RNA-seq data, through comparing reads' mapping information to the underneath gene model. Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Reference gene model Gene model in BED format. Number of usable sampled reads (default=200000) Number of usable reads sampled from SAM/BAM file. More reads will give more accurate estimation, but make program little slower. Outputs +++++++ For pair-end RNA-seq, there are two different ways to strand reads (such as Illumina ScriptSeq protocol): 1. 1++,1--,2+-,2-+ * read1 mapped to '+' strand indicates parental gene on '+' strand * read1 mapped to '-' strand indicates parental gene on '-' strand * read2 mapped to '+' strand indicates parental gene on '-' strand * read2 mapped to '-' strand indicates parental gene on '+' strand 2. 1+-,1-+,2++,2-- * read1 mapped to '+' strand indicates parental gene on '-' strand * read1 mapped to '-' strand indicates parental gene on '+' strand * read2 mapped to '+' strand indicates parental gene on '+' strand * read2 mapped to '-' strand indicates parental gene on '-' strand For single-end RNA-seq, there are also two different ways to strand reads: 1. ++,-- * read mapped to '+' strand indicates parental gene on '+' strand * read mapped to '-' strand indicates parental gene on '-' strand 2. +-,-+ * read mapped to '+' strand indicates parental gene on '-' strand * read mapped to '-' strand indicates parental gene on '+' strand Example Output ++++++++++++++ 
Example1
 :: ========================================================= This is PairEnd Data :: Fraction of reads explained by ""1++,1--,2+-,2-+"": 0.4992 Fraction of reads explained by ""1+-,1-+,2++,2--"": 0.5008 Fraction of reads explained by other combinations: 0.0000 ========================================================= 
Conclusion
: We can infer that this is NOT a strand specific because 50% of reads can be explained by ""1++,1--,2+-,2-+"", while the other 50% can be explained by ""1+-,1-+,2++,2--"". 
Example2
 :: ============================================================ This is PairEnd Data Fraction of reads explained by ""1++,1--,2+-,2-+"": 0.9644 :: Fraction of reads explained by ""1+-,1-+,2++,2--"": 0.0356 Fraction of reads explained by other combinations: 0.0000 ============================================================ 
Conclusion
: We can infer that this is a strand-specific RNA-seq data. strandness of read1 is consistent with that of gene model, while strandness of read2 is opposite to the strand of reference gene model. 
Example3
 :: ========================================================= This is SingleEnd Data :: Fraction of reads explained by ""++,--"": 0.9840 :: Fraction of reads explained by ""+-,-+"": 0.0160 Fraction of reads explained by other combinations: 0.0000 ========================================================= 
Conclusion
: This is single-end, strand specific RNA-seq data. Strandness of reads are concordant with strandness of reference gene. ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_inner_distance/5.0.3+galaxy0	"inner_distance.py +++++++++++++++++ This module is used to calculate the inner distance (or insert size) between two paired RNA reads. The distance is the mRNA length between two paired fragments. We first determine the genomic (DNA) size between two paired reads: D_size = read2_start - read1_end, then * if two paired reads map to the same exon: inner distance = D_size * if two paired reads map to different exons:inner distance = D_size - intron_size * if two paired reads map non-exonic region (such as intron and intergenic region): inner distance = D_size * The inner_distance might be a negative value if two fragments were overlapped. NOTE: Not all read pairs were used to estimate the inner distance distribution. Those low quality, PCR duplication, multiple mapped reads were skipped. Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Reference gene model Gene model in BED format. Estimated Upper/Lower Bounds (defaults=250 and -250) Estimated upper/lower bounds of inner distance (bp). Step size (default=5) Step size of histogram Output ++++++++++++++ 1. output.inner_distance.txt: - first column is read ID - second column is inner distance. Could be negative value if PE reads were overlapped or mapping error (e.g. Read1_start &lt; Read2_start, while Read1_end >> Read2_end due to spliced mapping of read1) - third column indicates how paired reads were mapped: PE_within_same_exon, PE_within_diff_exon,PE_reads_overlap 2. output..inner_distance_freq.txt: - inner distance starts - inner distance ends - number of read pairs - note the first 2 columns are left side half open interval 3. output.inner_distance_plot.r: R script to generate histogram 4. output.inner_distance_plot.pdf: histogram plot .. image:: $PATH_TO_IMAGES/inner_distance.png :height: 600 px :width: 600 px :scale: 80 % ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_insertion_profile/5.0.3+galaxy0	"insertion_profile.py ++++++++++++++++++++ Calculate the distributions of inserted nucleotides across reads. Note that to use this funciton, CIGAR strings within SAM/BAM file should have ‘I’ operation. Inputs ++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Minimum mapping quality Minimum mapping quality for an alignment to be considered as ""uniquely mapped"". default=30 Sequencing layout Denotes whether the sequecing was single-end (SE) or paired-end (PE). Sample Output ++++++++++++++ Read-1 insertion profile: .. image:: $PATH_TO_IMAGES/out.insertion_profile.R1.png :height: 600 px :width: 600 px :scale: 80 % Read-2 insertion profile: .. image:: $PATH_TO_IMAGES/out.insertion_profile.R2.png :height: 600 px :width: 600 px :scale: 80 % ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_junction_annotation/5.0.3+galaxy0	"junction_annotation.py ++++++++++++++++++++++ For a given alignment file (-i) in BAM or SAM format and a reference gene model (-r) in BED format, this program will compare detected splice junctions to reference gene model. splicing annotation is performed in two levels: splice event level and splice junction level. * splice event: An RNA read, especially long read, can be spliced 2 or more times, each time is called a splicing event; In this sense, 100 spliced reads can produce >= 100 splicing events. * splice junction: multiple splicing events spanning the same intron can be consolidated into one splicing junction. All detected junctions can be grouped to 3 exclusive categories: 1. Annotated: The junction is part of the gene model. Both splice sites, 5' splice site (5'SS) and 3'splice site (3'SS) can be annotated by reference gene model. 2. complete_novel: Complete new junction. Neither of the two splice sites cannot be annotated by gene model 3. partial_novel: One of the splice site (5'SS or 3'SS) is new, while the other splice site is annotated (known) Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Reference gene model Gene model in BED format. Minimum intron length (default=50) Minimum intron length (bp). Output ++++++++++++++ 1. output.junc.anno.junction.xls: - chrom ID - start position of junction (coordinate is 0 based) - end position of junction (coordinate is 1 based) - number of splice events supporting this junction - 'annotated', 'complete_novel' or 'partial_novel'. 2. output.anno.junction_plot.r: R script to generate pie chart 3. output.splice_junction.pdf: plot of splice junctions 4. output.splice_events.pdf: plot of splice events .. image:: $PATH_TO_IMAGES/junction.png :height: 400 px :width: 850 px :scale: 80 % ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_junction_saturation/5.0.3+galaxy0	"junction_saturation.py ++++++++++++++++++++++ It's very important to check if current sequencing depth is deep enough to perform alternative splicing analyses. For a well annotated organism, the number of expressed genes in particular tissue is almost fixed so the number of splice junctions is also fixed. The fixed splice junctions can be predetermined from reference gene model. All (annotated) splice junctions should be rediscovered from a saturated RNA-seq data, otherwise, downstream alternative splicing analysis is problematic because low abundance splice junctions are missing. This module checks for saturation by resampling 5%, 10%, 15%, ..., 95% of total alignments from BAM or SAM file, and then detects splice junctions from each subset and compares them to reference gene model. Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Reference gene model Gene model in BED format. Sampling Percentiles - Upper Bound, Lower Bound, Sampling Increment (defaults= 100, 5, and 5) Sampling starts from the Lower Bound and increments to the Upper Bound at the rate of the Sampling Increment. Minimum intron length (default=50) Minimum intron length (bp). Minimum coverage (default=1) Minimum number of supportting reads to call a junction. Output ++++++++++++++ 1. output.junctionSaturation_plot.r: R script to generate plot 2. output.junctionSaturation_plot.pdf .. image:: $PATH_TO_IMAGES/junction_saturation.png :height: 600 px :width: 600 px :scale: 80 % In this example, current sequencing depth is almost saturated for ""known junction"" (red line) detection because the number of ""known junction"" reaches a plateau. In other words, nearly all ""known junctions"" (expressed in this particular tissue) have already been detected, and continue sequencing will not detect additional ""known junction"" and will only increase junction coverage (i.e. junction covered by more reads). While current sequencing depth is not saturated for novel junctions (green). ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_mismatch_profile/5.0.3+galaxy0	"mismatch_profile.py +++++++++++++++++++ Calculate the distribution of mismatches across reads. Note that the “MD” tag must exist in BAM file. Inputs ++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Alignment length of read It is usually set to the orignial read length. For example, all these cigar strings (""101M"", ""68M140N33M"", ""53M1D48M"") suggest the read alignment length is 101. [required] Number of aligned reads used Number of aligned reads with deletions used to calculate the deletion profile. default=1000000 Minimum mapping quality Minimum mapping quality for an alignment to be considered as ""uniquely mapped"". default=30 Sample Output ++++++++++++++ .. image:: $PATH_TO_IMAGES/mismatch_profile.png :height: 600 px :width: 600 px :scale: 80 % ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_RNA_fragment_size/5.0.3+galaxy0	"RNA_fragment_size.py ++++++++++++++++++++ Calculate fragment size for each gene/transcript. For each transcript, it will report : 1) Number of fragment that was used to estimate mean, median, std (see below). 2) mean of fragment size 3) median of fragment size 4) stdev of fragment size. Inputs ++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Reference gene model Reference gene model in BED format. Must be strandard 12-column BED file. [required] Minimum mapping quality Minimum mapping quality for an alignment to be considered as ""uniquely mapped"". default=30 Minimum number of fragments Minimum number of fragments. default=3 ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_RPKM_saturation/5.0.3+galaxy0	"RPKM_saturation.py ++++++++++++++++++ The precision of any sample statitics (RPKM) is affected by sample size (sequencing depth); \'resampling\' or \'jackknifing\' is a method to estimate the precision of sample statistics by using subsets of available data. This module will resample a series of subsets from total RNA reads and then calculate RPKM value using each subset. By doing this we are able to check if the current sequencing depth was saturated or not (or if the RPKM values were stable or not) in terms of genes' expression estimation. If sequencing depth was saturated, the estimated RPKM value will be stationary or reproducible. By default, this module will calculate 20 RPKM values (using 5%, 10%, ... , 95%,100% of total reads) for each transcripts. In the output figure, Y axis is ""Percent Relative Error"" or ""Percent Error"" which is used to measures how the RPKM estimated from subset of reads (i.e. RPKMobs) deviates from real expression level (i.e. RPKMreal). However, in practice one cannot know the RPKMreal. As a proxy, we use the RPKM estimated from total reads to approximate RPKMreal. .. image:: $PATH_TO_IMAGES/RelativeError.png :height: 80 px :width: 400 px :scale: 100 % Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Reference gene model Gene model in BED format. Strand sequencing type (default=none) See Infer Experiment tool if uncertain. Options ++++++++++++++ Skip Multiple Hit Reads Use Multiple hit reads or use only uniquely mapped reads. Only use exonic reads Renders program only used exonic (UTR exons and CDS exons) reads, otherwise use all reads. Output ++++++++++++++ 1. output..eRPKM.xls: RPKM values for each transcript 2. output.rawCount.xls: Raw count for each transcript 3. output.saturation.r: R script to generate plot 4. output.saturation.pdf: .. image:: $PATH_TO_IMAGES/saturation.png :height: 600 px :width: 600 px :scale: 80 % - All transcripts were sorted in ascending order according to expression level (RPKM). Then they are divided into 4 groups: 1. Q1 (0-25%): Transcripts with expression level ranked below 25 percentile. 2. Q2 (25-50%): Transcripts with expression level ranked between 25 percentile and 50 percentile. 3. Q3 (50-75%): Transcripts with expression level ranked between 50 percentile and 75 percentile. 4. Q4 (75-100%): Transcripts with expression level ranked above 75 percentile. - BAM/SAM file containing more than 100 million alignments will make module very slow. - Follow example below to visualize a particular transcript (using R console):: pdf(""xxx.pdf"") #starts the graphics device driver for producing PDF graphics x &lt;- seq(5,100,5) #resampling percentage (5,10,15,...,100) rpkm &lt;- c(32.95,35.43,35.15,36.04,36.41,37.76,38.96,38.62,37.81,38.14,37.97,38.58,38.59,38.54,38.67, 38.67,38.87,38.68, 38.42, 38.23) #Paste RPKM values calculated from each subsets scatter.smooth(x,100*abs(rpkm-rpkm[length(rpkm)])/(rpkm[length(rpkm)]),type=""p"",ylab=""Precent Relative Error"",xlab=""Resampling Percentage"") dev.off() #close graphical device .. image:: $PATH_TO_IMAGES/saturation_eg.png :height: 600 px :width: 600 px :scale: 80 % ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_read_distribution/5.0.3+galaxy0	"read_distribution.py ++++++++++++++++++++ Provided a BAM/SAM file and reference gene model, this module will calculate how mapped reads were distributed over genome feature (like CDS exon, 5'UTR exon, 3' UTR exon, Intron, Intergenic regions). When genome features are overlapped (e.g. a region could be annotated as both exon and intron by two different transcripts) , they are prioritize as: CDS exons > UTR exons > Introns > Intergenic regions, for example, if a read was mapped to both CDS exon and intron, it will be assigned to CDS exons. * ""Total Reads"": This does NOT include those QC fail,duplicate and non-primary hit reads * ""Total Tags"": reads spliced once will be counted as 2 tags, reads spliced twice will be counted as 3 tags, etc. And because of this, ""Total Tags"" >= ""Total Reads"" * ""Total Assigned Tags"": number of tags that can be unambiguously assigned the 10 groups (see below table). * Tags assigned to ""TSS_up_1kb"" were also assigned to ""TSS_up_5kb"" and ""TSS_up_10kb"", tags assigned to ""TSS_up_5kb"" were also assigned to ""TSS_up_10kb"". Therefore, ""Total Assigned Tags"" = CDS_Exons + 5'UTR_Exons + 3'UTR_Exons + Introns + TSS_up_10kb + TES_down_10kb. * When assign tags to genome features, each tag is represented by its middle point. RSeQC cannot assign those reads that: * hit to intergenic regions that beyond region starting from TSS upstream 10Kb to TES downstream 10Kb. * hit to regions covered by both 5'UTR and 3' UTR. This is possible when two head-to-tail transcripts are overlapped in UTR regions. * hit to regions covered by both TSS upstream 10Kb and TES downstream 10Kb. Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Reference gene model Gene model in BED format. Sample Output ++++++++++++++ Output: =============== ============ =========== =========== Group Total_bases Tag_count Tags/Kb =============== ============ =========== =========== CDS_Exons 33302033 20002271 600.63 5'UTR_Exons 21717577 4408991 203.01 3'UTR_Exons 15347845 3643326 237.38 Introns 1132597354 6325392 5.58 TSS_up_1kb 17957047 215331 11.99 TSS_up_5kb 81621382 392296 4.81 TSS_up_10kb 149730983 769231 5.14 TES_down_1kb 18298543 266161 14.55 TES_down_5kb 78900674 729997 9.25 TES_down_10kb 140361190 896882 6.39 =============== ============ =========== =========== ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_read_duplication/5.0.3+galaxy0	"read_duplication.py +++++++++++++++++++ Two strategies were used to determine reads duplication rate: * Sequence based: reads with exactly the same sequence content are regarded as duplicated reads. * Mapping based: reads mapped to the same genomic location are regarded as duplicated reads. For splice reads, reads mapped to the same starting position and splice the same way are regarded as duplicated reads. Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Upper Limit of Plotted Duplicated Times (default=500) Only used for plotting. Output ++++++++++++++ 1. output.dup.pos.DupRate.xls: Read duplication rate determined from mapping position of read. First column is ""occurrence"" or duplication times, second column is number of uniquely mapped reads. 2. output.dup.seq.DupRate.xls: Read duplication rate determined from sequence of read. First column is ""occurrence"" or duplication times, second column is number of uniquely mapped reads. 3. output.DupRate_plot.r: R script to generate pdf file 4. output.DupRate_plot.pdf: graphical output generated from R script .. image:: $PATH_TO_IMAGES/duplicate.png :height: 600 px :width: 600 px :scale: 80 % ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_read_GC/5.0.3+galaxy0	"read_GC.py ++++++++++ Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Output ++++++++++++++ 1. output.GC.xls: Two column, plain text file, first column is GC%, second column is read count 2. output.GC_plot.r: R script to generate pdf file. 3. output.GC_plot.pdf: graphical output generated from R script. .. image:: $PATH_TO_IMAGES/read_gc.png :height: 600 px :width: 600 px :scale: 80 % ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_read_NVC/5.0.3+galaxy0	"read_NVC.py +++++++++++ This module is used to check the nucleotide composition bias. Due to random priming, certain patterns are over represented at the beginning (5'end) of reads. This bias could be easily examined by NVC (Nucleotide versus cycle) plot. NVC plot is generated by overlaying all reads together, then calculating nucleotide composition for each position of read (or each sequencing cycle). In ideal condition (genome is random and RNA-seq reads is randomly sampled from genome), we expect A%=C%=G%=T%=25% at each position of reads. NOTE: this program expect a fixed read length Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Include N,X in NVC plot Plots N and X alongside A, T, C, and G in plot. Output ++++++++++++++ This module is used to check the nucleotide composition bias. Due to random priming, certain patterns are over represented at the beginning (5'end) of reads. This bias could be easily examined by NVC (Nucleotide versus cycle) plot. NVC plot is generated by overlaying all reads together, then calculating nucleotide composition for each position of read (or each sequencing cycle). In ideal condition (genome is random and RNA-seq reads is randomly sampled from genome), we expect A%=C%=G%=T%=25% at each position of reads. 1. output.NVC.xls: plain text file, each row is position of read (or sequencing cycle), each column is nucleotide (A,C,G,T,N,X) 2. output.NVC_plot.r: R script to generate NVC plot. 3. output.NVC_plot.pdf: NVC plot. .. image:: $PATH_TO_IMAGES/NVC_plot.png :height: 600 px :width: 600 px :scale: 80 % ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_read_quality/5.0.3+galaxy0	"read_quality.py +++++++++++++++ According to SAM specification, if Q is the character to represent ""base calling quality"" in SAM file, then Phred Quality Score = ord(Q) - 33. Here ord() is python function that returns an integer representing the Unicode code point of the character when the argument is a unicode object, for example, ord('a') returns 97. Phred quality score is widely used to measure ""reliability"" of base-calling, for example, phred quality score of 20 means there is 1/100 chance that the base-calling is wrong, phred quality score of 30 means there is 1/1000 chance that the base-calling is wrong. In general: Phred quality score = -10xlog(10)P, here P is probability that base-calling is wrong. Inputs ++++++++++++++ Input BAM/SAM file Alignment file in BAM/SAM format. Ignore phred scores less than this number (default=1000) To avoid making huge vector in R, nucleotide with certain phred score represented less than this number will be ignored. Increase this number save more memory while reduce precision. This option only applies to the 'boxplot'. Output ++++++++++++++ 1. output.qual.r 2. output.qual.boxplot.pdf .. image:: $PATH_TO_IMAGES/36mer.qual.plot.png :height: 600 px :width: 600 px :scale: 80 % 3. output.qual.heatmap.pdf .. image:: $PATH_TO_IMAGES/36mer.qual.heatmap.png :height: 600 px :width: 600 px :scale: 80 % Heatmap: use different color to represent nucleotide density (""blue""=low density,""orange""=median density,""red""=high density"") ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_tin/5.0.3+galaxy0	"tin.py This program is designed to evaluate RNA integrity at transcript level. TIN (transcript integrity number) is named in analogous to RIN (RNA integrity number). RIN (RNA integrity number) is the most widely used metric to evaluate RNA integrity at sample (or transcriptome) level. It is a very useful preventive measure to ensure good RNA quality and robust, reproducible RNA sequencing. However, it has several weaknesses: * RIN score (1 <= RIN <= 10) is not a direct measurement of mRNA quality. RIN score heavily relies on the amount of 18S and 28S ribosome RNAs, which was demonstrated by the four features used by the RIN algorithm: the “total RNA ratio” (i.e. the fraction of the area in the region of 18S and 28S compared to the total area under the curve), 28S-region height, 28S area ratio and the 18S:28S ratio24. To a large extent, RIN score was a measure of ribosome RNA integrity. However, in most RNA-seq experiments, ribosome RNAs were depleted from the library to enrich mRNA through either ribo-minus or polyA selection procedure. * RIN only measures the overall RNA quality of an RNA sample. However, in real situation, the degradation rate may differs significantly among transcripts, depending on factors such as “AU-rich sequence”, “transcript length”, “GC content”, “secondary structure” and the “RNA-protein complex”. Therefore, RIN is practically not very useful in downstream analysis such as adjusting the gene expression count. * RIN has very limited sensitivity to measure substantially degraded RNA samples such as preserved clinical tissues. (ref: https://www.scribd.com/document/352764986/DV200-Technote-Truseq-Rna-Access). To overcome these limitations, we developed TIN, an algorithm that is able to measure RNA integrity at transcript level. TIN calculates a score (0 <= TIN <= 100) for each expressed transcript, however, the medTIN (i.e. meidan TIN score across all the transcripts) can also be used to measure the RNA integrity at sample level. Below plots demonstrated TIN is a useful metric to measure RNA integrity in both transcriptome-wise and transcript-wise, as demonstrated by the high concordance with both RIN and RNA fragment size (estimated from RNA-seq read pairs). ## Inputs Input BAM/SAM file Alignment file in BAM/SAM format. Reference gene model Gene Model in BED format. Must be standard 12-column BED file. Minimum coverage Minimum number of reads mapped to a tracript (default is 10). Sample size Number of equal-spaced nucleotide positions picked from mRNA. Note: if this number is larger than the length of mRNA (L), it will be halved until it’s smaller than L (default is 100). Subtract background Subtract background noise (estimated from intronic reads). Only use this option if there are substantial intronic reads. ## Outputs Text Table that includes the gene identifier (geneID), chromosome (chrom), transcript start (tx_start), transcript end (tx_end), and transcript integrity number (TIN). Example output: ------ ----- ---------- --------- ------------- geneID chrom tx_start tx_end TIN ------ ----- ---------- --------- ------------- ABCC2 chr10 101542354 101611949 67.6446525761 IPMK chr10 59951277 60027694 86.383618429 RUFY2 chr10 70100863 70167051 43.8967503948 ------ ----- ---------- --------- ------------- ----- About RSeQC +++++++++++ The RSeQC_ package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. ""Basic modules"" quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while ""RNA-seq specific modules"" investigate sequencing saturation status of both splicing junction detection and expression estimation, mapped reads clipping profile, mapped reads distribution, coverage uniformity over gene body, reproducibility, strand specificity and splice junction annotation. The RSeQC package is licensed under the GNU GPL v3 license. .. image:: $PATH_TO_IMAGES/logo.png .. _RSeQC: http://rseqc.sourceforge.net/"
toolshed.g2.bx.psu.edu/repos/devteam/weightedaverage/wtavg/1.0.1	".. class:: infomark 
What it does
 For each interval in your first dataset, this tool calculates the weighted average value of the overlapping features in your second dataset. - When a genomic interval partially or totally overlaps a single genomic feature, the value of that genomic feature is assigned to the genomic interval. - When a genomic interval partially or totally overlaps with more than one genomic features, the average of the values of the overlapping genomic features weighted by the corresponding number of overlapping bases is assigned to the genomic interval. - When a genomic interval does not overlap with any genomic feature, 'NA' will be assigned as it's value. ----- .. class:: warningmark 
Note
 The input datasets should be in 
bed
 or 
interval
 format. Please use ""edit attributes""/pencil icon to specify the column containing the values for the features in the second dataset as 
name/identifier
 column. The output will contain all the columns in the first input plus a new column containing the assigned value for each interval. ----- 
Example
 - Suppose our first dataset contains the following 
genomic intervals
:: chr start stop chr1 1000 2000 chr1 3000 5000 chr1 8000 9000 - and our second dataset contains the following 
genomic features
 each having an associated value (in fourth column) :: chr start stop name chr1 900 1200 0.5 chr1 2900 3100 0.2 chr1 4800 5100 0.8 - For each 
genomic interval
 in our first dataset, this tool calculates the weighted average value of the overlapping 
genomic features
 in our second dataset :: chr1 1000 2000 0.5 chr1 3000 5000 0.6 chr1 8000 9000 NA"
toolshed.g2.bx.psu.edu/repos/devteam/getindelrates_3way/indelRates_3way/1.0.0	".. class:: infomark 
What it does
 This tool estimates the insertion and deletion rates for alignments in a window of specified size. Rates are computed over the total adjusted lengths (adjusted by disregarding masked bases) of all the alignments blocks from the indel file that fall within that window. ----- .. class:: warningmark 
Note
 This tool only works on the output of the 'Estimate Indel Rates for 3-way alignments' tool."
toolshed.g2.bx.psu.edu/repos/devteam/microsats_mutability/microsats_mutability1/1.1.0	".. class:: infomark 
What it does
 This tool computes microsatellite mutability for the orthologous microsatellites fetched from 'Extract Orthologous Microsatellites from pair-wise alignments' tool. Mutability is computed according to the method described in the following paper: 
Webster et al., Microsatellite evolution inferred from human-chimpanzee genomic sequence alignments, Proc Natl Acad Sci 2002 June 25; 99(13): 8748-8753
 ----- .. class:: warningmark 
Note
 The user selected group and subgroup by features, the computed mutability and the count of the number of repeats used to compute that mutability are added as columns to the output."
toolshed.g2.bx.psu.edu/repos/devteam/microsats_alignment_level/microsats_align1/1.0.0	".. class:: infomark 
What it does
 This tool uses a modified version of SPUTNIK to fetch microsatellite repeats from the input fasta sequences and extracts orthologous repeats from the sputnik output. The modified version allows detection of mononucleotide microsatellites. More information on SPUTNIK can be found on this website_. The modified version is available here_. ----- .. class:: warningmark 
Note
 - Any block/s not containing exactly 2 species will be omitted. - This tool will filter out microsatellites based on the user input values for minimum distance and repeat number thresholds. Further, this tool will also filter out microsatellites that have no orthologous microsatellites in one of the species. .. _website: http://espressosoftware.com/pages/sputnik.jsp .. _here: http://www.bx.psu.edu/svn/universe/dependencies/sputnik/"
toolshed.g2.bx.psu.edu/repos/devteam/featurecounter/featureCoverage1/2.0.0	".. class:: infomark 
What it does
 This tool finds the coverage of intervals in the first dataset on intervals in the second dataset. The coverage and count are appended as 4 new columns in the resulting dataset. ----- 
Example
 - If 
First dataset
 consists of the following windows:: chrX 1 10001 seg 0 - chrX 10001 20001 seg 0 - chrX 20001 30001 seg 0 - chrX 30001 40001 seg 0 - - and 
Second dataset
 consists of the following exons:: chrX 5000 6000 seg2 0 - chrX 5500 7000 seg2 0 - chrX 9000 22000 seg2 0 - chrX 24000 34000 seg2 0 - chrX 36000 38000 seg2 0 - - the 
Result
 is the coverage of exons of the second dataset in each of the windows contained in first dataset:: chrX 1 10001 seg 0 - 3001 0.3001 2 1 chrX 10001 20001 seg 0 - 10000 1.0 1 0 chrX 20001 30001 seg 0 - 8000 0.8 0 2 chrX 30001 40001 seg 0 - 5999 0.5999 1 1 - To clarify, the following line of output ( added columns are indexed by a, b and c ):: a b c d chrX 1 10001 seg 0 - 3001 0.3001 2 1 implies that 2 exons (c) fall fully in this window (chrX:1-10001), 1 exon (d) partially overlaps this window, and these 3 exons cover 30.01% (c) of the window size, spanning 3001 nucleotides (a). * a: number of nucleotides in this window covered by the features in (c) and (d) - features overlapping with each other will be merged to calculate (a) * b: fraction of window size covered by features in (c) and (d) - features overlapping with each other will be merged to calculate (b) * c: number of features in the 2nd dataset that fall 
completely
 within this window * d: number of features in the 2nd dataset that 
partially
 overlap this window"
toolshed.g2.bx.psu.edu/repos/devteam/getindels_2way/getIndels_2way/1.0.0	".. class:: infomark 
What it does
 This tool estimates the number of indels for every alignment block of the MAF file. ----- .. class:: warningmark 
Note
 Any block/s not containing exactly 2 species will be omitted. ----- 
Example
 - For the following alignment block:: a score=7233.0 s hg18.chr1 100 35 + 247249719 AT--GACTGAGGACTTAGTTTAAGATGTTCCTACT s rheMac2.chr11 200 31 + 134511895 ATAAG-CGGACGACTTAGTTTAAGATGTTCC---- - running this tool will return:: #Block Source Seq1_Start Seq1_End Seq2_Start Seq2_End Indel_length 1 hg18.chr1 101 102 202 204 2 1 rheMac2.chr11 103 104 204 205 1 1 rheMac2.chr11 129 133 229 230 4"
toolshed.g2.bx.psu.edu/repos/devteam/indels_3way/indels_3way/1.0.3	".. class:: infomark 
What it does
 This tool consists of the first module from the computational pipeline to identify indels as described in Kvikstad et al., 2007. Note that the generated output does not include subsequent filtering steps. Deletions in a particular species are identified as one or more consecutive gap columns within an alignment block, given that the orthologous positions in the other two species contain nucleotides of equal length. Similarly, insertions in a particular species are identified as one or more consecutive nucleotide columns within an alignment block, given that the orthologous positions in the other two species contain gaps. 
Kvikstad E. M. et al. (2007). A Macaques-Eye View of Human Insertions and Deletions: Differences in Mechanisms. PLoS Computational Biology 3(9):e176
 ----- .. class:: warningmark 
Note
 Any block/s not containing exactly 3 sequences will be omitted."
toolshed.g2.bx.psu.edu/repos/devteam/substitutions/substitutions1/1.0.1	".. class:: infomark 
What it does
 This tool takes a pairwise MAF file as input and fetches substitutions per alignment block. ----- .. class:: warningmark 
Note
 Any block/s not containing exactly two sequences, will be omitted."
toolshed.g2.bx.psu.edu/repos/guru-ananda/rhmm/hmm_1/1.0.0	".. class:: infomark 
What it does
 This tool uses the 'HMMFit' and 'viterbi' functions from 'RHmm' library from R statistical package to fit an Hidden Markov Model using Baum-Welch algorithm, and calculate the optimal hidden states sequence using Viterbi's algorithm. It returns two outputs - one containing summary statistics for HMMFit, and the other containing state numbers appended as a new column to the input data. 
Ollivier TARAMASCO and Sebastian Bauer (2010). RHmm: Hidden Markov Models simulations and estimations. R package version 1.4.4. http://CRAN.R-project.org/package=RHmm.
 ----- .. class:: warningmark 
Note
 The tool fails if any of the observation columns contain non-numeric data."
toolshed.g2.bx.psu.edu/repos/guru-ananda/heatmap/heatmap_1/1.0.0	".. class:: infomark 
What it does
 This tool uses the 'heatmap' function from R statistical package to draw heatmap using numeric data values contained in columns of a dataset. 
R Development Core Team (2009). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0, URL http://www.R-project.org.
 ----- .. class:: warningmark If ""Remove NA"" option is not set to ""yes"", this tool skips entire rows/columns with non-numeric data ----- 
Example
 Input file:: chromosome GC telo fRec SNP 1 38.6381 51797179 1.0662 0.012289 2 38.8651 31413753 1.2255 0.008923 3 41.4730 26482501 1.6308 0.015474 4 44.9184 12412377 1.4997 0.022099 5 39.4870 34991501 1.4439 0.013091 6 38.3041 20816323 1.4700 0.014043 Below is a heatmap using columns 2 to 5 from the above data, with centering and scaling by column: .. image:: ./static/operation_icons/heatmap_output.png"
toolshed.g2.bx.psu.edu/repos/guru-ananda/karyotype_plot/karyotype_Plot_1/1.0.0	".. class:: infomark 
What it does
 This tool allows you to paint genomic regions of interest on chromosomes arranged in a karyotype-like fashion. It also allows you to have multiple series in a plot, with each series corresponding to a different feature/dataset. ----- .. class:: warningmark Chromosome lengths (backbone line) are determined based on min and max co-ordinates (by chromosome) in start and end columns respectively. For the series, only the co-ordiantes in start column are used for plotting. ----- 
Example
 Below is an example of a two series karyotype plot: .. image:: ./static/operation_icons/karyotype_output.png :height: 540 :width: 540"
toolshed.g2.bx.psu.edu/repos/devteam/windowsplitter/winSplitter/1.0.1	".. class:: infomark 
What it does
 This tool splits the intervals in the input file into smaller intervals based on the specified window-size and window type. ----- .. class:: warningmark 
Note
 The positions at the end of the input interval which do not fit into the last window or a new window of required size, will be omitted from the output. ----- .. class:: infomark 
About formats
 
BED format
 Browser Extensible Data format was designed at UCSC for displaying data tracks in the Genome Browser. It has three required fields and several additional optional ones: The first three BED fields (required) are:: 1. chrom - The name of the chromosome (e.g. chr1, chrY_random). 2. chromStart - The starting position in the chromosome. (The first base in a chromosome is numbered 0.) 3. chromEnd - The ending position in the chromosome, plus 1 (i.e., a half-open interval). The additional BED fields (optional) are:: 4. name - The name of the BED line. 5. score - A score between 0 and 1000. 6. strand - Defines the strand - either '+' or '-'. 7. thickStart - The starting position where the feature is drawn thickly at the Genome Browser. 8. thickEnd - The ending position where the feature is drawn thickly at the Genome Browser. 9. reserved - This should always be set to zero. 10. blockCount - The number of blocks (exons) in the BED line. 11. blockSizes - A comma-separated list of the block sizes. The number of items in this list should correspond to blockCount. 12. blockStarts - A comma-separated list of block starts. All of the blockStart positions should be calculated relative to chromStart. The number of items in this list should correspond to blockCount. 13. expCount - The number of experiments. 14. expIds - A comma-separated list of experiment ids. The number of items in this list should correspond to expCount. 15. expScores - A comma-separated list of experiment scores. All of the expScores should be relative to expIds. The number of items in this list should correspond to expCount. ----- 
Example
 - For the following dataset:: chr22 1000 4700 NM_174568 0 + - running this tool with 
Window size as 1000
, will return:: chr22 1000 2000 NM_174568 0 + chr22 2000 3000 NM_174568 0 + chr22 3000 4000 NM_174568 0 + - running this tool to make 
Sliding windows
 of 
size 1000
 and 
offset 500
, will return:: chr22 1000 2000 NM_174568 0 + chr22 1500 2500 NM_174568 0 + chr22 2000 3000 NM_174568 0 + chr22 2500 3500 NM_174568 0 + chr22 3000 4000 NM_174568 0 + chr22 3500 4500 NM_174568 0 +"
toolshed.g2.bx.psu.edu/repos/devteam/maf_cpg_filter/cpgFilter/1.0.0	".. class:: infomark 
What it does
 This tool takes a MAF file as input and masks CpG sites in every alignment block of the MAF file. ----- .. class:: warningmark 
Note
 
Inclusive definition
 defines CpG sites as those sites that are CG in at least one of the species. 
Restricted definition
 considers sites to be CpG if they are CG in at least one of the species, however, sites that are part of overlapping CpGs are excluded. For more information on CpG site definitions, please refer this article_. .. _article: http://mbe.oxfordjournals.org/cgi/content/full/23/3/565"
toolshed.g2.bx.psu.edu/repos/devteam/bam_to_sam/bam_to_sam/2.0.7	"What it does
 Converts BAM dataset to SAM using the 
samtools view
 command."
toolshed.g2.bx.psu.edu/repos/iuc/bamutil_clip_overlap/bamutil_clip_overlap/1.0.15+galaxy1	"What it does
 Clips overlapping read pairs in a SAM or BAM file based on criteria. The input file and resulting output file are sorted by coordinate (or readName if specified in the options). When a read is clipped from the front: * the read start position is updated to reflect the clipping * the mate's mate start position is updated to reflect the record's new position * the record is placed in the output file in the correct location based on the updated position To handle coordinate-sorted files, SAM/BAM records are buffered up until it is known that all following records will have a later start position. To prevent the program from running away with memory, a limit is set to the number of records that can be buffered, see --poolSize for more information. When two mates overlap, this tool will clip the record's whose clipped region would have the lowest average quality. It also checks strand. If a forward strand extends past the end of a reverse strand, that will be clipped. Similarly, if a reverse strand starts before the forward strand, the region prior to the forward strand will be clipped. If the reverse strand occurs entirely before the forward strand, both strands will be entirely clipped. If the --unmapped option is specified, then rather than clipping an entire read, it will be marked as unmapped. The qualities on the two strands remain unchanged even with clipping. The excludeFlags option accepts a decimal value and skips the records with the specified flags set. The default is 3852 (0xF0C hex), so records with any of the following flags set will be skipped: * unmapped * mate unmapped * secondary alignment * fails QC checks * duplicate * supplementary 
Assumptions/ Restrictions
 * Assumes the file is sorted by Coordinate (or ReadName if using --readName option) * Assumes only 2 reads have matching ReadNames (Supplementary and Secondary reads are ignored/skipped by default so will not cause a problem) * It matches in pairs, so if there are 3, the first 2 will be matched and compared, but the 3rd won't. If there are 4, the first 2 will be matched and the last 2 will be matched and compared. * Only mapped reads will be clipped * Assumes that mate information in records are accurate 
Clipping from the front
 The first operation after the softclip will be a Match/Mismatch, meaning that any trailing pads, deletions, insertions, or skips will also be soft clipped. +------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+ | Clip location | How it is handled | +================================+=========================================================+======================================================================================+ | If the clip position falls in a skip/deletion | Removes the entire skip/deletion | +------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+ | If the position immediately after the clip is a skip/deletion | Also removes the skip/deletion | +------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+ | If the position immediately after the clip is an Insert | Softclips the insert | +------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+ | If the position immediately after the clip is a Pad | Removes the pad | +------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+ | Clip occurs at the last match/mismatch position of the read (the entire read is clipped) | Entire read is soft clipped, 0-based position is left as the original (not modified) | +------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+ | Clip occurs after the read ends | Entire read is soft clipped, 0-based position is left as the original (not modified) | +------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+ | Clip occurs before the read starts | Nothing is clipped. The read is not changed. | +------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+ 
Clipping from the back
 +----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+ | Clip location | How it is handled | +==================+=========================================================+===================================================================================================================+ | If the clip position falls in a skip/deletion | Removes the entire skip/deletion | +----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+ | If the position immediately before the clip is a deletion/skip/pad | Remove the deletion/skip/pad | +----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+ | If the position immediately before the clip is an insertion | Leave the insertion, even if it results in a 70M3I27S | +----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+ | Clip occurs at the first position of the read (the entire read is clipped) | Entire read is soft clipped, preceding insertions remain, 0-based position is left as the original (not modified) | +----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+ | Clip occurs before the read starts | Entire read is soft clipped, 0-based position is left as the original (not modified) | +----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+ | Clip occurs after the read ends | Nothing is clipped. The read is not changed. | +----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+"
toolshed.g2.bx.psu.edu/repos/devteam/sam2interval/sam2interval/1.0.2	"What it does
 Converts positional information from a SAM dataset into interval format with 0-based start and 1-based end. CIGAR string of SAM format is used to compute the end coordinate. ----- 
Example
 Converting the following dataset:: r001 163 ref 7 30 8M2I4M1D3M = 37 39 TTAGATAAAGGATACTA * r002 0 ref 9 30 3S6M1P1I4M * 0 0 AAAAGATAAGGATA * r003 0 ref 9 30 5H6M * 0 0 AGCTAA * NM:i:1 r004 0 ref 16 30 6M14N5M * 0 0 ATAGCTTCAGC * r003 16 ref 29 30 6H5M * 0 0 TAGGC * NM:i:0 r001 83 ref 37 30 9M = 7 -39 CAGCGCCAT * into Interval format will produce the following if 
Print all?
 is set to 
Yes
:: ref 6 22 + r001 163 ref 7 30 8M2I4M1D3M = 37 39 TTAGATAAAGGATACTA * ref 8 19 + r002 0 ref 9 30 3S6M1P1I4M * 0 0 AAAAGATAAGGATA * ref 8 14 + r003 0 ref 9 30 5H6M * 0 0 AGCTAA * NM:i:1 ref 15 40 + r004 0 ref 16 30 6M14N5M * 0 0 ATAGCTTCAGC * ref 28 33 - r003 16 ref 29 30 6H5M * 0 0 TAGGC * NM:i:0 ref 36 45 - r001 83 ref 37 30 9M = 7 -39 CAGCGCCAT * Setting 
Print all?
 to 
No
 will generate the following:: ref 6 22 + r001 ref 8 19 + r002 ref 8 14 + r003 ref 15 40 + r004 ref 28 33 - r003 ref 36 45 - r001"
toolshed.g2.bx.psu.edu/repos/devteam/bamtools_filter/bamFilter/2.5.2+galaxy3	"What is does
 BAMTools filter is a very powerful utility to perform complex filtering of BAM files. It is based on BAMtools suite of tools by Derek Barnett (https://github.com/pezmaster31/bamtools). ----- 
How it works
 The tool use logic relies on the three concepts: (1) input BAM, (2) groups, and (3) filters. 
Input BAM(s)
 The input BAM is self-explanatory. This is the dataset you will be filtering. The tool can accept just one or multiple BAM files. To filter on multiple BAMs just add them by clicking 
Add new BAM dataset(s) to filter
 
Conditions and Filters
 Conditions for filtering BAM files can be arranged in 
Groups and Filters
. While it can be confusing at first this is what gives ultimate power to this tools. So try to look at the examples we are supplying below. ----- 
Example 1. Using a single filter
 When filtering on a single condition there is no need to worry about filters and conditions. Just choose a filter from the 
Select BAM property to filter on:
 dropdown and enter a value (or click a checkbox for binary filters). For example, for retaining reads with mapping quality of at least 20 one would set the tool interface as shown below: .. image:: single-filter.png ----- 
Example 2. Using multiple filters
 Now suppose one needs to extract reads that (1) have mapping quality of at least 20, (2) contain at least 1 mismatch, and (3) are mapping onto forward strand only. To do so we will use three filters as shown below (multiple filters are added to the interface by clicking on the 
Add new Filter
 button): .. image:: multiple-filters.png In this case (you can see that the three filters are grouped within a single Condition - 
Condition 1
) the filter too use logical 
AND
 to perform filtering. In other words only reads that (1) have mapping quality of at least 20 
AND
 (2) contain at least 1 mismatch 
AND
 are mapping onto forward strand will be returned in this example. ----- 
Example 3. Complex filtering with multiple conditions
 Suppose now you would like to select 
either
 reads that (
1
) have (
1.1
) no mismatches and (
1.2
) are on the forward strand 
OR
 (
2
) reads that have (
2.1
) at least one mismatch and (
2.2
) are on the reverse strand. In this scenario we have to set up two conditions: (
1
) and (
2
) each with two filters: 
1.1
 and 
1.2
 as well as 
2.1
 and 
2.2
. The following screenshot expalins how this can be done: .. image:: complex-filters.png ----- 
Example 4. Even more complex filtering with Rules
 In the above example we have used two conditions (Condition 1 and Condition 2). Using multiple conditions allows to combine them and a variety of ways to enable even more powerful filtering. For example, suppose get all reads that (
1
) do NOT map to mitochondria and either (
2
) have mapping quality over 20, or (
3
) are in properly mapped pairs. The logical rule to enable such filtering will look like this:: !(1) & (2 | 3) Here, numbers 1, 2, and 3 represent conditions. The following screenshot illustrates how to do this in Galaxy: .. image:: rule.png There are three conditions here, each with a single filter. A text entry area that can be opened by clicking on the 
Would you like to set rules?
 checkbox enables you to enter a rule. Here numbers correspond to numbers of conditions as they are shown in the interface. E.g., 1 corresponds to condition 1, 2 to condition 2 and so on... In human language this means:: NOT condition 1 AND (condition 2 OR condition 3) ----- 
JSON script file
 This tool produces two outputs. One of the them is a BAM file containing filtered reads. The other is a JSONified script. It can help you to see how your instructions are sent to BAMTools. For instance, the example 4 looks like this in the JSON form:: { ""filters"": [ { ""id"": ""1"", ""tag"":""NM:=0"", ""isReverseStrand"":""false"" }, { ""id"": ""2"", ""tag"":""NM:>0"", ""isReverseStrand"":""true"" } ] } ----- 
More information
 .. class:: infomark Additional information about BAMtools can be found at https://github.com/pezmaster31/bamtools/wiki"
toolshed.g2.bx.psu.edu/repos/devteam/sam_bitwise_flag_filter/sam_bw_filter/1.0.0	"What it does
 Allows parsing of SAM datasets using bitwise flag (the second column). The bits in the flag are defined as follows:: Bit Info ------ -------------------------------------------------------------------------- 0x0001 the read is paired in sequencing, no matter whether it is mapped in a pair 0x0002 the read is mapped in a proper pair (depends on the protocol, normally inferred during alignment) 1 0x0004 the query sequence itself is unmapped 0x0008 the mate is unmapped 1 0x0010 strand of the query (0 for forward; 1 for reverse strand) 0x0020 strand of the mate 1 0x0040 the read is the first read in a pair (see below) 0x0080 the read is the second read in a pair (see below) 0x0100 the alignment is not primary (a read having split hits may have multiple primary alignment records) 0x0200 the read fails platform/vendor quality checks 0x0400 the read is either a PCR duplicate or an optical duplicate Note the following: - Flag 0x02, 0x08, 0x20, 0x40 and 0x80 are only meaningful when flag 0x01 is present. - If in a read pair the information on which read is the first in the pair is lost in the upstream analysis, flag 0x01 should be set, while 0x40 and 0x80 should both be zero. ----- 
Example
 Suppose the following dataset was generated with BWA mapper:: r001 163 ref 7 30 8M2I4M1D3M = 37 39 TTAGATAAAGGATACTA * r002 0 ref 9 30 3S6M1P1I4M * 0 0 AAAAGATAAGGATA * r003 0 ref 9 30 5H6M * 0 0 AGCTAA * NM:i:1 r004 0 ref 16 30 6M14N5M * 0 0 ATAGCTTCAGC * r003 16 ref 29 30 6H5M * 0 0 TAGGC * NM:i:0 r001 83 ref 37 30 9M = 7 -39 CAGCGCCAT * To select properly mapped pairs, click the 
Add new Flag
 button and set 
Read mapped in a proper pair
 to 
Yes
. The following two reads will be returned:: r001 163 ref 7 30 8M2I4M1D3M = 37 39 TTAGATAAAGGATACTA * r001 83 ref 37 30 9M = 7 -39 CAGCGCCAT * For more information, please consult the 
SAM format description
. .. 
: http://www.ncbi.nlm.nih.gov/pubmed/19505943"
toolshed.g2.bx.psu.edu/repos/devteam/samtool_filter2/samtool_filter2/1.8+galaxy1	"What it does
 This tool uses the samtools view command in SAMtools_ toolkit to filter a SAM or BAM file on the MAPQ (mapping quality), FLAG bits, Read Group, Library, or region. 
Input
 Input is either a SAM or BAM file. 
Output
 The output file will be SAM or BAM (depending on the chosen option), filtered by the selected options. 
Options
 Filtering by read group or library requires headers in the input SAM or BAM file. If regions are specified, only alignments overlapping the specified regions will be output. An alignment may be given multiple times if it is overlapping several regions. A region can be presented, for example, in the following format:: chr2 (the whole chr2) chr2:1000000 (region starting from 1,000,000bp) chr2:1,000,000-2,000,000 (region between 1,000,000 and 2,000,000bp including the end points). Note: The coordinate is 1-based. Multiple regions may be specified, separated by a space character:: chr2:1000000-2000000 chr2:1,000,000-2,000,000 chrX .. _SAMtools: http://www.htslib.org/"
toolshed.g2.bx.psu.edu/repos/boris/filter_on_md/MDtag_filter/1.0.2	"Mismatches at either end of a mapped read are most likely sequencing errors. This tool aims to control the variation noise due to potential sequencing errors. ----- .. class:: infomark 
What it does
 This tool reads the MD tag of mapped reads (see SAM format specification). The user defines the 5' and 3' windows 
n
 and 
m
 (in bp), respectively. The mapped read is discarded if it contains any number of mismatches within 
n
 bases of the read 5' end and within 
m
 bases of the read 3' end. Option: save discarded reads in an additional SAM file. ----- .. class:: warningmark 
Note
 Mapped reads without an MD tag will be removed from the output SAM file(s). ----- .. class:: infomark 
About formats
 
SAM format
 -- SAM stands for Sequence Alignment/Map format. It is a TAB-delimited text format consisting of a header section, which is optional, and an alignment section. Each alignment line has 11 mandatory fields for essential alignment information such as mapping position, and variable number of optional fields for flexible or aligner specific information. Each alignment line has 11 
mandatory
 fields:: 1. QNAME - Query template NAME 2. FLAG - bitwise FLAG 3. RNAME - Reference sequence NAME 4. POS - 1-based leftmost mapping POSition 5. MAPQ - MAPping Quality 6. CIGAR - CIGAR string 7. RNEXT - Ref. name of the mate/next segment 8. PNEXT - Position of the mate/next segment observed 9. TLEN - Template LENgth 10. SEQ - segment SEQuence 11. QUAL - ASCII of Phred-scaled base QUALity+33 All 
optional
 fields follow the TAG\:TYPE\:VALUE format, where TAG is a two-character string that matches [A-Za-z][A-Za-z0-9]. TYPE is a single case sensitive letter which defines the format of VALUE:: MD TAG MD:Z:[0-9]+(([A-Z]|\^[A-Z]+)[0-9]+)
 with Z = Printable string, including space. String for mismatching positions. The MD field aims to achieve SNP/indel calling without looking at the reference. For example, a string ‘10A5^AC6’ means from the leftmost reference base in the alignment, there are 10 matches followed by an A on the reference which is different from the aligned read base; the next 5 reference bases are matches followed by a 2bp deletion from the reference; the deleted sequence is AC; the last 6 bases are matches. The MD field ought to match the CIGAR string. ----- 
Example
 - For the following dataset:: SRR057527.13746413 16 1 1164232 35 1I35M * 0 0 CGAAAGTGAGGTCCTGGCTCCAATCCAATCCCCGGG 333333033333333333333333333333333333 X0:i:1 X1:i:0 OC:Z:36M RG:Z:rnaseq XG:i:0 NM:i:2 XM:i:2 XO:i:0 OP:i:1164231 OQ:Z:CCCCCCDCCCCBCCCCCCCCCCCCCCCCCCCCCCCC XT:A:U SRR057527.8574994 16 1 565901 23 36M * 0 0 GAGCCTAATCTACTCCACCTCAATCACACTACTCCC 333333333333333303333333333333333333 X0:i:1 X1:i:1 XA:Z:MT,-5351,36M,2; MD:Z:1C34 RG:Z:rnaseq XG:i:0 NM:i:1 XM:i:1 XO:i:0 OQ:Z:CCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCC XT:A:U SRR057528.178504 0 1 566573 23 36M * 0 0 ACTGGGCCAGCCAGGCAACCTTCTAGGTAACGACCA 233333323222222232333222222222222222 X0:i:1 X1:i:1 XA:Z:MT,+6023,36M,1; MD:Z:36 RG:Z:rnaseq XG:i:0 NM:i:0 XM:i:0 XO:i:0 OQ:Z::?CCCCBAB@AA@@A@B@??BA@A;AA@======:@ XT:A:U SRR057527.20391474 0 1 565512 23 36M * 0 0 GGCAGTTGAGGGGGATTAAACCAAACCCAACTACGC %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% X0:i:1 X1:i:1 XA:Z:MT,+4962,36M,2; MD:Z:11T24 RG:Z:rnaseq XG:i:0 NM:i:1 XM:i:1 XO:i:0 OQ:Z:%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% XT:A:U SRR057513.2261668 16 1 16267 15 36M * 0 0 CACTTCTGGATGCTAGGGTTACACTGGGAGTCACAG 333333333333333333333333333333333333 X0:i:1 X1:i:6 MD:Z:30A5 RG:Z:rnaseq XG:i:0 NM:i:1 XM:i:1 XO:i:0 OQ:Z:IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII XT:A:U - running this tool with 
n = 5
 and 
m =10
*, will return:: SRR057528.178504 0 1 566573 23 36M * 0 0 ACTGGGCCAGCCAGGCAACCTTCTAGGTAACGACCA 233333323222222232333222222222222222 X0:i:1 X1:i:1 XA:Z:MT,+6023,36M,1; MD:Z:36 RG:Z:rnaseq XG:i:0 NM:i:0 XM:i:0 XO:i:0 OQ:Z::?CCCCBAB@AA@@A@B@??BA@A;AA@======:@ XT:A:U SRR057527.20391474 0 1 565512 23 36M * 0 0 GGCAGTTGAGGGGGATTAAACCAAACCCAACTACGC %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% X0:i:1 X1:i:1 XA:Z:MT,+4962,36M,2; MD:Z:11T24 RG:Z:rnaseq XG:i:0 NM:i:1 XM:i:1 XO:i:0 OQ:Z:%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% XT:A:U SRR057513.2261668 16 1 16267 15 36M * 0 0 CACTTCTGGATGCTAGGGTTACACTGGGAGTCACAG 333333333333333333333333333333333333 X0:i:1 X1:i:6 MD:Z:30A5 RG:Z:rnaseq XG:i:0 NM:i:1 XM:i:1 XO:i:0 OQ:Z:IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII XT:A:U"
toolshed.g2.bx.psu.edu/repos/devteam/pileup_parser/pileup_parser/1.0.2	"What it does
 Allows one to find sequence variants and/or sites covered by a specified number of reads with bases above a set quality threshold. The tool works on six and ten column pileup formats produced with 
samtools pileup
 command. However, it also allows you to specify columns in the input file manually. The tool assumes the following: - the quality scores follow phred33 convention, where input qualities are ASCII characters equal to the Phred quality plus 33. - the pileup dataset was produced by the 
samtools pileup
 command (although you can override this by setting column assignments manually). -------- 
Types of pileup datasets
 The descriptions of the following pileup formats are largely based on information that can be found on the SAMTools_ documentation page. The 6- and 10-column variants are described below. .. _SAMTools: http://samtools.sourceforge.net/pileup.shtml 
Six column pileup
:: 1 2 3 4 5 6 --------------------------------- chrM 412 A 2 ., II chrM 413 G 4 ..t, IIIH chrM 414 C 4 ..Ta III2 chrM 415 C 4 TTTt III7 where:: Column Definition ------- ---------------------------- 1 Chromosome 2 Position (1-based) 3 Reference base at that position 4 Coverage (# reads aligning over that position) 5 Bases within reads 6 Quality values (phred33 scale, see Galaxy wiki for more) 
Ten column pileup
 The 
ten-column
 pileup incorporates additional consensus information generated with the 
-c
 option of the 
samtools pileup
 command:: 1 2 3 4 5 6 7 8 9 10 ------------------------------------------------ chrM 412 A A 75 0 25 2 ., II chrM 413 G G 72 0 25 4 ..t, IIIH chrM 414 C C 75 0 25 4 ..Ta III2 chrM 415 C T 75 75 25 4 TTTt III7 where:: Column Definition ------- ---------------------------- 1 Chromosome 2 Position (1-based) 3 Reference base at that position 4 Consensus bases 5 Consensus quality 6 SNP quality 7 Maximum mapping quality 8 Coverage (# reads aligning over that position) 9 Bases within reads 10 Quality values (phred33 scale, see Galaxy wiki for more) .. 
: http://samtools.sourceforge.net/cns0.shtml ------ 
The output format
 The tool modifies the input dataset in two ways: 1. It appends five columns to the end of every reported line: - Number of 
A
 variants - Number of 
C
 variants - Number of 
G
 variants - Number of 
T
 variants - Number of read bases covering this position, where quality is equal to or higher than the value set by 
Do not consider read bases with quality lower than
 option. Optionally, if 
Print total number of differences?
 is set to 
Yes
, the tool will append the sixth column with the total number of deviants (see below). 2. If 
Convert coordinates to intervals?
 is set to 
Yes
, the tool replaces the position column (typically the second column) with a pair of tab-delimited start/end values. For example, if you are calling variants with base quality above 20 on this dataset:: chrM 412 A 2 ., II chrM 413 G 4 ..t, III2 chrM 414 C 4 ..Ta III2 chrM 415 C 4 TTTt III7 you will get:: chrM 413 G 4 ..t, IIIH 0 0 2 1 3 chrM 414 C 4 ..Ta III2 1 1 0 1 3 chrM 415 C 4 TTTt III7 0 0 0 4 4 where:: Column Definition ------- ---------------------------- 1 Chromosome 2 Position (1-based) 3 Reference base at that position 4 Coverage (# reads aligning over that position) 5 Bases within reads where 6 Quality values (phred33 scale, see Galaxy wiki for more) 7 Number of A variants 8 Number of C variants 9 Number of G variants 10 Number of T variants 11 Quality adjusted coverage: 12 Number of read bases (i.e., # of reads) with quality above the set threshold 13 Total number of deviants (if Convert coordinates to intervals? is set to yes) if 
Print total number of differences?
 is set to 
Yes
, you will get:: chrM 413 G 4 ..t, IIIH 0 0 2 1 3 1 chrM 414 C 4 ..Ta III2 1 2 0 1 3 2 chrM 415 C 4 TTTt III7 0 0 0 4 4 0 Note the additional column 13, that contains the number of deviant reads (e.g., there are two deviants, T and a, for position 414). Finally, if 
Convert coordinates to intervals?
 is set to 
Yes
, you will get one additional column with the end coordinate:: chrM 412 413 G 4 ..t, III2 0 0 2 1 3 chrM 414 415 C 4 ..Ta III2 1 2 0 1 3 chrM 414 415 C 4 TTTt III7 0 0 0 4 4 where:: Column Definition ------- ---------------------------- 1 Chromosome 2 Start position (0-based) 3 End position (1-based) 4 Reference base at that position 5 Coverage (# reads aligning over that position) 6 Bases within reads 7 Quality values (phred33 scale, see Galaxy wiki for more) 8 Number of A variants 9 Number of C variants 10 Number of G variants 11 Number of T variants 12 Quality adjusted coverage 13 Total number of deviants (if Convert coordinates to intervals? is set to yes) Note that in this case the coordinates of SNPs were converted to intervals, where the start coordinate is 0-based and the end coordinate in 1-based using the UCSC Table Browser convention. Although three positions have variants in the original file (413, 414, and 415), only 413 and 415 are reported because the quality values associated with these two SNPs are above the threshold of 20. In the case of 414 the 
a
 allele has a quality value of 17 ( ord(""2"")-33 ), and is therefore not reported. Note that five columns have been added to each of the reported lines:: chrM 413 G 4 ..t, IIIH 0 0 2 1 3 Here, there is one variant, 
t
. Because the fourth column represents 
T
 counts, it is incremented by 1. The last column shows that at this position, three reads have bases above the quality threshold of 20. ----- 
Example 1
: Just variants In this mode, the tool only outputs the lines from the input datasets where at least one read contains a sequence variant with quality above the threshold set by the 
Do not consider read bases with quality lower than
 option. For example, suppose one has a pileup dataset like the following:: chrM 412 A 2 ., II chrM 413 G 4 ..t, III2 chrM 414 C 4 ..Ta III2 chrM 415 C 4 TTTt III7 To call all variants (with no restriction by coverage) with quality above phred value of 20, we will need to set the parameters as follows: .. image:: pileup_parser_help1.png Running the tool with these parameters will return:: chrM 413 G 4 ..t, IIIH 0 0 0 1 3 chrM 414 C 4 ..Ta III2 0 2 0 1 3 chrM 415 C 4 TTTt III7 0 0 0 4 4 
Note
 that position 414 is not reported because the 
a
 variant has associated quality value of 17 (because ord('2')-33 = 17) and is below the phred threshold of 20 set by the 
Count variants with quality above this value
 parameter. ----- 
Example 2
: Report everything In addition to calling variants, it is often useful to know the quality adjusted coverage. Running the tool with these parameters: .. image:: pileup_parser_help2.png will report everything from the original file:: chrM 412 A 2 ., II 2 0 0 0 2 chrM 413 G 4 ..t, III2 0 0 2 1 3 chrM 414 C 4 ..Ta III2 0 2 0 1 3 chrM 415 C 4 TTTt III7 0 0 0 4 4 Here, you can see that although the total coverage at position 414 is 4 (column 4), the quality adjusted coverage is 3 (last column). This is because only three out of four reads have bases with quality above the set threshold of 20 (the actual qualities are III2 or, after conversion, 40, 40, 40, 17). One can use the last column of this dataset to filter out (using Galaxy's 
Filter
 tool) positions where quality adjusted coverage (last column) is below a set threshold. ------ 
Example 3
: Report everything and print total number of differences If you set the 
Print total number of differences?
 to 
Yes
 the tool will print an additional column with the total number of reads where a devinat base is above the quality threshold. So, seetiing parametrs like this: .. image:: pileup_parser_help3.png will produce this:: chrM 412 A 2 ., II 2 0 0 0 2 0 chrM 413 G 4 ..t, III2 0 0 2 1 3 1 chrM 414 C 4 ..Ta III2 0 2 0 1 3 1 chrM 415 C 4 TTTt III7 0 0 0 4 4 0 ----- 
Example 4
: Report everything, print total number of differences, and ignore qualities and read bases Setting 
Print quality and base string?
 to 
Yes
 as shown here: .. image:: pileup_parser_help4.png will produce this:: chrM 412 A 2 2 0 0 0 2 0 chrM 413 G 4 0 0 2 1 3 1 chrM 414 C 4 0 2 0 1 3 1 chrM 415 C 4 0 0 0 4 4 0"
toolshed.g2.bx.psu.edu/repos/devteam/sam_pileup/sam_pileup/1.1.3	"What it does
 Uses SAMTools_' pileup command to produce a pileup dataset from a provided BAM dataset. It generates two types of pileup datasets depending on the specified options. If 
Call consensus according to MAQ model?
 option is set to 
No
, the tool produces simple pileup. If the option is set to 
Yes
, a ten column pileup dataset with consensus is generated. Both types of datasets are briefly summarized below. .. 
SAMTools: http://samtools.sourceforge.net/samtools.shtml ------ 
Types of pileup datasets
 The description of pileup format below is largely based on information that can be found on SAMTools Pileup
 documentation page. The 6- and 10-column variants are described below. .. 
Pileup: http://samtools.sourceforge.net/pileup.shtml 
Six column pileup
:: 1 2 3 4 5 6 --------------------------------- chrM 412 A 2 ., II chrM 413 G 4 ..t, IIIH chrM 414 C 4 ...a III2 chrM 415 C 4 TTTt III7 where:: Column Definition ------- ---------------------------- 1 Chromosome 2 Position (1-based) 3 Reference base at that position 4 Coverage (# reads aligning over that position) 5 Bases within reads where (see Galaxy wiki for more info) 6 Quality values (phred33 scale, see Galaxy wiki for more) 
Ten column pileup
 The 
ten-column
 (consensus
) pileup incorporates additional consensus information generated with 
-c
 option of 
samtools pileup
 command:: 1 2 3 4 5 6 7 8 9 10 ------------------------------------------------ chrM 412 A A 75 0 25 2 ., II chrM 413 G G 72 0 25 4 ..t, IIIH chrM 414 C C 75 0 25 4 ...a III2 chrM 415 C T 75 75 25 4 TTTt III7 where:: Column Definition ------- -------------------------------------------------------- 1 Chromosome 2 Position (1-based) 3 Reference base at that position 4 Consensus bases 5 Consensus quality 6 SNP quality 7 Maximum mapping quality 8 Coverage (# reads aligning over that position) 9 Bases within reads where (see Galaxy wiki for more info) 10 Quality values (phred33 scale, see Galaxy wiki for more) .. _consensus: http://samtools.sourceforge.net/cns0.shtml"
toolshed.g2.bx.psu.edu/repos/devteam/bamtools/bamtools/2.5.2+galaxy3	"What is does
 BAMTools is a collection of utilities for manipulation on BAM files. It is based on BAMtools suite of tools by Derek Barnett (https://github.com/pezmaster31/bamtools). This Galaxy implementation of BAMTools utilities includes seven utilities - Convert, Count, Coverage, Header, Merge, Random, and Revert - decsribed in detail below. ----- 
Convert
 Converts BAM dataset(s) into BED, FASTA, FASTQ, JSON, Pileup, SAM, or YAML formats. Note that the conversion to the pileup format requires providing a reference sequence either cached on this Galaxy instance, or provided by you as a FASTA dataset from History. ----- 
Count
 Counts the number of alignments in a BAM dataset(s). ----- 
Coverage
 Prints per-base coverage for a BAM dataset. ----- 
Header
 Prints the header of a BAM dataset. ------ 
Merge
 Merges multiple BAM datasets into a single one. ------ 
Random
 Grabs a specified number of random lines from BAM dataset(s). ------ 
Revert
 Removes duplicate marks and restores original (non-recalibrated) base qualities. ----- .. class:: infomark 
More information
 Additional information about BAMtools can be found at https://github.com/pezmaster31/bamtools/wiki"
toolshed.g2.bx.psu.edu/repos/devteam/pileup_interval/pileup_interval/1.0.3	"What is does
 Reduces the size of a results set by taking a pileup file and producing a condensed version showing consecutive sequences of bases meeting coverage criteria. The tool works on six and ten column pileup formats produced with 
samtools pileup
 command. You also can specify columns for the input file manually. The tool assumes that the pileup dataset was produced by 
samtools pileup
 command (although you can override this by setting column assignments manually). -------- 
Types of pileup datasets
 The description of pileup format below is largely based on information that can be found on SAMTools_ documentation page. The 6- and 10-column variants are described below. .. _SAMTools: http://samtools.sourceforge.net/pileup.shtml 
Six column pileup
:: 1 2 3 4 5 6 --------------------------------- chrM 412 A 2 ., II chrM 413 G 4 ..t, IIIH chrM 414 C 4 ...a III2 chrM 415 C 4 TTTt III7 where:: Column Definition ------ ---------------------------- 1 Chromosome 2 Position (1-based) 3 Reference base at that position 4 Coverage (# reads aligning over that position) 5 Bases within reads where (see Galaxy wiki for more info) 6 Quality values (phred33 scale, see Galaxy wiki for more) 
Ten column pileup
 The 
ten-column
 pileup incorporates additional consensus information generated with 
-c
 option of 
samtools pileup
 command:: 1 2 3 4 5 6 7 8 9 10 ------------------------------------------------ chrM 412 A A 75 0 25 2 ., II chrM 413 G G 72 0 25 4 ..t, IIIH chrM 414 C C 75 0 25 4 ...a III2 chrM 415 C T 75 75 25 4 TTTt III7 where:: Column Definition ------- ---------------------------- 1 Chromosome 2 Position (1-based) 3 Reference base at that position 4 Consensus bases 5 Consensus quality 6 SNP quality 7 Maximum mapping quality 8 Coverage (# reads aligning over that position) 9 Bases within reads where (see Galaxy wiki for more info) 10 Quality values (phred33 scale, see Galaxy wiki for more) .. 
: http://samtools.sourceforge.net/cns0.shtml ------ 
The output format
 The output file condenses the information in the pileup file so that consecutive bases are listed together as sequences. The starting and ending points of the sequence range are listed, with the starting value converted to a 0-based value. Given the following input with minimum coverage set to 3:: 1 2 3 4 5 6 --------------------------------- chr1 112 G 3 ..Ta III6 chr1 113 T 2 aT.. III5 chr1 114 A 5 ,,.. IIH2 chr1 115 C 4 ,., III chrM 412 A 2 ., II chrM 413 G 4 ..t, IIIH chrM 414 C 4 ...a III2 chrM 415 C 4 TTTt III7 chrM 490 T 3 a I the following would be the output:: 1 2 3 4 ------------------- chr1 111 112 G chr1 113 115 AC chrM 412 415 GCC chrM 489 490 T where:: Column Definition ------- ---------------------------- 1 Chromosome 2 Starting position (0-based) 3 Ending position (1-based) 4 Sequence of bases"
toolshed.g2.bx.psu.edu/repos/iuc/qualimap_bamqc/qualimap_bamqc/2.3+galaxy0	"What it does
 
Qualimap BAM QC
 lets you evaluate the quality of aligned reads data in BAM format. The tool summarizes basic statistics of the alignment (number of reads, coverage, GC-content, etc.) and produces a number of useful graphs for their interpretation. The analysis can be performed with any kind of sequencing data, such as whole-genome sequencing, exome sequencing, RNA-seq or ChIP-seq data. In addition, it is possible to provide an annotation file so the results are computed for the reads mapping inside (and optionally outside) of the corresponding genomic regions, which can be especially useful for evaluating target-enrichment sequencing studies. Input ===== 
Mapped reads input dataset
 The dataset holding the mapped reads to carry out the analysis with. 
Dataset specifying regions
 If you decide to calculate mapping statistics for selected regions of the reference genome (instead of for the whole genome), you need to specify the regions through this additional dataset in gtf, gff or bed format. .. class:: infomark A typical problem when working with regions (and genome annotation data, in general) is potential inconsistency between the chromosome names used in the mapped reads input versus those used to define the regions. In the case of the human genome, for example, UCSC data has chromosomes starting with a 'chr' prefix, which is lacking from Ensemble data. This simple form of the problem is handled by Qualimap: if chromosome names in the regions input have a 'chr' prefix, Qualimap will add that prefix to the mapped reads chromosome names as needed. For more complex cases you will have to adjust your inputs manually. Parameters ---------- 
Reference genome regions to calculate mapping statistics for
 Choose whether you would like to have mapping statistics reported across - the entire reference genome (as specified in the header of the mapped reads input) - specific regions of the reference In the second case, you need to select a 
Dataset specifying regions
 (see above). Using the 
Invert regions
 switch you can then indicate whether you want to select or exclude the regions in this dataset. 
Generate per-base coverage output
 
Skip duplicate reads
 The tool lets you skip alignments of duplicate reads from the analysis. Depending on whether you select none, either one, or both of the available options, you can decide to: - not correct for duplicate reads at all (
e.g.
 because you have removed them at an earlier step with some dedicated tool) - identify and flag duplicate reads with a dedicated tool (like 
Picard MarkDuplicates
 or 
samtools markdup
), then have Qualimap ignore the duplicate-flagged reads (recommended, most flexible option since other tools can be told to ignore the same reads) - have Qualimap identify potential duplicates by itself and ignore them - combine external and Qualimap-internal duplicate detection for extra stringency Independent of your selection, the HTML report will always list (in the 
Globals
 section of the 
Summary
) the number of duplicated reads estimated by Qualimap. If you choose to skip duplicates, you will also be informed about the number of skipped reads in that same section and, if you instruct Qualimap to look for the duplicate flag on reads, the number of reads flagged as duplicates will also be reported here. 
Section: Settings affecting specific plots
 Parameters in this section only affect some (or even only one) of the plots contained in the HTML report (and the corresponding part of the 
Raw Data
 output collection). For most of these options, the parameter help above should be descriptive enough. Just a few more words on two of them: 
Number of bins to use in across-reference plots
 This value is used for computing the various graphs that plot information across the reference. Basically, the reference genome gets split into the given number of bins, and reads falling in the same bin are aggregated in the statistics of that bin. Thus, the higher the number of bins, the higher the resolution of the plots, but more bins also require longer time for their statistics to be computed. Less bins, on the other hand, mean more reads will have to be aggregated per bin and this comes with higher memory requirements. Hence, if the tool fails with an 
Out Of Memory
 error, you may want to rerun it with a higher bin number. 
Plot expected GC-content distribution of the following reference genome
 The choice of reference genomes with pre-calculated GC distributions is built into Qualimap. Future releases of Qualimap may include more choices, but the current version is limited to those offered here. Outputs ======= HTML Report ----------- 
Summary Section
 
Globals
 This section contains information about the total number of reads, number of mapped reads, paired-end mapping performance, read length distribution, number of clipped reads and duplication rate (estimated from the start positions of read alignments). 
ACGT Content
 Nucleotide content and GC percentage in the mapped reads. 
Coverage
 Mean and standard deviation of the coverage depth. 
Mapping quality
 Mean mapping quality of the mapped reads. 
Insert size
 Mean, standard deviation and percentiles of the insert size distribution if applicable. The features are computed based on the TLEN field of the SAM file. 
Mismatches and indels
 The section reports general alignment error rate (computed as a ratio of total collected edit distance to the number of mapped bases), total number of mismatches and total number of indels (computed from the CIGAR values). Additionally fraction of the homopolymer indels among total indels is provided. Note, the error rate and mismatches metrics are based on optional fields of a SAM record (NM for edit distance, MD for mismatches). The features are not reported if these fields are missing in the SAM file. 
Chromosome stats
 Number of mapped bases, mean and standard deviation of the coverage depth for each chromosome as defined by the header of the SAM file. For region-based analysis the information is given inside of regions, including some additional information like, for example, number of correct strand reads. 
Plots
 
Coverage Across Reference
 This plot consists of two figures. The upper figure provides the coverage distribution (red line) and coverage deviation across the reference sequence. The lower figure shows GC content across reference (black line) together with its average value (red dotted line). 
Coverage Histogram
 Histogram of the number of genomic locations having a given coverage rate. The bins of the x-axis are conveniently scaled by aggregating some coverage values in order to produce a representative histogram also in presence of the usual NGS peaks of coverage. 
Coverage Histogram (0-50X)
 Similar to the previous plot, but in this graph genome locations with a coverage greater than 50X are grouped into the last bin. By doing so a higher resolution of the most common values for the coverage rate is obtained. 
Genome Fraction Coverage
 Provides a visual way of knowing how much reference has been sequenced to at least a given coverage rate. This graph should be interpreted as in this example: If one aims for a coverage rate of at least 25X (x-axis), how much of the reference (y-axis) will be considered? 
Duplication Rate Histogram
 This plot shows the distribution of duplicated reads. Due to several factors (
e.g.
 amount of starting material, sample preparation, 
etc.
) it is possible that the same fragments are sequenced several times. For some experiments where enrichment is used (
e.g.
 ChIP-seq ) this is expected to some degree. For most experiments, however, a high duplication level of the reads indicates some unwanted bias. 
Mapped Reads Nucleotide Content
 This plot shows the nucleotide content per position of the mapped reads. 
Mapped Reads GC Content Distribution
 This graph shows the distribution of GC-content per mapped read. If compared with a precomputed genome distribution, this plot allows to check if there is a shift in the GC content. 
Mapped Reads Clipping Profile
 Represents the percentage of clipped bases across the reads. Technically, the clipping is detected via SAM format CIGAR codes ‘H’ (hard clipping) and ‘S’ (soft clipping). In addition, the total number of clipped reads can be found in the report 
Summary
 section. This plot is not shown if no clipped reads are found. 
Homopolymer Indels
 This bar plot shows the number of indels that are located within A, C, G and T homopolymers, respectively, as well as the number of indels that are not within any homopolymer. Large numbers of homopolymer indels may indicate a problem in the sequencing process. Technically, Qualimap identifies indels from the CIGAR code of the aligned reads. Indel statistics can also be found in a dedicated section of the report 
Summary
. This graph is not shown if the sample doesn’t contain any indels. 
Mapping Quality Across Reference
 This plot provides the mapping quality distribution across the reference. To construct the plot, the mean mapping quality is computed for each bin. 
Mapping Quality Histogram
 Histogram of the number of genomic locations having a given mapping quality. To construct the histogram the mean mapping quality is computed at each genome position with non-zero coverage and collected. According to the SAM/BAM format specifications, the range for the mapping quality score is [0-255]. 
Insert Size Across Reference
 This plot provides the insert size distribution across the reference. Technically, the insert size of each pair of aligned reads is collected from the SAM alignment field 
TLEN
. Only positive values are taken into account. To construct the plot, the mean insert size is computed for each bin. 
Insert Size Histogram
 Histogram of insert size distribution. Raw Data -------- This is a 
Collection
 of 10 individual datasets. The 
genome_results
 dataset provides a plain-text summary of key statistics, most of which can also be found in the 
Summary
 section of the 
HTML Report
. The remaining 12 datasets hold the tabular raw data underlying the plots of the corresponding names in the 
HTML Report
. Per-base coverage ----------------- Optional. This is a tabular dataset listing the coverage of every base in the reference genome unless that coverage is zero. Since its content is uncompressed text, this dataset can easily become huge, and it is recommended that you generate this dataset only for very small genomes or very limited regions of larger genomes."
toolshed.g2.bx.psu.edu/repos/iuc/qualimap_multi_bamqc/qualimap_multi_bamqc/2.3+galaxy0	"What it does
 This tool lets you combine the summary statistics, obtained through multiple runs of the 
Qualimap BamQC
 tool, of several aligned reads datasets into a single report. This makes it easy to visualize the degree of similarities between the different inputs and to spot differences between them. Input ===== Several 
Raw Data
 collections obtained from previous runs of 
Qualimap BamQC
. Options ------- 
Report samples
 -> 
Individually
 / 
In groups
 You may decide to group the input data for reporting, in which case you will need to provide a name for each group that will be used in the plot legends. Output ====== The single HTML report generated by this tool contains the following: 
Input data and parameters
 This section lists the names of the BamQC Raw Data collections that served as input along with the names of the groups (if any) each input got assigned to. 
Summary
 The summary table contains comparison of selected critical alignment metrics for all samples. The metrics include mean and standard deviation of coverage, mean GC content, mean insert size and mean mapping qualities. If the sample groups are provided, they are also shown for each sample. 
PCA plot
 The alignment features presented in the Summary section undergo Principal Component Analysis. Afterwards the biplot presenting first and second principal component is constructed. It allows to detect if any samples group together and if there are any outliers among the analyzed samples. 
Other plots
 Here you will find plots of: - Coverage Across Reference, - Coverage Histogram, - Genome Fraction Coverage, - Duplication Rate Histogram, - Mapped Reads GC Content, - Mapped Reads GC Content Distribution, - Mapped Reads Clipping Profile, - Mapping Quality Across Reference, - Mapping Quality Histogram and, if applicable, - Insert Size Across Reference, - Insert Size Histogram Essentially, these are overlays of the plots of the individual inputs (or of the groups they have been assigned to) and that are explained in the help of the 
Qualimap BamQC
 tool."
toolshed.g2.bx.psu.edu/repos/devteam/samtools_rmdup/samtools_rmdup/2.0.1	"What it does
 Remove potential PCR duplicates: if multiple read pairs have identical external coordinates, only retain the pair with highest mapping quality. In the paired-end mode, this command ONLY works with FR orientation and requires ISIZE is correctly set. It does not work for unpaired reads (e.g. two ends mapped to different chromosomes or orphan reads)."
toolshed.g2.bx.psu.edu/repos/devteam/sam_to_bam/sam_to_bam/2.1.5	
toolshed.g2.bx.psu.edu/repos/bgruening/sambamba_flagstat/sambamba_flagstat/1.0.1+galaxy2	Sambamba_flagstat outputs statistics drawn from read flags. The first line contains numbers of QC-passed and QC-failed reads. Then come pairs of numbers, the former for QC-passed reads, the latter for QC-failed ones: - duplicates - mapped reads (plus percentage relative to the numbers from the first line) - reads with 'is_paired' flag set - paired reads which are first mates - paired reads which are second mates - paired reads with 'proper_pair' flag set (plus percentage relative to the numbers of QC-passed/failed reads with 'is_paired' flag set) - paired reads where both mates are mapped - paired reads where read itself is unmapped but mate is mapped - paired reads where mate is mapped to a different chromosome - the same as previous but mapping quality is not less than 5
toolshed.g2.bx.psu.edu/repos/bgruening/sambamba_markdup/sambamba_markdup/1.0.1+galaxy2	Sambamba_markdup marks (by default) or removes duplicate reads. To determine whether a read is a duplicate or not, the same criteria as in Picard are used.
toolshed.g2.bx.psu.edu/repos/bgruening/sambamba_merge/sambamba_merge/1.0.1+galaxy2	sambamba_merge is used to merge several sorted BAM files into one. The sorting order of all the files must be the same, and it is maintained in the output file. SAM headers are merged automatically like in Picard merging tool.
toolshed.g2.bx.psu.edu/repos/bgruening/sambamba_sort/sambamba_sort/1.0.1+galaxy2	Sambamba sort performs sorting of BAM files. BAM files can have either 'coordinate' or 'qname' sort order. - 'coordinate' sorting order : Sorts Reads by (integer) reference ID, and for each reference sort corresponding reads by start coordinate. - 'qname' sorting order : Reads are sorted lexicographically by their names.
toolshed.g2.bx.psu.edu/repos/iuc/samtools_ampliconclip/samtools_ampliconclip/1.22+galaxy1	"What it does
 Clips read alignments where they match BED file defined regions (e.g. for amplicon sequencing). samtools ampliconclip -b [INPUT BED] [INPUT BAM1] -o [OUTPUT]"
toolshed.g2.bx.psu.edu/repos/devteam/samtools_bedcov/samtools_bedcov/2.0.7	"What it does
 Calculates read depth for regions listed in a BED dataset using 
samtools bedcov
 command:: samtools bedcov [INPUT BED] [INPUT BAM1] ... [INPUT BAMn] > [OUTPUT]"
toolshed.g2.bx.psu.edu/repos/devteam/samtools_calmd/samtools_calmd/2.0.7	
toolshed.g2.bx.psu.edu/repos/iuc/samtools_coverage/samtools_coverage/1.22+galaxy3	"What it does
 This tool runs the 
samtools coverage
 command. Computes the depth at each position or region and draws an ASCII-art histogram or tabulated text. If you select to pool the bam files, then it calculates coverage for the combined files. The tabulated form uses the following headings: - rname Reference name / chromosome - startpos Start position - endpos End position (or sequence length) - numreads Number reads aligned to the region (after filtering) - covbases Number of covered bases with depth >= 1 - coverage Proportion of covered bases [0..1] - meandepth Mean depth of coverage - meanbaseq Mean baseQ in covered region - meanmapq Mean mapQ of selected reads"
toolshed.g2.bx.psu.edu/repos/iuc/samtools_depth/samtools_depth/1.22+galaxy1	"What it does
 Computes the depth at each position or region using the 
samtools depth
 command. The output is a tabular file, with one line for each base of each reference with any coverage (bases with no coverage may not appear). The first column is the reference name, the second column is the reference position, and then there is one column for each SAM/BAM file giving the coverage depth at that position."
toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.22+galaxy1	"disjoint subsets of the reads can be redirected to separate data sets. ""READ[12]"" are reads for which the READ[12] flag is set and the READ[21] flag is not set. ""unspecific"" reads are those for which both ore neither of these flags are set. ""singletons"" are the singleto reads. Note that if selected READ1 and READ2 contain only the paired reads. ""other"" are the remaining reads. If onlz 'others' or nothing is selected all reads go to a single data set."
toolshed.g2.bx.psu.edu/repos/iuc/samtools_fixmate/samtools_fixmate/1.22+galaxy1	"What it does
 Fill in mate coordinates, ISIZE and mate related flags from a name-sorted alignment."
toolshed.g2.bx.psu.edu/repos/devteam/samtools_flagstat/samtools_flagstat/2.0.8	"What it does
 Uses 
samtools flagstat
 command to print descriptive information for a BAM dataset. Here is an example of such information:: 200 + 0 in total (QC-passed reads + QC-failed reads) 0 + 0 secondary 0 + 0 supplementary 0 + 0 duplicates 25 + 0 mapped (12.50%:nan%) 200 + 0 paired in sequencing 100 + 0 read1 100 + 0 read2 0 + 0 properly paired (0.00%:nan%) 0 + 0 with itself and mate mapped 25 + 0 singletons (12.50%:nan%) 0 + 0 with mate mapped to a different chr 0 + 0 with mate mapped to a different chr (mapQ>=5) The results of samtools flagstat can be visualized with MultiQC."
toolshed.g2.bx.psu.edu/repos/devteam/samtools_idxstats/samtools_idxstats/2.0.8	"What it does
 Runs the 
samtools idxstats
 command. It retrieves and prints stats in the index file. Input is a sorted and indexed BAM file, the output is tabular with four columns (one row per reference sequence plus a final line for unmapped reads):: Column Description ------ ----------------------------- 1 Reference sequence identifier 2 Reference sequence length 3 Number of mapped reads 4 Number of placed but unmapped reads (typically unmapped partners of mapped reads) ------ 
Example
 output from a 
de novo
 assembly:: contig_1 170035 98397 0 contig_2 403835 199564 0 contig_3 553102 288189 0 ... ... ... ... contig_603 653 50 0 contig_604 214 6 0 * 0 0 50320 In this example there were 604 contigs, each with one line in the output table, plus the final row (labelled with an asterisk) representing 50320 unmapped reads. In this BAM file, the final column was otherwise zero. The results of samtools ixdstats can be visualized with MultiQC. ------ Peter J.A. Cock (2013), 
Galaxy wrapper &lt;https://github.com/peterjc/pico_galaxy/tree/master/tools/samtools_idxstats&gt;
_ for the samtools idxstats command"
toolshed.g2.bx.psu.edu/repos/iuc/samtools_markdup/samtools_markdup/1.22+galaxy1	Mark duplicate alignments from a coordinate sorted file that has been run through fixmate with the -m option. This program relies on the MC and ms tags that fixmate provides. Note: The Galaxy tool sorts the data automatically if the input is SAM or query name sorted. The output is BAM (which is query name sorted again if the input is). The optional basic statistics output of samtools markdup can be visualized with MultiQC.
toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.22+galaxy1	"What it does
 Merge multiple sorted alignment files, producing a single sorted output file that contains all the input records and maintains the existing sort order. If a file to take @headers from is specified the @SQ headers of input files will be merged into the specified header, otherwise they will be merged into a composite header created from the input headers. If in the process of merging @SQ lines for coordinate sorted input files, a conflict arises as to the order (for example input1.bam has @SQ for a,b,c and input2.bam has b,a,c) then the resulting output file will need to be re-sorted back into coordinate order. Unless the @PG/@RG headers are made unique when merging @RG and @PG records into the output header then any IDs found to be duplicates of existing IDs in the output header will have a suffix appended to them to differentiate them from similar header records from other files and the read records will be updated to reflect this."
toolshed.g2.bx.psu.edu/repos/devteam/samtools_mpileup/samtools_mpileup/2.2.0	
toolshed.g2.bx.psu.edu/repos/devteam/samtools_phase/samtools_phase/2.0.2	"What it does
 Call and phase heterozygous SNPs."
toolshed.g2.bx.psu.edu/repos/devteam/samtools_reheader/samtools_reheader/2.0.6	"What it does
 Generates a new BAM dataset with the contents of 
target
 dataset, but the header of 
source
 dataset using the 
samtools reheader
 command."
toolshed.g2.bx.psu.edu/repos/devteam/samtools_sort/samtools_sort/2.0.8	"What it does
 Sort alignments by leftmost coordinates, or by read name when -n is used. An appropriate @HD-SO sort order header tag will be added or an existing one updated if necessary. 
Ordering Rules
 The following rules are used for ordering records. If option -t is in use, records are first sorted by the value of the given alignment tag, and then by position or name (if using -n). For example, “-t RG” will make read group the primary sort key. The rules for ordering by tag are: - Records that do not have the tag are sorted before ones that do. - If the types of the tags are different, they will be sorted so that single character tags (type A) come before array tags (type B), then string tags (types H and Z), then numeric tags (types f and i). - Numeric tags (types f and i) are compared by value. Note that comparisons of floating-point values are subject to issues of rounding and precision. - String tags (types H and Z) are compared based on the binary contents of the tag using the C strcmp(3) function. - Character tags (type A) are compared by binary character value. - No attempt is made to compare tags of other types — notably type B array values will not be compared. When the -n option is present, records are sorted by name. Names are compared so as to give a “natural” ordering — i.e. sections consisting of digits are compared numerically while all other sections are compared based on their binary representation. This means “a1” will come before “b1” and “a9” will come before “a10”. Records with the same name will be ordered according to the values of the READ1 and READ2 flags (see flags). When the -n option is not present, reads are sorted by reference (according to the order of the @SQ header records), then by position in the reference, and then by the REVERSE flag. This has now been removed. The previous out.prefix argument (and -f option, if any) should be changed to an appropriate combination of -T PREFIX and -o FILE. The previous -o option should be removed, as output defaults to standard output. When the -M (minash collation) option is present, then samtools sort groups unmapped reads with similar sequence together. This can sometimes significantly reduce the file size."
toolshed.g2.bx.psu.edu/repos/devteam/samtools_split/samtools_split/1.22+galaxy1	"What it does
 Splits BAM files on readgroups. This tool is based on 
samtools split
 command. It will generate multiple output datasets for each readgroup from the input dataset."
toolshed.g2.bx.psu.edu/repos/devteam/samtools_stats/samtools_stats/2.0.8	Required for GC-depth and mismatches-per-cycle calculation
toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.22+galaxy1	Reference data as fasta(.gz). Required for SAM input without @SQ headers and useful/required for writing CRAM output (see help).
toolshed.g2.bx.psu.edu/repos/devteam/samtools_slice_bam/samtools_slice_bam/2.0.6	"What it does
 Allows to restrict (slice) input BAM dataset to a list of intervals defined in a BED file, individual chromosomes, or manually set list of coordinates. BED datasets can be obtained from 
Get Data -> UCSC Main
. This tool is based on 
samtools view
 command. @no-chrom-options@"
toolshed.g2.bx.psu.edu/repos/devteam/bamtools_split/bamSplit/2.4.0	"What is does
 BAMTools split is a utility for splitting BAM files. It is based on BAMtools suite of tools by Derek Barnett (https://github.com/pezmaster31/bamtools). ----- .. class:: warningmark 
DANGER: Multiple Outputs
 As described below, splitting a BAM dataset(s) on reference name or a tag value can produce very large numbers of outputs. Read below and know what you are doing. ----- 
How it works
 The following options can be specified via ""
Split BAM dataset(s) by
"" dropdown:: Mapping status (-mapped) split mapped/unmapped and generate two output files named (MAPPED) and (UNMAPPED) containing mapped and unmapped reads, respectively. Pairing status (-paired) split single-end/paired-end alignments and generate two output files named (SINGLE_END) and (PAIRED_END) containing paired and unpaired reads, respectively. Reference name (-reference) split alignments by reference name. In cases of unfinished genomes with very large number of reference sequences (scaffolds) it can generate thousands (if not millions) of output datasets. Specific tag (-tag) split alignments based on all values of TAG encountered. Choosing this option from the menu will allow you to enter the tag name. As was the case with the reference splitting above, this option can produce very large number of outputs if a tag has a large number of unique values. ----- .. class:: infomark 
More information
 Additional information about BAMtools can be found at https://github.com/pezmaster31/bamtools/wiki"
toolshed.g2.bx.psu.edu/repos/devteam/bamtools_split/bamtools_split/2.5.2+galaxy0	"What is does
 BAMTools split is a utility for splitting BAM files. It is based on BAMtools suite of tools by Derek Barnett (https://github.com/pezmaster31/bamtools). ----- .. class:: warningmark 
DANGER: Multiple Outputs
 As described below, splitting a BAM dataset(s) on reference name or a tag value can produce very large numbers of outputs. Read below and know what you are doing. ----- 
How it works
 The following options can be specified via ""
Split BAM dataset(s) by
"" dropdown:: Mapping status (-mapped) split mapped/unmapped and generate two output files named (MAPPED) and (UNMAPPED) containing mapped and unmapped reads, respectively. Pairing status (-paired) split single-end/paired-end alignments and generate two output files named (SINGLE_END) and (PAIRED_END) containing paired and unpaired reads, respectively. Reference name (-reference) split alignments by reference name. In cases of unfinished genomes with very large number of reference sequences (scaffolds) it can generate thousands (if not millions) of output datasets. Specific tag (-tag) split alignments based on all values of TAG encountered. Choosing this option from the menu will allow you to enter the tag name. As was the case with the reference splitting above, this option can produce very large number of outputs if a tag has a large number of unique values. ----- .. class:: infomark 
More information
 Additional information about BAMtools can be found at https://github.com/pezmaster31/bamtools/wiki"
toolshed.g2.bx.psu.edu/repos/iuc/bamtools_split_tag/bamtools_split_tag/2.5.2+galaxy2	"What is does
 BAMTools split is a utility for splitting BAM files. It is based on the BAMtools suite of tools by Derek Barnett (https://github.com/pezmaster31/bamtools). ----- 
How it works
 Split alignments by tag name into a dataset list collection. .. class:: warningmark This can generate a collection with a huge number of elements depending on the number of distinct values of the TAG. ----- .. class:: infomark 
More information
 Additional information about BAMtools can be found at https://github.com/pezmaster31/bamtools/wiki"
toolshed.g2.bx.psu.edu/repos/iuc/bamtools_split_mapped/bamtools_split_mapped/2.5.2+galaxy2	"What is does
 BAMTools split is a utility for splitting BAM files. It is based on BAMtools suite of tools by Derek Barnett (https://github.com/pezmaster31/bamtools). ----- 
How it works
 Splits the input BAM file into 2 output files containing mapped and unmapped reads, respectively. ----- .. class:: infomark 
More information
 Additional information about BAMtools can be found at https://github.com/pezmaster31/bamtools/wiki"
toolshed.g2.bx.psu.edu/repos/iuc/bamtools_split_ref/bamtools_split_ref/2.5.2+galaxy2	"Click and type in the box above to see options. You can select multiple entries. If ""No options available"" is displayed, you need to re-detect metadata on the input dataset."
toolshed.g2.bx.psu.edu/repos/iuc/bamtools_split_paired/bamtools_split_paired/2.5.2+galaxy2	"What is does
 BAMTools split is a utility for splitting BAM files. It is based on BAMtools suite of tools by Derek Barnett (https://github.com/pezmaster31/bamtools). ----- 
How it works
 Splits the input BAM file into 2 output files named containing single_end and paired_end reads, respectively. ----- .. class:: infomark 
More information
 Additional information about BAMtools can be found at https://github.com/pezmaster31/bamtools/wiki"
toolshed.g2.bx.psu.edu/repos/iuc/mosdepth/mosdepth/0.3.12+galaxy0	"mosdepth_ is a tool for fast and flexible calculation of read depths from BAM or CRAM files. It can compute: * mean (or median) depth in fixed-sized windows * mean (or median) depth in regions specified by a BED file * per base depths * a histogram of read depths * the mean or median coverage histogram for windows / regions * a distribution of proportion of based covered over a particular threshold * a BED format report on regions that are defined by coverage thresholds By default, only a summary and depth histogram is computed, but the other options mentioned above can be enabled using different options in the ""Compute depth by region"" selector and some of the Advanced options. .. _mosdepth: https://github.com/brentp/mosdepth"
toolshed.g2.bx.psu.edu/repos/lldelisle/revertr2orientationinbam/revertR2orientationInBam/0.0.2	"This tool is very useful when you have paired-end stranded RNA-seq. Using this tool prior to a bedtools genome coverage allow to have strand specific coverage using both mates. It uses samtools to convert input to sam format and then awk to modify the flag ""reverse strand"" for the second mate of pairs."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/run_sccaf/run_sccaf/0.0.9+galaxy0	"SCCAF explained =============== Single Cell Clustering Assessment Framework (SCCAF) is a novel method for automated identification of putative cell types from single cell RNA-seq (scRNA-seq) data. By iteratively applying clustering and a machine learning approach to gene expression profiles of a given set of cells, SCCAF simultaneously identifies distinct cell groups and a weighted list of feature genes for each group. The feature genes, which are overexpressed in the particular cell group, jointly discriminate the given cell group from other cells. Each such group of cells corresponds to a putative cell type or state, characterised by the feature genes as markers. Inputs ------ * AnnData object which contains the expression matrix and pre-calculated coordinates for UMAP. The AnnData object can include already clustering data, in which case the user will need to know on which AnnData slot/label is contained. * Optional external text file with mappings between cells and clusters (when no clustering is given inside the AnnData file). Modes of operation ------------------ * Optimisation with exit condition based on accuracy cut-off. In this case the user provides a minimum cut-off for the accuracy to be achieved and the optimisation process will exit at that point. * Optimisation with exit condition based on under-clustered scenario. In the case the AnnData object given must include a low resolution, under clustered clustering, and its label must be know to be specified. * Assesment only (no optimisation), where an existing clustering is assessed. This is resource intensive. Distributed assesment --------------------- If running the optimisation you can distribute assessments of the optimisation results. For this, activate the ""Produce parameter walk for assessment distribution"" option, which will generate a ""Rounds for assesment distribution"". Then feed the AnnData output of the optimisation process and the rounds output to the SCCAF Assesment module. Then merge all assessment results with SCCAF Assesment Merger (this also receives the rounds output). The workflow would look like this: .. image:: example_sccaf_workflow.png :height: 400 px :width: 850 px :scale: 80 %"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/sccaf_asses/sccaf_asses/0.0.9+galaxy0	SCCAF explained =============== Single Cell Clustering Assessment Framework (SCCAF) is a novel method for automated identification of putative cell types from single cell RNA-seq (scRNA-seq) data. By iteratively applying clustering and a machine learning approach to gene expression profiles of a given set of cells, SCCAF simultaneously identifies distinct cell groups and a weighted list of feature genes for each group. The feature genes, which are overexpressed in the particular cell group, jointly discriminate the given cell group from other cells. Each such group of cells corresponds to a putative cell type or state, characterised by the feature genes as markers. SCCAF Assesment =============== This module is used to independently asses either SCCAF Optimisation rounds results or just any clustering contained in an AnnData object that contains also UMAP embeddings (necessary for the plots shown). The main purpose of this independent assesment module is to distribute assesment runs after an optimisation.
toolshed.g2.bx.psu.edu/repos/ebi-gxa/sccaf_asses_merger/sccaf_asses_merger/0.0.9+galaxy0	"SCCAF explained =============== Single Cell Clustering Assessment Framework (SCCAF) is a novel method for automated identification of putative cell types from single cell RNA-seq (scRNA-seq) data. By iteratively applying clustering and a machine learning approach to gene expression profiles of a given set of cells, SCCAF simultaneously identifies distinct cell groups and a weighted list of feature genes for each group. The feature genes, which are overexpressed in the particular cell group, jointly discriminate the given cell group from other cells. Each such group of cells corresponds to a putative cell type or state, characterised by the feature genes as markers. The main tool in the SCCAF suite is Run SCCAF. The purpose of this tool is to merge multiple runs of SCCAF Assesment and produce a single plot that compares Test accuracy and Cross Validations, towards deciding on the best solution provided by the optimisation process. To produce the required Round files collection, RUN SCCAF needs to be run in optimisation mode and enable the 
Produce parameter walk for asessment distribution
 option."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/sccaf_regress_out/sccaf_regress_out/0.0.9+galaxy1	"============================= Operations on AnnData objects ============================= Performs the following operations: * Change observation fields, mostly for downstreaming processes convenience. Multiple fields can be changed as one. * Flag genes that start with a certain text: useful for flagging mitochondrial, spikes or other groups of genes. * For the flags created, calculates qc metrics (pct_<flag>
counts). * Calculates 
n_genes
, 
n_counts
 for cells and 
n_cells
, 
n_counts
 for genes. * For top <N> genes specified, calculate qc metrics (pct_counts_in_top
<N>_genes). This functionality will probably be added in the future to a larger package."
toolshed.g2.bx.psu.edu/repos/arkarachai-fungtammasan/str_fm/microsatcompat/1.0.0	".. class:: infomark 
What it does
 This tool is used to select only those input lines that have compatible STR motifs between the two user-specified columns. Two STR motifs are called compatible if they are either identical, or complementary, or produce the same sequence on rotating the start of the motif. For example, 
A
 is considered compatible with 
A
 and its reverse complement 
T
. Similarly, 
AGG
 considered compatible with 
AGG
, its reverse complement 
TCC
, and their rotations 
GGA
, 
GAG
, 
CCT
 and 
CTC
. For STR-FM pipeline (profiling STRs in short read data), this tool can be used to make sure that the STRs in the reads have the compatible motif as the STRs in the reference at the corresponding mapped location. 
Citation
 When you use this tool, please cite 
Fungtammasan A, Ananda G, Hile SE, Su MS, Sun C, Harris R, Medvedev P, Eckert K, Makova KD. 2015. Accurate Typing of Short Tandem Repeats from Genome-wide Sequencing Data and its Applications, Genome Research
 
Input
 The input files can be any tab delimited file. If this tool is used in STR-FM pipeline for STRs profiling, it should contains: - Column 1 = STR location in reference chromosome - Column 2 = STR location in reference start - Column 3 = STR location in reference stop - Column 4 = STR location in reference motif - Column 5 = STR location in reference length - Column 6 = STR location in reference motif size - Column 7 = length of STR (bp) - Column 8 = length of left flanking region (bp) - Column 9 = length of right flanking region (bp) - Column 10 = repeat motif (bp) - Column 11 = hamming distance - Column 12 = read name - Column 13 = read sequence with soft masking of STR - Column 14 = read quality (the same Phred score scale as input) - Column 15 = read name (The same as column 12) - Column 16 = chromosome - Column 17 = left flanking region start - Column 18 = left flanking region stop - Column 19 = STR start as infer from pair-end - Column 20 = STR stop as infer from pair-end - Column 21 = right flanking region start - Column 22 = right flanking region stop - Column 23 = STR length in reference - Column 24 = STR sequence in reference 
Output
 The same as input format."
toolshed.g2.bx.psu.edu/repos/arkarachai-fungtammasan/str_fm/PEsortedSAM2readprofile/1.1.0	".. class:: infomark 
What it does
 - This tool will take SAM file (sorted by read name), remove unpaired reads, and combine paired faux read-pairs into a single row. It also reports Short Tandem Repeats (STRs) sequences in the reference genome that correspond to the space between the faux paired end reads and the coordinate of start and stop for left and right flanking regions of STRs. 
Citation
 When you use this tool, please cite 
Fungtammasan A, Ananda G, Hile SE, Su MS, Sun C, Harris R, Medvedev P, Eckert K, Makova KD. 2015. Accurate Typing of Short Tandem Repeats from Genome-wide Sequencing Data and its Applications, Genome Research
 
Input
 - Sorted SAM files by read name. 
Output
 The output will combine the two faux paired-end read lines of input ito the following single line format: - Column 1 = read name - Column 2 = chromosome - Column 3 = left flanking region start - Column 4 = left flanking region stop - Column 5 = STR start - Column 6 = STR stop - Column 7 = right flanking region start - Column 8 = right flanking region stop - Column 9 = STR length in reference - Column 10= STR sequence in reference"
toolshed.g2.bx.psu.edu/repos/arkarachai-fungtammasan/str_fm/combineproballelecom/2.0.0	".. class:: infomark 
What it does
 - This tool will combine the read profile probabilities for each allele combination in the input and calculates the probability to detect heterozygote for each allele combination and each depth. 
Citation
 When you use this tool, please cite 
Fungtammasan A, Ananda G, Hile SE, Su MS, Sun C, Harris R, Medvedev P, Eckert K, Makova KD. 2015. Accurate Typing of Short Tandem Repeats from Genome-wide Sequencing Data and its Applications, Genome Research
 
Input
 The input format is the same as output from 
Evaluate the probability of the allele combination to generate read profile
 tool. - Column 1 = location of STR locus. - Column 2 = length profile (length of STR in each read that mapped to this location in comma separated format). - Column 3 = motif of STR in this locus. The input file can contain more than three columns. - Column 4 = homozygote/heterozygote label. - Column 5 = log based 10 of (the probability of homozygote/the probability of heterozygote) - Column 6 = Allele for most probable homozygote. - Column 7 = Allele 1 for most probable heterozygote. - Column 8 = Allele 2 for most probable heterozygote. - Column 9 = Probability of the allele combination to generate given read profile. - Column 10 = Number of possible rearrangement of given read profile. - Column 11 = Probability of the allele combination to generate read profile with any rearrangement (Product of column 9 and column 10) - Column 12 = Read depth Only columns 2,3,4,7,8,11 were used in calculation. 
Output
 The output will contain the following header and columns - Line 1 header: read_depth allele heterozygous_prob motif - Column 1 = read depth - Column 2 = allele combination - Column 3 = probability to detect heterozygote of that allele combination - Column 4 = motif"
toolshed.g2.bx.psu.edu/repos/arkarachai-fungtammasan/str_fm/readdepth2seqdepth/1.0.0	".. class:: infomark 
What it does
 This tool is used to convert informative read depth (specified by user) to sequencing depth when the STRs is mapped using STR-FM pipeline. The locus specific sequencing depth (yrequired) is the sequencing depth that will make an STR locus to have a certain informative read depth based on uniform mapping of reads. It is calculated as follows: :: yrequired = ( X * L ) / (L - (2F+r-1)) where X = informative read depth, L = read length, F = the number of flanking bases required on either side, r = the expected repeat length of the STR of interest. The genome wide sequencing depth is the sequencing depth that will make certain percentage of genome (e.g. 90 percent or 95 percent) to have certain locus specific sequencing depth. It's calculated using numerical guessing to find smallest lambda that: :: 0.90 (or other proportion specified by user) < = P(Y=0) + P(Y=1) + …+ P(Y=yrequired-1) where P(Y=y) = (lambda^(y) * e ^(-lambda)) /y! y = specific level of sequencing depth. Lambda = genome wide sequencing depth Please refer the Methods section of the paper cited below for further details. 
Citation
 When you use this tool, please cite 
Fungtammasan A, Ananda G, Hile SE, Su MS, Sun C, Harris R, Medvedev P, Eckert K, Makova KD. 2015. Accurate Typing of Short Tandem Repeats from Genome-wide Sequencing Data and its Applications, Genome Research"
toolshed.g2.bx.psu.edu/repos/arkarachai-fungtammasan/str_fm/GenotypeSTR/2.0.0	".. class:: infomark 
What it does
 - This tool will correct for STR sequencing and library preparation errors using error rates estimated from hemizygous male X chromosome (https://usegalaxy.org/u/guru%40psu.edu/h/error-rates-files) or rates provided by user. The STR length profile for each locus will be processed independently. - First, this tool will find three most common STR lengths from input STR length profile. If the STR length profile has only one length of STR, the length of one motif longer than the observed length will be used as the second most common STR length. - Second, it will calculate probability of three forms of homozygotes and use the form with the highest probability. The same goes for heterozygotes. - Third, this tools will calculate log10 of the ratio of the probability of homozygote to the probability of heterozygote. If this value is more than 0, it will predict this locus to be homozygote. If this value is less than 0, it will predict this locus to be heterozygote. If this value is 0, read profile at this locus will be discarded. 
Citation
 When you use this tool, please cite 
Fungtammasan A, Ananda G, Hile SE, Su MS, Sun C, Harris R, Medvedev P, Eckert K, Makova KD. 2015. Accurate Typing of Short Tandem Repeats from Genome-wide Sequencing Data and its Applications, Genome Research
 
Input
 - The input files need to contain at least three columns. - Column 1 = location of STR locus. - Column 2 = length profile (length of STR in each read that mapped to this location in comma separated format). - Column 3 = motif of STR in this locus. The input file can contain more than three columns. 
Output
 The output will be contain original three (or more) columns as the input. However, it will also have these following columns. - Additional column 1 = homozygote/heterozygote label. - Additional column 2 = log based 10 of (the probability of homozygote/the probability of heterozygote) - Additional column 3 = Allele for most probable homozygote. - Additional column 4 = Allele 1 for most probable heterozygote. - Additional column 5 = Allele 2 for most probable heterozygote. 
Example
 - Suppose that we sequence a locus of STR with NGS. This locus has 
A
 motif and the following STR length (bp) profile. :: chr1_100_106 5, 6, 6, 6, 6, 7, 7, 8, 8 A - We want to figure out if this locus is a homoozygote or heterozygote and the corresponding allele(s). Therefore, we use this tool to refine genotype. - This tool will calculate the probability of homozygote A6A6, A7A7, and A8A8 to generate the observed STR length profile. Among this A7A7 has the highest probability. Therefore, we use this form as the representative for homozygote. - Then, this tool will calculate the probability of heterozygote A6A7, A7A8, and A6A8 to generate the observed STR length profile. Among this A6A8 has the highest probability. Therefore, we use this form as the representative for heterozygote. - Finally, it will compare the representative homozygous and heterozygous forms. The A6A8 has higher probability than A7A7. Therefore, the program will report that this locus as a heterozygous locus of form A6A8. :: chr1 5,6,6,6,6,7,7,8,8 A hetero -14.8744881854 7 6 8"
toolshed.g2.bx.psu.edu/repos/arkarachai-fungtammasan/str_fm/heteroprob/2.0.0	".. class:: infomark 
What it does
 - This tool will calculate the probability that the allele combination can generated the given the STR length profile. This tool is part of the pipeline to estimate minimum read depth. - The calculation of probability is very similar to the tool 
Correct genotype for STR errors
. However, this tool will restrict the calculation to only the allele combination indicated in input. Also, when it encounter allele combination that cannot be generated from error profile, the total probability will be zero instead of using base substitution rate. 
Citation
 When you use this tool, please cite 
Fungtammasan A, Ananda G, Hile SE, Su MS, Sun C, Harris R, Medvedev P, Eckert K, Makova KD. 2015. Accurate Typing of Short Tandem Repeats from Genome-wide Sequencing Data and its Applications, Genome Research
 
Input
 The input format is the same as output from 
Correct genotype for STR errors
 tool. - Column 1 = location of STR locus. - Column 2 = length profile (length of STR in each read that mapped to this location in comma separated format). - Column 3 = motif of STR in this locus. The input file can contain more than three column. - Column 4 = homozygote/heterozygote label. - Column 5 = log based 10 of (the probability of homozygote/the probability of heterozygote) - Column 6 = Allele for most probable homozygote. - Column 7 = Allele 1 for most probable heterozygote. - Column 8 = Allele 2 for most probable heterozygote. Only column 2,3,7,8 were used in calculation. 
Output
 The output will contain the original eight columns from the input and the following additional columns. - Column 9 = Probability of the allele combination to generate given read profile. - Column 10 = Number of possible rearrangements of the given read profile. - Column 11 = Probability of the allele combination to generate read profile with any rearrangement (Product of column 9 and column 10) - Column 12 = Read depth"
toolshed.g2.bx.psu.edu/repos/arkarachai-fungtammasan/str_fm/fetchflank/1.0.0	".. class:: infomark 
What it does
 This tool will fetch flanking regions around STRs from the reads output by ""STR detection"" step, screen for quality score at STRs and adjacent flanking regions, and output two fastq files containing flanking regions in forward-forward direction. - This tool assumes that the quality score is Phred+33, such as Sanger fastq. - Reads that have either left or right flanking regions shorter than the length of flanking regions that require quality screening will be removed. 
Citation
 When you use this tool, please cite 
Fungtammasan A, Ananda G, Hile SE, Su MS, Sun C, Harris R, Medvedev P, Eckert K, Makova KD. 2015. Accurate Typing of Short Tandem Repeats from Genome-wide Sequencing Data and its Applications, Genome Research
 
Input
 The input file needs to be in the same format as output from 
STR detection
 step. This format contains 
length of repeat
, 
length of left flanking region
, 
length of right flanking region
, 
repeat motif
, 
hamming (editing) distance
, 
read name
, 
read sequence
, 
read quality score
 
Output
 The output will be two fastq files. The first file contains left flanking bases. The second file contains right flanking bases. 
Example
 - Starting with this test input :: 6 40 54 G 0 SRR345592.75000006 HS2000-192_107:1:63:5822:176818_1_per1_1 TACCCTCCTGTCTTCCCAGACTGATTTCTGTTCCTGCCCTggggggTTCTTGACTCCTCTGAATGGGTACGGGAGTGTGGACCTCAGGGAGGCCCCCTTG GGGGGGGGGGGGGGGGGFGGGGGGGGGFEGGGGGGGGGGG?FFDFGGGGGG?FFFGGGGGDEGGEFFBEFCEEBD@BACB
?=99(/=5'6=4:CCC
AA - If we want to get fastq files of flanking regions around the detected STRs with quality score of at least 20, the program will report these two fastq files :: @SRR345592.75000006 HS2000-192_107:1:63:5822:176818_1_per1_1 TACCCTCCTGTCTTCCCAGACTGATTTCTGTTCCTGCCCT +SRR345592.75000006 HS2000-192_107:1:63:5822:176818_1_per1_1 GGGGGGGGGGGGGGGGGFGGGGGGGGGFEGGGGGGGGGGG @SRR345592.75000006 HS2000-192_107:1:63:5822:176818_1_per1_1 TTCTTGACTCCTCTGAATGGGTACGGGAGTGTGGACCTCAGGGAGGCCCCCTTG +SRR345592.75000006 HS2000-192_107:1:63:5822:176818_1_per1_1 GGGGG?FFFGGGGGDEGGEFFBEFCEEBD@BACB
?=99(/=5'6=4:CCC
AA"
toolshed.g2.bx.psu.edu/repos/arkarachai-fungtammasan/str_fm/Profilegenerator/2.0.0	".. class:: infomark 
What it does
 This tool will generate all possible combination of observed STR length profiles of the consecutive alleles from given error profile. The range of observed read lengths can be filtered to contain only those that are frequently occur using ""Minimum error rate to be considered"" parameter. This program will collect the lists of valid (pass ""Minimum error rate to be considered"" threshold) observed length profiles from combination of consecutive allele lengths. The lists that are equivalent or the subset of the other lists will be removed. For each depth and each list, length profile were generated from combination with replacement which compatible with python 2.7. There could be redundant error profiles generated from different lists if more than one combination of allele is generated due to overlap range of observed microsatellite lengths. The user need to remove them which can be done easily using 
sort | uniq
 command in unix. 
Citation
 When you use this tool, please cite 
Fungtammasan A, Ananda G, Hile SE, Su MS, Sun C, Harris R, Medvedev P, Eckert K, Makova KD. 2015. Accurate Typing of Short Tandem Repeats from Genome-wide Sequencing Data and its Applications, Genome Research
 
Input
 - The error profile needs to contain these three columns. - Column 1 = Correct STR length - Column 2 = Observed STR length - Column 3 = Number of observation 
Output
 - Column 1 = Place holder for location of STR locus. (just ""chr"") - Column 2 = length profile (length of STR in each read that mapped to this location in comma separated format). - Column 3 = motif of STR in this locus. 
Example
 - Suppose that we provide the following STR length profile :: true obs. reads 9 9 100000 10 10 91456 10 9 1259 11 11 39657 11 10 1211 11 12 514 - Using the default minimum probability (fraction of reads) of 0.00000001 and motif = A, all observed STR lengths are valid. The program will generated lists of observed length profiles from consecutive allele lengths :: 9:10 = [9,10] 10:11 = [9,10,11,12] - Lists that are subsets of other lists will be removed. In this example, [9,10] will not be considered. - The program will then generate all combinations with replacement for each depth from each list. Using 
maximum read depth levels =3
, we will get the following output. :: chr 9,9 A chr 9,10 A chr 9,11 A chr 9,12 A chr 10,10 A chr 10,11 A chr 10,12 A chr 11,11 A chr 11,12 A chr 12,12 A chr 9,9,9 A chr 9,9,10 A chr 9,9,11 A chr 9,9,12 A chr 9,10,10 A chr 9,10,11 A chr 9,10,12 A chr 9,11,11 A chr 9,11,12 A chr 9,12,12 A chr 10,10,10 A chr 10,10,11 A chr 10,10,12 A chr 10,11,11 A chr 10,11,12 A chr 10,12,12 A chr 11,11,11 A chr 11,11,12 A chr 11,12,12 A chr 12,12,12 A"
toolshed.g2.bx.psu.edu/repos/arkarachai-fungtammasan/str_fm/space2underscore_readname/1.0.0	".. class:: infomark 
What it does
 The readname produced by the ""STR detection"" step may contain spaces instead of underscores, which will cause downstream tools that use space as a column delimiter to fail. This tool will help convert space to underscore. If your input does not have spaces in readname column, this step can be skipped. 
Citation
 When you use this tool, please cite 
Fungtammasan A, Ananda G, Hile SE, Su MS, Sun C, Harris R, Medvedev P, Eckert K, Makova KD. 2015. Accurate Typing of Short Tandem Repeats from Genome-wide Sequencing Data and its Applications, Genome Research
 
Input
 The input files can be any tab delimited file. If this tool is used in STR-FM for STRs profiling, it should be in the same format as output from 
STR detection program
. This format contains 
length of repeat
, 
length of left flanking region
, 
length of right flanking region
, 
repeat motif
, 
hamming (editing) distance
, 
read name
, 
read sequence
, 
read quality score
 
Output
 The same as input format."
toolshed.g2.bx.psu.edu/repos/arkarachai-fungtammasan/str_fm/microsatellite/1.0.0	".. class:: infomark 
What it does
 This tool identifies simple as well interrupted STRs. Choosing a hamming distance of zero will return simple STRs. Choosing a hamming distance of greater than zero will return both simple and interrupted STRs. The algorithms used to identify simple and interrupted STRs are described oin the manuscript cited below (see TABLE XXXX). 
Citation
 When you use this tool, please cite 
Fungtammasan A, Ananda G, Hile SE, Su MS, Sun C, Harris R, Medvedev P, Eckert K, Makova KD. 2015. Accurate Typing of Short Tandem Repeats from Genome-wide Sequencing Data and its Applications, Genome Research
 This tool is developed by Chen Sun (cxs1031@cse.psu.edu) and Bob Harris (rsharris@bx.psu.edu) 
Input
 - The input files can be fastq, fasta, fastq without quality score, and SAM format. 
Output
 For fastq, the output will contain the following columns: - Column 1 = length of STR (bp) - Column 2 = length of left flanking region (bp) - Column 3 = length of right flanking region (bp) - Column 4 = repeat motif (bp) - Column 5 = hamming distance - Column 6 = read name - Column 7 = read sequence with soft masking of STR - Column 8 = read quality (the same Phred score scale as input) For fasta, fastq without quality score and sam format, column 8 will be replaced with dot(.). If the users have mapped file (SAM) and would like to profile STRs from premapped data instead of using flank-based mapping approach, they can select SAM format input and specify that they want correspond STRs in reference for comparison. The output will be as follow: - Column 1 = length of STR (bp) - Column 2 = length of left flanking region (bp) - Column 3 = length of right flanking region (bp) - Column 4 = repeat motif (bp) - Column 5 = hamming distance - Column 6 = read name - Column 7 = read sequence with soft masking of STR - Column 8 = read quality (the same Phred score scale as input) - Column 9 = read name (The same as column 6) - Column 10 = chromosome - Column 11 = left flanking region start - Column 12 = left flanking region stop - Column 13 = STR start as infer from pair-end - Column 14 = STR stop as infer from pair-end - Column 15 = right flanking region start - Column 16 = right flanking region stop - Column 17 = STR length in reference - Column 18 = STR sequence in reference"
toolshed.g2.bx.psu.edu/repos/arkarachai-fungtammasan/str_fm/microsatpurity/1.0.0	".. class:: infomark 
What it does
 This tool is used to select only the uninterrupted STRs/microsatellites. Interrupted STRs (e.g. ATATATATAATATAT) or sequences of STRs with non-STR parts (e.g. ATATATATATG) will be removed. As another application of this tool, specifically for STR-FM pipeline (profiling STRs in short read data), it can be used to avoid the cases where flanking bases were misread as STRs (sequencing errors). Thus, the remaining read profile will only reflect the variation of TR length from expansion/contraction. For example, suppose that the sequence around an STR in the reference genome is AGCGACGaaaaaaGCGATCA. If we observe a read with sequence AGCGACGaaaaaaaaaaGCGATCA, we can indicate that this is an STR expansion. However, if we observe another read with sequence AGCGACGaaaaaaaCGATCA, this is likely a substitution of G to A. Such incidents can be removed with this tool. You can use the tool 
combine mapped flanking bases
 to get the STRs in reference that correspond to sequence between mapped reads. If the user map these reads around the uninterrupted STRs in reference, the corresponding sequences between these pairs should be the uninterrupted STRs regardless of expansion/contraction of STRs in short read data. However, if the substitution of flanking base or if the fluorescent signal from the previous run make it look like substitution, the corresponding sequences in reference in between the pairs will not be uninterrupted STRs. Thus this tool can remove those cases and keep only STR expansion/contraction. 
Citation
 When you use this tool, please cite 
Fungtammasan A, Ananda G, Hile SE, Su MS, Sun C, Harris R, Medvedev P, Eckert K, Makova KD. 2015. Accurate Typing of Short Tandem Repeats from Genome-wide Sequencing Data and its Applications, Genome Research
 
Input
 The input files can be any tab delimited file. If this tool is used in STR-FM for STRs profiling, it should contains: - Column 1 = STR location in reference chromosome - Column 2 = STR location in reference start - Column 3 = STR location in reference stop - Column 4 = STR location in reference motif - Column 5 = STR location in reference length - Column 6 = STR location in reference motif size - Column 7 = length of STR (bp) - Column 8 = length of left flanking region (bp) - Column 9 = length of right flanking region (bp) - Column 10 = repeat motif (bp) - Column 11 = hamming distance - Column 12 = read name - Column 13 = read sequence with soft masking of STR - Column 14 = read quality (the same Phred score scale as input) - Column 15 = read name (The same as column 12) - Column 16 = chromosome - Column 17 = left flanking region start - Column 18 = left flanking region stop - Column 19 = STR start as infer from pair-end - Column 20 = STR stop as infer from pair-end - Column 21 = right flanking region start - Column 22 = right flanking region stop - Column 23 = STR length in reference - Column 24 = STR sequence in reference 
Output
 The same as input format."
export_remote	
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_comp/1.5+galaxy0	"What it does
 Reports composition of fasta/fastq sequences. For an example sequence like :: >test0 ACTGACTGAA >ambig_ref ACGTCGTGTTVHDBN The seqtk tool will report: :: #chr length #A #C #G #T #2 #3 #4 #CpG #tv #ts #CpG-ts test0 11 4 2 2 2 0 0 1 0 0 0 0 ambig_ref 15 1 2 3 4 0 4 1 4 0 0 0 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_cutN/1.5+galaxy0	"What it does
 Splits long sequences with runs of Ns :: >test AACTGATCGATCGATCGNNNNNNNNNNNACATG This will be split into the component sequences without the ambiguity. :: >test:1-17 AACTGATCGATCGATCG >test:29-33 ACATG 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_dropse/1.5+galaxy0	"What it does
 Remove unpaired reads in an interleaved paired-end FASTA/Q file. Given a fastq file with unpaired reads: :: @test-6/1 AGCTTGACGC + ?.HCF@C>>F @test-6/2 TGCGAAGACC + >2?A?A@@7? @test-4/1 CTTGACGCTG + I@>3EFCG@C @test-2/1 AGACCAAAAT + ??><6E?IFC @test-2/2 CTGGCGAATT + ?=?
?A?<?@ This tool will remove the offending reads (test-4/1), leaving just the paired data. :: @test-6/1 AGCTTGACGC + ?.HCF@C>>F @test-6/2 TGCGAAGACC + >2?A?A@@7? @test-2/1 AGACCAAAAT + ??><6E?IFC @test-2/2 CTGGCGAATT + ?=?
?A?<?@ 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_fqchk/1.5+galaxy0	"What it does
 Returns quality score information base-by-base. :: @SEQ_ID1 GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + !''
((((
+))%%%++)(%%%%).1
-+
''))
55CCF>>>>>>CCCCCCC65 each based is examined individually and information reported: :: #min_len: 60; max_len: 60; avg_len: 60.00; 15 distinct quality values #POS #bases %A %C %G %T %N avgQ errQ %low %high #ALL 60 31.7 16.7 18.3 33.3 0.0 15.1 8.7 66.7 33.3 1 1 0.0 0.0 100.0 0.0 0.0 0.0 3.0 100.0 0.0 2 1 100.0 0.0 0.0 0.0 0.0 6.0 6.0 100.0 0.0 3 1 0.0 0.0 0.0 100.0 0.0 6.0 6.0 100.0 0.0 4 1 0.0 0.0 0.0 100.0 0.0 9.0 9.0 100.0 0.0 5 1 0.0 0.0 0.0 100.0 0.0 7.0 7.0 100.0 0.0 6 1 0.0 0.0 100.0 0.0 0.0 7.0 7.0 100.0 0.0 7 1 0.0 0.0 100.0 0.0 0.0 7.0 7.0 100.0 0.0 8 1 0.0 0.0 100.0 0.0 0.0 7.0 7.0 100.0 0.0 9 1 0.0 0.0 100.0 0.0 0.0 9.0 9.0 100.0 0.0 10 1 0.0 0.0 0.0 100.0 0.0 9.0 9.0 100.0 0.0 
Attribution** This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_hety/1.5+galaxy0	"What it does
 Reports on heterozygosity over a region :: >het_region ACTTTACATCGAGCMMMMMMMACAGTACTG As can be seen in the following output: :: #chr start end A B num_het het_region 0 8 0.00 8 0 het_region 8 9 0.00 8 0 het_region 9 10 0.00 8 0 het_region 10 11 0.00 8 0 het_region 11 12 0.00 8 0 het_region 12 13 0.00 8 0 het_region 13 14 0.00 8 0 het_region 14 15 1.00 8 1 het_region 15 16 2.00 8 2 het_region 16 17 3.00 8 3 het_region 17 18 4.00 8 4 het_region 18 19 5.00 8 5 het_region 19 20 6.00 8 6 het_region 20 21 7.00 8 7 het_region 21 22 7.00 8 7 het_region 22 23 6.00 8 6 het_region 23 24 5.00 8 5 het_region 24 25 4.00 8 4 het_region 25 26 3.00 8 3 het_region 26 27 2.00 8 2 het_region 27 28 1.00 8 1 het_region 28 29 0.00 8 0 het_region 29 30 0.00 1 0 If you know what A and B are measures of, please 
submit an issue &lt;https://github.com/galaxyproject/tools-iuc/issues&gt;
__ and it will be corrected 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_listhet/1.5+galaxy0	"What it does
 Lists regions of heterozygosity. :: >ambig ACGTMRWSYKVHDBN The seqtk suite recognises MRWSYK: :: #chr position base ambig 5 M ambig 6 R ambig 7 W ambig 8 S ambig 9 Y ambig 10 K 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_mergefa/1.5+galaxy1	"What it does
 This tool merges two FASTA or FASTQ files into a single FASTA file using IUPAC ambiguity codes where appropriate. When differences occur between the sequences, ambiguity codes are used to represent possible variations. Example:: >seq1 ACTGACTGAAA >seq2 ACTGAMTGCGN will result in:: >seq1 ACTGAMTGMRN If the 
-m
 option is in use, however, the tool will pick the least ambiguous base if there is no contradiction between the symbols in the inputs. Conflicts are indicated by using x in the merged sequence and the picked base is converted to lowercase if the less specific symbol is an N to express uncertainty. With this logic the input sequences above will result in the merge result:: >seq1 ACTGACTGxxa 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_mergepe/1.5+galaxy0	"What it does
 Merge two files which constitute a paired-end file into a single, interleaved, paired-end FASTA/Q file :: # r1.fq @test-6/1 AGCTTGACGC + ?.HCF@C>>F # r2.fq @test-6/2 TGCGAAGACC + >2?A?A@@7? will produce the following paired file: :: @test-6/1 AGCTTGACGC + ?.HCF@C>>F @test-6/2 TGCGAAGACC + >2?A?A@@7? While this may not have been an illuminating example, it is important to note that this tool will properly interleave data. For example if you have the ids: :: @r-1/1 @r-2/1 @r-3/1 @r-4/1 and :: @r-1/2 @r-2/2 @r-3/2 @r-4/2 These will be interleaved as :: @r-1/1 @r-1/2 @r-2/1 @r-2/2 @r-3/1 @r-3/2 @r-4/1 @r-4/2 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_mutfa/1.5+galaxy0	"What it does
 the SNP inputs consist of 4 columns: chromosome, 1-based-pos, any, base-changed-to. :: # Input fasta >test0 ACTGACTGAA # Input SNP file test0 1 . G test0 4 . A This will effect the desired mutations in the output file :: # Output result >test0 GCTAACTGAA 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_randbase/1.5+galaxy0	"What it does
 Randomly resolves ambiguous bases :: # Input fasta >ambig ACGTMRWSYK results in :: # Output result >ambig ACGTCGTGTT 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_sample/1.5+galaxy0	"What it does
 Takes a random subsample of FASTA or FASTQ sequences. The RNG is seedable to allow for reproducible results, and defaults to 
4 &lt;http://xkcd.com/221/&gt;
__. The subsample size can be a decimal fraction <=1, where 1 implies 100% of the reads should be used. If a number >1 is provided, that many reads will be taken from the dataset. Two pass sampling mode reads the input file once to build a list of reads to output then again to output the reads. It is twice as slow, but uses much less RAM. It is only in effect when an integer number of reads (not a fraction) is specified as subsample size. 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_seq/1.5+galaxy1	"What it does
 Various utilities for transforming FASTA/Q data 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_subseq/1.5+galaxy0	"What it does
 Given a list of newline separated IDs from a fasta file, this will extract those named fasta sequences from the input file. :: # Input ID list seq1 # Input fasta >seq1 ACGTMRWSYK >seq2 RWSYKACGTM results in :: # Output result >seq1 ACGTMRWSYK 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_telo/1.5+galaxy0	"What it does
 Identifies telomeres using a default vertebrate repeat sequence CCCTAA, or a user supplied one :: Usage: seqtk telo [options] <in.fq> Options: -m STR motif [CCCTAA] -p INT penalty [1] -d INT max drop [2000] -s INT min score [300] -P print scoring 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_trimfq/1.5+galaxy0	"What it does
 Trim a fastq file based on Phred scores, or by length 
Attribution
 This Galaxy tool relies on the seqtk toolkit from 
lh3/seqtk &lt;https://github.com/lh3/seqtk/&gt;
_, developed by Heng Li at the Broad Institute"
toolshed.g2.bx.psu.edu/repos/richard-burhans/ncbi_fcs_adaptor/ncbi_fcs_adaptor/0.5.0+galaxy0	FCS-adaptor is a high-throughput implementation of NCBI VecScreen. FCS-adaptor runs a pipeline to screen input sequences against a non-redudant database of adaptors and vectors using stringent BLAST searches and remove contaminants from your genome. FCS-adaptor removes terminal and internal matches to foreign sequences. Sequences identified as mostly adaptor/vector are removed entirely. FCS-adaptor produces a tabular output with details on the contaminant sequences identified as well as a cleaned FASTA. Please see the wiki for more information. https://github.com/ncbi/fcs/wiki/FCS-adaptor
toolshed.g2.bx.psu.edu/repos/iuc/ncbi_fcs_gx/ncbi_fcs_gx/0.5.5+galaxy2	FCS-GX detects contamination from foreign organisms in genome sequences using the genome cross-species aligner (GX). The FCS-GX executable retrieves a Docker or Singularity container and runs a pipeline to align sequences to a large database of NCBI genomes through modified k-mer seeds and assign a most likely taxonomic division. FCS-GX classifies sequences as contaminant when their taxonomic assignment is different from the user provided taxonomic identifier. A contamination summary provides an overview of observed contaminant divisions, counts, and total sizes, and an action report provides details and recommended actions for each problematic sequence. https://github.com/ncbi/fcs/wiki/FCS-GX
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_create_seurat_object/seurat_create_seurat_object/2.3.1+galaxy0	".. class:: infomark 
What it does
 Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. This tool creates a Seurat object from an RDS object. ----- 
Inputs
 * RDS object from Seurat read_10x module. ----- 
Outputs
 * Seurat RDS object .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ 
Version history
 0.0.1: Initial contribution. Maria Doyle, https://github.com/mblue9. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski(https://github.com/drosofff) and Lea Bellenger(https://github.com/bellenger-l)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_export_cellbrowser/seurat_export_cellbrowser/4.0.4+galaxy0	".. class:: infomark 
What it does
 This tool converts a Seurat object (hopefully with t-SNE results) and its accompanying marker genes file (optional) to a tar that can be feed to the UCSC CellBrowser tool. Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. ----- 
Inputs
 * RDS object ----- 
Outputs
 * Text file .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_filter_cells/seurat_filter_cells/4.0.4+galaxy0	".. class:: infomark 
What it does
 This tool filters cells in a Seurat object. Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. ----- 
Inputs
 * Seurat RDS object. Probably the one produced by Seurat create object. * Subset names. A list of attributes to subset on, colon separated (:). * Low thresholds. A minimum value for each of the attributes set in subset names, again, colon separated (:). Optional. * High thresholds. A maximum value for each of the attributes set in subset names, again, colon separated (:). Optional. * Cells to use. A list of cell names/idenfifiers to filter positively by. ----- 
Outputs
 * Seurat RDS object filtered according to the inputs. .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_find_clusters/seurat_find_clusters/4.0.4+galaxy0	".. class:: infomark 
What it does
 Identify clusters of cells by a shared nearest neighbor (SNN) modularity optimization based clustering algorithm. First calculate k-nearest neighbors and construct t he SNN graph (using Seurat find neighbours). Then optimize the modularity function to determine clusters. For a full description of the algorithms, see Waltman and van Eck (2013) The European Physical Journal B. Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. ----- 
Inputs
 * RDS object ----- 
Outputs
 * Seurat RDS object .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_find_markers/seurat_find_markers/4.0.4+galaxy0	".. class:: infomark 
What it does
 This tool finds markers (differentially expressed genes) for each of the identity classes in a dataset. It outputs a text file containing a ranked list of putative markers, and associated statistics (p-values, ROC score, etc.) p-value adjustment is performed using bonferroni correction based on the total number of genes in the dataset. Other correction methods are not recommended, as Seurat pre-filters genes using the arguments above, reducing the number of tests performed. Lastly, as Aaron Lun has pointed out, p-values should be interpreted cautiously, as the genes used for clustering are the same genes tested for differential expression. Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. ----- 
Inputs
 * RDS object ----- 
Outputs
 * Text file .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_find_neighbours/seurat_find_neighbours/4.0.4+galaxy0	".. class:: infomark 
What it does
 Constructs a Shared Nearest Neighbor (SNN) Graph for a given dataset. We first determine the k-nearest neighbors of each cell. We use this knn graph to construct the SNN graph by calculating the neighborhood overlap (Jaccard index) between every cell and its k.param nearest neighbors. Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. ----- 
Inputs
 * Seurat RDS object. Probably the one produced by Seurat create object. * Subset names. A list of attributes to subset on, colon separated (:). * Low thresholds. A minimum value for each of the attributes set in subset names, again, colon separated (:). Optional. * High thresholds. A maximum value for each of the attributes set in subset names, again, colon separated (:). Optional. * Cells to use. A list of cell names/idenfifiers to filter positively by. ----- 
Outputs
 * Seurat RDS object filtered according to the inputs. .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_find_variable_genes/seurat_find_variable_genes/4.0.4+galaxy0	".. class:: infomark 
What it does
 This tool identifies genes that are outliers on a 'mean variability plot'. First, uses a function to calculate average expression (mean.function) and dispersion (dispersion.function) for each gene. Next, divides genes into num.bin (deafult 20) bins based on their average expression, and calculates z-scores for dispersion within each bin. The purpose of this is to identify variable genes while controlling for the strong relationship between variability and average expression. For the mean.var.plot method: Exact parameter settings may vary empirically from dataset to dataset, and based on visual inspection of the plot. Setting the y.cutoff parameter to 2 identifies features that are more than two standard deviations away from the average dispersion within a bin. The default X-axis function is the mean expression level, and for Y-axis it is the log(Variance/mean). All mean/variance calculations are not performed in log-space, but the results are reported in log-space - see relevant functions for exact details. Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. ----- 
Inputs
 * Seurat RDS object * Mean function. Function to compute x-axis value (average expression). Default is to take the mean of the detected (i.e. non-zero) values. * Dispersion function. Function to compute y-axis value (dispersion). Default is to take the standard deviation of all values. * Bottom cutoff on x-axis for identifying variable genes. * Top cutoff on x-axis for identifying variable genes. * Bottom cutoff on y-axis for identifying variable genes. * Top cutoff on y-axis for identifying variable genes. ----- 
Outputs
 * Seurat RDS object. Places variable genes in object@var.genes. The result of all analysis is stored in object@hvg.info * Tabular file of variable genes .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_normalise_data/seurat_normalise_data/4.0.4+galaxy0	".. class:: infomark 
What it does
 This tool normalises a Seurat RDS object. Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. ----- 
Inputs
 * Seurat RDS object. Possibly the output of Seurat filter cells or Seurat create object. * Normalisation method. Method for normalization. Default is log-normalization (LogNormalize). * Assay type. Type of assay to normalize for (default is RNA), but can be changed for multimodal analyses. * Scale factor. Sets the scale factor for cell-level normalization. Default: 1000 ----- 
Outputs
 * Seurat RDS object with normalised matrix. .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_dim_plot/seurat_dim_plot/4.0.4+galaxy0	".. class:: infomark 
What it does
 Graphs the output of a dimensional reduction technique on a 2D scatter plot where each point is a cell and it's positioned based on the cell embeddings determined by the reduction technique. By default, cells are colored by their identity class (can be changed with the group.by parameter). Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. ----- 
Inputs
 * Seurat RDS object ----- 
Outputs
 * PNG-format image file .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_read10x/seurat_read10x/4.0.4+galaxy0	".. class:: infomark 
What it does
 Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. 
Read expression data from a tabular file or a 10x-Genomics-formatted mtx directory (
read_10x_mtx
)
 For mtx, the directory should contain: 1) Raw expression quantification as a sparse matrix in Matrix Market format, where the each column is a gene and each row is a barcode/cell. 2) A gene table of at least two columns where the first column gives the gene IDs. 3) A barcode/cell table of at least one column giving the barcode/cell IDs. The above-mentioned files can be obtained by running 
EBI SCXA Data Retrieval
 with a dataset accession. Otherwise, they need to be provided as separate Galaxy datasets. 
Inputs
 * A tabular file of expression data OR * Raw expression quantification as a sparse matrix in Matrix Market format, where the each column is a gene and each row is a barcode/cell. * A gene table of at least two columns where the first column gives the gene IDs. * A barcode/cell table of at least one column giving the barcode/cell IDs. * Optionally, a file with cell metadata. 
Outputs
 * R object for Seurat .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ .. _Seurat Guided Clustering tutorial: https://satijalab.org/seurat/pbmc3k_tutorial.html 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_run_pca/seurat_run_pca/4.0.4+galaxy0	".. class:: infomark 
What it does
 This tool runs a PCA dimensionality reduction. Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. ----- 
Inputs
 * Seurat RDS object, normalised and scaled potentially. * Genes used to scale. File of gene names to scale/center. Default is all genes in object. * Principal components to compute. Total Number of PCs to compute and store (20 by default). Less PCs might be faster, but will explain less variance. * Use imputed. Boolean indicating whether to run PCA on imputed values or not. ----- 
Outputs
 * Seurat RDS object with PCA calculations and embeddings. * Embeddings on CSV file. File with a csv-format embeddings table with principal components by cell. * Loadings on CSV file. File with a csv-format loadings table with principal components by gene. * Standard deviation on CSV file. Contains principal components std. deviations. .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_run_tsne/seurat_run_tsne/4.0.4+galaxy0	".. class:: infomark 
What it does
 Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. Run t-SNE dimensionality reduction on selected features. Has the option of running in a reduced dimensional space (i.e. spectral tSNE, recommended), or running based on a set of genes. ----- 
Inputs
 * RDS object ----- 
Outputs
 * Seurat RDS object .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/seurat_scale_data/seurat_scale_data/4.0.4+galaxy0	".. class:: infomark 
What it does
 This tool regresses out variables in a Seurat object to mitigate the effect of confounding factors. Seurat_ is a toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. It is developed and maintained by the 
Satija Lab
_ at NYGC. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. ----- 
Inputs
 * Seurat RDS object, probably normalised. * Genes to use. A file with a vector of gene names to scale/center (one gene per line). Default is all genes in object@data. * Variables to regress * Statistical model to use. * Use UMIs (boolean) * Do centering (boolean) * Scale maximum * Block size * Minimum number of cells to block * Check that data is normalised ----- 
Outputs
 * Seurat RDS object, scaled. .. _Seurat: https://www.nature.com/articles/nbt.4096 .. _Satija Lab: https://satijalab.org/seurat/ 
Version history
 4.0.0: Moves to Seurat 4.0.0, introducing a number of methods for merging datasets, plus the whole suite of Seurat plots. Pablo Moreno with funding from AstraZeneca. 3.2.3+galaxy0: Moves to Seurat 3.2.3 and introduce convert method, improving format interconversion support. 3.1.2_0.0.8: Update metadata parsing 3.1.1_0.0.7: Exposes perplexity and enables tab input. 3.1.1_0.0.6+galaxy0: Moved to Seurat 3. Find clusters: removed dims-use, k-param, prune-snn. 2.3.1+galaxy0: Improved documentation and further exposition of all script's options. Pablo Moreno, Jonathan Manning and Ni Huang, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/. Parts obtained from wrappers from Christophe Antoniewski (GitHub drosofff) and Lea Bellenger (GitHub bellenger-l). 0.0.1: Initial contribution. Maria Doyle (GitHub mblue9)."
toolshed.g2.bx.psu.edu/repos/bgruening/alevin/alevin/1.10.1+galaxy4	"Salmon is a lightweight method for quantifying transcript abundance from RNA–seq reads, combining a dual-phase parallel inference algorithm and feature-rich bias models with an ultra-fast read mapping procedure. The salmon package contains 4 tools: * Index: creates a salmon index * Quant: quantifies a sample (Reads or mapping-based) * Alevin: Single-cell analysis * Quantmerge: Merges multiple quantifications into a single file Galaxy divides these four into three separate tools in the IUC toolshed: * Salmon quant * Salmon quantmerge * Alevin Alevin is a tool — integrated with the salmon software — that introduces a family of algorithms for quantification and analysis of 3’ tagged-end single-cell sequencing data. Currently alevin supports the following two major droplet based single-cell protocols: * Drop-seq * 10x-Chromium v1/2/3 Alevin works under the same indexing scheme (as salmon) for the reference, and consumes the set of FASTA/Q files(s) containing the Cellular Barcode(CB) + Unique Molecule identifier (UMI) in one read file and the read sequence in the other. Given just the transcriptome and the raw read files, alevin generates a cell-by-gene count matrix (in a fraction of the time compared to other tools). Alevin works in two phases. In the first phase it quickly parses the read file containing the CB and UMI information to generate the frequency distribution of all the observed CBs, and creates a lightweight data-structure for fast-look up and correction of the CB. In the second round, alevin utilizes the read-sequences contained in the files to map the reads to the transcriptome, identify potential PCR/sequencing errors in the UMIs, and performs hybrid de-duplication while accounting for UMI collisions. Finally, a post-abundance estimation CB whitelisting procedure is done and a cell-by-gene count matrix is generated. For further information regarding the tool and its optional parameters, visit the 
Alevin &lt;https://salmon.readthedocs.io/en/latest/alevin.html?highlight=alevin&gt;
 and 
Salmon &lt;https://salmon.readthedocs.io/en/latest/index.html&gt;
 wikis."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/anndata_ops/anndata_ops/1.9.3+galaxy1	"============================= Operations on AnnData objects ============================= Performs the following operations: * Change observation/var fields, mostly for downstreaming processes convenience. Multiple fields can be changed at once. * Flag genes that start with a certain text: useful for flagging mitochondrial, spikes or other groups of genes. * For the flags created, calculates qc metrics (pct_<flag>
counts). * Calculates 
n_genes
, 
n_counts
 for cells and 
n_cells
, 
n_counts
 for genes. * For top <N> genes specified, calculate qc metrics (pct_counts_in_top
<N>_genes). * Make a specified column of var or obs unique (normally useful for gene symbols). * Copy from a set of compatible AnnData objects (same cells and genes): * Observations, such as clustering results. * Embeddings, such as tSNE or UMAPs. * Unstructure annotations, like gene markers. This functionality will probably be added in the future to a larger package. History ------- 1.9.5+galaxy1: Makes cell metadata optional for workflow optional steps. 1.8.1+galaxy10: Adds field to be made unique in obs or var. 1.6.0+galaxy0: Moves to Scanpy Scripts 0.3.0 (Scanpy 1.6.0), versioning switched to track Scanpy as other tools. 0.0.3+galaxy0: Adds ability to merge AnnData objects (Scanpy 1.4.3)."
toolshed.g2.bx.psu.edu/repos/iuc/episcanpy_build_matrix/episcanpy_build_matrix/0.3.2+galaxy1	".. class:: infomark 
What it does
 Builds single-cell ATAC-seq count matrix in Anndata format. ----- 
Inputs
 - ATAC fragments containing the positions of Tn5 integration sites, the cell barcode that the DNA fragment originated from, and the number of times the fragment was sequenced. An example:: chrY 2650256 2650533 GACCAATGTCCGTAGC 1 chrY 2650420 2650463 TGACAACGTACTTCAG 1 chrY 2650444 2650643 GTGGATTGTACAAGCG 3 chrY 2650639 2650990 ATAGGCTAGGGCTCTC 2 chrY 2650650 2650692 GACTAACAGCAACGGT 1 chrY 2650699 2650942 TCAAAGCTCAAAGTAG 1 chrY 2650768 2650809 TTGTTGTAGGGCATTG 2 chrY 2650841 2650873 TTGTTGTAGGGCATTG 1 chrY 2650957 2650995 GACTAACAGCAACGGT 1 chrY 2651205 2651265 TCAAAGCTCAAAGTAG 1 chrY 2651215 2651268 TCACAAGGTCAAGACG 1 - Features file. A plain BED file with peak locations or narrowPeak file from MACS2. 
Output
 - Count matrix in Anndata format."
toolshed.g2.bx.psu.edu/repos/iuc/cite_seq_count/cite_seq_count/1.4.4+galaxy0	CITE-seq-Count is a program that outputs UMI and read counts from raw fastq CITE-seq or hashing data. Here is an image explaining the expected structure of read1 and read2 from the sequencer: .. image:: read_structure.png :alt: Read1: --barcode--|--umi--|TTTT and Read2: --CMO/HTO--|AAA
toolshed.g2.bx.psu.edu/repos/iuc/raceid_inspectclusters/raceid_inspectclusters/3.1	RaceID3 ========= RaceID is a clustering algorithm for the identification of cell types from single-cell RNA-sequencing data. It was specifically designed for the detection of rare cells which correspond to outliers in conventional clustering methods. This module inspects the clusters generated from the previous clustering step (and requires the output RData file from it as input). The tool offers three modes of inspection which can all be activated at the same time, resulting in a single PDF report: * Plot All Clusters: * Produces a tSNE of all clusters, as well as a force-directed Fruchterman-Reingold (F-R) layout which may have tidier plots. * Perform Subset Analysis: * tSNE and F-R plots with cells whose name match the specified regex highlighted * Examine Genes of Interest: * Expression plots highlighting a gene or genes of interest across all clusters * Differential Gene Testing: * Examining the expression between * A list of the most differentially expressed genes in each cluster * An output PDF that provides heatmaps for: * The initial and final clustering (as determined using random forest) * Heatmaps for each of the most differentially expressed genes in each cluster The tool requires the RData input from the previous filtering / normalisation / confounder removal step to work.
toolshed.g2.bx.psu.edu/repos/iuc/raceid_clustering/raceid_clustering/3.1	RaceID3 ========= RaceID is a clustering algorithm for the identification of cell types from single-cell RNA-sequencing data. It was specifically designed for the detection of rare cells which correspond to outliers in conventional clustering methods. This module performs clustering, and outlier detection and ultimately tells you how well defined your clusters are (even if the resultant tSNE plots look messy). The tool generates the following: * A list of the most differentially expressed genes in each cluster * Cluster stability plots: * The mean within-cluster dispersion as a function of the cluster number and highlights the saturation point inferred based on the saturation criterion applied by RaceID3. The number of clusters where the change in within-cluster dispersion upon adding further clusters approaches linear behaviour demarcates the saturation point is highlighted in blue. * The point where this flattens out gives you a rough estimate of how many clusters there are in your analysis. * A scatter plot showing the gene expression variance as a function of the mean and the inferred polynomial fit of the background model, as well as a local regression. * This tells you which genes are the most significant against a background model of random expression. * A Jaccard stability plot which tells you how well defined your clusters are prior to outlier identification. * Good stable clusters should near the 0.8 mark, but 0.6 is acceptable, and for a large number of clusters, one or two low scoring clusters are also acceptable. * Heatmaps: * The initial and final clustering (as determined using random forest) * For each of the most differentially expressed genes in each cluster The tool requires the RData input from the previous filtering / normalisation / confounder removal step to work.
toolshed.g2.bx.psu.edu/repos/iuc/baredsc_combine_1d/baredsc_combine_1d/1.1.3+galaxy0	".. class:: infomark 
BARED (Bayesian Approach to Retreive Expression Distribution of) Single Cell
 baredSC is a tool that uses a Monte-Carlo Markov Chain to estimate a confidence interval on the probability density function (PDF) of expression of one or two genes from single-cell RNA-seq data. It uses the raw counts and the total number of UMI for each cell. The PDF is approximated by a number of 1d or 2d gaussians provided by the user. The likelihood is estimated using the asumption that the raw counts follow a Poisson distribution of parameter equal to the proportion of mRNA for the gene in the cell multiplied by the total number of UMI identified in this cell. To get a description of outputs, please read the 
Documentation &lt;https://baredsc.readthedocs.io/en/latest/index.html&gt;
_ This is a description of the figure with the results. - When the 1d version is used, it displays the mean PDF in solid red line, the median in black dashed lines (/!\backslash the integral of the median is not equal to 1) with the confidence interval of 1 sigma (68%), 2 sigma (95%) and 3 sigma (99.7%) as well as in green, the kernel density estimate of the input values, the detected expression (
log(1 + targetSum * raw / total UMI)
). - When the 2d version is used, it displays the PDF as a heatmap as well as a projection on the x and y axis. On the projection, the confidence interval 68% is indicated as a shaded area as well as the mean with a solid red line and the median with a dashed black line. On the top right corner, the correlation is indicated with the confidence interval 68% as well as a confidence interval on the one-sided p-value (the probability that the correlation is the opposite sign of the mean, one sigma confidence interval). Usually you should run baredSC_1d or baredSC_2d with 1 to 4 gaussians. Then you combine the different models with combineMultipleModels_1d or combineMultipleModels_2d."
toolshed.g2.bx.psu.edu/repos/iuc/baredsc_combine_2d/baredsc_combine_2d/1.1.3+galaxy0	".. class:: infomark 
BARED (Bayesian Approach to Retreive Expression Distribution of) Single Cell
 baredSC is a tool that uses a Monte-Carlo Markov Chain to estimate a confidence interval on the probability density function (PDF) of expression of one or two genes from single-cell RNA-seq data. It uses the raw counts and the total number of UMI for each cell. The PDF is approximated by a number of 1d or 2d gaussians provided by the user. The likelihood is estimated using the asumption that the raw counts follow a Poisson distribution of parameter equal to the proportion of mRNA for the gene in the cell multiplied by the total number of UMI identified in this cell. To get a description of outputs, please read the 
Documentation &lt;https://baredsc.readthedocs.io/en/latest/index.html&gt;
_ This is a description of the figure with the results. - When the 1d version is used, it displays the mean PDF in solid red line, the median in black dashed lines (/!\backslash the integral of the median is not equal to 1) with the confidence interval of 1 sigma (68%), 2 sigma (95%) and 3 sigma (99.7%) as well as in green, the kernel density estimate of the input values, the detected expression (
log(1 + targetSum * raw / total UMI)
). - When the 2d version is used, it displays the PDF as a heatmap as well as a projection on the x and y axis. On the projection, the confidence interval 68% is indicated as a shaded area as well as the mean with a solid red line and the median with a dashed black line. On the top right corner, the correlation is indicated with the confidence interval 68% as well as a confidence interval on the one-sided p-value (the probability that the correlation is the opposite sign of the mean, one sigma confidence interval). Usually you should run baredSC_1d or baredSC_2d with 1 to 4 gaussians. Then you combine the different models with combineMultipleModels_1d or combineMultipleModels_2d."
toolshed.g2.bx.psu.edu/repos/bgruening/music_construct_eset/music_construct_eset/0.1.1+galaxy4	"Construct an ExpressionSet object from a variety of input attributes, such as experimentData, phenotype data, and annotations. For more options and information, consult 
the manual &lt;http://www.bioconductor.org/packages/release/bioc/vignettes/Biobase/inst/doc/ExpressionSetIntroduction.pdf&gt;
 and the 
rdocumentation &lt;https://www.rdocumentation.org/packages/Biobase/versions/2.32.0/topics/ExpressionSet&gt;
 ."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/decoupler_pseudobulk/decoupler_pseudobulk/1.4.0+galaxy9	"This tool generates a count matrix for pseudo-bulk analysis (to be done with a separate tool like EdgeR or DESeq2) and filtering using Decoupler-py. Provide the input AnnData file and specify the necessary parameters. - Input AnnData file: The input AnnData file to be processed. - Contrasts file: optional file with a header and a single column, with arithmetic operations (contrasts definitions) as expected by EdgeR or DESeq2 based on the existing groups in the AnnData. This is optional and only required if you want to get a list of genes to filter out later on based on the percetage of cells that express those genes per contrast's conditions. - Min % expression per contrast (in at least one condition of the contrast): Percentage of cells (within at least one condition of a contrast) that need to express a gene for that genes not to be marked for later filtering. This requires the contrast file to be provided. - Obs Fields to Merge: Fields in adata.obs to merge, comma separated (optional). - Groupby column: The column in adata.obs that defines the groups. - Sample Key column: The column in adata.obs that defines the samples. - Layer (optional): The name of the layer of the AnnData object to use. - Mode: The mode for Decoupler pseudobulk analysis (sum, mean, median). Sum by default. - Factor Fields (optional): Fields in adata.obs to use as factors, comma separated (optional). For EdgeR make sure that the first field is the main contrast field desired and the rest of the fields are the covariates desired. - Use Raw: Whether to use the raw part of the AnnData object. - Minimum Cells: Minimum number of cells for pseudobulk analysis (optional). - Minimum Counts: Minimum count threshold for filtering by expression (optional). - Minimum Total Counts: Minimum total count threshold for filtering by expression (optional). - Enable Filtering by Expression: Check this box to enable filtering by expression. - Plot Samples Figsize: Size of the samples plot as a tuple (two arguments). - Plot Filtering Figsize: Size of the filtering plot as a tuple (two arguments). - Number of Pseudo Replicates: Number of pseudo replicates to create per sample (at least 3). The tool will output the filtered AnnData, count matrix, samples metadata, genes metadata (in DESeq2 format), and the pseudobulk plot and filter by expression plot (if enabled). Files for filtering genes later on are also generated (to ignore after the DE model). You can obtain more information about Decoupler pseudobulk at the developers documentation 
here &lt;https://decoupler-py.readthedocs.io/en/latest/notebooks/pseudobulk.html&gt;
_ ."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/droplet_barcode_plot/_dropletBarcodePlot/1.6.1+galaxy2	".. class:: infomark 
What it does
 Given a barcode freqeuncy table or an MTX-format matrix from which one can be calculated, produces a barcode rank plot to assess distinctness of droplets with cells over those without (a key mark of good-quality droplet single-cell RNA-seq data). Thresholds are calculated and plotted, either with DropletUtils or by custom method discussed at https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490160480. 
Inputs
 * two-column tab-delimted text file with barcode frequencies OR an MTX-format matrix file ----- 
Outputs
 * PNG-format plot file."
toolshed.g2.bx.psu.edu/repos/iuc/dropletutils/dropletutils/1.10.0+galaxy2	"Utilities to process 10X data in a variety of formats. Methods include: 
Filter data:
 * 
DefaultDrops
 emulates the native 10X method to produce a filtered count matrix * 
EmptyDrops
 is a more informative quantile-based approach to produce a filtered count matrix. 
Metrics:
 * 
RankBarcodes
 produces a gap statistic plot to estimate the number of barcodes detected in the sample matrix. 
Background
 Droplet-based single-cell RNA sequencing (scRNA-seq) technologies allow researchers to obtain transcriptome-wide expression profiles for thousands of cells at once. Briefly, each cell is encapsulated in a droplet in a oil-water emulsion, along with a bead containing reverse transcription primers with a unique barcode sequence. After reverse transcription inside the droplet, each cell’s cDNA is labelled with that barcode (referred to a “cell barcode”). Bursting of the droplets yields a pool of cDNA for library preparation and sequencing. Debarcoding of the sequences can then be performed to obtain the expression profile for each cell. This tool implements some general utilities for handling these data after quantification of expression. In particular, we focus on the 10X Genomics platform, providing functions to load in the matrix of unique molecule identifier (UMI) counts as well as the raw molecule information. Functions are also available for downsampling the UMI count matrix or the raw reads; for distinguishing cells from empty droplets, based on the UMI counts; and to eliminate the effects of barcode swapping on Illumina 4000 sequencing machine."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/dropletutils_read_10x/dropletutils_read_10x/1.0.4+galaxy0	"============================================================= Read 10x-Genomics-formatted mtx directory (
Read10xCounts()
) ============================================================= The mtx directory should contain: 1) Raw expression quantification as a sparse matrix in Matrix Market format, where the each column is a gene and each row is a barcode/cell. 2) A gene table of at least two columns where the first column gives the gene IDs. 3) A barcode/cell table of at least one column giving the barcode/cell IDs. The above-mentioned files can be obtained by running 
EBI SCXA Data Retrieval
 with a dataset accession. @HELP@"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/dropletutils_empty_drops/dropletutils_empty_drops/1.0.4+galaxy0	"======================================================================================================================================== Distinguish between droplets containing cells and ambient RNA in a droplet-based single-cell RNA sequencing experiment. (
emptyDrops()
) ======================================================================================================================================== This tool takes a SingleCellExeriment object and esitmates empty cells. From ?emptyDrops(): The emptyDrops function combines the results of testEmptyDrops with barcodeRanks to identify droplets that are likely to contain cells. Barcodes that contain more than retain total counts are always retained. This ensures that large cells with profiles that are very similar to the ambient pool are not inadvertently discarded. If retain is not specified, it is set to the total count at the knee point detected by barcodeRanks. Manual specification of retain may be useful if the knee point was not correctly identified in complex log-rank curves. Users can also set retain=Inf to disable automatic retention of barcodes with large totals. The Benjamini-Hochberg correction is also applied to the Monte Carlo p-values to correct for multiple testing. Cells can then be defined by taking all barcodes with significantly non-ambient profiles, e.g., at a false discovery rate of 0.1%. All barcodes with total counts above K (or retain) are assigned p-values of zero during correction, reflecting our assumption that they are true positives. This ensures that their Monte Carlo p-values do not affect the correction of other genes, and also means that they will have FDR values of zero. Nonetheless, their original Monte Carlo p-values are still reported in the output. @HELP@"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/retrieve_scxa/retrieve_scxa/v0.0.2+galaxy2	"================================================================================= Gene expression analysis in single cells across species and biological conditions ================================================================================= Single Cell Expression Atlas supports research in single cell transcriptomics. The Atlas annotates publicly available single cell RNA-Seq experiments with ontology identifiers and re-analyses them using standardised pipelines available through iRAP, our RNA-Seq analysis toolkit. The browser enables visualisation of clusters of cells, their annotations and supports searches for gene expression within and across studies. For more information check https://www.ebi.ac.uk/gxa/sc/home EBI SCXA Data Retrieval ----------------------- The data retrieval tool presented here allows the user to retrieve expression matrices and metadata for any public experiment available at EBI Single Cell Expression Atlas. To use it, simply set the accession for the desired experiment and choose the type of matrix that you want to download: :Raw filtered counts: This should be the default choice for running clustering and another analysis methods where you will introduce scaling and normalization of the data. The filtering is based on the quality control applied by iRAP prior to pseudo-alignment and quantification. :TPMs: TPM stands for Transcripts Per Kilobase Million, and as the name implies, this has been already normalized/scaled. You should keep this in mind when using this data on methods that will try to normalise data as part of their procedure. Due to technical particularities in the current Atlas SC pipeline, TPMs available here are not filtered. 
Note: droplet databases won't have TPM data
 Outputs will be: :Matrix (txt): Contains the expression values for genes (rows) and samples/runs/cells (columns), in either raw filtered counts or filtered tpms depending on the choice made. This text file is formatted as a Matrix Market file, and as such it is accompanied by separate files for the gene identifiers and the samples/runs/cells identifiers. :Genes (tsv): Identifiers (column repeated) for the genes present in the matrix of expression, in the same order as the matrix rows. :Barcodes (tsv): Identifiers for the cells, samples or runs of the data matrix. The file is ordered to match the columns of the matrix. :Experiment Design file (tsv): Contains metadata for the different cells/samples/runs of the experiment. Please note that this file is generated before the filtering step, and while not often, it might be the case that it contains more cells/samples/runs than the matrix."
toolshed.g2.bx.psu.edu/repos/iuc/anndata_export/anndata_export/0.10.9+galaxy2	"This tool exports an AnnData dataset to a tabular files (
write_csvs method &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.write_csvs.html&gt;
) 
AnnData
 AnnData provides a scalable way of keeping track of data together with learned annotations. It is used within 
Scanpy &lt;https://github.com/theislab/scanpy&gt;
, for which it was initially developed. AnnData stores a data matrix 
X
 together with annotations of observations 
obs
, variables 
var
 and unstructured annotations 
uns
. .. image:: https://falexwolf.de/img/scanpy/anndata.svg AnnData stores observations (samples) of variables (features) in the rows of a matrix. This is the convention of the modern classics of statistics (
Hastie et al., 2009 &lt;https://web.stanford.edu/~hastie/ElemStatLearn/&gt;
) and machine learning (Murphy, 2012), the convention of dataframes both in R and Python and the established statistics and machine learning packages in Python (statsmodels, scikit-learn). More details on the 
AnnData documentation &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.html&gt;
 
Loom data
 Loom files are an efficient file format for very large omics datasets, consisting of a main matrix, optional additional layers, a variable number of row and column annotations, and sparse graph objects. .. image:: https://linnarssonlab.org/loompy/_images/Loom_components.png Loom files to store single-cell gene expression data: the main matrix contains the actual expression values (one column per cell, one row per gene); row and column annotations contain metadata for genes and cells, such as Name, Chromosome, Position (for genes), and Strain, Sex, Age (for cells)."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/gtf2gene_list/_ensembl_gtf2gene_list/1.52.0+galaxy0	".. class:: infomark 
What it does
 Given an Ensembl GTF file, it will extract all information on chromosomes, coordinates, and attributes provided at the specified feature level. Mitochondrial features can also be flagged. See https://github.com/ebi-gene-expression-group/atlas-gene-annotation-manipulation. You can also supply a fasta-format file of sequences, which can be filtered by identifier to match annotation and/or used a source of information for transcripts un-annotated in the GTF. This can be useful for tools such as Alevin which need a transcript-to-gene mapping and a transcriptome file without any missing entries (with respect to annotation). 
Inputs
 * Ensembl GTF file ----- 
Outputs
 * Gene annotations in tsv."
toolshed.g2.bx.psu.edu/repos/iuc/anndata_import/anndata_import/0.10.9+galaxy2	"What it does
 This tool creates an AnnData from several input types: - Loom (
read_loom method &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.io.read_loom.html&gt;
) - Tabular (
read_csv method &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.io.read_csv.html&gt;
) - Matrix Market (mtx), from Cell ranger or not (
read_mtx method &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.io.read_mtx.html&gt;
) - UMI tools (
read_umi_tools method &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.io.read_umi_tools.html&gt;
) 
AnnData
 AnnData provides a scalable way of keeping track of data together with learned annotations. It is used within 
Scanpy &lt;https://github.com/theislab/scanpy&gt;
, for which it was initially developed. AnnData stores a data matrix 
X
 together with annotations of observations 
obs
, variables 
var
 and unstructured annotations 
uns
. .. image:: https://falexwolf.de/img/scanpy/anndata.svg AnnData stores observations (samples) of variables (features) in the rows of a matrix. This is the convention of the modern classics of statistics (
Hastie et al., 2009 &lt;https://web.stanford.edu/~hastie/ElemStatLearn/&gt;
) and machine learning (Murphy, 2012), the convention of dataframes both in R and Python and the established statistics and machine learning packages in Python (statsmodels, scikit-learn). More details on the 
AnnData documentation &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.html&gt;
__ 
Loom data
 Loom files are an efficient file format for very large omics datasets, consisting of a main matrix, optional additional layers, a variable number of row and column annotations, and sparse graph objects. .. image:: https://linnarssonlab.org/loompy/_images/Loom_components.png Loom files to store single-cell gene expression data: the main matrix contains the actual expression values (one column per cell, one row per gene); row and column annotations contain metadata for genes and cells, such as Name, Chromosome, Position (for genes), and Strain, Sex, Age (for cells)."
toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/3.1	"RaceID3 ======= RaceID is a clustering algorithm for the identification of cell types from single-cell RNA-sequencing data. It was specifically designed for the detection of rare cells which correspond to outliers in conventional clustering methods. This module performs filtering, normalisation, and batch effect removal in the same step. Example Usage: Inspecting the Aggregated Expression for a Group of Genes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Our cells come from 5 different batches (I5,II5,III5,IV5,V5) and are labelled to reflect this (i.e. ""I5_1"", ""I5_2"", ..., ""I5_129"", ""II5_1"", ..., ""V5_236"" ) We wish to filter out the gene Lpca5 and Atk2 which we know in advance will saturate our analysis with unwanted expression. We will also be interested in the cluster that contains significant expression for Apoa genes (Apoa1, Apoa1bp, Apoa2, Apoa4, Apoa5). First, we must load in our count matrix in order to correct for batch effects, filter out unwanted genes, and compute our clusters and outliers. * 
Mode of Analysis
 → 
Cluster
 * 
Count Matrix
 → [input tabular] * Filtering: * 
Use Defaults?
 → 
No
 * 
Batch Regex
 → ""^I5,^II5,^III5,^IV5,^V5"" * 
FGenes
 → ""Lpca5,Atk2"" A PDF report will be generated giving metrics about the library size and number of features as histograms, and additional metrics relating to cell-cycle correction will be produced if that option has been selected."
toolshed.g2.bx.psu.edu/repos/iuc/anndata_inspect/anndata_inspect/0.10.9+galaxy2	"What it does
 This tool inspects a AnnData dataset and returns: - General information about the object as text - The full data matrix (
X
) as a Tabular - A chunk of the data matrix as a Tabular, using the 
chunk_X method &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.chunk_X.html&gt;
 - Key-indexed observations annotation (
obs
) as a Tabular - Key-indexed annotation of variables/features (
var
) as a Tabular 
AnnData
 AnnData provides a scalable way of keeping track of data together with learned annotations. It is used within 
Scanpy &lt;https://github.com/theislab/scanpy&gt;
, for which it was initially developed. AnnData stores a data matrix 
X
 together with annotations of observations 
obs
, variables 
var
 and unstructured annotations 
uns
. .. image:: https://falexwolf.de/img/scanpy/anndata.svg AnnData stores observations (samples) of variables (features) in the rows of a matrix. This is the convention of the modern classics of statistics (
Hastie et al., 2009 &lt;https://web.stanford.edu/~hastie/ElemStatLearn/&gt;
) and machine learning (Murphy, 2012), the convention of dataframes both in R and Python and the established statistics and machine learning packages in Python (statsmodels, scikit-learn). More details on the 
AnnData documentation &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.html&gt;
 
Loom data
 Loom files are an efficient file format for very large omics datasets, consisting of a main matrix, optional additional layers, a variable number of row and column annotations, and sparse graph objects. .. image:: https://linnarssonlab.org/loompy/_images/Loom_components.png Loom files to store single-cell gene expression data: the main matrix contains the actual expression values (one column per cell, one row per gene); row and column annotations contain metadata for genes and cells, such as Name, Chromosome, Position (for genes), and Strain, Sex, Age (for cells)."
toolshed.g2.bx.psu.edu/repos/bgruening/music_inspect_eset/music_inspect_eset/0.1.1+galaxy4	"Inspect an ExpressionSet object by a variety of attributes, such as experimentData, phenotype data, and annotations. For more options and information, consult 
the manual &lt;http://www.bioconductor.org/packages/release/bioc/vignettes/Biobase/inst/doc/ExpressionSetIntroduction.pdf&gt;
 and the 
rdocumentation &lt;https://www.rdocumentation.org/packages/Biobase/versions/2.32.0/topics/ExpressionSet&gt;
 ."
toolshed.g2.bx.psu.edu/repos/iuc/raceid_inspecttrajectory/raceid_inspecttrajectory/3.1	StemID2 and FateID ===================== Given a previous lineage tree generated an RDS file generated from the previous Trajectory step, we can explore the variation of gene expression for all cells that lie on a given branch or trajectory. This will generate a PDF containing a heatmap of expression for all neighboring clusters that share links with cluster 5, as well as a plot of all cells along the branches between 1 to 3 and 3 to 10. A table of the most differentially expressed genes across these projection will also be output, which will provide a more qualitative assessment of how signficant our Apoa-expressing genes are along this projection. For more information on the different types cluster and trajectory inspection that can be performed, please consult the RaceID vignette_. .. _vignette: https://github.com/dgrun/RaceID3_StemID2_package/blob/master/vignettes/RaceID.Rmd
toolshed.g2.bx.psu.edu/repos/iuc/raceid_trajectory/raceid_trajectory/3.1	StemID2 ========= StemID is an algorithm for the inference of lineage trees and differentiation trajectories based on pseudo-temporal ordering of single-cell transcriptomes. It utilizies the clusters predicted by RaceID clustering tool. In a nutshell, StemID infers links between clusters which are more populated with intermediate single-cell transcriptomes than expected by chance. This will generate an RDS object, and a PDF showing a minimal spanning tree between the clusters, with the strength between clusters given by the colour and thickness of the branches.
toolshed.g2.bx.psu.edu/repos/iuc/modify_loom/modify_loom/0.10.9+galaxy2	This tool allows the user to modify an existing loom data file by adding column attributes, row attributes or additional layers via tsv files.
toolshed.g2.bx.psu.edu/repos/iuc/anndata_manipulate/anndata_manipulate/0.10.9+galaxy2	"What it does
 This tool takes an AnnData dataset, manipulates it and returns it. The possible manipulations are: - Concatenate along the observations axis (
concatenate method &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.concatenate.html&gt;
) The 
uns
, 
varm
 and 
obsm
 attributes are ignored. If you use 
join='outer'
 this fills 0s for sparse data when variables are absent in a batch. Use this with care. Dense data is filled with 
NaN
 - Makes the obs index unique by appending '1', '2', etc (
obs_names_make_unique method &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.obs_names_make_unique.html&gt;
) The first occurrence of a non-unique value is ignored. - Makes the var index unique by appending '1', '2', etc (
var_names_make_unique method &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.var_names_make_unique.html&gt;
) The first occurrence of a non-unique value is ignored. - Rename categories of annotation 
key
 in 
obs
, 
var
 and 
uns
 (
rename_categories method &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.rename_categories.html&gt;
) Besides calling 
self.obs[key].cat.categories = categories
 - similar for 
var
 - this also renames categories in unstructured annotation that uses the categorical annotation 
key
 - Remove keys from obs or var annotations Helps in cleaning up andata with many annotations. For example, helps in removing qc metrics calculated during the preprocesing or already existing cluster annotations. - Flag genes start with a pattern Useful for flagging the mitochondrial or ribosomal protein genes - Transform string annotations to categoricals (
strings_to_categoricals method &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.strings_to_categoricals.html&gt;
) Only affects string annotations that lead to less categories than the total number of observations. - Transpose the data matrix, leaving observations and variables interchanged (
transpose method &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.transpose.html&gt;
) Data matrix is transposed, observations and variables are interchanged. - Add annotation for variables or - Split the AnnData object into multiple AnnData objects based on the values of a given obs key For example, helps in splitting an anndata objects based on cluster annotation. This function generates a collection with a number of elements equal to the number of categories in the input obs key. - Filter data variables or observations, by index or key - Freeze the current state into the 'raw' attribute 
AnnData
 AnnData provides a scalable way of keeping track of data together with learned annotations. It is used within 
Scanpy &lt;https://github.com/theislab/scanpy&gt;
, for which it was initially developed. AnnData stores a data matrix 
X
 together with annotations of observations 
obs
, variables 
var
 and unstructured annotations 
uns
. .. image:: https://falexwolf.de/img/scanpy/anndata.svg AnnData stores observations (samples) of variables (features) in the rows of a matrix. This is the convention of the modern classics of statistics (
Hastie et al., 2009 &lt;https://web.stanford.edu/~hastie/ElemStatLearn/&gt;
) and machine learning (Murphy, 2012), the convention of dataframes both in R and Python and the established statistics and machine learning packages in Python (statsmodels, scikit-learn). More details on the 
AnnData documentation &lt;https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.html&gt;
__ 
Loom data
 Loom files are an efficient file format for very large omics datasets, consisting of a main matrix, optional additional layers, a variable number of row and column annotations, and sparse graph objects. .. image:: https://linnarssonlab.org/loompy/_images/Loom_components.png Loom files to store single-cell gene expression data: the main matrix contains the actual expression values (one column per cell, one row per gene); row and column annotations contain metadata for genes and cells, such as Name, Chromosome, Position (for genes), and Strain, Sex, Age (for cells)."
toolshed.g2.bx.psu.edu/repos/bgruening/music_manipulate_eset/music_manipulate_eset/0.1.1+galaxy4	"Manipulate an ExpressionSet object by concatenation and or subsetting. For more options and information, consult 
the manual &lt;http://www.bioconductor.org/packages/release/bioc/vignettes/Biobase/inst/doc/ExpressionSetIntroduction.pdf&gt;
 and the 
rdocumentation &lt;https://www.rdocumentation.org/packages/Biobase/versions/2.32.0/topics/ExpressionSet&gt;
 ."
toolshed.g2.bx.psu.edu/repos/bgruening/music_compare/music_compare/0.1.1+galaxy4	MuSiC Compare produces boxplots and heatmaps of cell type proportions within bulk RNA datasets, learned from single-cell RNA datasets. To discover the proportion of single-cell cell types within a bulk RNA dataset, create a scRNA group for each scRNA dataset, and the bulk datasets that you wish to discover the types in. Phenotype factors can also be specified, either for the entire dataset or for specific samples within a dataset given by a phenotype data column identifier. The resulting plots will combine all the bulk datasets and their learned cell type proportions into several summarizing plots.
toolshed.g2.bx.psu.edu/repos/bgruening/music_deconvolution/music_deconvolution/0.1.1+galaxy4	MuSiC utilizes cell-type specific gene expression from single-cell RNA sequencing (RNA-seq) data to characterize cell type compositions from bulk RNA-seq data in complex tissues. By appropriate weighting of genes showing cross-subject and cross-cell consistency, MuSiC enables the transfer of cell type-specific gene expression information from one dataset to another. Solid tissues often contain closely related cell types which leads to collinearity. To deal with collinearity, MuSiC employs a tree-guided procedure that recursively zooms in on closely related cell types. Briefly, we first group similar cell types into the same cluster and estimate cluster proportions, then recursively repeat this procedure within each cluster.
toolshed.g2.bx.psu.edu/repos/iuc/sceasy_convert/sceasy_convert/0.0.7+galaxy2	"SCeasy ====== Convert scRNA data object between formats 
sceasy::convertFormat()
 Supports the following conversion: .. image:: $PATH_TO_IMAGES/conv.png :width: 80 % :align: center"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/sceasy_convert/sceasy_convert/0.0.5+galaxy1	"=================================================================== Convert scRNA data object between formats 
sceasy::convertFormat()
 =================================================================== Support the following conversion: * Loom <-> SingleCellExperiment (full) * SingleCellExperiment -> AnnData (matrix, metadata, reducedDim) * Seurat -> AnnData (matrix, metadata, reducedDim) More information can be found at https://github.com/cellgeni/sceasy 
Version history
 0.0.1+galaxy0: Initial version based on sceasy 0.0.5 by Ni Huang, WTSI."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_compute_graph/scanpy_compute_graph/1.9.3+galaxy0	"============================================================= Compute a neighborhood graph of observations (
pp.neighbors
) ============================================================= The neighbor search efficiency of this heavily relies on UMAP (McInnes et al, 2018), which also provides a method for estimating connectivities of data points - the connectivity of the manifold (
method=='umap'
). If 
method=='Gaussian'
, connectivities are computed according to Coifman et al (2005), in the adaption of Haghverdi et al (2016). More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_run_dpt/scanpy_run_dpt/1.9.3+galaxy0	"================================================ Calculate Diffusion Pseudotime (
scanpy.tl.dpt
) ================================================ Calculate diffusion pseudotime from single cell KNN graphs. This requires to run 
Scanpy DiffusionMap
 and 
Scanpy FindCluster
, first. It yields 
dpt_pseudotime
, diffusion pseudotime as an attribute for each cell. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_run_diffmap/scanpy_run_diffmap/1.9.3+galaxy0	"==================================================== Calculate Diffusion Components (
scanpy.tl.diffmap
) ==================================================== Calculate diffusion components from single cell KNN graphs. This requires to run 
Scanpy ComputeGraph
, first. It yields 
X_diffmap
, the dimension-reduced representation in diffusion components space. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_filter_cells/scanpy_filter_cells/1.9.3+galaxy0	"=================================================================== Filter cells based on various QC metrics (
scanpy.pp.filter_cells
) =================================================================== For instance, only keep cells with at least 
min_counts
 and at most 
max_counts
 UMI and/or at least 
min_genes
 expressed genes and/or at most 
max_mito_percent
 mitocondria expression. This is to filter measurement outliers, i.e., ""unreliable"" observations. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_filter_genes/scanpy_filter_genes/1.9.3+galaxy0	"===================================================================== Filter genes based on arbitrary attributes (
scanpy.pp.filter_genes
) ===================================================================== Keep genes that have at least 
min_counts
 counts or are expressed in at least 
min_cells
 cells or have at most 
max_counts
 counts or are expressed in at most 
max_cells
 cells. Other gene attributes can be used for filtering too if available. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_find_cluster/scanpy_find_cluster/1.9.3+galaxy0	"====================================================================== Cluster cells into subgroups (
scanpy.tl.louvain
, 
scanpy.tl.leiden
) ====================================================================== Cluster cells using the Louvain algorithm (Blondel et al, 2008) in the implementation of Traag et al, 2017, or the Leiden algorithm (Traag et al, 2019). The Louvain algorithm has been proposed for single-cell analysis by Levine et al, 2015. This requires to run 
Scanpy ComputeGraph
, first. It by default yields 
louvain
 or 
leiden
, generated cluster label. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_find_markers/scanpy_find_markers/1.9.3+galaxy0	"============================================================= Rank genes for characterizing groups (
tl.rank_genes_groups
) ============================================================= Rank genes for characterizing groups. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_find_variable_genes/scanpy_find_variable_genes/1.9.3+galaxy1	"============================================================== Mark highly variable genes (
scanpy.pp.highly_variable_genes
) ============================================================== Depending on 
flavor
, this reproduces the R-implementations of Seurat or Cell Ranger. The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_integrate_harmony/scanpy_integrate_harmony/1.9.3+galaxy0	".. class:: infomark 
What it does
 Uses harmonypy [Korunsky19] to integrate different experiments at the principal components level. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/iuc/scanpy_inspect/scanpy_inspect/1.10.2+galaxy3	"Calculate quality control metrics., using 
pp.calculate_qc_metrics
 =================================================================== Calculates a number of qc metrics for an AnnData object, largely based on calculateQCMetrics from scater. Currently is most efficient on a sparse CSR or dense matrix. It updates the observation level metrics with - total_{var_type}
by
{expr_type} (e.g. ""total_genes_by_counts"", number of genes with positive counts in a cell) - total_{expr_type} (e.g. ""total_counts"", total number of counts for a cell) - pct_{expr_type}
in_top
{n}
{var_type} - for n in percent_top (e.g. ""pct_counts_in_top_50_genes"", cumulative percentage of counts for 50 most expressed genes in a cell) - total
{expr_type}
{qc_var} - for qc_var in qc_vars (e.g. ""total_counts_mito"", total number of counts for variabes in qc_vars) - pct
{expr_type}
{qc_var} - for qc_var in qc_vars (e.g. ""pct_counts_mito"", proportion of total counts for a cell which are mitochondrial) And also the variable level metrics: - total
{expr_type} (e.g. ""total_counts"", sum of counts for a gene) - mean_{expr_type} (e.g. ""mean counts"", mean expression over all cells) - n_cells_by_{expr_type} (e.g. ""n_cells_by_counts"", number of cells this expression is measured in) - pct_dropout_by_{expr_type} (e.g. ""pct_dropout_by_counts"", percentage of cells this feature does not appear in) More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.calculate_qc_metrics.html&gt;
 Compute a neighborhood graph of observations, using 
pp.neighbors
 ================================================================== The neighbor search efficiency of this heavily relies on UMAP (McInnes et al, 2018), which also provides a method for estimating connectivities of data points - the connectivity of the manifold (
method=='umap'
). If 
method=='diffmap'
, connectivities are computed according to Coifman et al (2005), in the adaption of Haghverdi et al (2016). The returned AnnData object contains: - Weighted adjacency matrix of the neighborhood graph of data points (connectivities). Weights should be interpreted as connectivities. - Distances for each pair of neighbors (distances) This data are stored in the unstructured annotation (uns) and can be accessed using the inspect tool for AnnData objects More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pp.neighbors.html&gt;
 Score a set of genes, using 
tl.score_genes
 ============================================ The score is the average expression of a set of genes subtracted with the average expression of a reference set of genes. The reference set is randomly sampled from the 
gene_pool
 for each binned expression value. This reproduces the approach in Seurat (Satija et al, 2015) and has been implemented for Scanpy by Davide Cittaro. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.score_genes.html&gt;
 Score cell cycle genes, using 
tl.score_genes_cell_cycle
 ========================================================= Given two lists of genes associated to S phase and G2M phase, calculates scores and assigns a cell cycle phase (G1, S or G2M). See 
score_genes
 for more explanation. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.score_genes_cell_cycle.html&gt;
 Rank genes for characterizing groups, using 
tl.rank_genes_groups
 ================================================================== The returned AnnData object contains: - Gene names, ordered according to scores - Z-score underlying the computation of a p-value for each gene for each group, prdered according to scores - Log2 fold change for each gene for each group, ordered according to scores. It is only provided if method is ‘t-test’ like. This is an approximation calculated from mean-log values. - P-values - Ajusted p-values This data are stored in the unstructured annotation (uns) and can be accessed using the inspect tool for AnnData objects More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.rank_genes_groups.html&gt;
 Calculate an overlap score between data-deriven marker genes and provided markers (
tl.marker_gene_overlap
) ============================================================================================================ Marker gene overlap scores can be quoted as overlap counts, overlap coefficients, or jaccard indices. The method returns a pandas dataframe which can be used to annotate clusters based on marker gene overlaps. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.marker_gene_overlap.html&gt;
 Logarithmize the data matrix (
pp.log1p
) ========================================= More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.log1p.html&gt;
 Scale data to unit variance and zero mean (
pp.scale
) ====================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.scale.html&gt;
 Computes the square root the data matrix (
pp.sqrt
) ==================================================== 
X = sqrt(X)"
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_normalise_data/scanpy_normalise_data/1.9.3+galaxy0	"============================================================= Normalise total counts per cell (
scanpy.pp.normalize_total
) ============================================================= Normalise each cell by total counts over all genes (excluding top expressed genes if so required), so that every cell has the same total count after normalisation. Similar functions are used, for example, by Seurat, Cell Ranger or SPRING. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_run_paga/scanpy_run_paga/1.9.3+galaxy0	"=================================================================== Perform PAGA (partition-based graph abstraction) (
scanpy.tl.paga
) =================================================================== Infer trajectories by mapping out the coarse-grained connectivity structures of complex manifolds of single cell KNN graphs (Wolf et al, 2019). This requires to run 
Scanpy FindCluster
, first. It yields 
paga
, connectivity graph between clusters. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_parameter_iterator/scanpy_parameter_iterator/0.0.1+galaxy9	".. class:: infomark 
What it does
 Given start, step and end, it will iterate parameters for either perplexity (for t-SNE), resolution (clustering) or number of neighbours (neighbour graph construction). 
Inputs
 * Parameter name: either Resolution or Perplexity. * Starting value: float for Resolution, integer for Perplexity. * Step: float for Resolution, integer for Perplexity. * End value: float for Resolution, integer for Perplexity. 
Outputs
 * Collection of parameters to be passed to either Scanpy run tSNE (perplexity) or Scanpy find clusters (resolution). 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_plot_embed/scanpy_plot_embed/1.9.3+galaxy0	"============================================================= Plot embeddings of a given method of dimensionality reduction ============================================================= It yields a scatter plot in png format, wherein cells are placed in space of reduced dimensionality and coloured by attribute of choice. Requires calculating the specified embeddings first. For example, to make UMAP/TSNE plots, run 
Scanpy RunUMAP
/
Scanpy RunTSNE
 first, then enter ""umap""/""tsne"" as the name of the embedding to plot here. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_plot_trajectory/scanpy_plot_trajectory/1.9.3+galaxy0	"=============================================================== Plot PAGA-inferred trajectories (
scanpy.pl.paga/paga_compare
) =============================================================== Depending on the selected options, it yields a plot of a graph representing inferred trajectory, or a trajectory graph side-by-side with a scatter plot of cells embedded to space of reduced dimensionality in png format. It requires running PAGA, first. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_read_10x/scanpy_read_10x/1.9.3+galaxy0	"================================================================= Read 10x-Genomics-formatted mtx directory (
scanpy.read_10x_mtx
) ================================================================= The mtx directory should contain: 1) Raw expression quantification as a sparse matrix in Matrix Market format, where the each column is a gene and each row is a barcode/cell. 2) A gene table of at least two columns where the first column gives the gene IDs. 3) A barcode/cell table of at least one column giving the barcode/cell IDs. The above-mentioned files can be obtained by running 
EBI SCXA Data Retrieval
 with a dataset accession or 
Human Cell Atlas Matrix Downloader
 with a project name/label/UUID. Additionally, cell and/or gene metadata table can be provided as tab-separated text with a header row and an index column that matches the respective barcode/cell and/or gene table. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_regress_variable/scanpy_regress_variable/1.9.3+galaxy0	".. class:: infomark 
What it does
 Regress out unwanted source of variance (
scanpy.pp.regress_out
) More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_run_fdg/scanpy_run_fdg/1.9.3+galaxy0	"================================================================================ Embed the neighborhood graph using Force-directed Graph (
scanpy.tl.draw_graph
) ================================================================================ For making FDG plots, please use 
Scanpy PlotEmbed
 with the output of this tool and enter ""draw_graph_{layout}"" as the name of the embedding to plot. It yields 
X_draw_graph_{layout}
, FDG coordinates of data. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_run_pca/scanpy_run_pca/1.9.3+galaxy0	"================================================================================ Dimensionality reduction by PCA (principal component analysis) (
scanpy.pp.pca
) ================================================================================ It uses the implementation of 
scikit-learn
. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_run_tsne/scanpy_run_tsne/1.9.3+galaxy0	"========================================================================= t-distributed stochastic neighborhood embedding (tSNE) (
scanpy.tl.tsne
) ========================================================================= For making TSNE plots, please use 
Scanpy PlotEmbed
 with the output of this tool and enter ""tsne"" as the name of the embedding to plot. t-distributed stochastic neighborhood embedding (tSNE) (Maaten et al, 2008) has been proposed for visualizating single-cell data by (Amir et al, 2013). Here, by default, we use the implementation of 
scikit-learn
 (Pedregosa et al, 2011). It yields 
X_tsne
, tSNE coordinates of data. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_run_umap/scanpy_run_umap/1.9.3+galaxy0	"========================================================== Embed the neighborhood graph using UMAP (
scanpy.tl.umap
) ========================================================== For making UMAP plots, please use 
Scanpy PlotEmbed
 with the output of this tool and enter ""umap"" as the name of the embedding to plot. UMAP (Uniform Manifold Approximation and Projection) is a manifold learning technique suitable for visualizing high-dimensional data. Besides tending to be faster than tSNE, it optimizes the embedding such that it best reflects the topology of the data, which we represent throughout Scanpy using a neighborhood graph. tSNE, by contrast, optimizes the distribution of nearest-neighbor distances in the embedding such that these best match the distribution of distances in the high-dimensional space. We use the implementation of 
umap-learn &lt;https://github.com/lmcinnes/umap&gt;
 (McInnes et al, 2018). For a few comparisons of UMAP with tSNE, see this 
preprint &lt;https://doi.org/10.1101/298430&gt;
. It yields 
X_umap
, UMAP coordinates of data. More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_scale_data/scanpy_scale_data/1.9.3+galaxy0	".. class:: infomark 
What it does
 Scale data to unit variance and (optionally) zero mean (
scanpy.pp.scale
) More information can be found at https://scanpy.readthedocs.io 
Version history
 1.9.301+galaxy0: Moves to Scanpy 1.9.3 1.8.1+galaxy9: Fixes version label to get versions sorted properly on Galaxy (equivalent to 1.8.1+3+galaxy0). In addition, adds ability to make fields unique in AnnData operations, adds additional handler on Scrubblet for workflows convenience, enables Scanpy tools to avoid outputing AnnData/matrix files when not required. 1.8.1+3+galaxy0: Upate to scanpy-scripts 1.1.3 (running scanpy ==1.8.1), including a fix to MTX output and a bugfix for the Scrublet wrapper. 1.8.1+2+galaxy0: Upate to scanpy-scripts 1.1.2 (running scanpy ==1.8.1), including improved boolean handling for mito etc. 1.8.1+1+galaxy0: Upate to scanpy-scripts 1.1.1 build 1 (running scanpy ==1.8.1), including improved Scrublet integration with batch handling. 1.8.1+galaxy0: Upate to scanpy-scripts 1.0.1 (running scanpy ==1.8.1), including Scrublet integration. 1.7.2+galaxy0: Upate to scanpy-scripts 0.3.3 (running scanpy ==1.7.2) to incorporate fix for object output from PAGA plotting, to allow PAGA init of FDG. 1.6.0+galaxy0: Update to scanpy-scripts 0.2.13 (running scanpy ==1.6.0) to incorporate new options, code simplifications, and batch integration methods. Jonathan Manning, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ 1.4.3+galaxy10: Update to scanpy-scripts 0.2.10 (running scanpy ==1.4.3) to address bugfixes in run-pca. 1.4.3+galaxy10: Update to scanpy-scripts 0.2.9 (running scanpy ==1.4.3) to address bugfixes in find-variable-genes. 1.4.3+galaxy10: Use profile 18.01 for modules. 1.4.3+galaxy6: Update to scanpy-scripts 0.2.8 (running scanpy ==1.4.3) and wider compatibility with other Galaxy modules. Bug fixes in filtering and plotting improvements. 1.4.3+galaxy0: Update to scanpy-scripts 0.2.5 (running scanpy ==1.4.3). 1.4.2+galaxy0: Update to scanpy-scripts 0.2.4 (requires scanpy >=1.4.2). 1.3.2+galaxy1: Normalise-data and filter-genes: Exposes ability to output 10x files. 1.3.2+galaxy0: Initial contribution. Ni Huang and Pablo Moreno, Expression Atlas team https://www.ebi.ac.uk/gxa/home at EMBL-EBI https://www.ebi.ac.uk/ and Teichmann Lab at Wellcome Sanger Institute."
toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.10.2+galaxy3	"Cluster cells into subgroups (
tl.louvain
) =========================================== Cluster cells using the Louvain algorithm (Blondel et al, 2008) in the implementation of Traag et al,2017. The Louvain algorithm has been proposed for single-cell analysis by Levine et al, 2015. This requires to run 
pp.neighbors
, first. More details on the 
tl.louvain scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.louvain.html&gt;
 Cluster cells into subgroups (
tl.leiden
) ========================================== Cluster cells using the Leiden algorithm (Traag et al, 2018), an improved version of the Louvain algorithm (Blondel et al, 2008). The Louvain algorithm has been proposed for single-cell analysis by Levine et al, 2015. More details on the 
tl.leiden scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html&gt;
 Computes PCA (principal component analysis) coordinates, loadings and variance decomposition, using 
pp.pca
 ============================================================================================================ More details on the 
pp.pca scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.pca.html&gt;
 Diffusion Maps, using 
tl.diffmap
 ================================== Diffusion maps (Coifman et al 2005) has been proposed for visualizing single-cell data by Haghverdi et al (2015). The tool uses the adapted Gaussian kernel suggested by Haghverdi et al (2016) in the implementation of Wolf et al (2017). The width (""sigma"") of the connectivity kernel is implicitly determined by the number of neighbors used to compute the single-cell graph in 
pp.neighbors
. To reproduce the original implementation using a Gaussian kernel, use 
method=='gauss'
 in 
pp.neighbors
. To use an exponential kernel, use the default 
method=='umap'
. Differences between these options shouldn't usually be dramatic. The diffusion map representation of data are added to the return AnnData in the multi-dimensional observations annotation (obsm). It is the right eigen basis of the transition matrix with eigenvectors as colum. It can be accessed using the inspect tool for AnnData More details on the 
tl.diffmap scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.diffmap.html&gt;
 t-distributed stochastic neighborhood embedding (tSNE), using 
tl.tsne
 ======================================================================= t-distributed stochastic neighborhood embedding (tSNE) (Maaten et al, 2008) has been proposed for visualizating single-cell data by (Amir et al, 2013). Here, by default, we use the implementation of 
scikit-learn
 (Pedregosa et al, 2011). It returns 
X_tsne
, tSNE coordinates of data. More details on the 
tl.tsne scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.tsne.html&gt;
 Embed the neighborhood graph using UMAP, using 
tl.umap
 ======================================================== UMAP (Uniform Manifold Approximation and Projection) is a manifold learning technique suitable for visualizing high-dimensional data. Besides tending to be faster than tSNE, it optimizes the embedding such that it best reflects the topology of the data, which we represent throughout Scanpy using a neighborhood graph. tSNE, by contrast, optimizes the distribution of nearest-neighbor distances in the embedding such that these best match the distribution of distances in the high-dimensional space. We use the implementation of 
umap-learn &lt;https://github.com/lmcinnes/umap&gt;
 (McInnes et al, 2018). For a few comparisons of UMAP with tSNE, see this 
paper &lt;https://www.nature.com/articles/nbt.4314&gt;
. The UMAP coordinates of data are added to the return AnnData in the multi-dimensional observations annotation (obsm). This data is accessible using the inspect tool for AnnData More details on the 
tl.umap scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.umap.html&gt;
 Force-directed graph drawing, using 
tl.draw_graph
 =================================================== Force-directed graph drawing describes a class of long-established algorithms for visualizing graphs. It has been suggested for visualizing single-cell data by Islam et al, 11. Many other layouts as implemented in igraph are available. Similar approaches have been used by Zunder et al, 2015 or Weinreb et al, 2016. This is an alternative to tSNE that often preserves the topology of the data better. This requires to run 
pp.neighbors
, first. The default layout (ForceAtlas2) uses the package fa2. The coordinates of graph layout are added to the return AnnData in the multi-dimensional observations annotation (obsm). This data is accessible using the inspect tool for AnnData. More details on the 
tl.draw_graph scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.draw_graph.html&gt;
__ Infer progression of cells through geodesic distance along the graph (
tl.dpt
) =============================================================================== Reconstruct the progression of a biological process from snapshot data. 
Diffusion Pseudotime
 has been introduced by Haghverdi et al (2016) and implemented within Scanpy (Wolf et al, 2017). Here, we use a further developed version, which is able to deal with disconnected graphs (Wolf et al, 2017) and can be run in a 
hierarchical
 mode by setting the parameter 
n_branchings&gt;1
. We recommend, however, to only use 
tl.dpt
 for computing pseudotime (
n_branchings=0
) and to detect branchings via 
paga
. For pseudotime, you need to annotate your data with a root cell. This requires to run 
pp.neighbors
, first. In order to reproduce the original implementation of DPT, use 
method=='gauss'
 in this. Using the default 
method=='umap'
 only leads to minor quantitative differences, though. If 
n_branchings==0
, no field 
dpt_groups
 will be written. - dpt_pseudotime : Array of dim (number of samples) that stores the pseudotime of each cell, that is, the DPT distance with respect to the root cell. - dpt_groups : Array of dim (number of samples) that stores the subgroup id ('0','1', ...) for each cell. The groups typically correspond to 'progenitor cells', 'undecided cells' or 'branches' of a process. The tool is similar to the R package 
destiny
 of Angerer et al (2016). More details on the 
tl.dpt scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.dpt.html&gt;
 Generate cellular maps of differentiation manifolds with complex topologies (
tl.paga
) ======================================================================================= By quantifying the connectivity of partitions (groups, clusters) of the single-cell graph, partition-based graph abstraction (PAGA) generates a much simpler abstracted graph (
PAGA graph
) of partitions, in which edge weights represent confidence in the presence of connections. By tresholding this confidence in 
paga
, a much simpler representation of data can be obtained. The confidence can be interpreted as the ratio of the actual versus the expected value of connetions under the null model of randomly connecting partitions. We do not provide a p-value as this null model does not precisely capture what one would consider ""connected"" in real data, hence it strongly overestimates the expected value. See an extensive discussion of this in Wolf et al (2017). Together with a random walk-based distance measure, this generates a partial coordinatization of data useful for exploring and explaining its variation. The returned AnnData object contains: - Full adjacency matrix of the abstracted graph, weights correspond to confidence in the connectivities of partition (connectivities) - Adjacency matrix of the tree-like subgraph that best explains the topology (connectivities_tree) These datasets are stored in the unstructured annotation (uns) and can be accessed using the inspect tool for AnnData objects More details on the 
tl.paga scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.paga.html&gt;
 Calculates the density of cells in an embedding (per condition). (
tl.embedding_density
) ========================================================================================= Gaussian kernel density estimation is used to calculate the density of cells in an embedded space. This can be performed per category over a categorical cell annotation. Note that density values are scaled to be between 0 and 1. Thus, the density value at each cell is only comparable to densities in the same category. More details on the 
tl.embedding_density scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.embedding_density.html&gt;
_"
toolshed.g2.bx.psu.edu/repos/iuc/scanpy_filter/scanpy_filter/1.10.2+galaxy3	"Filter cells outliers based on counts and numbers of genes expressed (
pp.filter_cells
) ======================================================================================== For instance, only keep cells with at least 
min_counts
 counts or 
min_genes
 genes expressed. This is to filter measurement outliers, i.e., ""unreliable"" observations. Only provide one of the optional parameters 
min_counts
, 
min_genes
, 
max_counts
, 
max_genes
 per call. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_cells.html&gt;
 Filter genes based on number of cells or counts (
pp.filter_genes
) =================================================================== Keep genes that have at least 
min_counts
 counts or are expressed in at least 
min_cells
 cells or have at most 
max_counts
 counts or are expressed in at most 
max_cells
 cells. Only provide one of the optional parameters 
min_counts
, 
min_cells
, 
max_counts
, 
max_cells
 per call. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes.html&gt;
 Filters out genes based on fold change and fraction of genes expressing the gene within and outside the groupby categories (
tl.filter_rank_genes_groups
) ========================================================================================================================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.filter_rank_genes_groups.html&gt;
 Annotate highly variable genes (
pp.highly_variable_genes
) =========================================================== It expects logarithmized data. Depending on flavor, this reproduces the R-implementations of Seurat or Cell Ranger. The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. Subsample to a fraction of the number of observations (
pp.subsample
) ====================================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.subsample.html&gt;
 Downsample counts (
pp.downsample_counts
) ========================================== Downsample counts so that each cell has no more than 
target_counts
. Cells with fewer counts than 
target_counts
 are unaffected by this. This has been implemented by M. D. Luecken. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.downsample_counts.html&gt;
 Filter marker genes (
filter_marker
) ===================================== This option is specific for celltype marker gene detection. You can generate a celltype marker gene file (tsv) with 
COSG
 provided at Galaxy. The marker gene file should have as rows celltypes and columns as marker genes. Each celltype can have varying number of marker genes. A marker gene is returned (retained in the list) if the mean expression of the marker gene is bigger than the threshold of mean expression (thresh_mean) and if the fraction of cells with the marker gene expression is equal or higher than the cell fraction threshold (thresh_frac). More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.downsample_counts.html&gt;
 Predict cell doublets using a nearest-neighbor classifier of observed transcriptomes and simulated doublets. (
pp.scrublet
) ============================================================================================================================ Works best if the input is a raw (unnormalized) counts matrix from a single sample or a collection of similar samples from the same experiment. This function is a wrapper around functions that pre-process using Scanpy and directly call functions of Scrublet(). More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pp.scrublet.html&gt;
__"
toolshed.g2.bx.psu.edu/repos/iuc/scanpy_normalize/scanpy_normalize/1.10.2+galaxy3	"Normalize total counts per cell (
pp.normalize_total
) ====================================================== Normalize each cell by total counts over all genes, so that every cell has the same total count after normalization. If choosing target_sum=1e6, this is CPM normalization. Similar functions are used, for example, by Seurat, Cell Ranger or SPRING. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.normalize_total.html&gt;
 Normalization and filtering as of Zheng et al. (2017), the Cell Ranger R Kit of 10x Genomics (
pp.recipe_zheng17
) ================================================================================================================== Expects non-logarithmized data. If using logarithmized data, pass 
log=False
. The recipe runs the following steps: - only consider genes with more than 1 count - normalize with total UMI count per cell - select highly-variable genes - subset the genes - renormalize after filtering - log transform (if needed) - scale to unit variance and shift to zero mean More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pp.recipe_zheng17.html&gt;
 Normalization and filtering as of Weinreb et al (2017) (
pp.recipe_weinreb17
) ============================================================================== Expects non-logarithmized data. If using logarithmized data, pass 
log=False
. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pp.recipe_weinreb17.html&gt;
 Normalization and filtering as of Seurat et al (2015) (
pp.recipe_seurat
) ========================================================================== This uses a particular preprocessing. Expects non-logarithmized data. If using logarithmized data, pass 
log=False
. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pp.recipe_seurat.html&gt;
 Markov Affinity-based Graph Imputation of Cells (MAGIC) as of Van Dijk D et al. (2018) (
external.pp.magic
) ============================================================================================================ MAGIC is an algorithm for denoising and transcript recover of single cells applied to single-cell sequencing data. MAGIC builds a graph from the data and uses diffusion to smooth out noise and recover the data manifold. The algorithm implemented here has changed primarily in two ways compared to the algorithm described in Van Dijk D et al. (2018). - Firstly, we use the adaptive kernel described in Moon et al, (2019) for improved stability. - Secondly, data diffusion is applied in the PCA space, rather than the data space, for speed and memory improvements. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.external.pp.magic.html&gt;
__"
toolshed.g2.bx.psu.edu/repos/iuc/scanpy_plot/scanpy_plot/1.10.2+galaxy3	"Generic: Scatter plot along observations or variables axes (
pl.scatter
) ========================================================================= Color the plot using annotations of observations (
.obs
), variables (
.var
) or expression of genes (
.var_names
). More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.scatter.html&gt;
 Generic: Heatmap of the expression values of set of genes (
pl.heatmap
) ======================================================================== If 
groupby
 is given, the heatmap is ordered by the respective group. For example, a list of marker genes can be plotted, ordered by clustering. If the 
groupby
 observation annotation is not categorical the observation annotation is turned into a categorical by binning the data into the number specified in 
num_categories
. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.heatmap.html&gt;
 Generic: Makes a dot plot of the expression values (
pl.dotplot
) ================================================================= For each var_name and each 
groupby
 category a dot is plotted. Each dot represents two values: mean expression within each category (visualized by color) and fraction of cells expressing the var_name in the category. (visualized by the size of the dot). If groupby is not given, the dotplot assumes that all data belongs to a single category. A gene is not considered expressed if the expression value in the adata (or adata.raw) is equal to zero. For instance, for each marker gene, the mean value and the percentage of cells expressing the gene can be visualized for each cluster. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html&gt;
 Generic: Tracks plot (
pl.tracksplot
) ====================================== In this type of plot each var_name is plotted as a filled line plot where the y values correspond to the var_name values and x is each of the cells. Best results are obtained when using raw counts that are not log. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/generated/scanpy.pl.tracksplot.html&gt;
 Generic: Violin plot (
pl.violin
) ================================== Wraps 
seaborn.violinplot
 for 
anndata.AnnData
. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.violin.html&gt;
 Generic: Stacked violin plots (
pl.stacked_violin
) =================================================== Makes a compact image composed of individual violin plots (from 
seaborn.violinplot
) stacked on top of each other. Useful to visualize gene expression per cluster. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.stacked_violin.html&gt;
 Generic: Heatmap of the mean expression values per cluster (
pl.matrixplot
) ============================================================================ Creates a heatmap of the mean expression values per cluster of each var_names If groupby is not given, the matrixplot assumes that all data belongs to a single category. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.matrixplot.html&gt;
 Generic: Hierarchically-clustered heatmap (
pl.clustermap
) =========================================================== Wraps 
seaborn.clustermap &lt;https://seaborn.pydata.org/generated/seaborn.clustermap.html&gt;
 for 
anndata.AnnData
. The returned object has a savefig() method that should be used if you want to save the figure object without clipping the dendrograms. To access the reordered row indices, use: clustergrid.dendrogram_row.reordered_ind Column indices, use: clustergrid.dendrogram_col.reordered_ind More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.clustermap.html&gt;
 Preprocessing: Plot the fraction of counts assigned to each gene over all cells (
pl.highest_expr_genes
) ========================================================================================================= Computes, for each gene, the fraction of counts assigned to that gene within a cell. The 
n_top
 genes with the highest mean fraction over all cells are plotted as boxplots. This plot is similar to the 
scater
 package function 
plotHighestExprs(type= ""highest-expression"")
 More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.highest_expr_genes.html&gt;
 Preprocessing: Plot dispersions versus means for genes (
pl.highly_variable_genes
) =================================================================================== It produces Supp. Fig. 5c of Zheng et al. (2017) and MeanVarPlot() of Seurat. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.highly_variable_genes.html&gt;
 Preprocessing: Plot histogram of doublet scores for observed transcriptomes and simulated doublets (
pl.scrublet_score_distribution
) ===================================================================================================================================== The histogram for simulated doublets is useful for determining the correct doublet score threshold. Scrublet must have been run previously with the input object. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.scrublet_score_distribution.html&gt;
 PCA: Scatter plot in PCA coordinates (
pl.pca
) =============================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.pca.html&gt;
 PCA: Rank genes according to contributions to PCs (
pl.pca_loadings
) ===================================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.pca_loadings.html&gt;
 PCA: Plot the variance ratio (
pl.pca_variance_ratio
) ====================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.pca_variance_ratio.html&gt;
 PCA: Plot PCA results (
pl.pca_overview
) ========================================= The parameters are the ones of the scatter plot. Call pca_ranking separately if you want to change the default settings. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.pca_overview.html&gt;
 Embedding: Scatter plot in tSNE basis (
pl.tsne
) ================================================= More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.tsne.html&gt;
 Embeddings: Scatter plot in UMAP basis (
pl.umap
) ================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.umap.html&gt;
 Embeddings: Scatter plot in Diffusion Map basis (
pl.diffmap
) ============================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.diffmap.html&gt;
 Embeddings: Scatter plot in graph-drawing basis (
pl.draw_graph
) ================================================================= More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.draw_graph.html&gt;
 Embeddings: Plot the density of cells in an embedding (per condition) (
pl.embedding_density
) ============================================================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.embedding_density.html&gt;
 .. This function is commented out because it is not compatible with pandas version. If the issue is not resolved in the next update, this should be removed. .. Branching trajectories and pseudotime, clustering: Plot groups and pseudotime (
pl.dpt_groups_pseudotime
) .. =========================================================================================================== .. More details on the 
scanpy documentation .. &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.dpt_groups_pseudotime.html&gt;
 Branching trajectories and pseudotime, clustering: Heatmap of pseudotime series (
pl.dpt_timeseries
) ===================================================================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.dpt_timeseries.html&gt;
 Branching trajectories and pseudotime, clustering: Plot the abstracted graph through thresholding low-connectivity edges (
pl.paga
) ==================================================================================================================================== This uses ForceAtlas2 or igraph's layout algorithms for most layouts. When initializing the positions, note that - for some reason - igraph mirrors coordinates along the x axis... that is, you should increase the 
maxiter
 parameter by 1 if the layout is flipped. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.paga.html&gt;
 Branching trajectories and pseudotime, clustering: Scatter and PAGA graph side-by-side (
pl.paga_compare
) ========================================================================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.paga_compare.html&gt;
 Branching trajectories and pseudotime, clustering: Gene expression and annotation changes along paths (
pl.paga_path
) ====================================================================================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.paga_path.html&gt;
 Marker genes: Plot ranking of genes using dotplot plot (
pl.rank_genes_groups
) =============================================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.rank_genes_groups.html&gt;
 Marker genes: Plot ranking of genes as violin plot (
pl.rank_genes_groups_violin
) ================================================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.rank_genes_groups_violin.html&gt;
 Marker genes: Plot ranking of genes as stacked violin plot (
pl.rank_genes_groups_stacked_violin
) ================================================================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.rank_genes_groups_stacked_violin.html&gt;
 Marker genes: Plot ranking of genes as heatmap plot (
pl.rank_genes_groups_heatmap
) ==================================================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.rank_genes_groups_heatmap.html&gt;
 Marker genes: Plot ranking of genes as dotplot plot (
pl.rank_genes_groups_dotplot
) ==================================================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.rank_genes_groups_dotplot.html&gt;
 Marker genes: Plot ranking of genes as matrixplot plot (
pl.rank_genes_groups_matrixplot
) ========================================================================================== More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.rank_genes_groups_matrixplot.html&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/scanpy_remove_confounders/scanpy_remove_confounders/1.10.2+galaxy3	"Regress out unwanted sources of variation, using 
pp.regress_out
 ================================================================= Regress out unwanted sources of variation, using simple linear regression. This is inspired by Seurat's 
regressOut
 function in R. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.regress_out.html&gt;
 .. This function is commented out because the conda package is not working. Please add this if there is user demand and the conda package is fixed. If not please remove in the next update. .. Correct batch effects by matching mutual nearest neighbors, using 
external.pp.mnn_correct
 .. =========================================================================================== .. This uses the implementation of mnnpy. Depending on do_concatenate, it returns AnnData objects in the .. original order containing corrected expression values or a concatenated matrix or AnnData object. .. Be reminded that it is not advised to use the corrected data matrices for differential expression testing. .. More details on the 
scanpy documentation .. &lt;https://scanpy.readthedocs.io/en/stable/generated/scanpy.external.pp.mnn_correct.html&gt;
 Correct batch effects with ComBat function (
pp.combat
) ======================================================== Corrects for batch effects by fitting linear models, gains statistical power via an EB framework where information is borrowed across genes. This uses the implementation of ComBat More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pp.combat.html&gt;
 Correct batch effects with bbknn function (
external.pp.bbknn
) =============================================================== Batch balanced kNN alters the kNN procedure to identify each cell’s top neighbours in each batch separately instead of the entire cell pool with no accounting for batch. The nearest neighbours for each batch are then merged to create a final list of neighbours for the cell. Aligns batches in a quick and lightweight manner. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/generated/scanpy.external.pp.bbknn.html&gt;
 Correct batch effects with harmony function (
external.pp.harmony_integrate
) ============================================================================= Harmony is an algorithm for integrating single-cell data from multiple experiments. As Harmony works by adjusting the principal components, this function should be run after performing PCA but before computing the neighbor graph. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/generated/scanpy.external.pp.harmony_integrate.html&gt;
 Correct batch effects with scanprama function (
external.pp.scanorama_integrate
) ================================================================================= Scanprama is an algorithm for integrating single-cell data from multiple experiments stored in an AnnData object. This function should be run after performing PCA but before computing the neighbor graph. More details on the 
scanpy documentation &lt;https://scanpy.readthedocs.io/en/stable/generated/scanpy.external.pp.scanorama_integrate.html&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/seurat_create/seurat_create/5.0+galaxy1	"Seurat ====== Seurat is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data. Creating a Seurat Object ======================== Seurat objects can be created from single cell data in matrix market or tab-delimited table formats, using the Read10X or read.table functions followed by CreateSeuratObject. The input should be a single cell matrix with cells as rows and genes as columns. Both RNA-seq and combined RNA and CITE-seq data can be used as inputs. Read10X ======== Load sparse data matrices provided by 10X genomics. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/read10x&gt;
 read.table ========== Read a tab-delimited tsv or tabular file into an RDS file as a table. More details on the 
R documentation &lt;https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/read.table&gt;
 CreateSeuratObject ================== Create a Seurat Object from raw data in RDS format. names.field For the initial identity class for each cell, choose this field from the cell's name. E.g. If your cells are named as BARCODE_CLUSTER_CELLTYPE in the input matrix, set names.field to 3 to set the initial identities to CELLTYPE. names.delim For the initial identity class for each cell, choose this delimiter from the cell's column name. E.g. If your cells are named as BARCODE-CLUSTER-CELLTYPE, set this to “-” to separate the cell name into its component parts for picking the relevant field. meta.data Additional cell-level metadata to add to the Seurat object. Should be a data.frame where the rows are cell names and the columns are additional metadata fields. Row names in the metadata need to match the column names of the counts matrix. Filtering can also be performed on: min.cells = only include features/genes detected in at least this many cells min.features = only include cells where at least this many features are detected Some QC metrics are added when creating a Seurat Object (nCount_RNA and nFeature_RNA). Mito percentage can optionally be calculated - it will be based on gene names starting with ""MT-"". If this pattern does not work for your gene names then you can use the separate 'Calculate QC Metrics' function instead. More details on the 
seurat documentation &lt;https://satijalab.github.io/seurat-object/reference/CreateSeuratObject.html&gt;
 Calculate QC Metrics ==================== Calculate the percentage of all the counts belonging to a subset of the possible features for each cell. This is useful when trying to compute the percentage of transcripts that map to mitochondrial genes for example. The calculation here is simply the column sum of the matrix present in the counts slot for features belonging to the set divided by the column sum for all features times 100. Feature sets can be defined by entering a list of genes or using a shared pattern in the gene names, such as ""^MT-"" or ""^RP[LS]"" for human mitochondrial or ribosomal genes. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/percentagefeatureset&gt;
 Filter Cells ============ Filter cells based on QC metrics. nFeature_RNA = number of unique genes identified in the cell ncounts_RNA = total number of RNAs found in the cell percent.mt = percentage of mitochondrial genes in the cell More details on the 
R documentation &lt;https://rdrr.io/r/base/subset.html&gt;
__"
toolshed.g2.bx.psu.edu/repos/iuc/seurat_data/seurat_data/5.0+galaxy0	"Seurat ====== Seurat is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data. Inspect and Manipulate ====================== A selection of essential functions to display information about a Seurat Object or make basic changes to the object. More details on these essential commands can be found in the 
seurat documentation &lt;https://satijalab.org/seurat/articles/essential_commands&gt;
 Inspect ======= General - basic information about the Seurat Object Features - list of features, select which assay to display features from if object contains multimodal data Cells - list of cell barcodes/ids, select layer to display cells for if object contains multiple layers Idents - list showing Ident for each cell Metadata - table of cell metadata Matrix - show the full matrix Manipulate ========== Functions to add, change, or remove selected elements of a Seurat Object. Change Idents ============= Change which annotation in your cell metadata is used as the Ident column. Rename Idents ============= Rename the classes in the Ident column (e.g. to replace cluster numbers with cell types). You have the option to stash the original idents in a new column called 'old.ident' before renaming the classes in the Ident column. AddMetaData =========== Merge ===== Combine two Seurat Objects into a single Seurat Object. Each object will be placed in a separate layer, but you can choose to run the JoinLayers function after merging to combine the objects into a single layer. Subset ====== Subset a group of cells based on their ident or another grouping in your cell metadata. DietSeurat ========== Keep only certain aspects of the Seurat object. Can be useful in functions that utilize merge as it reduces the amount of data in the merge More details on these essential commands can be found in the 
seurat documentation &lt;https://satijalab.org/seurat/reference/dietseurat&gt;
 AggregateExpression =================== Returns summed counts (""pseudobulk"") for each identity class. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/aggregateexpression&gt;
__ DefaultAssay ============ Set the default assay for multimodal data. You can use the Inspect - General function to check which assay is currently active and which other assays are available."
toolshed.g2.bx.psu.edu/repos/iuc/seurat_clustering/seurat_clustering/5.0+galaxy0	"Seurat ====== Seurat is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data. FindNeighbors ============= Compute the k.param nearest neighbors for a given dataset. Can also optionally (via compute.SNN), construct a shared nearest neighbor graph by calculating the neighborhood overlap (Jaccard index) between every cell and its k.param nearest neighbors. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/findneighbors&gt;
 FindMultiModalNeighbors ======================= This function will construct a weighted nearest neighbor (WNN) graph for two modalities (e.g. RNA-seq and CITE-seq). For each cell, we identify the nearest neighbors based on a weighted combination of two modalities. Takes as input two dimensional reductions, one computed for each modality. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/findmultimodalneighbors&gt;
 FindClusters ============ Identify clusters of cells by a shared nearest neighbor (SNN) modularity optimization based clustering algorithm. First calculate k-nearest neighbors and construct the SNN graph. Then optimize the modularity function to determine clusters. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/findclusters&gt;
 FindAllMarkers ============== Find markers (differentially expressed genes) for each of the identity classes in a dataset Outputs a matrix containing a ranked list of putative markers, and associated statistics (p-values, ROC score, etc.) Methods: ""wilcox"" : Identifies differentially expressed genes between two groups of cells using a Wilcoxon Rank Sum test (default); will use a fast implementation by Presto if installed ""wilcox_limma"" : Identifies differentially expressed genes between two groups of cells using the limma implementation of the Wilcoxon Rank Sum test; set this option to reproduce results from Seurat v4 ""bimod"" : Likelihood-ratio test for single cell gene expression, (McDavid et al., Bioinformatics, 2013) ""roc"" : Identifies 'markers' of gene expression using ROC analysis. For each gene, evaluates (using AUC) a classifier built on that gene alone, to classify between two groups of cells. An AUC value of 1 means that expression values for this gene alone can perfectly classify the two groupings (i.e. Each of the cells in cells.1 exhibit a higher level than each of the cells in cells.2). An AUC value of 0 also means there is perfect classification, but in the other direction. A value of 0.5 implies that the gene has no predictive power to classify the two groups. Returns a 'predictive power' (abs(AUC-0.5) * 2) ranked matrix of putative differentially expressed genes. ""t"" : Identify differentially expressed genes between two groups of cells using Student's t-test. ""negbinom"" : Identifies differentially expressed genes between two groups of cells using a negative binomial generalized linear model. Use only for UMI-based datasets ""poisson"" : Identifies differentially expressed genes between two groups of cells using a poisson generalized linear model. Use only for UMI-based datasets ""LR"" : Uses a logistic regression framework to determine differentially expressed genes. Constructs a logistic regression model predicting group membership based on each feature individually and compares this to a null model with a likelihood ratio test. ""MAST"" : Identifies differentially expressed genes between two groups of cells using a hurdle model tailored to scRNA-seq data. Utilizes the MAST package to run the DE testing. ""DESeq2"" : Identifies differentially expressed genes between two groups of cells based on a model using DESeq2 which uses a negative binomial distribution (Love et al, Genome Biology, 2014).This test does not support pre-filtering of genes based on average difference (or percent detection rate) between cell groups. However, genes may be pre-filtered based on their minimum detection rate (min.pct) across both cell groups. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/findallmarkers&gt;
 FindMarkers =========== Find markers (differentially expressed genes) for identity classes (clusters) or groups of cells Outputs a data.frame with a ranked list of putative markers as rows, and associated statistics as columns (p-values, ROC score, etc., depending on the test used (test.use)). Methods - as for FindAllMarkers More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/findmarkers&gt;
 FindConservedMarkers ==================== Finds markers that are conserved between the groups Uses metap::minimump as meta.method. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/findconservedmarkers&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/seurat_integrate/seurat_integrate/5.0+galaxy0	"Seurat ====== Seurat is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data. Split ===== The split function is used here to split the data into separate layers based on the groups defined by f. Each group becomes its own layer in the Seurat Object. More details on the 
R documentation &lt;https://rdrr.io/r/base/split.html&gt;
 Integrate ========= Multiple layers are integrated to enable them to be analysed together. Available methods are: CCA, Harmony, JointPCA, RPCA, FastMNN and scVI. More details on the 
seurat documentation &lt;https://satijalab.github.io/seurat-object/reference/CreateSeuratObject.html&gt;
 JoinLayers ========== Multiple layers (e.g. those created using the split function) can be joined together in a single layer. No integration is performed by this function. More details on the 
seurat documentation &lt;https://satijalab.github.io/seurat-object/reference/SplitLayers.html&gt;
 PrepSCTFindMarkers ================== Given a merged object with multiple SCT models, this function uses minimum of the median UMI (calculated using the raw UMI counts) of individual objects to reverse the individual SCT regression model using minimum of median UMI as the sequencing depth covariate. The counts slot of the SCT assay is replaced with recorrected counts and the data slot is replaced with log1p of recorrected counts. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/prepsctfindmarkers&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/seurat_preprocessing/seurat_preprocessing/5.0+galaxy0	"Seurat ====== Seurat is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data. NormalizeData ============= Normalize the count data present in a given assay. Methods: “LogNormalize”: Feature counts for each cell are divided by the total counts for that cell and multiplied by the scale.factor. This is then natural-log transformed using log1p “CLR”: Applies a centered log ratio transformation “RC”: Relative counts. Feature counts for each cell are divided by the total counts for that cell and multiplied by the scale.factor. No log-transformation is applied. For counts per million (CPM) set scale.factor = 1e6 More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/normalizedata&gt;
 FindVariableFeatures ==================== Identify features that are outliers on a 'mean variability plot'. Methods: “vst”: First, fits a line to the relationship of log(variance) and log(mean) using local polynomial regression (loess). Then standardizes the feature values using the observed mean and expected variance (given by the fitted line). Feature variance is then calculated on the standardized values after clipping to a maximum (see clip.max parameter). “mean.var.plot” (mvp): First, uses a function to calculate average expression (mean.function, using FastExpMean) and dispersion (dispersion.function, using FastLogVMR) for each feature. Next, divides features into num.bin (deafult 20) bins based on their average expression, and calculates z-scores for dispersion within each bin. The purpose of this is to identify variable features while controlling for the strong relationship between variability and average expression “dispersion” (disp): selects the genes with the highest dispersion values More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/findvariablefeatures&gt;
 Scale and regress the data with ScaleData ========================================= Scale and center features in the dataset. If variables are provided in vars.to.regress, they are individually regressed against each feature, and the resulting residuals are then scaled and centered. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/scaledata&gt;
 SCTransform =========== Use this function as an alternative to the NormalizeData, FindVariableFeatures, ScaleData workflow. Results are saved in a new assay (named SCT by default) with counts being (corrected) counts, data being log1p(counts), scale.data being pearson residuals; sctransform::vst intermediate results are saved in misc slot of new assay. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/sctransform&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/seurat_reduce_dimension/seurat_reduce_dimension/5.0+galaxy0	"Seurat ====== Seurat is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data. RunPCA ====== Run a PCA dimensionality reduction. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/runpca&gt;
 RuntSNE ======= Run t-SNE dimensionality reduction on selected features. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/runtsne&gt;
 RunUMAP ======= Runs the Uniform Manifold Approximation and Projection (UMAP) dimensional reduction technique. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/runumap&gt;
__"
toolshed.g2.bx.psu.edu/repos/iuc/seurat_plot/seurat_plot/5.0+galaxy0	"Seurat ====== Seurat is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data. You can choose which type of file you want to produce, but be aware that some types (e.g. tex) may not be viewable on Galaxy, although you will be able to download them to use elsewhere. VlnPlot ======= Draw a violin plot of single cell data (gene expression, metrics, PC scores, etc.) More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/vlnplot&gt;
 FeatureScatter ============== Create a scatter plot of two features (typically feature expression), across a set of single cells. Cells are colored by their identity class. Pearson correlation between the two features is displayed above the plot. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/featurescatter&gt;
 CellScatter =========== Creates a plot of scatter plot of features across two single cells. Pearson correlation between the two cells is displayed above the plot. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/cellscatter&gt;
 VariableFeaturePlot =================== View selected variable features on a plot of variances. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/variablefeatureplot&gt;
 VizDimLoadings ============== Visualize top genes associated with reduction components - default reduction is PCA. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/vizdimloadings&gt;
 DimPlot ======= Graph the output of a dimensional reduction technique on a 2D scatter plot where each point is a cell and it's positioned based on the cell embeddings determined by the reduction technique. By default, cells are colored by their identity class (can be changed with the group.by parameter). More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/dimplot&gt;
 DimHeatmap ========== Draw a heatmap focusing on a principal component. Both cells and genes are sorted by their principal component scores. Allows for nice visualization of sources of heterogeneity in the dataset. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/dimheatmap&gt;
 ElbowPlot ========= Plot the standard deviations of the principal components for easy identification of an elbow in the graph - plots PCA as default reduction. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/elbowplot&gt;
 FeaturePlot =========== Color single cells on a dimensional reduction plot according to a 'feature' (i.e. gene expression, PC scores, number of genes detected, etc.) More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/featureplot&gt;
 DoHeatmap ========= Draw a heatmap of single cell feature expression. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/doheatmap&gt;
 DotPlot ======= Visualize how feature expression changes across different identity classes (e.g. clusters). The size of the dot encodes the percentage of cells within a class that express the gene, while the color encodes the AverageExpression level across all cells within a class. More details on the 
seurat documentation &lt;https://satijalab.org/seurat/reference/dotplot&gt;
__"
toolshed.g2.bx.psu.edu/repos/iuc/sinto_barcode/sinto_barcode/0.10.1+galaxy0	"Sinto is a toolkit for processing aligned single-cell data. -------------------------------------------------------------------------------------------------------------- Cell barcodes from one FASTQ file added to the read names of another, or the same, FASTQ file. This is useful when processing raw single-cell sequencing data, as the cell barcode information can easily be propagated to the aligned BAM file by encoding the cell barcode in the read name. 
Inputs
 FASTQ files containing barcodes and forward reads. An optional reverse reads FASTQ file can be provided for paired-end experiments. Note that all the FASTQs must contain the same number of reads and the reads must appear in the same order. 
Outputs
 FASTQ files with the read names modified to contain the cell barcode sequence at the beginning of the read name, separated from the original read name by a : character."
toolshed.g2.bx.psu.edu/repos/iuc/sinto_fragments/sinto_fragments/0.10.1+galaxy0	"Sinto: single-cell analysis tools -------------------------------------------------------------------------------------------------------------- An ATAC-seq fragment file can be created from a BAM file using the fragments command. The fragment file contains the position of each Tn5 integration site, the cell barcode associated with the fragment, and the number of times the fragment was sequenced. PCR duplicates are collapsed. 
Input
 A BAM file. Alignments in the input BAM file must contain cell barcodes either as a part of read names or in a tag (typically, in CB tag). 
Output
 The fragment file. It contains the positions of Tn5 integration sites, the cell barcode that the DNA fragment originated from, and the number of times the fragment was sequenced."
toolshed.g2.bx.psu.edu/repos/iuc/snapatac2_clustering/snapatac2_clustering/2.8.0+galaxy0	"Perform dimension reduction using Laplacian Eigenmap, using 
tl.spectral
 ========================================================================= Perform dimension reduction using Laplacian Eigenmaps. Convert the cell-by-feature count matrix into lower dimensional representations using the spectrum of the normalized graph Laplacian defined by pairwise similarity between cells. This function utilizes the matrix-free spectral embedding algorithm to compute the embedding when 
distance_metric
 is “cosine”, which scales linearly with the number of cells. For other types of similarity metrics, the time and space complexity scale quadratically with the number of cells. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.tl.spectral.html&gt;
 Compute Laplacian Eigenmaps simultaneously on multiple modalities, with linear space and time complexity, using 
tl.multi_spectral
 =================================================================================================================================== This is similar to 
spectral
, but it can work on multiple modalities. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.tl.multi_spectral.html&gt;
 Compute Umap, using 
tl.umap
 ============================= Compute Umap More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.tl.umap.html&gt;
 Compute a neighborhood graph of observations, using 
pp.knn
 ============================================================ Compute a neighborhood graph of observations. Computes a neighborhood graph of observations stored in adata using the method specified by method. The distance metric used is Euclidean. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.knn.html&gt;
 Cluster cells into subgroups, using 
tl.leiden
 =============================================== Cluster cells into subgroups. Cluster cells using the Leiden algorithm, an improved version of the Louvain algorithm. It has been proposed for single-cell analysis by. This requires having ran 
knn
. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.tl.leiden.html&gt;
 Cluster cells into subgroups using the K-means algorithm, using 
tl.kmeans
 =========================================================================== Cluster cells into subgroups using the K-means algorithm, a classical algorithm in data mining. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.tl.kmeans.html&gt;
 Cluster cells into subgroups using the DBSCAN algorithm, using 
tl.dbscan
 ========================================================================== Cluster cells into subgroups using the DBSCAN algorithm. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.tl.dbscan.html&gt;
 Cluster cells into subgroups using the HDBSCAN algorithm, using 
tl.hdbscan
 ============================================================================ Cluster cells into subgroups using the HDBSCAN algorithm. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.tl.hdbscan.html&gt;
 .. Aggregate values in adata.X in a row-wise fashion, using 
tl.aggregate_X
 .. ========================================================================= .. Aggregate values in adata.X in a row-wise fashion. .. Aggregate values in adata.X in a row-wise fashion. This is used to compute RPKM or RPM values stratified by user-provided groupings. .. More details on the 
SnapATAC2 documentation .. &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.tl.aggregate_X.html&gt;
 Aggregate cells into pseudo-cells, using 
tl.aggregate_cells
 ============================================================= Aggregate cells into pseudo-cells. Aggregate cells into pseudo-cells by iterative clustering. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.tl.aggregate_cells.html&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/snapatac2_plotting/snapatac2_plotting/2.8.0+galaxy0	"Plot fragment size distribution, using 
pl.frag_size_distr
 =========================================================== Plot fragment size distribution. 
metrics.frag_size_distr
 must be ran first in order to use this function. Plot the TSS enrichment vs. number of fragments density figure, using 
pl.tsse
 =============================================================================== Plot the TSS enrichment vs. number of fragments density figure. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pl.tsse.html&gt;
 Plot the UMAP embedding, using 
pl.umap
 ======================================== Plot the UMAP embedding. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pl.umap.html&gt;
 Plot the eigenvalues of spectral embedding, using 
pl.spectral_eigenvalues
 =========================================================================== Plot the eigenvalues of spectral embedding. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pl.spectral_eigenvalues.html&gt;
__"
toolshed.g2.bx.psu.edu/repos/iuc/snapatac2_preprocessing/snapatac2_preprocessing/2.8.0+galaxy0	"Convert a BAM file to a fragment file, using 
pp.make_fragment_file
 ==================================================================== Convert a BAM file to a fragment file. Convert a BAM file to a fragment file by performing the following steps: - Filtering: remove reads that are unmapped, not primary alignment, mapq < 30, fails platform/vendor quality checks, or optical duplicate. For paired-end sequencing, it also removes reads that are not properly aligned. - Deduplicate: Sort the reads by cell barcodes and remove duplicated reads for each unique cell barcode. - Output: Convert BAM records to fragments (if paired-end) or single-end reads. The bam file needn’t be sorted or filtered. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.make_fragment_file.html&gt;
 Generate cell by bin count matrix, using 
pp.import_fragments
 ============================================================== Import data fragment files and compute basic QC metrics. This function is used to generate and add a cell by bin count matrix to the AnnData object. This function accepts both single-end and paired-end reads. If the records in the fragment file contain 6 columns with the last column representing the strand of the fragment, the fragments are considered single-ended. Otherwise, the fragments are considered paired-ended. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.import_fragments.html&gt;
 Generate cell by bin count matrix, using 
pp.import_contacts
 ============================================================= Import chromatin contacts. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.import_contacts.html&gt;
 Generate cell by bin count matrix, using 
pp.add_tile_matrix
 ============================================================= Generate cell by bin count matrix. This function is used to generate and add a cell by bin count matrix to the AnnData object. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.add_tile_matrix.html&gt;
 Generate cell by gene activity matrix, using 
pp.make_gene_matrix
 ================================================================== Generate cell by gene activity matrix. Generate cell by gene activity matrix by counting the TN5 insertions in gene body regions. The result will be stored in a new file and a new AnnData object will be created. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.make_gene_matrix.html&gt;
 Filter cell outliers based on counts and numbers of genes expressed, using 
pp.filter_cells
 ============================================================================================ Filter cell outliers based on counts and numbers of genes expressed. For instance, only keep cells with at least 
min_counts
 counts or 
min_ts`` TSS enrichment scores. This is to filter measurement outliers, i.e. “unreliable” observations. More details on the
SnapATAC2 documentation <https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.filter_cells.html>
__ Perform feature selection, using
pp.select_features
===================================================== Perform feature selection by selecting the most accessible features across all cells unless
max_iter
&gt; 1 More details on the
SnapATAC2 documentation <https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.select_features.html>
__ Compute probability of being a doublet using the scrublet algorithm, using
pp.scrublet
======================================================================================== Compute probability of being a doublet using the scrublet algorithm. This function identifies doublets by generating simulated doublets using randomly pairing chromatin accessibility profiles of individual cells. The simulated doublets are then embedded alongside the original cells using the spectral embedding algorithm in this package. A k-nearest-neighbor classifier is trained to distinguish between the simulated doublets and the authentic cells. This trained classifier produces a “doublet score” for each cell. The doublet scores are then converted into probabilities using a Gaussian mixture model. More details on the
SnapATAC2 documentation <https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.scrublet.html>
__ Remove doublets according to the doublet probability or doublet score, using
pp.filter_doublets
================================================================================================= Remove doublets according to the doublet probability or doublet score. The user can choose to remove doublets by either the doublet probability or the doublet score.
scrublet
must be ran first in order to use this function. More details on the
SnapATAC2 documentation <https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.filter_doublets.html>
__ A modified MNN-Correct algorithm based on cluster centroid, using
pp.mnc_correct
================================================================================== A modified MNN-Correct algorithm based on cluster centroid. More details on the
SnapATAC2 documentation <https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.mnc_correct.html>
__ Use harmonypy to integrate different experiments,using
pp.harmony
=================================================================== Use harmonypy to integrate different experiments. Harmony is an algorithm for integrating single-cell data from multiple experiments. This function uses the python port of Harmony,
harmonypy
, to integrate single-cell data stored in an AnnData object. This function should be run after performing dimension reduction. More details on the
SnapATAC2 documentation <https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.harmony.html>
__ Use Scanorama to integrate different experiments, using
pp.scanorama_integrate
================================================================================ Use Scanorama to integrate different experiments. Scanorama is an algorithm for integrating single-cell data from multiple experiments stored in an AnnData object. This function should be run after performing
tl.spectral
but before computing the neighbor graph. More details on the
SnapATAC2 documentation <https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.scanorama_integrate.html>
__ Export fragments for each group of cells, using
ex.export_fragments
===================================================================== Export and save fragments for a group of cells in a BED format file. More details on the
SnapATAC2 documentation <https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.ex.export_fragments.html>
__ Export fragments for each group of cells, using
ex.export_coverage
===================================================================== Export and save coverage for a group of cells in a bedgraph or bigwig format file. More details on the
SnapATAC2 documentation <https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.ex.export_coverage.html>`"
toolshed.g2.bx.psu.edu/repos/iuc/snapatac2_peaks_and_motif/snapatac2_peaks_and_motif/2.8.0+galaxy0	"Call peaks using MACS3, using 
tl.macs3
 ======================================== Call peaks using MACS3. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.tl.macs3.html&gt;
 Merge peaks from different groups, using 
tl.merge_peaks
 ========================================================= Merge peaks from different groups. Merge peaks from different groups. It is typically used to merge results from 
macs3
. This function initially expands the summits of identified peaks by 
half_width
 on both sides. Following this expansion, it addresses the issue of overlapping peaks through an iterative process. The procedure begins by prioritizing the most significant peak, determined by the smallest p-value. This peak is retained, and any peak that overlaps with it is excluded. Subsequently, the same method is applied to the next most significant peak. This iteration continues until all peaks have been evaluated, resulting in a final list of non-overlapping peaks, each with a fixed width determined by the initial extension. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.tl.merge_peaks.html&gt;
 Generate cell by peak count matrix, using 
pp.make_peak_matrix
 =============================================================== This function will generate a cell by peak count matrix. 
import_fragments
 must be ran first in order to use this function. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.pp.make_peak_matrix.html&gt;
 A quick-and-dirty way to get marker regions, using 
tl.marker_regions
 ====================================================================== A quick-and-dirty way to get marker regions. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.tl.marker_regions.html&gt;
 Identify differentially accessible regions, using 
tl.diff_test
 ==================================================================== Identify differentially accessible regions. More details on the 
SnapATAC2 documentation &lt;https://scverse.org/SnapATAC2/api/_autosummary/snapatac2.tl.diff_test.html&gt;
__"
toolshed.g2.bx.psu.edu/repos/iuc/baredsc_1d/baredsc_1d/1.1.3+galaxy0	".. class:: infomark 
BARED (Bayesian Approach to Retreive Expression Distribution of) Single Cell
 baredSC is a tool that uses a Monte-Carlo Markov Chain to estimate a confidence interval on the probability density function (PDF) of expression of one or two genes from single-cell RNA-seq data. It uses the raw counts and the total number of UMI for each cell. The PDF is approximated by a number of 1d or 2d gaussians provided by the user. The likelihood is estimated using the asumption that the raw counts follow a Poisson distribution of parameter equal to the proportion of mRNA for the gene in the cell multiplied by the total number of UMI identified in this cell. To get a description of outputs, please read the 
Documentation &lt;https://baredsc.readthedocs.io/en/latest/index.html&gt;
_ This is a description of the figure with the results. - When the 1d version is used, it displays the mean PDF in solid red line, the median in black dashed lines (/!\backslash the integral of the median is not equal to 1) with the confidence interval of 1 sigma (68%), 2 sigma (95%) and 3 sigma (99.7%) as well as in green, the kernel density estimate of the input values, the detected expression (
log(1 + targetSum * raw / total UMI)
). - When the 2d version is used, it displays the PDF as a heatmap as well as a projection on the x and y axis. On the projection, the confidence interval 68% is indicated as a shaded area as well as the mean with a solid red line and the median with a dashed black line. On the top right corner, the correlation is indicated with the confidence interval 68% as well as a confidence interval on the one-sided p-value (the probability that the correlation is the opposite sign of the mean, one sigma confidence interval). Usually you should run baredSC_1d or baredSC_2d with 1 to 4 gaussians. Then you combine the different models with combineMultipleModels_1d or combineMultipleModels_2d."
toolshed.g2.bx.psu.edu/repos/iuc/baredsc_2d/baredsc_2d/1.1.3+galaxy0	".. class:: infomark 
BARED (Bayesian Approach to Retreive Expression Distribution of) Single Cell
 baredSC is a tool that uses a Monte-Carlo Markov Chain to estimate a confidence interval on the probability density function (PDF) of expression of one or two genes from single-cell RNA-seq data. It uses the raw counts and the total number of UMI for each cell. The PDF is approximated by a number of 1d or 2d gaussians provided by the user. The likelihood is estimated using the asumption that the raw counts follow a Poisson distribution of parameter equal to the proportion of mRNA for the gene in the cell multiplied by the total number of UMI identified in this cell. To get a description of outputs, please read the 
Documentation &lt;https://baredsc.readthedocs.io/en/latest/index.html&gt;
_ This is a description of the figure with the results. - When the 1d version is used, it displays the mean PDF in solid red line, the median in black dashed lines (/!\backslash the integral of the median is not equal to 1) with the confidence interval of 1 sigma (68%), 2 sigma (95%) and 3 sigma (99.7%) as well as in green, the kernel density estimate of the input values, the detected expression (
log(1 + targetSum * raw / total UMI)
). - When the 2d version is used, it displays the PDF as a heatmap as well as a projection on the x and y axis. On the projection, the confidence interval 68% is indicated as a shaded area as well as the mean with a solid red line and the median with a dashed black line. On the top right corner, the correlation is indicated with the confidence interval 68% as well as a confidence interval on the one-sided p-value (the probability that the correlation is the opposite sign of the mean, one sigma confidence interval). Usually you should run baredSC_1d or baredSC_2d with 1 to 4 gaussians. Then you combine the different models with combineMultipleModels_1d or combineMultipleModels_2d."
toolshed.g2.bx.psu.edu/repos/ebi-gxa/salmon_kallisto_mtx_to_10x/_salmon_kallisto_mtx_to_10x/0.0.1+galaxy6	".. class:: infomark 
What it does
 Kallisto and Alevin (and possibly other tools) output an MTX file and associated labels that are not consistent with the old-style 10X, meaning that routines designed to parse those files cannot be used. This tool transforms (in the mathematical sense) the matrix, and reformats the genes file (essentially duplicating the column) to match those earlier conventions. 
Inputs
 MTX and row and gene labels from the relevant tool. For Alevin this will be * Matrix file: quants_mat.mtx.gz * Genes file: quants_mat_cols.txt * Barcodes file: quants_mat_rows.txt For Kallisto it will be: * Matrix file: [name].mtx.gz * Genes file: [name].genes.txt * Barcodes file: [name].barcodes.txt ----- 
Outputs
 * MTX, gene and cell labels in 10X style"
toolshed.g2.bx.psu.edu/repos/iuc/episcanpy_preprocess/episcanpy_preprocess/0.3.2+galaxy1	"convert the count matrix into a binary matrix (
pp.binarize
) ============================================================================================ convert the count matrix into a binary matrix More details on the 
episcanpy documentation &lt;https://colomemaria.github.io/episcanpy_doc/api/episcanpy.api.pp.binarize.html&gt;
 Filter cells outliers based on counts and numbers of features expressed (
pp.filter_cells
) ============================================================================================ For instance, only keep cells with at least 
min_counts
 counts or 
min_features
 genes expressed. This is to filter measurement outliers, i.e. ""unreliable"" observations. Only provide one of the optional parameters 
min_counts
, 
min_features
, 
max_counts
, 
max_features
 per call. More details on the 
episcanpy documentation &lt;https://colomemaria.github.io/episcanpy_doc/api/episcanpy.api.pp.filter_cells.html&gt;
 Filter features based on number of cells or counts (
pp.filter_features
) ======================================================================================== Keep features that have at least 
min_counts
 counts or are expressed in at least 
min_cells
 cells or have at most 
max_counts
 counts or are expressed in at most 
max_cells
 cells. Only provide one of the optional parameters 
min_counts
, 
min_cells
, 
max_counts
, 
max_cells
 per call. More details on the 
episcanpy documentation &lt;https://colomemaria.github.io/episcanpy_doc/api/episcanpy.api.pp.filter_features.html&gt;
 Histogram of the number of open features (
pp.coverage_cells
) ======================================================================================== Histogram of the number of open features (in the case of ATAC-seq data) per cell. More details on the 
episcanpy documentation &lt;https://colomemaria.github.io/episcanpy_doc/api/episcanpy.api.pp.coverage_cells.html&gt;
 Distribution of the feature commoness in cells (
pp.coverage_features
) ======================================================================================== Display how often a feature is measured as open (for ATAC-seq). Distribution of the feature commoness in cells. More details on the 
episcanpy documentation &lt;https://colomemaria.github.io/episcanpy_doc/api/episcanpy.api.pp.coverage_features.html&gt;
 Selects the most variable features according to either a specified number of features or minimum variance score (
pp.select_var_feature
) ========================================================================================================================================= This function computes a variability score to rank the most variable features across all cells. Then it selects the most variable features according to either a specified number of features (nb_features) or a minimum variance score (min_score). More details on the 
episcanpy documentation &lt;https://colomemaria.github.io/episcanpy_doc/api/episcanpy.api.pp.select_var_feature.html&gt;
 Distribution of cells sharing features and variability score (
pp.cal_var
) ============================================================================= Show distribution plots of cells sharing features and variability score. More details on the 
episcanpy documentation &lt;https://colomemaria.github.io/episcanpy_doc/api/episcanpy.api.pp.cal_var.html&gt;
 Compute a variability score to rank the most variable features across all cells (
pp.variability_features
) ============================================================================================================ This function computes a variability score to rank the most variable features across all cells. Then it selects the most variable features according to either a specified number of features (nb_features) or a minimum variance score (min_score). Find and add gene annotations (
tl.find_genes
) ======================================================================================== This function adds a gene annotation to an AnnData (h5ad) file from annotations file (.annotation.gtf). More details on the 
episcanpy documentation &lt;https://colomemaria.github.io/episcanpy_doc/api/episcanpy.api.pp.variability_features.html&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/velocyto_cli/velocyto_cli/0.17.17+galaxy3	"Requirements on the input files velocyto assumes that the bam file that is passed to the CLI contains a set of information and that some upstream analysis was performed on them already. In particular the bam file will have to: Be sorted by mapping position. Represents either a single sample (multiple cells prepared using a certain barcode set in a single experiment) or single cell. Contain an error corrected cell barcodes as a TAG named CB or XC. Contain an error corrected molecular barcodes as a TAG named UB or XM. Note For SmartSeq2 bam files (3) and (4) are not required because it consists of one bam file per cell and no umi are present. velocyto assumes that the gtf file follows the GENCODE gtf format description. However some mandatory field are relaxed to extend compatibility to a wider set of gtf files. In particular the gtf file will have to: Contain the 3rd column entry feature-type. Note that only the exon entry of the gtf file marked as exon in this column will be considered and therefore the requirements below only apply to the 
exon
 labeled lines. Contain, in the 9th column, the key-value pair transcript_id, containing an unique identified for the transcript model. Contain, in the 9th column, the key-value pair transcript_name (Optional, if not present it will be set to the value of transcript_id) Contain, in the 9th column, the key-value pair gene_id, containing an unique identified for the gene. Contain, in the 9th column, the key-value pair gene_name (Optional, if not present it will be set to the value of gene_id) Contain, in the 9th column, the key-value pair exon_number (Recommended but optional, if not provided velocyto will sort exons in memory and number them)"
toolshed.g2.bx.psu.edu/repos/goeckslab/squidpy/squidpy_spatial/1.5.0+galaxy0	"What it does
 This tool includes various of single cell spatial analysis utils provided by Squidpy. 
Input
 
AnnData
 
Output
 
Anndata
 
Plotting (PNG) if applicable"
toolshed.g2.bx.psu.edu/repos/perssond/quantification/quantification/1.6.0+galaxy0	"-------- MCQUANT -------- 
MCQUANT
 module for single cell quantification given a segmentation mask and multi-channel image. The CSV structure is aligned with histoCAT output. 
Inputs
 1. A fully stitched and registered image in .ome.tif format. Nextflow will use images in the registration/ and dearray/ subfolders as appropriate. 2. One or more segmentation masks in .tif format. Nextflow will use files in the segmentation/ subfolder within the project. 3. A .csv file containing a marker_name column specifying names of individual channels. Nextflow will look for this file in the project directory. 
Outputs
 A cell-by-feature table mapping Cell IDs to marker expression and morphological features (including x,y coordinates)."
toolshed.g2.bx.psu.edu/repos/goeckslab/cell_intensity_processing/cell_intensity_processing/0.0.2+galaxy1	"This tool can be used to perform several different common signal processing operations for single-cell mean marker intensities from multiplex tissue imaging data. 
Inputs
 1. Comma-separated feature observation matrix that is generated by 
MCQuant
 2. Comma-separated channel metadata file that maps marker names to exposure times (optional) and respective AF/bg channels (optional) 
Options
 1. Exposure correction - Divide single-cell intensities by respective exposure time in channel metadata 2. Background subtraction - Subtract single-cell mmean marker intensities by respective AF/bg channel mean intensity specified in channel metadata 3. Signal-to-background ratio - Divide single-cell mmean marker intensities by respective AF/bg channel mean intensity specified in channel metadata 
Outputs
 1. Feature observation matrix with processed intensities for all markers in channel metadata file. CellIDs, centroids, and morphological data remain unchanged."
toolshed.g2.bx.psu.edu/repos/imgteam/slice_image/ip_slice_image/0.3-4	"Slices an image into multiple smaller, square-shaped patches.
 For overlapping patches, set the stride to a value smaller than the patch size. For non-overlapping patches, set the stride to a value identical to the patch size (or larger). If the stride is set to a value larger than the patch size, parts of the original image will be skipped. Optionally, patches entirely corresponding to image background are discarded. To decide whether a patch corresponds to image background, the 
homogeneity
 of its 
gray-level co-occurrence matrix
 is considered. .. _homogeneity: https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.graycoprops .. _gray-level co-occurrence matrix: https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.graycomatrix In addition, the number of the remaining patches can be reduced by specifying a maximum number of patches to retain. Those will be selected randomly."
toolshed.g2.bx.psu.edu/repos/goeckslab/vitessce_spatial/vitessce_spatial/3.5.1+galaxy0	"What it does
 This tools provides web-based, interactive and scalable visualizations of single cell data. 
Input
 OME-TIFF image. Segmentation masks (optional). AnnData with marker intensities. 
Output
 An HTML file with Vitessce component."
toolshed.g2.bx.psu.edu/repos/perssond/s3segmenter/s3segmenter/1.3.12+galaxy0	"------------------- S3segmenter ------------------- 
S3segmenter
 is a Python-based set of functions that generates single cell (nuclei and cytoplasm) label masks. Inputs are: 1. an .ome.tif (preferably flat field corrected) 2. a 3-class probability maps derived from a deep learning model such as UNet. Classes include background, nuclei contours, and nuclei foreground. The centers of each nuclei are obtained by finding local maxima from the nuclei foreground. These are used for marker-controlled watershed constrained by the nuclei contours. To segment cytoplasm, the nuclei are in turn used for a marker-controlled watershed segmentation constrained by a cytoplasmic marker such as B-catenin. The channel number of this marker must be specified. A 3-pixel annulus around each nucleus will also be used to segment cytoplasm. The source repository can be found here: https://github.com/HMS-IDAC/S3segmenter"
toolshed.g2.bx.psu.edu/repos/lecorguille/anova/abims_anova/1.2.1	".. class:: infomark 
Authors
 Gildas Le Corguille ABiMS - UPMC/CNRS - Station Biologique de Roscoff - gildas.lecorguille|at|sb-roscoff.fr Melanie Petera - PFEM ; INRA ; MetaboHUB --------------------------------------------------- ===== Anova ===== ----------- Description ----------- Analysis of variance (ANOVA) is used to analyze the differences between group means and their associated procedures, in which the observed variance in a particular variable is partitioned into components attributable to different sources of variation. 
Note about sum of squares (SS) calculation of N-way ANOVA in this module.
 This module use R function 
manova()
 (and thus R function 
aov()
) to establish N-way ANOVA. Therefore calculated sum of squares are sequential ones (sometimes called ""Type I SS""). If your design is unbalanced, this may not correspond to the type of hypothesis being of interest. Note that you can obtain adjusted sums of squares (""Type II SS"") by running several times this module with different orders in factors. ----------- Input files ----------- +----------------------------+------------+ | Parameter : num + label | Format | +============================+============+ | 1 : Data Matrix file | Tabular | +----------------------------+------------+ | 2 : Sample Metadata file | Tabular | +----------------------------+------------+ | 3 : Variable Metadata file | Tabular | +----------------------------+------------+ ------------ Output files ------------ 
.anova_pvalue.tabular
* | Your variable metadata file completed with columns of p-values, result of selection method and means of subgroups. 
.anova_signif.pdf
 | A pdf file containing a Venn diagram and boxplots of significant variables. ------ .. class:: infomark The outputs 
.anova_filtered.tabular
 is a tabular file. You can continue your analysis using it in the following tools: | Generic_filter | Hierarchical Clustering ---------------------------------------------------"
toolshed.g2.bx.psu.edu/repos/ethevenot/biosigner/biosigner/2.2.7	".. class:: infomark 
Author
 Philippe Rinaudo and Etienne Thevenot (CEA, LIST, MetaboHUB Paris, etienne.thevenot@cea.fr) --------------------------------------------------- .. class:: infomark 
Please cite
 Rinaudo P., Boudah S., Junot C. and Thevenot E.A. (2016). 
biosigner
: a new method for the discovery of significant molecular signatures from omics data. 
Frontiers in Molecular Biosciences
, 
3
 (http://dx.doi.org/10.3389/fmolb.2016.00026). --------------------------------------------------- .. class:: infomark 
R package
 The 
biosigner
 package is available from the bioconductor repository (http://bioconductor.org/packages/biosigner). --------------------------------------------------- .. class:: infomark 
Tool updates
 See the 
NEWS
 section at the bottom of this page --------------------------------------------------- ========================================================== 
biosigner
: Molecular signature discovery from omics data ========================================================== ----------- Description ----------- High-throughput, non-targeted, technologies such as transcriptomics, proteomics and metabolomics, are widely used to 
discover molecules
 which allow to efficiently discriminate between biological or clinical conditions of interest (e.g., disease vs control states). Powerful 
machine learning
 approaches such as Partial Least Square Discriminant Analysis (PLS-DA), Random Forest (RF) and Support Vector Machines (SVM) have been shown to achieve high levels of prediction accuracy. 
Feature selection
, i.e., the selection of the few features (i.e., the molecular signature) which are of highest discriminating value, is a critical step in building a robust and relevant classifier (Guyon and Elisseeff, 2003): First, dimension reduction is usefull to limit the risk of overfitting and reduce the prediction variability of the model; second, intrepretation of the molecular signature is facilitated; third, in case of the development of diagnostic product, a restricted list is required for the subsequent validation steps (Rifai et al, 2006). Since the comprehensive analysis of all combinations of features is not computationally tractable, several selection techniques have been described (Saeys et al, 2007). The major challenge for such methods is to be fast and extract 
restricted and stable molecular signatures
 which still provide high performance of the classifier (Gromski et al, 2014; Determan, 2015). The 
biosigner
 module implements a new feature selection algorithm to assess the relevance of the variables for the prediction performances of the classifier (Rinaudo et al, submitted). Three binary classifiers can be run in parallel, namely 
PLS-DA
, 
Random Forest
 and 
SVM
, as the performances of each machine learning approach may vary depending on the structure of the dataset. The algorithm computes the 
tier
 of each feature for the selected classifer(s): tier 
S
 corresponds to the final signature, i.e., features which have been found significant in all the selection steps; features with tier 
A
 have been found significant in all but the last selection, and so on for tier 
B
 to 
E
. It returns the 
signature
 (by default from the 
S
 tier) for each of the selected classifier as an additional column of the 
variableMetadata
 table. In addition the 
tiers
 and 
individual boxplots
 of the selected features are returned. The module has been successfully applied to 
transcriptomics
 and 
metabolomics
 data. Note: | 1) Only 
binary
 classification is currently available, | 2) If the 
dataMatrix
 contains 
missing
 values (NA), these features will be removed prior to modeling with Random Forest and SVM (in contrast, the NIPALS algorithm from PLS-DA can handle missing values), | 3) As the algorithm relies on bootstrapping, re-running the module may result in slightly different results. To ensure that returned results are exactly the same, the 
seed
 (advanced) parameter can be used. | --------------------------------------------------- .. class:: infomark 
References
 | Determan C. (2015). Optimal algorithm for metabolomics classification and feature selection varies by dataset. International 
Journal of Biology
 7, 100-115. | Gromski P.S., Xu Y., Correa E., Ellis D.I., Turner M.L. and Goodacre R. (2014). A comparative investigation of modern feature selection and classification approaches for the analysis of mass spectrometry data . 
Analytica Chimica Acta
 829, 1-8. | Guyon I. and Elisseeff A. (2003). An introduction to variable and feature selection. 
Journal of Machine Learning Research
 3, 1157-1182. | Rifai N., Gillette M.A. and Carr S.A. (2006). Protein biomarker discovery and validation: the long and uncertain path to clinical utility. 
Nature Biotechnology
 24, 971-983. | Rinaudo P., Junot C. and Thevenot E.A. 
biosigner
: A new method for the discovery of restricted and stable molecular signatures from omics data. 
submitted
. | Saeys Y., Inza I. and Larranaga P. (2007). A review of feature selection techniques in bioinformatics. 
Bioinformatics
 23, 2507-2517. --------------------------------------------------- ----------------- Workflow position ----------------- .. image:: biosigner_workflowPositionImage.png :width: 600 ----------- Input files ----------- +---------------------------+------------+ | File | Format | +===========================+============+ | 1) Data matrix | tabular | +---------------------------+------------+ | 2) Sample metadata | tabular | +---------------------------+------------+ | 3) Variable metadata | tabular | +---------------------------+------------+ ---------- Parameters ---------- Data matrix file | variable x sample 
dataMatrix
 tabular separated file of the numeric intensities, with . as decimal, and NA for missing values; use the 
Check Format
 tool in the 
LC-MS/Quality Control
 section to check the formats of your 
dataMatrix
, 
sampleMetadata
 and 
variableMetadata
 files | Sample metadata file | sample x metadata 
sampleMetadata
 tabular separated file of the numeric and/or character sample metadata, with . as decimal and NA for missing values; use the 
Check Format
 tool in the 
LC-MS/Quality Control
 section to check the formats of your 
dataMatrix
, 
sampleMetadata
 and 
variableMetadata
 files | Variable metadata file | variable x metadata 
variableMetadata
 tabular separated file of the numeric and/or character variable metadata, with . as decimal and NA for missing values; use the 
Check Format
 tool in the 
LC-MS/Quality Control
 section to check the formats of your 
dataMatrix
, 
sampleMetadata
 and 
variableMetadata
 files | Classes of samples | Column of the sample metadata table to be used as the qualitative 
binary
 response to be modelled; the column should contain only two types of strings (e.g., 'case' and 'control') | Advanced: Classification method(s) (default = all) | Either one or all of the following classifiers: Partial Least Squares Discriminant Analysis (PLS-DA), or Random Forest, or Support Vector Machine (SVM) | Advanced: Number of bootstraps (default = 50) | This parameter controls the number of times the model performance is compared to the prediction on a test subset where the intensities of the candidate feature have been randomly permuted. | Advanced: Selection tier(s) (default = S) | Tier 
S
 corresponds to the final signature, i.e., features which have been found significant in all the backward selection steps; features with tier 
A
 have been found significant in all but the last selection, and so on for tier 
B
 to 
E
. Default selection tier is 
S
, meaning that the final signature only is returned; to view a larger number of candidate features, the 
S+A
 tiers can be selected. | Advanced: p-value threshold (default = 0.05) | This threshold controls the selection of the features at each selection round (tier): to be selected, the proportion of times the prediction on the test set with the randomized intensities of the feature is more accurate than on the original test set must be inferior to this threshold. For example, if the number of bootstraps is 50, no more than 2 out of the 50 predictions on the randomized test set must not be more accurate than on the original test set (since 1/50 = 0.02). Advanced: Seed (default = 0) | As the algorithm relies on resampling (bootstrap), re-running the module may result in slightly different signatures. To ensure that returned results are exactly the same, the 
seed
 parameter (integer) can be used; the default, 0, means that no seed is used. | ------------ Output files ------------ variableMetadata_out.tabular | When a least one feature has been selected, a 
tier
 column is added indicating for each feature the classifier(s) it was selected from. | figure-tier.pdf | Graphic summarizing which features were selected, with their corresponding tier (i.e., round(s) of selection) for each classifier. | figure-boxplot.pdf | Individual boxplots of the features which were selected in at least one of the signatures. Features selected for a single classifier are colored (
red
 for PLS-DA, 
green
 for Random Forest, and 
blue
 for SVM) | information.txt | Text file with all messages and warnings generated during the computation. | --------------------------------------------------- --------------- Working example --------------- See the 
W4M00001a_sacurine-subset-statistics
 and 
W4M00003_diaplasma
 shared histories in the 
Shared Data/Published Histories
 menu (https://galaxy.workflow4metabolomics.org/history/list_published) Figure output ============= .. image:: biosigner_workingExampleImage.png :width: 600 --------------------------------------------------- ---- NEWS ---- CHANGES IN VERSION 2.2.6 ======================== INTERNAL MODIFICATIONS Minor internal modifications CHANGES IN VERSION 2.2.4 ======================== INTERNAL MODIFICATIONS Creating additional files for planemo and travis running and installation validation CHANGES IN VERSION 2.2.2 ======================== INTERNAL MODIFICATIONS Internal updates to biosigner package versions of 1.0.0 and above, and ropls versions of 1.4.0 and above (i.e. using S4 methods instead of S3) CHANGES IN VERSION 2.2.1 ======================== NEW FEATURE Creation of the tool"
toolshed.g2.bx.psu.edu/repos/iuc/seqcomplexity/seqcomplexity/0.1.2	Calculates Per-Read and Total Sequence Complexity from FastQ file. Complexity is the number of base pairs of unique or nonrepeating DNA in a given segment of DNA, or component of the genome.
toolshed.g2.bx.psu.edu/repos/devteam/rcve/rcve1/1.0.0	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Edit Datasets->Convert characters
 ----- .. class:: infomark 
What it does
 This tool computes the RCVE (Relative Contribution to Variance) for all possible variable subsets using the following formula: 
RCVE(i) = [R-sq (full: 1,2,..,i..,p-1) - R-sq(without i: 1,2,...,p-1)] / R-sq (full: 1,2,..,i..,p-1)
, which denotes the case where the 'i'th predictor is dropped. In general, 
RCVE(X+) = [R-sq (full: {X,X+}) - R-sq(reduced: {X})] / R-sq (full: {X,X+})
, where, - {X,X+} denotes the set of all predictors, - X+ is the set of predictors for which we compute RCVE (and therefore drop from the full model to obtain a reduced one), - {X} is the set of the predictors that are left in the reduced model after excluding {X+} The 4 columns in the output are described below: - Column 1 (Model): denotes the variables present in the model ({X}) - Column 2 (R-sq): denotes the R-squared value corresponding to the model in Column 1 - Column 3 (RCVE_Terms): denotes the variable/s for which RCVE is computed ({X+}). These are the variables that are absent in the reduced model in Column 1. A '-' in this column indicates that the model in Column 1 is the Full model. - Column 4 (RCVE): denotes the RCVE value corresponding to the variable/s in Column 3. A '-' in this column indicates that the model in Column 1 is the Full model."
toolshed.g2.bx.psu.edu/repos/devteam/partialr_square/partialRsq/1.0.0	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Edit Datasets->Convert characters
 ----- .. class:: infomark 
What it does
 This tool computes the Partial R squared for all possible variable subsets using the following formula: 
Partial R squared = [SSE(without i: 1,2,...,p-1) - SSE (full: 1,2,..,i..,p-1) / SSE(without i: 1,2,...,p-1)]
, which denotes the case where the 'i'th predictor is dropped. In general, 
Partial R squared = [SSE(without i: 1,2,...,p-1) - SSE (full: 1,2,..,i..,p-1) / SSE(without i: 1,2,...,p-1)]
, where, - SSE (full: 1,2,..,i..,p-1) = Sum of Squares left out by the full set of predictors SSE(X1, X2 … Xp) - SSE (full: 1,2,..,i..,p-1) = Sum of Squares left out by the set of predictors excluding; for example, if we omit the first predictor, it will be SSE(X2 … Xp). The 4 columns in the output are described below: - Column 1 (Model): denotes the variables present in the model - Column 2 (R-sq): denotes the R-squared value corresponding to the model in Column 1 - Column 3 (Partial R squared_Terms): denotes the variable/s for which Partial R squared is computed. These are the variables that are absent in the reduced model in Column 1. A '-' in this column indicates that the model in Column 1 is the Full model. - Column 4 (Partial R squared): denotes the Partial R squared value corresponding to the variable/s in Column 3. A '-' in this column indicates that the model in Column 1 is the Full model. 
R Development Core Team (2010). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0, URL http://www.R-project.org."
Count1	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
Syntax
 This tool counts occurrences of unique values in selected column(s). - If multiple columns are selected, counting is performed on each unique group of all values in the selected columns. - The first column of the resulting dataset will be the count of unique values in the selected column(s) and will be followed by each value. ----- 
Example
 - Input file:: chr1 10 100 gene1 chr1 105 200 gene2 chr1 205 300 gene3 chr2 10 100 gene4 chr2 1000 1900 gene5 chr3 15 1656 gene6 chr4 10 1765 gene7 chr4 10 1765 gene8 - Counting unique values in column c1 will result in:: 3 chr1 2 chr2 1 chr3 2 chr4 - Counting unique values in the grouping of columns c2 and c3 will result in:: 2 10 100 2 10 1765 1 1000 1900 1 105 200 1 15 1656 1 205 300"
toolshed.g2.bx.psu.edu/repos/devteam/count_gff_features/count_gff_features/0.2	Counts the number of features in a GFF dataset. GFF features are often spread across multiple lines; this tool counts the number of features in dataset rather than the number of lines.
toolshed.g2.bx.psu.edu/repos/devteam/plot_from_lda/plot_for_lda_output1/1.0.1	".. class:: infomark 
What it does
 This tool generates a Receiver Operating Characteristic (ROC) plot that shows LDA classification success rates for different values of the tuning parameter tau as Figure 3 in Carrel et al., 2006 (PMID: 17009873). 
Carrel L, Park C, Tyekucheva S, Dunn J, Chiaromonte F, et al. (2006) Genomic Environment Predicts Expression Patterns on the Human Inactive X Chromosome. PLoS Genet 2(9): e151. doi:10.1371/journal.pgen.0020151
 ----- .. class:: warningmark 
Note
 - Output from ""Perform LDA"" tool is used as input file for this tool."
toolshed.g2.bx.psu.edu/repos/devteam/generate_pc_lda_matrix/generate_matrix_for_pca_and_lda1/1.0.0	".. class:: infomark 
What it does
 This tool consists of a module to generate a matrix to be used for running the Linear Discriminant Analysis as described in Carrel et al., 2006 (PMID: 17009873) 
Carrel L, Park C, Tyekucheva S, Dunn J, Chiaromonte F, et al. (2006) Genomic Environment Predicts Expression Patterns on the Human Inactive X Chromosome. PLoS Genet 2(9): e151. doi:10.1371/journal.pgen.0020151
 ----- 
Example
 - Input file (Source file First) .. image:: tools/lda/first_matrix_generator_example_file.png - Input file (Source file Second) .. image:: tools/lda/second_matrix_generator_example_file.png"
toolshed.g2.bx.psu.edu/repos/iuc/graphembed/graphembed/2.4.0	"============ GraphEmbed ============ 
Compute a 2D embedding of a data matrix given supervised class information.
 Input: A discrete label for each instance is expected. A graph is built where nodes are instances and there exist two types of edges: * 'knn' edges An edge to the k-th nearest instance that has the same label. * 'k_shift' edges An edge to the k-th nearest instance that is denser and has a different label Density is defined as the sum of the pairwise cosine similarity between an instance and all the other instances. The desired edge length is the euclidean distance between the instances. If the endpoints of an edge have the same label then the desired distance is divided by 1 + class_confidence. A k-shift edge is deleted if at least one of the endpoints is an outlier. Outlier nodes are defined as those instances that have no mutual k neighbors. Finally the embedding is computed as the 2D coordinates of the corresponding graph embedding using the force layout algorithm from Tomihisa Kamada, and Satoru Kawai. ""An algorithm for drawing general undirected graphs."", Information processing letters 31, no. 1 (1989): 7-15."
toolshed.g2.bx.psu.edu/repos/ethevenot/heatmap/Heatmap/2.2.2	".. class:: infomark | 
Tool update: See the 'NEWS' section at the bottom of the page
 --------------------------------------------------- .. class:: infomark 
Author
 Etienne Thevenot (W4M Core Development Team, MetaboHUB Paris, CEA) --------------------------------------------------- .. class:: infomark 
References
 | Etienne A. Thevenot, Aurelie Roux, Ying Xu, Eric Ezan, and Christophe Junot (2015). Analysis of the human adult urinary metabolome variations with age, body mass index and gender by implementing a comprehensive workflow for univariate and OPLS statistical analyses. 
Journal of Proteome Research
, 
14
:3322-3335 (http://dx.doi.org/10.1021/acs.jproteome.5b00354). | R Core Team (2013). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria (http://www.r-project.org) | --------------------------------------------------- .. class:: infomark 
Tool updates
 See the 
NEWS
 section at the bottom of this page --------------------------------------------------- ======================== Heatmap ======================== ----------- Description ----------- | Performs hierarchical clustering on both the samples (rows) and variables (columns) of the dataMatrix | Displays the dataMatrix with sorted rows and samples and the dendrograms (heatmap) | In the output dataMatrix, sampleMetadata and variableMetadata files sample and variables are sorted according to the dendrograms | Optionally, indicates the groups of samples and/or variables obtained by cutting the dendrograms into a specific number of partitions | | Note: 1) Computations rely on the 'hclust' function. The dissimilarity is 1 - cor (where cor is the Spearman correlation) and the 'ward.D' aggregating method is used. | 2) A ""blue-orange-red"" palette is generated with the function 'colorRampPalette'; 
By default, variables are standardized (mean-centered and unit-scaled) to enhance contrast on the figure
; standardization can be turned off by using the full list of parameters; in any case, standardizing is performed after the computation of clusters, for display only | 3) When a specific number of sample and/or variable groups (i.e. > 1) are selected, the group numbers are indicated on the plot and in an additional 'heat_clust"" column in the sampleMetadata and/or variableMetadata | 4) Example of computation times: for 126 variables: a few seconds; for 4324 variables: 30 min | ----------------- Workflow position ----------------- | In the workflow example below, the structure of the dataset (dataMatrix) is visualized by using first the ""Quality Metrics"" (for checking potential signal drift, sample outliers, etc.), then the ""Heatmap"" (for correlations between samples or variables), and finally the ""Multivariate"" (for PCA or PLS) modules. | .. image:: heatmap_workflowPositionImage.png :width: 600 ----------- Input files ----------- +--------------------------+-------------+ | File type | Format | +==========================+=============+ | 1 : Data matrix | tabular | +--------------------------+-------------+ | 2 : Sample metadata | tabular | +--------------------------+-------------+ | 3 : Variable metadata | tabular | +--------------------------+-------------+ | | Required formats for the dataMatrix, sampleMetadata and variableMetadata files are described in the HowTo entitled 'Format Data For Postprocessing' available on the main page of Workflow4Metabolomics.org; formats of the three files can be further checked with the 'Check Data' module (in the 'Quality Control' section) | ---------- Parameters ---------- Number of sample clusters | By default (cluster = 1), only dendrograms are displayed; when a specific number of sample clusters is selected, the sample dendrogram is cut at the corresponding level: the sample groups are displayed on the dendrogram and a ""heat_clust"" column is added in the sampleMetadata file with the group of each sample | Number of variable clusters | Same as above for variables | Standardization (Full list) | By default, variables are standardized for display to enhance contrast of the heatmap (note that standardization is performed after the clustering for display only and does not modify cluster computation nor intensities in the output files) | Size of labels (Full list) | The size of sample and variable names on the heatmap is 0.8 (note that names with more than 14 characters are truncated); this number may be lowered (or uppered) in case of many (few) names to display ------------ Output files ------------ dataMatrix_out.tabular | dataMatrix file with rows and columns sorted according to the dendrogram | sampleMetadata_out.tabular | sampleMetadata file with rows sorted according to the sample dendrogram; in case a number of sample groups is specified, and additional ""heat_clust"" column is added with the cluster group of each sample | variableMetadata_out.tabular | variableMetadata file with rows sorted according to the variable dendrogram; in case a number of variable groups is specified, and additional ""heat_clust"" column is added with the cluster group of each variable | figure.pdf | Heatmap | information.txt | File with all messages and warnings generated during the computation | --------------------------------------------------- --------------- Working example --------------- .. class:: infomark See the 
W4M00001a_sacurine-subset-statistics
 shared history in the 
Shared Data/Published Histories
 menu (https://galaxy.workflow4metabolomics.org/history/list_published) --------------------------------------------------- ---- NEWS ---- CHANGES IN VERSION 2.2.2 ======================== INTERNAL MODIFICATIONS Minor internal modifications CHANGES IN VERSION 2.2.0 ======================== NEW FEATURES Default method for the correlation coefficient is now 'pearson', instead of 'spearman' previously (the latter can still be selected in the advanced parameters) The 1-abs(correlation) dissimilarity is now available (in addition to the default '1-correlation') in case the sign of correlations between samples and between variables does not matter, as well as the euclidean, maximum, manhattan, canberra, binary, and minkowski dissimilarities A new red-green color scale is available CHANGES IN VERSION 2.1.2 ======================== INTERNAL MODIFICATIONS Creating additional files for planemo and travis running and installation validation CHANGES IN VERSION 2.1.1 ======================== INTERNAL MODIFICATIONS Internal replacement of the as.hclust function which happened to produce error messages"
toolshed.g2.bx.psu.edu/repos/devteam/kernel_canonical_correlation_analysis/kcca1/1.0.0	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Edit Datasets->Convert characters
 ----- .. class:: infomark 
What it does
 This tool uses functions from 'kernlab' library from R statistical package to perform Kernel Canonical Correlation Analysis (kCCA) on the input data. 
Alexandros Karatzoglou, Alex Smola, Kurt Hornik, Achim Zeileis (2004). kernlab - An S4 Package for Kernel Methods in R. Journal of Statistical Software 11(9), 1-20. URL http://www.jstatsoft.org/v11/i09/
 ----- .. class:: warningmark 
Note
 This tool currently treats all variables as continuous numeric variables. Running the tool on categorical variables might result in incorrect results. Rows containing non-numeric (or missing) data in any of the chosen columns will be skipped from the analysis."
toolshed.g2.bx.psu.edu/repos/devteam/kernel_principal_component_analysis/kpca1/1.0.0	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Edit Datasets->Convert characters
 ----- .. class:: infomark 
What it does
 This tool uses functions from 'kernlab' library from R statistical package to perform Kernel Principal Component Analysis (kPCA) on the input data. It outputs two files, one containing the summary statistics of the performed kPCA, and the other containing a scatterplot matrix of rotated values reported by kPCA. 
Alexandros Karatzoglou, Alex Smola, Kurt Hornik, Achim Zeileis (2004). kernlab - An S4 Package for Kernel Methods in R. Journal of Statistical Software 11(9), 1-20. URL http://www.jstatsoft.org/v11/i09/
 ----- .. class:: warningmark 
Note
 This tool currently treats all variables as continuous numeric variables. Running the tool on categorical variables might result in incorrect results. Rows containing non-numeric (or missing) data in any of the chosen columns will be skipped from the analysis."
toolshed.g2.bx.psu.edu/repos/devteam/mine/maximal_information_based_nonparametric_exploration/0.0.1	"What it does
 Applies the Maximal Information-based Nonparametric Exploration strategy to an input dataset. See http://www.exploredata.net/ for more information. ------ 
Citation
 For the underlying tool, please cite 
David N. Reshef, Yakir A. Reshef, Hilary K. Finucane5, Sharon R. Grossman, Gilean McVean, Peter J. Turnbaugh, Eric S. Lander, Michael Mitzenmacher, Pardis C. Sabeti Detecting Novel Associations in Large Data Sets. Science. 2011 Dec. &lt;http://www.sciencemag.org/content/334/6062/1518&gt;
_ If you use this tool in Galaxy, please cite Blankenberg D, et al. 
In preparation."
toolshed.g2.bx.psu.edu/repos/ethevenot/multivariate/Multivariate/2.3.10	".. class:: infomark 
Author
 Etienne Thevenot (CEA, LIST, MetaboHUB Paris, etienne.thevenot@cea.fr) --------------------------------------------------- .. class:: infomark 
Please cite
 Etienne A. Thevenot, Aurelie Roux, Ying Xu, Eric Ezan, and Christophe Junot (2015). Analysis of the human adult urinary metabolome variations with age, body mass index and gender by implementing a comprehensive workflow for univariate and OPLS statistical analyses. 
Journal of Proteome Research
, 
14
:3322-3335 (http://dx.doi.org/10.1021/acs.jproteome.5b00354). --------------------------------------------------- .. class:: infomark 
R package
 The 
ropls
 package is available from the bioconductor repository (http://bioconductor.org/packages/ropls). --------------------------------------------------- .. class:: infomark 
Tool updates
 See the 
NEWS
 section at the bottom of this page --------------------------------------------------- ============================================== Multivariate analysis with PCA and (O)PLS(-DA) ============================================== ----------- Description ----------- 
Latent variable modeling
 with Principal Component Analysis (
PCA
) and Partial Least Squares (
PLS
) are powerful methods for 
visualization
, 
regression
, 
classification
, and feature selection of 
omics data
 where the number of variables exceeds the number of samples and with multicollinearity among variables (Wold et al, 2001; Thenenhaus, 1998; Wehrens, 2011; Eriksson et al, 2006; Trygg et al, 2007). Orthogonal Partial Least Squares (
OPLS
) enables to separately model the variation correlated (predictive) to the factor of interest and the uncorrelated (orthogonal) variation (Trygg and Wold, 2002). While performing similarly to PLS, OPLS facilitates interpretation. Successful applications of these chemometrics techniques include spectroscopic data such as Raman spectroscopy, nuclear magnetic resonance (NMR), mass spectrometry (MS) in metabolomics and proteomics, but also transcriptomics data. In addition to 
scores
, 
loadings
 and 
weights
 plots, the module provides metrics and graphics to determine the optimal number of components (e.g. with the 
R2
 and 
Q2
 coefficients; Wold et al, 2001; Thenenhaus, 1998; Eriksson et al, 2006), check the 
validity of the model
 by permutation testing (Szymanska et al, 2012), detect 
outliers
 (Wold et al, 2001; Thenenhaus, 1998; Hubert et al, 2005), and provide several metrics to assess the importance of the variables in the model (e.g. 
Variable Importance in Projection
 or regression coefficients; Wold et al, 2001; Mehmood et al, 2012; Galindo-Prieto et al, 2014). The module is an implementation of the 
ropls
 R package available from Bioconductor (Thevenot et al, 2015). -------- Comments -------- 1) Overfitting | Overfitting (i.e., building a model with good performances on the training set but poor performances on a new test set) is a major caveat of machine learning techniques applied to data sets with more variables than samples. A simple simulation with a random dataMatrix and a random response shows that perfect PLS-DA classification can be achieved as soon as the number of variables exceeds the number of samples (Wehrens, 2011). It is therefore essential to check that the Q2 value of the model is significant by random permutation of the labels: the number of permutations (advanced computational parameter) is set to 20 by default but should be increased for confirmation of the results. 2) VIP from OPLS models | The classical VIP metric is not useful for OPLS modeling of a single response since (Galindo-Prieto et al, 2014; Thevenot et al, 2015). In fact, when features are standardized, we can demonstrate a mathematical relationship between VIP and 
p
-values from a Pearson correlation test (Thevenot et al, 2015): classical VIP are therefore univariate for OPLS(-DA) models (and identical whatever the number of orthogonal components of the model). | Galindo-Prieto et al. (2014) have therefore recently suggested new VIP metrics for OPLS, VIP
pred
 and VIP
ortho
, to separately measure the influence of the features in the modeling of the dispersion correlated to, and orthogonal to the response, respectively. | For OPLS(-DA) models, the output variableMetadata contains the 2 metrics: VIP_pred is a measure of the variable importance in prediction and VIP_ortho is a measure of the variable importance in orthogonal modeling. VIP_pred and VIP_ortho are scaled as the classical VIP (i.e., the mean of their squared values equals 1). 3) (Orthogonal) Partial Least Squares Discriminant Analysis: (O)PLS-DA | The approach for discriminant analysis implemented in the module relies on internal conversion of the response into a dummy vector (resp. a matrix when the number of classes is > 2), mean-centering and unit-variance scaling of the vector (resp. the matrix), and PLS (resp. PLS2) regression modeling. | When the sizes of the 2 classes are unbalanced, Brereton and Lloyd (2014) have demonstrated that a bias is introduced in the computation of the decision rule, which penalizes the class with the highest size. In the multiclass case, the proportions of 0 and 1 in the columns is usually unbalanced (even in the case of balanced size of the classes) resulting in a bias (Brereton and Llyod, 2014). | With the current implementation of the module, we thus recommend to stick to binary discrimination and use balanced classes for optimal use. ---------- References ---------- | Brereton R.G. and Lloyd G.R. (2014). Partial least squares discriminant analysis: taking the magic away. 
Journal of Chemometrics
, 28:213-225. http://dx.doi.org/10.1002/cem.2609 | Eriksson I., Johansson E., Kettaneh-Wold N. and Wold S. (2001). Multi- and megavariate data analysis. Principles and applications. 
Umetrics Academy
. | Galindo-Prieto B., Eriksson L. and Trygg J. (2014). Variable influence on projection (VIP) for orthogonal projections to latent structures (OPLS). 
Journal of Chemometrics
, 28:623-632. http://dx.doi.org/10.1002/cem.2627 | Hubert M., Rousseeuw P. and Vanden Branden K. (2005). ROBPCA: a new approach to robust principal component analysis. 
Technometrics
, 47:64-79. http://dx.doi.org/10.1198/004017004000000563 | Mehmood T., Liland K.H., Snipen L. and Saebo S. (2012). A review of variable selection methods in Partial Least Squares Regression. 
Chemometrics and Intelligent Laboratory Systems
, 118:62-69. http://dx.doi.org/10.1016/j.chemolab.2012.07.010 | Szymanska E., Saccenti E., Smilde A. and Westerhuis J. (2012). Double-check: validation of diagnostic statistics for PLS-DA models in metabolomics studies. 
Metabolomics
, 8:3-16. http://dx.doi.org/10.1007/s11306-011-0330-3 | Tenenhaus M. (1998). La regression PLS : theorie et pratique. 
Technip
. | Thevenot E.A., Roux A., Xu Y., Ezan E. and Junot C. (2015). Analysis of the human adult urinary metabolome variations with age, body mass index and gender by implementing a comprehensive workflow for univariate and OPLS statistical analyses. 
Journal of Proteome Research
, 14:3322-3335. http://dx.doi.org/10.1021/acs.jproteome.5b00354 | Trygg J. and Wold S. (2002). Orthogonal projection to latent structures (O-PLS). 
Journal of Chemometrics
, 16:119-128. http://dx.doi.org/10.1002/cem.695 | Trygg J., Holmes E. and Lundstedt T. (2007). Chemometrics in Metabonomics. 
Journal of Proteome Research
, 6:469-479. http://dx.doi.org/10.1021/pr060594q | Wehrens W. (2011). Chemometrics with R. 
Springer
. | Wold S., Sjostrom M. and Eriksson L. (2001). PLS-regression: a basic tool of chemometrics. 
Chemometrics and Intelligent Laboratory Systems
, 58:109-130. http://dx.doi.org/10.1016/S0169-7439(01)00155-1 ----------------- Workflow position ----------------- .. image:: multivariate_workflowPositionImage.png :width: 600 ----------- Input files ----------- +---------------------------+------------+ | File | Format | +===========================+============+ | 1) Data matrix | tabular | +---------------------------+------------+ | 2) Sample metadata | tabular | +---------------------------+------------+ | 3) Variable metadata | tabular | +---------------------------+------------+ ---------- Parameters ---------- Data matrix file | variable x sample 
dataMatrix
 tabular separated file of the numeric data matrix, with . as decimal, and NA for missing values; the table must not contain metadata apart from row and column names; the row and column names must be identical to the rownames of the sample and variable metadata, respectively (see below) | Sample metadata file | sample x metadata 
sampleMetadata
 tabular separated file of the numeric and/or character sample metadata, with . as decimal and NA for missing values | Variable metadata file | variable x metadata 
variableMetadata
 tabular separated file of the numeric and/or character variable metadata, with . as decimal and NA for missing values | Y Response (mandatory for PLS and OPLS; keep the default, none, for PCA) | Column of the sample metadata table to be used as (qualitative or quantitative) response for (O)PLS(-DA) | Number of (predictive) components (default = NA) | For OPLS(-DA), this number is automatically converted to 1; otherwise if set to 
NA
, the optimal number of components is automatically determined by cross-validation: components are extracted until (i) PCA case: the variance is less than the mean variance of all components (note that this rule requires all components to be computed and can be quite time-consuming for large datasets) or (ii) PLS case: either R2Y of the component is less than 0.01 or Q2Y is less than 0 (when the dataset contains more than 100 samples) or 0.05 otherwise | Number of orthogonal components (mandatory for OPLS(-DA); default = 0 otherwise) | When set to 
0
 [default], PLS will be performed; otherwise OPLS will be peformed; when set to 
NA
, OPLS is performed and the number of orthogonal components is automatically computed by using cross-validation | Samples for prediction (for (O)PLS(-DA) only; default is no) | In case predictions should be computed on test samples, provide in your 
sampleMetadata
 a column named 
test.
 (use exactly this column name, with the dot at the end) and containing 
yes
 and 
no
 values to indicate which samples should be tested; for those samples, the values of the response will not be used (you can leave 
NA
 in the response column of the 
sampleMetadata
) | Advanced graphical parameters | Graphic type (default = summary) | 
summary
 4-plot graphics showing 
overview
 (or 
permutation
 when the number of permutations is superior to 0; see below), 
outlier
, 
x-loading
 and 
x-score
 | 
correlation
 Variable correlations with the components | 
outlier
 Observation diagnostics (score and orthogonal distances) | 
overview
 Model overview showing R2Ycum and Q2cum (or 'Variance explained' for PCA) | 
permutation
 Scatterplot of R2Y and Q2Y actual and simulated models after random permutation of response values | 
predict-train
 Predicted vs Actual Y for the reference set (only if Y has a single column) | 
x-loading
 X-Loadings | 
x-score
 X-Scores | 
x-varcor
 Spread of raw variables corresp. to quantile variances and, if the number of variables is less than 100, correlations between the X-variables | 
xy-score
 XY-Scores, | 
xy-weight
 XY-Weights | .pdf image files can be converted to high-resolution .tif images (e.g. for publication) by using in the open-source Gimp software: open the .pdf with resolution = 300, and export as a .tif image without compression | Ellipses (default = NA) | If 'NA' ellipses are drawn automatically for (O)PLS-DA, or for PCA, when a column of characters is selected in the 'sample colors' argument below. If you do not want ellipses, set to none. | Sample colors (default = none) | Name of the column of the sample table with the classes to be used for coloring the samples on plots (e.g. for PCA or if you wish to highlight a factor distinct from the response above); by default (none) sample names are converted into a color palette"" | Sample labels (default = none) | Name of the column of the sample table with the classes to be used for labeling the samples on plots; by default (none), sample names will be used | Component to be displayed as abscissa (default = 1) | In case of OPLS(-DA), the first component (i.e. the predictive component) must be set to 1"" | Component to be displayed as ordinate (default = 2) | Note: In case of OPLS(-DA), the orthogonal component of the value below - 1 will be displayed (e.g. to see the first orthogonal component, select the value 
2
 (default) | Number of variables most contributing to loadings to be highlighted (default = 3) | Such variables will be colored in red on the loading plot; In addition, the loading values and the correlation with the components will be printed in the text summary | Advanced computational parameters | Scaling (default = standard) | Either mean-centering alone (
center
), or followed by pareto scaling (
pareto
), or unit-variance scaling (
standard
) | Permutation testing for (O)PLS(-DA) models: Number of permutations (default = 20) | Number of random permutations of response labels to estimate R2Y and Q2Y significance; Default is 10 for single response models and 0 otherwise | Log10 transformation (default = no) | Should the data matrix values be log10 transformed? Note: zeros are set to 1 prior to transformation | Train/test partition (default = none) | When set to 
odd
, samples with odd indices are used to train the model, which is subsequently tested on the samples with even indices; a RMSEP (root mean square error estimation of prediction) is computed, in addition to the RMSEE (error of estimation). Note that in case of a qualitative response, the proportion of samples in each class in the full dataset is preserved within the reference and train subsets | Algorithm (default = svd for PCA and nipals for PLS and OPLS) | When using 
svd
 (singular value decomposition) for PCA on an 
dataMatrix
 containing missing values (NA), the latters are set to half the minimum of non-missing values and a warning is generated; an alternative is to use the 
nipals
 algorithm (non-linear iterative partial least squares, based on a power method to find eigenvalues, and able to handle a small amount of missing values); For PLS and OPLS, only the 
nipals
 algorithm is available | Number of cross-validation segments (default = 7) | ------------ Output files ------------ sampleMetadata_out.tabular | 
sampleMetadata
 tabular separated file identical to the file given as argument, except that two columns with the x-scores of the displayed components have been added | variableMetadata_out.tabular | 
variableMetadata
 tabular separated file identical to the file given as argument, except that i) 3 columns with the x-loadings of the displayed components, and the regression coefficients, have been added, ii) in the case of PLS, a column with the VIP values (variable importance in projection of the model with all components) has been added, iii) in the case of OPLS, 2 columns with the VIP_pred and VIP_ortho have been added. | figure.pdf | Graphic | information.txt | Text file with all messages and warnings generated during the computation | --------------------------------------------------- ---------------- Working examples ---------------- | .. class:: infomark See the 
W4M00001a_sacurine-subset-statistics
, 
W4M00001b_sacurine-complete
, 
W4M00002_mtbls2
 or 
W4M00003_diaplasma
 shared histories in the 
Shared Data/Published Histories
 menu (https://galaxy.workflow4metabolomics.org/history/list_published) Figure output ============= .. image:: multivariate_workingExampleImage.png :width: 600 --------------------------------------------------- ---- NEWS ---- CHANGES IN VERSION 2.3.8 ======================== MINOR CORRECTION (O)PLS(-DA) coefficients display in case of multiple quantitative (or multiclass) response: now the column names of the coefficients for each response are correctly labelled in the variableMetadata file CHANGES IN VERSION 2.3.6 ======================== INTERNAL MODIFICATIONS Minor internal modifications CHANGES IN VERSION 2.3.4 ======================== INTERNAL MODIFICATIONS Minor update in .shed.yml file CHANGES IN VERSION 2.3.2 ======================== NEW FEATURES Error messages are generated in OPLS(-DA) models in case of non-significance of either the predictive or the first orthogonal component INTERNAL MODIFICATIONS Modifications of the 
wrapper
 file to handle the recent 
ropls
 package versions (i.e. 1.3.15 and above) which use S4 classes CHANGES IN VERSION 2.3.0 ======================== NEW FEATURES 1) 
Predictions
 now available (see the 'Samples to be tested' argument) 2) OPLS(-DA): 
Predictive and Orthogonal VIP
 are now computed (see the 'comments' section) 3) 
Multiclass PLS-DA
 implemented (see the 'comments' section) MINOR MODIFICATIONS 1) Changes in color palette: black/grey colors for diagnostics and other colors for scores 2) Default number of permutations set to 20 (instead of 10) 3) Predictive components denoted in the tables by 'p' (instead of 'h' previously) CHANGES IN VERSION 2.2.4 ======================== 1) Correction in the Galaxy wrapper (in the previous version, the number of predictive components was sometimes set to the maximum by mistake) 2) The regression coefficients are now provided as a new column of the variableMetadata output CHANGES IN VERSION 2.2.3 ======================== The default number of permutations is set to 10 (instead of 100) as a compromise to enable both a quick computation and a first hint at model significance CHANGES IN VERSION 2.2.2 ======================== 1) A default of 100 permutations has been set in order to check for overfitting; in addition, 'permutation', 'overview', and 'outlier' plots are now displayed by default 2) Classification is currently implemented for two-class responses only 3) 
dataMatrix
 is not modified by the tool, so it does not appear as an output files 4) Double cross-validation (advanced computational parameters): 'odd' now refers to train (instead to test) indices"
toolshed.g2.bx.psu.edu/repos/devteam/best_regression_subsets/BestSubsetsRegression1/1.0.0	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Edit Datasets->Convert characters
 ----- .. class:: infomark 
What it does
 This tool uses the 'regsubsets' function from R statistical package for regression subset selection. It outputs two files, one containing a table with the best subsets and the corresponding summary statistics, and the other containing the graphical representation of the results. ----- .. class:: warningmark 
Note
 - This tool currently treats all predictor and response variables as continuous variables. - Rows containing non-numeric (or missing) data in any of the chosen columns will be skipped from the analysis. - The 6 columns in the output are described below: - Column 1 (Vars): denotes the number of variables in the model - Column 2 ([c2 c3 c4...]): represents a list of the user-selected predictor variables (full model). An asterix denotes the presence of the corresponding predictor variable in the selected model. - Column 3 (R-sq): the fraction of variance explained by the model - Column 4 (Adj. R-sq): the above R-squared statistic adjusted, penalizing for higher number of predictors (p) - Column 5 (Cp): Mallow's Cp statistics - Column 6 (bic): Bayesian Information Criterion."
toolshed.g2.bx.psu.edu/repos/devteam/lda_analysis/lda_analy1/1.0.1	".. class:: infomark 
TIP:
 If you want to perform Principal Component Analysis (PCA) on the give numeric input data (which corresponds to the ""Source file First in ""Generate A Matrix"" tool), please use 
Multivariate Analysis/Principal Component Analysis
 ----- .. class:: infomark 
What it does
 This tool consists of the module to perform the Linear Discriminant Analysis as described in Carrel et al., 2006 (PMID: 17009873) 
Carrel L, Park C, Tyekucheva S, Dunn J, Chiaromonte F, et al. (2006) Genomic Environment Predicts Expression Patterns on the Human Inactive X Chromosome. PLoS Genet 2(9): e151. doi:10.1371/journal.pgen.0020151
 ----- .. class:: warningmark 
Note
 - Output from ""Generate A Matrix"" tool is used as input file for this tool - Output of this tool contains LDA classification success rates for different values of the turning parameter tau (from 0 to 1 with 0.005 interval). This output file will be used to establish the ROC plot, and you can obtain more detail information from this plot."
toolshed.g2.bx.psu.edu/repos/devteam/linear_regression/LinearRegression1/1.0.1	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Edit Datasets->Convert characters
 ----- .. class:: infomark 
What it does
 This tool uses the 'lm' function from R statistical package to perform linear regression on the input data. It outputs two files, one containing the summary statistics of the performed regression, and the other containing diagnostic plots to check whether model assumptions are satisfied. 
R Development Core Team (2009). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0, URL http://www.R-project.org.
 ----- .. class:: warningmark 
Note
 - This tool currently treats all predictor and response variables as continuous numeric variables. Running the tool on categorical variables might result in incorrect results. - Rows containing non-numeric (or missing) data in any of the chosen columns will be skipped from the analysis. - The summary statistics in the output are described below: - sigma: the square root of the estimated variance of the random error (standard error of the residiuals) - R-squared: the fraction of variance explained by the model - Adjusted R-squared: the above R-squared statistic adjusted, penalizing for the number of the predictors (p) - p-value: p-value for the t-test of the null hypothesis that the corresponding slope is equal to zero against the two-sided alternative."
toolshed.g2.bx.psu.edu/repos/imgteam/landmark_registration/ip_landmark_registration/0.1.0-2	"What it does
 1) Estimation of the affine transformation matrix between two sets of 2d points; 2) Piecewise affine transformation of points based on landmark pairs. About the format of landmark/point coordinates in the input TSV table: Columns with header ""x"" and ""y"" are for x- and y-coordinate, respectively."
toolshed.g2.bx.psu.edu/repos/devteam/principal_component_analysis/pca1/1.0.2	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Edit Datasets->Convert characters
 ----- .. class:: infomark 
What it does
 This tool performs Principal Component Analysis on the given numeric input data using functions from R statistical package - 'princomp' function (for Eigenvector based solution) and 'prcomp' function (for Singular value decomposition based solution). It outputs two files, one containing the summary statistics of PCA, and the other containing biplots of the observations and principal components. 
R Development Core Team (2009). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0, URL http://www.R-project.org.
 ----- .. class:: warningmark 
Note
 - This tool currently treats all variables as continuous numeric variables. Running the tool on categorical variables might result in incorrect results. Rows containing non-numeric (or missing) data in any of the chosen columns will be skipped from the analysis. - The summary statistics in the output are described below: - Std. deviation: Standard deviations of the principal components - Loadings: a list of eigen-vectors/variable loadings - Scores: Scores of the input data on the principal components"
toolshed.g2.bx.psu.edu/repos/iuc/rgcca/rgcca/3.0.2+galaxy1	"================================== ABOUT ================================== 
Author:
 Etienne CAMENEN 
Contact:
 etienne.camenen@gmail.com 
R package:
 | The RGCCA package is available from the CRAN repository (v2.1.2; https://cran.r-project.org/web/packages/RGCCA). | This tool is based on a version available on github (v3.0; https://github.com/rgcca-factory/RGCCA). --------------------------------------------------- ================================== R/SGCCA ================================== A user-friendly multi-blocks analysis (Regularized Generalized Canonical Correlation Analysis, RGCCA) as described in [1] and [2] with all default settings predefined. The software produces figures to explore the analysis' results: individuals and variables projected on two components of the multi-block analysis, list of top variables and explained variance in the model. 
Working example
 | From Russett data (RGCCA package): https://github.com/BrainAndSpineInstitute/rgcca_ui/tree/master/inst/extdata | Use 
agriculture.tsv
 as a block. Add 
industry.tsv
 and 
politic.tsv
 for multiblock analysis. 
connection.tsv
 could be used as a design matrix and 
political_system.tsv
 as a response variable respectively in analysis and graphic settings. 
Documentation
 - RGCCA: https://cran.r-project.org/web/packages/RGCCA/vignettes/vignette_RGCCA.pdf - accepted input / https://github.com/BrainAndSpineInstitute/rgcca_ui#input-files - tutorial: https://github.com/BrainAndSpineInstitute/rgcca_galaxy#readme"
toolshed.g2.bx.psu.edu/repos/bgruening/pandas_rolling_window/pandas_rolling_window/0.1	"What it does
 Provides rolling window calculations, e.g. for smoothing values."
toolshed.g2.bx.psu.edu/repos/peterjc/seq_composition/seq_composition/0.0.5	"What it does
 Takes input files of sequences (typically FASTA or FASTQ, but also Standard Flowgram Format (SFF) is supported), counts all the letters in each sequence, and returns a summary table of their counts and percentages. 
Citation
 This tool uses Biopython, so if you use this Galaxy tool in work leading to a scientific publication please cite the following paper: Cock et al (2009). Biopython: freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics 25(11) 1422-3. http://dx.doi.org/10.1093/bioinformatics/btp163 pmid:19304878. This tool is available to install into other Galaxy Instances via the Galaxy Tool Shed at http://toolshed.g2.bx.psu.edu/view/peterjc/seq_composition"
Summary_Statistics1	".. class:: warningmark This tool expects input datasets consisting of tab-delimited columns (blank or comment lines beginning with a # character are automatically skipped). .. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert delimiters to TAB
 .. class:: infomark 
TIP:
 Computing summary statistics may throw exceptions if the data value in every line of the columns being summarized is not numerical. If a line is missing a value or contains a non-numerical value in the column being summarized, that line is skipped and the value is not included in the statistical computation. The number of invalid skipped lines is documented in the resulting history item. .. class:: infomark 
USING R FUNCTIONS:
 Most functions (like 
abs
) take only a single expression. 
log
 can take one or two parameters, like 
log(expression,base)
 Currently, these R functions are supported: 
abs, sign, sqrt, floor, ceiling, trunc, round, signif, exp, log, cos, sin, tan, acos, asin, atan, cosh, sinh, tanh, acosh, asinh, atanh, lgamma, gamma, gammaCody, digamma, trigamma, cumsum, cumprod, cummax, cummin
 ----- 
Syntax
 This tool computes basic summary statistics on a given column, or on a valid expression containing one or more columns. - Columns are referenced with 
c
 and a 
number
. For example, 
c1
 refers to the first column of a tab-delimited file. - For example: - 
log(c5)
 calculates the summary statistics for the natural log of column 5 - 
(c5 + c6 + c7) / 3
 calculates the summary statistics on the average of columns 5-7 - 
log(c5,10)
 summary statistics of the base 10 log of column 5 - 
sqrt(c5+c9)
 summary statistics of the square root of column 5 + column 9 ----- 
Examples
 - Input Dataset:: c1 c2 c3 c4 c5 c6 586 chrX 161416 170887 41108_at 16990 73 chrX 505078 532318 35073_at 1700 595 chrX 1361578 1388460 33665_s_at 1960 74 chrX 1420620 1461919 1185_at 8600 - Summary Statistics on column c6 of the above input dataset:: #sum mean stdev 0% 25% 50% 75% 100% 29250.000 7312.500 7198.636 1700.000 1895.000 5280.000 10697.500 16990.000"
toolshed.g2.bx.psu.edu/repos/devteam/t_test_two_samples/t_test_two_samples/1.0.1	".. class:: infomark 
What it does
 This program implements the non-pooled t-test for two samples where the alternative hypothesis is two-sided or one-sided. The program takes four inputs: - The first input file is a TABULAR format file representing the first sample and consisting of one column only. - The second input file is a TABULAR format file representing the first sample and consisting of one column only. - The third input is the sidedness of the t-test: either two-sided or, one-sided with m1 less than m2 or, one-sided with m1 greater than m2. - The fourth input is the equality status of the standard deviations of both populations. - The output file is a TXT file representing the result of the two-sample t-test. 
Example
 Let us have the first input file representing the first sample as follows:: 5 4 8 6 7 2 1 1 0 6 4 5 7 5 3 2 5 8 7 6 4 And the second input file representing the second sample as follows:: 2 3 5 1 2 7 5 4 3 2 7 6 0 8 4 6 9 2 4 5 6 Runnig the program and choosing ""Two-sided"" and ""Equal"" as parameters will give the following output:: Two Sample t-test data: sample1 and sample2 t = -0.3247, df = 40, p-value = 0.7471 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -1.720030 1.243839 sample estimates: mean of x mean of y 4.333333 4.571429"
toolshed.g2.bx.psu.edu/repos/ethevenot/transformation/Transformation/2.2.0	".. class:: infomark 
Author
 Etienne Thevenot (W4M Core Development Team, MetaboHUB Paris, CEA) --------------------------------------------------- .. class:: infomark 
Tool updates
 See the 
NEWS
 section at the bottom of this page --------------------------------------------------- ======================== Transformation ======================== ----------- Description ----------- | Performs transformation of the dataMatrix intensity values aimed at stabilizing variance. | For mass spectrometry data, where multiplicative noise has been reported, logarithm transformation can be useful to make the peak intensity variance independent of the intensity mean (see for example Veselkov et al, 2011). | For the logarithm transformations, log(1+X) is used (to avoid returning -Inf for 0 values); NA values remain unchanged | 
Veselkov et al (2011). Optimized Preprocessing of Ultra-Performance Liquid Chromatography/Mass Spectrometry Urinary Metabolic Profiles for Improved Information Recovery. Analytical Chemistry, 83:5864-5872. &lt;http://dx.doi.org/10.1021/ac201065j&gt;
_ ----------------- Workflow position ----------------- | In the workflow example below, the intensities are first transformed, before the signal drift and batch-effects are corrected; finally, the data quality is evaluated numerically and visually. | .. image:: transformation_workflowPositionImage.png ----------- Input files ----------- +----------------------------+---------+ | Parameter : num + label | Format | +============================+=========+ | 1 : Data matrix file | tabular | +----------------------------+---------+ | | 
Required format for the dataMatrix is described in the HowTo entitled 'Format Data For Postprocessing' available on the main page of Workflow4Metabolomics.org
 | ---------- Parameters ---------- Method | Method to be used for transforming the intensity values of the dataMatrix: | 
log2
 (resp. 
log10
): intensities values are log2 (resp. log10) transformed: log(1+X) is used to avoid generating -Inf for 0 values | in case of negative intensities in the initial dataMatrix, an error is returned | in case of missing (NA) intensities in the initial dataMatrix, these intensities remain set to NA after transformation ------------ Output files ------------ dataMatrix_out.tabular | dataMatrix data file with the transformed intensity values | information.txt | Text file giving some informations about the computation (eg, number of NA and 0 valuesin the initial dataMatrix) | --------------------------------------------------- --------------- Working example --------------- .. class:: infomark See the 
W4M00001b_sacurine-complete
 shared history in the 
Shared Data/Published Histories
 menu (https://galaxy.workflow4metabolomics.org/history/list_published) --------------------------------------------------- ---- NEWS ---- CHANGES IN VERSION 2.2.0 ======================== NEW FEATURE Square root transformation now available CHANGES IN VERSION 2.0.2 ======================== INTERNAL MODIFICATIONS Creating tests for R code"
toolshed.g2.bx.psu.edu/repos/ethevenot/univariate/Univariate/2.2.4	".. class:: infomark | 
Tool update: See the 'NEWS' section at the bottom of the page
 --------------------------------------------------- .. class:: infomark 
Authors
 | 
Marie Tremblay-Franco (marie.tremblay-franco@toulouse.inra.fr)
 and 
Etienne Thevenot (etienne.thevenot@cea.fr)
 wrote this wrapper of R univariate statistical tests. | MetaboHUB: The French National Infrastructure for Metabolomics and Fluxomics (http://www.metabohub.fr/en) --------------------------------------------------- .. class:: infomark 
Please cite
 R Core Team (2013). R: A language and Environment for Statistical Computing. http://www.r-project.org --------------------------------------------------- .. class:: infomark 
References
 | Benjamini Y. and Hochberg Y. (1995). Controlling the false discovery rate: a practical and powerful approach for multiple testing. Journal of the Royal Statistical Society. Series B (Methodological), 57:289-300. | Dalgaard P. (2002). Introductory statistics with R. Springer. | Kvam P. and Vidakovic B. (2007). Nonparametric statistics with applications to science and engineering. Wiley. | Van Belle G., Fisher L., Heagerty P. and Lumley T. (2004). Biostatistics - a methodology for the health sciences. Wiley. | Pohlert T. (2015). PMCMR: Calculate pairwise multiple comparisons of mean rank sums. R package on CRAN. --------------------------------------------------- ===================== Univariate statistics ===================== ----------- Description ----------- | The module performs two sample tests (t-test and Wilcoxon rank test), analysis of variance and Kruskal-Wallis rank test, and correlation tests (by using either the pearson or the spearman correlation) ----------------- Workflow position ----------------- .. image:: univariate_workflowPositionImage.png :width: 584 ----------- Input files ----------- +------------------------------+------------+ | File | Format | +==============================+============+ | 1) Data matrix | tabular | +------------------------------+------------+ | 2) Sample metadata | tabular | +------------------------------+------------+ | 3) Variable metadata | tabular | +------------------------------+------------+ ---------- Parameters ---------- Data matrix file | variable x sample 
dataMatrix
 tabular separated file of the numeric data matrix, with . as decimal, and NA for missing values; the table must not contain metadata apart from row and column names; the row and column names must be identical to the rownames of the sample and variable metadata, respectively (see below) | Sample metadata file | sample x metadata 
sampleMetadata
 tabular separated file of the numeric and/or character sample metadata, with . as decimal and NA for missing values | Variable metadata file | variable x metadata 
variableMetadata
 tabular separated file of the numeric and/or character variable metadata, with . as decimal and NA for missing values | Factor | Column of the sample metadata table to be used as qualitative factor (t-test, Wilcoxon test, Analysis of variance, Kruskal-Wallis test) or quantitative variable (correlation) | Test | Depending on the factor of interest (qualitative with 2 or more levels, or quantitative), and on the normality of the sample values (determining whether a parametric or nonparametric test is required), you can choose one of the 6 tests available: +---------------------------+------------------+----------------------+----------------------+ | Factor to be tested | Number of levels | Parametric test | Nonparametric test | +===========================+==================+======================+======================+ | Qualitative | 2 | t-test | Wilcoxon test | + +------------------+----------------------+----------------------+ | | > 2 | Analysis of variance | Kruskal-Wallis | +---------------------------+------------------+----------------------+----------------------+ | Quantitative | | Pearson correlation | Spearman correlation | +---------------------------+------------------+----------------------+----------------------+ Method for multiple testing correction | The 7 methods implemented in the 'p.adjust' R function are available and documented as follows: | ""The adjustment methods include the Bonferroni correction (""bonferroni"") in which the p-values are multiplied by the number of comparisons. Less conservative corrections are also included by Holm (1979) (""holm""), Hochberg (1988) (""hochberg""), Hommel (1988) (""hommel""), Benjamini and Hochberg (1995) (""BH"" or its alias ""fdr""), and Benjamini and Yekutieli (2001) (""BY""), respectively. A pass-through option (""none"") is also included. The set of methods are contained in the p.adjust.methods vector for the benefit of methods that need to have the method as an option and pass it on to p.adjust. The first four methods are designed to give strong control of the family-wise error rate. There seems no reason to use the unmodified Bonferroni correction because it is dominated by Holm's method, which is also valid under arbitrary assumptions. Hochberg's and Hommel's methods are valid when the hypothesis tests are independent or when they are non-negatively associated (Sarkar, 1998; Sarkar and Chang, 1997). Hommel's method is more powerful than Hochberg's, but the difference is usually small and the Hochberg p-values are faster to compute. The ""BH"" (aka ""fdr"") and ""BY"" method of Benjamini, Hochberg, and Yekutieli control the false discovery rate, the expected proportion of false discoveries amongst the rejected hypotheses. The false discovery rate is a less stringent condition than the family-wise error rate, so these methods are more powerful than the others."" | (Corrected) p-value significance threshold | | ------------ Output files ------------ variableMetadata_out.tabular | 
variableMetadata
 file identical to the file given as argument, except that (at least) three columns have been added: | 1) [factor]
[test]
[class'a'].[class'b']
dif or [factor]
[test]
cor: difference of the means (ttest) or the medians (wilcoxon) between the two classes, or 'pearson' or 'spearman' correlations | 2) [factor]
[test]
[class'a'].[class'b']
[method] or [factor]
[test]
[method]: adjusted p-values | 3) [factor]
[test]
[class'a'].[class'b']
sig or [factor]
[test]_sig: significance (coded as '1' if below the threshold and '0' otherwise) | In the case of 'anova' and 'kruskal', the columns 2) and 3) appear first to give the results from the ANOVA or Kruskal Wallis test, and then the results of the pairwise comparisons are reported in additional columns (otherwise NA in these columns): in the case of ANOVA, the Tukey HSD post-hoc analysis is used (for each comparison, the difference between means, p value, and significance are provided); in the case of Kruskal Wallis, the Nemenyi is performed (PMCMR package) (for each pairwise comparison, the difference between medians, p value and significance are provided); note that since version 2.2.0, the p-values of the post-hoc pairwise comparisons (ANOVA or Kruskal) are further corrected for multiple testing over all variables (as the p-value of the omnibus test in column 2) | figure.pdf | File containing the graphics of all significant tests: boxplots (respectively scatterplots with the regression line in red and the R2 value) are displayed when the factor of interest is qualitative (respectively quantitative). The variable name and the corrected p-value is indicated in the title of each plot | information.txt | File with all messages and warnings generated during the computation | --------------------------------------------------- --------------- Working example --------------- .. class:: infomark See the 
W4M00001a_sacurine-subset-statistics
, 
W4M00001b_sacurine-subset-complete
, 
W4M00002_mtbls2
, 
W4M00003_diaplasma
 shared histories in the 
Shared Data/Published Histories
 menu (https://galaxy.workflow4metabolomics.org/history/list_published) --------------------------------------------------- ---- NEWS ---- CHANGES IN VERSION 2.2.4 ======================== MINOR MODIFICATION Internal minor modifications for building and testing CHANGES IN VERSION 2.2.0 ======================== MAJOR MODIFICATION ANOVA and Kruskal-Wallis: The p-values of the post-hoc tests (i.e. from pairwise comparisons) are now further corrected for multiple testing over all variables (previously, only the p-value of the omnibus test was corrected over all variables) MINOR MODIFICATION All values in the 'dif', adjusted p-value, and 'sig' columns are now displayed (previously, the values were set to NA when the p-value of the omnibus test was not significant) NEW FEATURE Graphic: a single pdf file containing the graphics of all significant tests is now produced as '_figure.pdf' output: boxplots (respectively scatterplots with the regression line in red and the R2 value) are displayed when the factor of interest is qualitative (respectively quantitative). The corrected p-value is indicated in the title of each plot CHANGES IN VERSION 2.1.4 ======================== NEW FEATURE Level names are now separated by '.' instead of '-' previously in the column names of the output variableMetadata table (e.g., 'jour_ttest_J3.J10_fdr' instead of 'jour_ttest_J3-J10_fdr' previously) INTERNAL MODIFICATIONS Minor internal changes for toolshed export CHANGES IN VERSION 2.1.2 ======================== INTERNAL MODIFICATIONS Minor internal changes for toolshed export CHANGES IN VERSION 2.1.1 ======================== INTERNAL MODIFICATIONS Internal handling of 'NA' p-values (e.g. when intensities are identical in all samples) CHANGES IN VERSION 2.0.1 ======================== NEW FEATURE (corrected) p-value threshold can be set to any value between 0 and 1"
toolshed.g2.bx.psu.edu/repos/devteam/dwt_var_perfeature/dwt_var1/1.0.2	".. class:: infomark 
What it does
 This tool computes the scale-specific variance in wavelet coeffients obtained from the discrete wavelet transform of a feature of interest. Input data consists of an ordered series of data, S, equispaced and of sample size N, where N is of the form N = 2^k, and k is a positive integer and represents the number of levels of wavelet decomposition. S could be a time series, or a set of DNA sequences. The user calculates a statistic of interest for each feature in each interval of S: say, expression level of a particular gene in a time course, or the number of LINE elements per window across a chromosome. This tool then performs a discrete wavelet transform of the feature of interest, and plots the resulting variance in wavelet coefficients per wavelet scale. In addition, statistical significance of variances are determined by 1,000 random permutations of the intervals in S, to generate null bands (representing the user provided alpha value) corresponding to the empirical distribution of wavelet variances under the null hypothesis of no inherent order to the series in S. This tool generates two output files: - The first output file is a TABULAR format file representing the variances, p-values, and test orientation for the features at each wavelet scale based on a random permutation test. - The second output file is a PDF image plotting the wavelet variances of each feature at each scale. ----- .. class:: warningmark 
Note
 In order to obtain empirical p-values, a random perumtation scheme is implemented by the tool, such that the output may generate slightly variations in results each time it is run on the same input file."
toolshed.g2.bx.psu.edu/repos/richard-burhans/rdeval/rdeval/0.0.7+galaxy7	"What it does ============ 
rdeval
 processes sequencing files and optionally 
filters
, 
subsamples
, and/or 
compresses homopolymers
 within the reads. The retained reads can be saved in multiple formats, and metrics on these reads can be stored in a '
sketch
' file. Statistics can then be efficiently retrieved from these sketch files for further processing. When provided with a collection of files, this tool processes each file individually and produces corresponding output collections. This allows for efficient parallel processing of multiple samples while maintaining individual file tracking and error handling. .. image:: pipeline.svg :alt: pipeline diagram :align: left Filtering ========= Input reads can be filtered using one of the three methods listed below, applied sequentially in the specified order. 1. Retain reads whose header lines are listed in the include dataset. 2. Discard reads whose header lines are listed in the exclude dataset. 3. Retain reads that match the provided filter expression. The filter expression can be used to select reads based on read length (l), average read quality (q), or a combination of both. The grammar for constructing filter expressions is outlined below: * filter-expression ::= <length-expression> | <quality-expression> | <length-expression> <combination-operator> <quality-expression> | <quality-expression> <combination-operator> <length-expression> * length-expression ::= ""l"" <comparison-operator> <integer> * quality-expression ::= ""q"" <comparison-operator> <integer> * combination-operator := ""&"" | ""|"" * comparison-operator ::= ""<"" | ""="" | "">"" * integer ::= <digit> | <digit><integer> * digit ::= ""0"" | ""1"" | ""2"" | ""3"" | ""4"" | ""5"" | ""6"" | ""7"" | ""8"" | ""9"" Retain reads longer than 10 base pairs l>10 Retain reads with average quality greather than 20 q>20 Retain reads longer than 10 base pairs with average quality greather than 20 l>10 & q>20 .. _sampling-label: Sub-sampling ============ 4. Retain a subsample of the reads by specifying the fraction to be kept. Use the 
random seed
 option to keep subsampling reproducible. Homopolymer Compression ======================= 5. Runs of repeated nucleotides in each read are collapsed, with any associated quality data discarded. For example, CAGGCTTT would become CAGCT."
toolshed.g2.bx.psu.edu/repos/richard-burhans/rdeval/rdeval_report/0.0.7+galaxy7	This tool creates a report containing a read summary and figures from input RD files.
addValue	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
What it does
 You can enter any value and it will be added as a new column to your dataset ----- 
Example
 If your original data looks like this:: chr1 10 100 geneA chr2 200 300 geneB chr2 400 500 geneC Typing 
+
 in the text box will generate:: chr1 10 100 geneA + chr2 200 300 geneB + chr2 400 500 geneC + You can also add line numbers by selecting 
Iterate: YES
. In this case if you enter 
1
 in the text box you will get:: chr1 10 100 geneA 1 chr2 200 300 geneB 2 chr2 400 500 geneC 3"
toolshed.g2.bx.psu.edu/repos/devteam/add_value/addValue/1.0.1	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
What it does
 You can enter any value and it will be added as a new column to your dataset ----- 
Example
 If you original data looks like this:: chr1 10 100 geneA chr2 200 300 geneB chr2 400 500 geneC Typing 
+
 in the text box will generate:: chr1 10 100 geneA + chr2 200 300 geneB + chr2 400 500 geneC + You can also add line numbers by selecting 
Iterate: YES
. In this case if you enter 
1
 in the text box you will get:: chr1 10 100 geneA 1 chr2 200 300 geneB 2 chr2 400 500 geneC 3"
toolshed.g2.bx.psu.edu/repos/mvdbeek/add_input_name_as_column/addName/0.2.0	"What it does
 Adds a new column with the name of the input file as it appears in the history. By default the column is appended. ----- .. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert"
toolshed.g2.bx.psu.edu/repos/bgruening/add_line_to_file/add_line_to_file/0.1.0	"What it does
 This tool adds the input text to the beginning (header) or the end (footer) of the input text file."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy3	"What it does
 This tool runs the 
cut
 unix command, which extract or delete columns from a file. ----- Field List Example: 
1,3,7
 - Cut specific fields/characters. 
3-
 - Cut from the third field/character to the end of the line. 
2-5
 - Cut from the second to the fifth field/character. 
-8
 - Cut from the first to the eight field/characters. Input Example:: fruit color price weight apple red 1.4 0.5 orange orange 1.5 0.3 banana yellow 0.9 0.3 Output Example ( 
Keeping fields 1,3,4
 ):: fruit price weight apple 1.4 0.5 orange 1.5 0.3 banana 0.9 0.3 Output Example ( 
Discarding field 2
 ):: fruit price weight apple 1.4 0.5 orange 1.5 0.3 banana 0.9 0.3 Output Example ( 
Keeping 3 characters
 ):: fru app ora ban @REFERENCES@"
ChangeCase	".. class:: warningmark 
This tool breaks column assignments.
 To re-establish column assignments run the tool and click on the pencil icon in the resulting history item. .. class:: warningmark The format of the resulting dataset from this tool is always tabular. ----- 
What it does
 This tool selects specified columns from a dataset and converts the values of those columns to upper or lower case. - Columns are specified as 
c1
, 
c2
, and so on. - Columns can be specified in any order (e.g., 
c2,c1,c6
) ----- 
Example
 Changing columns 1 and 3 ( delimited by Comma ) to upper case in:: apple,is,good windows,is,bad will result in:: APPLE is GOOD WINDOWS is BAD"
toolshed.g2.bx.psu.edu/repos/galaxyp/regex_find_replace/regexColumn1/1.0.3	".. class:: warningmark 
This tool will attempt to reuse the metadata from your first input.
 To change metadata assignments click on the ""edit attributes"" link of the history item generated by this tool. .. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- This tool goes line by line through the specified input file and if the text in the selected column matches a specified regular expression pattern replaces the text with the corresponding specified replacement. This tool can be used to change between the chromosome naming conventions of UCSC and Ensembl. For example to remove the 
chr
 part of the reference sequence name in the first column of this GFF file:: ##gff-version 2 ##Date: Thu Mar 23 11:21:17 2006 ##bed2gff.pl $Rev: 601 $ ##Input file: ./database/files/61c6c604e0ef50b280e2fd9f1aa7da61.dat chr1 bed2gff CCDS1000.1_cds_0_0_chr1_148325916_f 148325916 148325975 . + . score ""0""; chr21 bed2gff CCDS13614.1_cds_0_0_chr21_32707033_f 32707033 32707192 . + . score ""0""; chrX bed2gff CCDS14606.1_cds_0_0_chrX_122745048_f 122745048 122745924 . + . score ""0""; Setting:: using column: c1 Find Regex: chr([0-9]+|X|Y|M[Tt]?) Replacement: \1 produces:: ##gff-version 2 ##Date: Thu Mar 23 11:21:17 2006 ##bed2gff.pl $Rev: 601 $ ##Input file: ./database/files/61c6c604e0ef50b280e2fd9f1aa7da61.dat 1 bed2gff CCDS1000.1_cds_0_0_chr1_148325916_f 148325916 148325975 . + . score ""0""; 21 bed2gff CCDS13614.1_cds_0_0_chr21_32707033_f 32707033 32707192 . + . score ""0""; X bed2gff CCDS14606.1_cds_0_0_chrX_122745048_f 122745048 122745924 . + . score ""0""; This tool uses Python regular expressions with the 
re.sub()
 function. More information about Python regular expressions can be found here: http://docs.python.org/library/re.html. The regex 
chr([0-9]+|X|Y|M)
 means start with text 
chr
 followed by either: one or more digits, or the letter X, or the letter Y, or the letter M (optionally followed by a single letter T or t). Note that the parentheses 
()
 capture patterns in the text that can be used in the replacement text by using a backslash-number reference: 
\1
 In the replacement pattern, use the special token #{input_name} to insert the input dataset's display name. The name can be modified by a second find/replace check. Suppose you want to insert the sample id of your dataset, named 
Sample ABC123
, into the dataset itself, which currently contains the lines:: Data 1 Data 2 Data 3 You can use the following checks:: Find Regex: Data Replacement: #{input_name} Data Find Regex: Sample (\S+) Replacement: \1 The result will be:: ABC123 Data 1 ABC123 Data 2 ABC123 Data 3 Galaxy aggressively escapes input supplied to tools, so if something is not working please let us know and we can look into whether this is the cause. Also if you would like help constructing regular expressions for your inputs, please let us know at help@msi.umn.edu."
toolshed.g2.bx.psu.edu/repos/bgruening/column_arrange_by_header/bg_column_arrange_by_header/0.2	"What it does
 With this tool you can specify - by name - the order of columns for tabular data. Columns not specified will remain ordered as before and be moved to the right of the specified columns, as shown in the following example. Input file:: AHeader BHeader CHeader DHeader a b c d a b c d Specifying 
CHeader
 and 
BHeader
, as the columns that should be leftmost, generates:: CHeader BHeader AHeader DHeader c b a d c b a d Alternatively, you can choose to retain 
only
 the specified columns in their new arrangement and discard all other columns."
toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
What it does
 This tool computes an expression on every row of a dataset and appends or inserts the result as a new column (field). Several expressions can be specified and will be applied sequentially to each row. 
Expression rules
 - Columns are referenced with 
c
 and a 
number
. For example, 
c1
 refers to the first column of a tab-delimited file - The following built-in Python functions are available for use in expressions:: abs | all | any | ascii | bin | bool | chr | complex | divmod | float | format | hex | int | len | list map | max | min | oct | ord | pow | range | reversed | round | set | sorted | str | sum | type acos | acosh | asin | asinh | atan | atan2 | atanh | cbrt | ceil | comb | copysign | cos | cosh | degrees dist | erf | erfc | exp | exp2 | expm1 | fabs | factorial | floor | fmod | frexp | fsum | gamma | gcd hypot | inf | isclose | isfinite | isinf | isnan | isqrt | ldexp | lgamma | log | log10 | log1p | log2 modf | nextafter | perm | pow | prod | remainder | sin | sqrt | tan | tanh | tau | trunc | ulp - In addition the numpy function 
format_float_positional
 is available to control the formatting of floating point numbers. - Expressions can be chained, and the tool will keep track of newly added columns while working through the chain. This means you can reference a column that was created as the result of a previous expression in later ones. ----- 
Simple examples
 If this is your input:: chr1 151077881 151077918 2 200 - chr1 151081985 151082078 3 500 + computing ""c4 * c5"" will produce:: chr1 151077881 151077918 2 200 - 400 chr1 151081985 151082078 3 500 + 1500 You can also use this tool to evaluate expressions. For example, computing ""c3 >= c2"" for the input above will result in the following:: chr1 151077881 151077918 2 200 - True chr1 151081985 151082078 3 500 + True Similarly, computing ""type(c2) == type(c3) will return:: chr1 151077881 151077918 2 200 - True chr1 151081985 151082078 3 500 + True ----- 
Error handling
 The tool will always fail on syntax errors in and other unrecoverable parsing errors with any of your expressions. For other problems, however, it offers control over how they should be handled: 1. The default for ""Autodetect column types"" is ""Yes"", which means the tool will evaluate each column value as the type that Galaxy assumes for the column. This default behavior will allow you to write simpler expressions. The arithmetic expression ""c4 * c5"" from the first simple example, for instance, works only because Galaxy realizes that c4 and c5 are integer columns. Occasionally, this autodetection can cause issues. A common such situation are missing values in columns that Galaxy thinks are of numeric type. If you're getting errors like ""Failed to convert some of the columns in line #X ..."", a solution might be to turn off column type autodetection. The price you will have to pay for doing so is that now you will have to handle type conversions yourself. In the first example you would now have to use the epression: ""int(c4) * int(c5)"". 2. By default, if any expression references columns that are not existing before that expression gets computed, the tool will fail, but you can uncheck the ""Fail on references to non-existent columns"" option. If you do so, the result will depend on your choice for ""If an expression cannot be computed for a row"" (see 3.) 3. The default for rows, for which an expression fails to compute is, again, to fail the tool run, but you can also choose to: - skip the row on output This is a simple way to only keep lines conforming to an expected standard. It is also easy to mask problems with your expressions with this option so take a look at the results and try to understand what gets skipped and for what reasons (the stdout of the tool will contain information about both). - keep the row unchanged This can be a good solution if your input contains special separator lines that don't follow the general tabular format of other lines and you would like to keep those lines - produce an empty column value for the row This will use the empty string as a substitute for non-computable items. Different from the ""keep the row unchanged option"" the problematic line will have a column added or changed. This option is a good choice for inputs in which all rows have the same tabular layout where you want to make sure that the same is true for the output, i.e. that all output lines still have the same number of columns. - fill in a replacement value This option is very similar to the previous one, but lets you control the replacement value. 
Example
 In the following input:: chr1 151077881 151077918 2 200 - chr1 151081985 151082078 3 500 + chr1 151090031 151090938 4 700 the last line does not have a strand column. This violates the bed file format specification, which says that unknown strand is to be encoded as 
.
 in the strand column. You can fix the file with the following tool run: 
Add expression
: 
c6
 
Mode of the operation
: 
Replace
 
Use new column to replace column number
: 
6
 
Fail on references to non-existent columns
: 
No
 
If an expression cannot be computed for a row
: 
Fill in a replacement value
 
Replacement value
: 
."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cat/9.5+galaxy3	".. class:: warningmark 
WARNING:
 Be careful not to concatenate datasets of different kinds (e.g., sequences with intervals). This tool does not check if the datasets being concatenated are in the same format. ----- 
What it does
 Concatenates datasets ----- 
Example
 Concatenating Dataset:: chrX 151087187 151087355 A 0 - chrX 151572400 151572481 B 0 + with Dataset1:: chr1 151242630 151242955 X 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - and with Dataset2:: chr2 100000030 200000955 P 0 + chr2 100000015 200000999 Q 0 + will result in the following:: chrX 151087187 151087355 A 0 - chrX 151572400 151572481 B 0 + chr1 151242630 151242955 X 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - chr2 100000030 200000955 P 0 + chr2 100000015 200000999 Q 0 +"
toolshed.g2.bx.psu.edu/repos/artbio/concatenate_multiple_datasets/cat_multi_datasets/1.4.3	".. class:: warningmark 
WARNINGS:
 - This tool does not check if the datasets being concatenated are in the same format. - When concatenating 2 collections make sure the first collection is the one with the most items. - This tool can't handle nested collection deeper than list:list. ----- 
What it does
 Concatenates datasets and paired collections with multiple options: - When the input is a paired collection: - concatenation by strand : forward and reverse datasets are concatenated separately and a list with a single forward - reverse dataset pair is returned - concatenation by pair : forward - reverse dataset pairs are concatenated and a simple dataset collection is returned - whole collection concatenation : all datasets in the collection are concatenated and a single dataset is returned - When the inputs are 2 collections: datasets are concatenated in a pairwise combination and a single dataset collection is returned - When nested collection concatenation: datasets in each sub-collection are concatenated and a simple dataset collection is returned - Skipping lines before concatenation to avoid headers - Add the name of the concatenated files as separator ----- 
Single datasets concatenation example
 Concatenating Dataset:: chrX 151087187 151087355 A 0 - chrX 151572400 151572481 B 0 + with Dataset1:: chr1 151242630 151242955 X 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - and with Dataset2:: chr2 100000030 200000955 P 0 + chr2 100000015 200000999 Q 0 + will result in the following:: chrX 151087187 151087355 A 0 - chrX 151572400 151572481 B 0 + chr1 151242630 151242955 X 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - chr2 100000030 200000955 P 0 + chr2 100000015 200000999 Q 0 + ----- 
2 Collections concatenation
 1rst collection:: a b c d 2nd collection:: 1 2 3 4 Concatenation result:: A single collection containing: a concatenated with 1 b concatenated with 2 c concatenated with 3 d concatenated with 4 ----- 
Paired collection concatenation example
 1rst pair:: forward - reverse 2nd pair:: forward - reverse - Concatenation by strand:: concatenates: 1rst forward + 2nd forward 1rst reverse + 2nd reverse outputs: 1 pair - Concatenation by pair:: concatenates: 1rst forward + 1rst reverse 2nd forward + 2nd reverse outputs: 2 datasets - Concatenate all:: concatenates: 1rst forward + 1rst reverse + 2nd forward + 2nd reverse outputs: 1 dataset ----- 
Nested collection concatenation example
 Nested collection: - Experiment - Sample_1 - Sample_1_file_1 - Sample_1_file_2 - Sample_2 - Sample_2_file_1 - Sample_2_file_2 - Sample_2_file_3 Concatenation result:: A single collection containing: - Sample_1: (Sample_1_file_1 + Sample_1_file_2) - Sample_2: (Sample_2_file_1 + Sample_2_file_2 + Sample_2_file_3) ----- 
When selecting ""Include dataset names"" when concatenating files
: 1rst file name=""first_tabular"":: chrX 151087187 151087355 A 0 - chrX 151572400 151572481 B 0 + 2nd file name=""second_tabular"":: chr1 151242630 151242955 X 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - output:: # first_tabular chrX 151087187 151087355 A 0 - chrX 151572400 151572481 B 0 + # second_tabular chr1 151242630 151242955 X 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - ----- 
Skiping lines
 1rst file:: chrX 151087187 151087355 A 0 - chrX 151572400 151572481 B 0 + 2nd file:: chr1 151242630 151242955 X 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - skipping 1 line output:: chrX 151572400 151572481 B 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - ----- Adapted from galaxy's catWrapper.xml to allow multiple input files."
toolshed.g2.bx.psu.edu/repos/mvdbeek/concatenate_multiple_datasets/cat_multiple/0.2	".. class:: warningmark 
WARNING:
 Be careful not to concatenate datasets of different kinds (e.g., sequences with intervals). This tool does not check if the datasets being concatenated are in the same format. ----- 
What it does
 Concatenates datasets ----- 
Example
 Concatenating Dataset:: chrX 151087187 151087355 A 0 - chrX 151572400 151572481 B 0 + with Dataset1:: chr1 151242630 151242955 X 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - and with Dataset2:: chr2 100000030 200000955 P 0 + chr2 100000015 200000999 Q 0 + will result in the following:: chrX 151087187 151087355 A 0 - chrX 151572400 151572481 B 0 + chr1 151242630 151242955 X 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - chr2 100000030 200000955 P 0 + chr2 100000015 200000999 Q 0 + ----- Adapted from galaxy's catWrapper.xml to allow multiple input files"
cat1	".. class:: warningmark 
WARNING:
 Be careful not to concatenate datasets of different kinds (e.g., sequences with intervals). This tool does not check if the datasets being concatenated are in the same format. ----- 
What it does
 Concatenates datasets ----- 
Example
 Concatenating Dataset:: chrX 151087187 151087355 A 0 - chrX 151572400 151572481 B 0 + with Dataset1:: chr1 151242630 151242955 X 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - and with Dataset2:: chr2 100000030 200000955 P 0 + chr2 100000015 200000999 Q 0 + will result in the following:: chrX 151087187 151087355 A 0 - chrX 151572400 151572481 B 0 + chr1 151242630 151242955 X 0 + chr1 151271715 151271999 Y 0 + chr1 151278832 151279227 Z 0 - chr2 100000030 200000955 P 0 + chr2 100000015 200000999 Q 0 +"
Condense characters1	"What it does
 This tool condenses all consecutive characters of a specified type. ----- 
Example
 - Input file:: geneX,,,10,,,,,20 geneY,,5,,,,,12,15,9, - Condense all consecutive commas. The above file will be converted into:: geneX,10,20 geneY,5,12,15,9"
Convert characters1	"What it does
 Converts all delimiters of a specified type into TABs. Consecutive delimiters can be condensed in a single TAB. ----- 
Example
 - Input file:: chrX||151283558|151283724|NM_000808_exon_8_0_chrX_151283559_r|0|- chrX|151370273|151370486|NM_000808_exon_9_0_chrX_151370274_r|0|- chrX|151559494|151559583|NM_018558_exon_1_0_chrX_151559495_f|0|+ chrX|151564643|151564711|NM_018558_exon_2_0_chrX_151564644_f||||0|+ - Converting all pipe delimiters of the above file to TABs and condensing delimiters will get:: chrX 151283558 151283724 NM_000808_exon_8_0_chrX_151283559_r 0 - chrX 151370273 151370486 NM_000808_exon_9_0_chrX_151370274_r 0 - chrX 151559494 151559583 NM_018558_exon_1_0_chrX_151559495_f 0 + chrX 151564643 151564711 NM_018558_exon_2_0_chrX_151564644_f 0 +"
createInterval	".. class:: warningmark 
TIP
. Once your interval appears in history, you must tell Galaxy which genome it belongs to by clicking pencil icon or the ""?"" link in the history item. ----- 
What it does
 This tool allows you to create a single genomic interval. The resulting history item will be in the BED format. ----- 
Example
 Typing the following values in the form:: Chromosome: chrX Start position: 151087187 End position: 151370486 Name: NM_000808 Strand: minus will create a single interval:: chrX 151087187 151370486 NM_000808 0 -"
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_text_file_with_recurring_lines/9.5+galaxy3	".. class:: infomark 
What it does
 This tool creates a text file with recurring lines. You can specify a bunch of characters or entire sentences. The entire string will be printed X times separated by a line break. X can be either given by the use as a number or calculated by a given file. In case the user provides a file, the line number will be used as X."
Cut1	".. class:: warningmark 
WARNING: This tool breaks column assignments.
 To re-establish column assignments run the tools and click on the pencil icon in the latest history item. .. class:: infomark The output of this tool is always in tabular format (e.g., if your original delimiters are commas, they will be replaced with tabs). For example: Cutting columns 1 and 3 from:: apple,is,good windows,is,bad will give:: apple good windows bad ----- 
What it does
 This tool selects (cuts out) specified columns from the dataset. - Columns are specified as 
c1
, 
c2
, and so on. Column count begins with 
1
 - Columns can be specified in any order (e.g., 
c2,c1,c6
) - If you specify more columns than actually present - empty spaces will be filled with dots ----- 
Example
 Input dataset (six columns: c1, c2, c3, c4, c5, and c6):: chr1 10 1000 gene1 0 + chr2 100 1500 gene2 0 + 
cut
 on columns ""
c1,c4,c6
"" will return:: chr1 gene1 + chr2 gene2 + 
cut
 on columns ""
c6,c5,c4,c1
"" will return:: + 0 gene1 chr1 + 0 gene2 chr2 
cut
 on columns ""
c1-c3
"" will return:: chr1 10 1000 chr2 100 1500 
cut
 on columns ""
c8,c7,c4
"" will return:: . . gene1 . . gene2"
toolshed.g2.bx.psu.edu/repos/iuc/filter_tabular/filter_tabular/3.3.1	Python offset indexes or slices. Examples: <ul> <li>Column offset indexes: 0,3,1 (selects the first,fourth, and second columns)</li> <li>Negative column numbers: -1,-2 (selects the last, and second last columns)</li> <li>python slices ( slice(start, stop[, step]) select a range of columns): <li> <ul> <li>0:3 or :3 (selects the first 3 columns)</li> <li>3:5 (selects the fourth and fifth columns)</li> <li>2: (selects all columns after the second)</li> <li>-2: (selects the last 2 columns)</li> <li>2::-1 (selects the first 3 columns n reverse order: third,second,first)</li> </ul> </ul>
toolshed.g2.bx.psu.edu/repos/iuc/jq/jq/1.0	"JQ == jq is a lightweight and flexible JSON processor. Brief Examples -------------- See 
the manual &lt;https://stedolan.github.io/jq/manual/&gt;
__ for a much more detailed guide on using JQ. Select an Attribute ~~~~~~~~~~~~~~~~~~~ Given an input like the following :: {""foo"": 42, ""bar"": ""less interesting data""} To select just the value of 
foo
, supply the filter 
.foo
 Loop over an Array ~~~~~~~~~~~~~~~~~~ Given an input like the following :: [{""foo"": 1123}, {""foo"": 6536}, {""foo"": 5321}] To select the values of 
foo
, supply the filter 
.[].foo
 or 
.[] | .foo
. This will produce a file with one number per line. If you wish to select multiple things: :: [{""foo"": 1123, ""bar"": ""a""}, {""foo"": 6536, ""bar"": ""b""}, {""foo"": 5321, ""bar"": ""c""}] To select the values of 
foo
 AND 
bar
, supply the filter 
.[] | [.foo, .bar]
. This will produce and output array like: :: [ [1123, ""a""] [6536, ""b""] [5321, ""c""] ] A common next step is to turn this into a tabular output which more Galaxy tools can work with. This can be done by checking the box for tabular. This will invoke the JQ filter of 
@tsv
 at the end of the processing chain, and produce a tabular file."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_easyjoin_tool/9.5+galaxy3	"What it does
 This tool joins two tabular files based on a common key column. ----- 
Example
 
First file
:: Fruit Color Apple red Banana yellow Orange orange Melon green 
Second File
:: Fruit Price Orange 7 Avocado 8 Apple 4 Banana 3 
Joining
 both files, using 
key column 1
 and a 
header line
, will return:: Fruit Color Price Apple red 4 Avocado . 8 Banana yellow 3 Melon green . Orange orange 7 .. class:: infomark * Input files need not be sorted. * The header line (
Fruit Color Price
) was joined and kept as first line. * Missing values ( Avocado's color, missing from the first file ) are replaced with a period character."
wc_gnu	"What it does
 This tool outputs counts of specified attributes (lines, words, characters) of a dataset. ----- 
Example Output
 :: #lines words characters 7499 41376 624971 ------ 
Citation
 If you use this tool in Galaxy, please cite Blankenberg D, et al. 
In preparation."
mergeCols1	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 ----- 
What it does
 This tool merges columns together. Any number of valid columns can be merged in any order. ----- 
Example
 Input dataset (five columns: c1, c2, c3, c4, and c5):: 1 10 1000 gene1 chr 2 100 1500 gene2 chr merging columns ""
c5,c1
"" will return:: 1 10 1000 gene1 chr chr1 2 100 1500 gene2 chr chr2 .. class:: warningmark Note that all original columns are preserved and the result of merge is added as the rightmost column."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_multijoin_tool/9.5+galaxy3	"What it does
 This tool joins multiple tabular files based on a common key column. ----- 
Example
 To join three files, based on the 4th column, and keeping the 7th,8th,9th columns: 
First file (AAA)
:: chr4 888449 890171 FBtr0308778 0 + 266 1527 1722 chr4 972167 979017 FBtr0310651 0 - 3944 6428 6850 chr4 972186 979017 FBtr0089229 0 - 3944 6428 6831 chr4 972186 979017 FBtr0089231 0 - 3944 6428 6831 chr4 972186 979017 FBtr0089233 0 - 3944 6428 6831 chr4 995793 996435 FBtr0111046 0 + 7 166 642 chr4 995793 997931 FBtr0111044 0 + 28 683 2138 chr4 995793 997931 FBtr0111045 0 + 28 683 2138 chr4 1034029 1047719 FBtr0089223 0 - 5293 13394 13690 ... 
Second File (BBB)
:: chr4 90286 134453 FBtr0309803 0 + 657 29084 44167 chr4 251355 266499 FBtr0089116 0 + 56 1296 15144 chr4 252050 266506 FBtr0308086 0 + 56 1296 14456 chr4 252050 266506 FBtr0308087 0 + 56 1296 14456 chr4 252053 266528 FBtr0300796 0 + 56 1296 14475 chr4 252053 266528 FBtr0300800 0 + 56 1296 14475 chr4 252055 266528 FBtr0300798 0 + 56 1296 14473 chr4 252055 266528 FBtr0300799 0 + 56 1296 14473 chr4 252541 266528 FBtr0300797 0 + 56 1296 13987 ... 
Third file (CCC)
:: chr4 972167 979017 FBtr0310651 0 - 9927 6738 6850 chr4 972186 979017 FBtr0089229 0 - 9927 6738 6831 chr4 972186 979017 FBtr0089231 0 - 9927 6738 6831 chr4 972186 979017 FBtr0089233 0 - 9927 6738 6831 chr4 995793 996435 FBtr0111046 0 + 5 304 642 chr4 995793 997931 FBtr0111044 0 + 17 714 2138 chr4 995793 997931 FBtr0111045 0 + 17 714 2138 chr4 1034029 1047719 FBtr0089223 0 - 17646 13536 13690 ... 
Joining
 the files, using 
key column 4
, 
value columns 7,8,9
 and a 
header line
, will return:: key AAA__V7 AAA__V8 AAA__V9 BBB__V7 BBB__V8 BBB__V9 CCC__V7 CCC__V8 CCC__V9 FBtr0089116 0 0 0 56 1296 15144 0 0 0 FBtr0089223 5293 13394 13690 0 0 0 17646 13536 13690 FBtr0089229 3944 6428 6831 0 0 0 9927 6738 6831 FBtr0089231 3944 6428 6831 0 0 0 9927 6738 6831 FBtr0089233 3944 6428 6831 0 0 0 9927 6738 6831 FBtr0111044 28 683 2138 0 0 0 17 714 2138 FBtr0111045 28 683 2138 0 0 0 17 714 2138 FBtr0111046 7 166 642 0 0 0 5 304 642 FBtr0300796 0 0 0 56 1296 14475 0 0 0 ... .. class:: infomark Input files need not be sorted."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/nl/9.5+galaxy3	"What it does
 Add line numbers to the data set."
Paste1	".. class:: infomark Paste preserves column assignments of the first dataset. ----- 
What it does
 This tool merges two datasets side by side. If the first (left) dataset contains column assignments such as chromosome, start, end and strand, these will be preserved. However, if you would like to change column assignments, click the pencil icon in the history item. ----- 
Example
 First dataset:: a 1 a 2 a 3 Second dataset:: 20 30 40 Pasting them together will produce:: a 1 20 a 2 30 a 3 40"
toolshed.g2.bx.psu.edu/repos/iuc/query_tabular/query_tabular/3.3.2	Python offset indexes or slices. Examples: <ul> <li>Column offset indexes: 0,3,1 (selects the first,fourth, and second columns)</li> <li>Negative column numbers: -1,-2 (selects the last, and second last columns)</li> <li>python slices ( slice(start, stop[, step]) select a range of columns): <li> <ul> <li>0:3 or :3 (selects the first 3 columns)</li> <li>3:5 (selects the fourth and fifth columns)</li> <li>2: (selects all columns after the second)</li> <li>-2: (selects the last 2 columns)</li> <li>2::-1 (selects the first 3 columns n reverse order: third,second,first)</li> </ul> </ul>
toolshed.g2.bx.psu.edu/repos/iuc/gff3_rebase/gff3.rebase/1.2	"What it does
 Often the genomic data processing/analysis process requires a workflow like the following: - select some features from a genome - export the sequences associated with those regions - analyse those exports with some tool like Blast For display, especially in software like JBrowse, it is convenient to know where in the original genome the analysis results would fall. E.g. if a transmembrane domain is detected at bases 10-20 of an analysed protein, where should this be displayed relative to the parent genome? This tool helps fill that gap, by rebasing some analysis results against the parent features which were originally analysed. 
Example Inputs
 For a ""child"" set of annotations like:: #gff-version 3 cds42 blastp match_part 1 50 1e-40 . . ID=m00001;Notes=RNAse A Protein And a parent set of annotations like:: #gff-version 3 PhageBob maker cds 300 600 . + . ID=cds42 One could imagine that during the analysis process, the user had exported the parent annotation into some sequence:: >cds42 M...... and then analysed those results, producing the ""child"" annotation file. This tool will then localize the results properly against the parent:: #gff-version 3 PhageBob blastp match_part 300 449 1e-40 + . ID=m00001;Notes=RNAse A Protein which will allow you to display the results in the correct location in visualizations. 
Options
 There are two optional flags which can be passed. The Interpro flag selectively ignores features which shouldn't be included in the output (i.e. 
polypeptide
), and a couple of qualifiers that aren't useful (
status
, 
Target
) The ""Map Protein..."" flag says that you translated the sequences during the genomic export process, analysing protein sequences. This indicates to the software that the bases should be multiplied by three to obtain the correct DNA locations."
toolshed.g2.bx.psu.edu/repos/galaxyp/regex_find_replace/regex1/1.0.3	"This tool goes line by line through the specified input file and replaces text which matches the specified regular expression patterns with its corresponding specified replacement. This tool uses Python regular expressions. More information about Python regular expressions can be found here: http://docs.python.org/library/re.html. To convert an Ilumina FATSQ sequence id from the CAVASA 8 format:: @EAS139:136:FC706VJ:2:2104:15343:197393 1:Y:18:ATCACG GGGTGATGGCCGCTGCCGATGGCGTCAAATCCCACC +EAS139:136:FC706VJ:2:2104:15343:197393 1:Y:18:ATCACG IIIIIIIIIIIIIIIIIIIIIIIIIIIIII9IG9IC To the CASAVA 7 format:: @EAS139_FC706VJ:2:2104:15343:197393#0/1 GGGTGATGGCCGCTGCCGATGGCGTCAAATCCCACC +EAS139_FC706VJ:2:2104:15343:197393#0/1 IIIIIIIIIIIIIIIIIIIIIIIIIIIIII9IG9IC Use Settings:: Find Regex: ^([@+][A-Z0-9]+):\d+:(\S+)\s(\d).
$ Replacement: \1_\2#0/\3 Note that the parentheses 
()
 capture patterns in the text that can be used in the replacement text by using a backslash-number reference: 
\1
 The regex 
^([@+][A-Z0-9]+):\d+:(\S+) (\d).
$
 means:: ^ - start the match at the beginning of the line of text ( - start a group (1), that is a string of matched text, that can be back-referenced in the replacement as \1 [@+] - matches either a @ or + character [A-Z0-9]+ - matches an uppercase letter or a digit, the plus sign means to match 1 or more such characters ) - end a group (1), that is a string of matched text, that can be back-referenced in the replacement as \1 :\d+: - matches a colon followed by one or more digits followed by a colon character (\S+) - matches one or more non-whitespace charcters, the enclosing parentheses make this a group (2) that can back-referenced in the replacement text as \2 \s - matches a whitespace character (\d) - matches a single digit character, the enclosing parentheses make this a group (3) that can back-referenced in the replacement text as \3 .* - dot means match any character, asterisk means zero more more matches $ - the regex must match to the end of the line of text In the replacement pattern, use the special token #{input_name} to insert the input dataset's display name. The name can be modified by a second find/replace check. Suppose you want to insert the sample id of your dataset, named 
Sample ABC123**, into the dataset itself, which currently contains the lines:: Data 1 Data 2 Data 3 You can use the following checks:: Find Regex: Data Replacement: #{input_name} Data Find Regex: Sample (\S+) Replacement: \1 The result will be:: ABC123 Data 1 ABC123 Data 2 ABC123 Data 3 Galaxy aggressively escapes input supplied to tools, so if something is not working please let us know and we can look into whether this is the cause. Also if you would like help constructing regular expressions for your inputs, please let us know at help@msi.umn.edu."
Remove beginning1	"What it does
 This tool removes a specified number of lines from the beginning of a dataset. ----- 
Example
 Input File:: chr7 56632 56652 D17003_CTCF_R6 310 + chr7 56736 56756 D17003_CTCF_R7 354 + chr7 56761 56781 D17003_CTCF_R4 220 + chr7 56772 56792 D17003_CTCF_R7 372 + chr7 56775 56795 D17003_CTCF_R4 207 + After removing the first 3 lines the dataset will look like this:: chr7 56772 56792 D17003_CTCF_R7 372 + chr7 56775 56795 D17003_CTCF_R4 207 +"
toolshed.g2.bx.psu.edu/repos/iuc/column_remove_by_header/column_remove_by_header/1.0	Removes or keeps columns based upon user provided values. Hint: If any of the column names you would like to specify contains special (non-ASCII) characters, you can specify these using their Unicode escape sequences.
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy3	"What it does
 This tool finds $ replaces text in an input dataset. .. class:: infomark The 
pattern to find
 can be a simple text string, or a perl 
regular expression
 string (depending on 
pattern is a regex
 check-box). .. class:: infomark When using regular expressions, the 
replace pattern
 can contain back-references ( e.g. \1 ) .. class:: infomark This tool uses 
Perl regular expression &lt;https://perldoc.perl.org/perlre&gt;
_ syntax. ----- 
Examples of 
regular-expression
 Find Patterns
 - 
HELLO
 The word 'HELLO' (case sensitive). - 
AG.T
 The letters A,G followed by any single character, followed by the letter T. - 
A{4,}
 Four or more consecutive A's. - 
chr2[012]\t
 The words 'chr20' or 'chr21' or 'chr22' followed by a tab character. - 
hsa-mir-([^ ]+)
 The text 'hsa-mir-' followed by one-or-more non-space characters. When using parenthesis, the matched content of the parenthesis can be accessed with 
\1
 in the 
replace
 pattern. 
Examples of Replace Patterns
 - 
WORLD
 The word 'WORLD' will be placed whereever the find pattern was found. - 
FOO-$&-BAR
 Each time the find pattern is found, it will be surrounded with 'FOO-' at the begining and '-BAR' at the end. 
$&
 (dollar-ampersand) represents the matched find pattern. - 
$1
 The text which matched the first parenthesis in the Find Pattern. ----- 
Example 1
 
Find Pattern:
 HELLO 
Replace Pattern:
 WORLD 
Regular Expression:
 no 
Replace what:
 entire line Every time the word HELLO is found, it will be replaced with the word WORLD. ----- 
Example 2
 
Find Pattern:
 ^chr 
Replace Pattern:
 (empty) 
Regular Expression:
 yes 
Replace what:
 column 11 If column 11 (of every line) begins with ther letters 'chr', they will be removed. Effectively, it'll turn ""chr4"" into ""4"" and ""chrXHet"" into ""XHet"" ----- 
Perl's Regular Expression Syntax
 The Find & Replace tool searches the data for lines containing or not containing a match to the given pattern. A Regular Expression is a pattern descibing a certain amount of text. - 
( ) { } [ ] . * ? + \ ^ $
 are all special characters. 
\
 can be used to ""escape"" a special character, allowing that special character to be searched for. - 
^
 matches the beginning of a string(but not an internal line). - 
(
 .. 
)
 groups a particular pattern. - 
{
 n or n, or n,m 
}
 specifies an expected number of repetitions of the preceding pattern. - 
{n}
 The preceding item is matched exactly n times. - 
{n,}
 The preceding item ismatched n or more times. - 
{n,m}
 The preceding item is matched at least n times but not more than m times. - 
[
 ... 
]
 creates a character class. Within the brackets, single characters can be placed. A dash (-) may be used to indicate a range such as 
a-z
. - 
.
* Matches any single character except a newline. - 
 The preceding item will be matched zero or more times. - 
?
 The preceding item is optional and matched at most once. - 
+
 The preceding item will be matched one or more times. - 
^
 has two meaning: - matches the beginning of a line or string. - indicates negation in a character class. For example, [^...] matches every character except the ones inside brackets. - 
$
 matches the end of a line or string. - 
\|
 Separates alternate possibilities. - 
\d
 matches a single digit - 
\w
 matches a single letter or digit or an underscore. - 
\s** matches a single white-space (space or tabs)."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_column/9.5+galaxy3	"What it does
 This tool performs find & replace operation on a specified column in a given file. .. class:: infomark The 
pattern to find
 uses the 
extended regular
 expression syntax (same as running 'awk --re-interval'). .. class:: infomark 
TIP:
 If you need more complex patterns, use the 
awk
 tool. ----- 
Examples of Find Patterns
 - 
HELLO
 The word 'HELLO' (case sensitive). - 
AG.T
 The letters A,G followed by any single character, followed by the letter T. - 
A{4,}
 Four or more consecutive A's. - 
chr2[012]\t
 The words 'chr20' or 'chr21' or 'chr22' followed by a tab character. - 
hsa-mir-([^ ]+)
 The text 'hsa-mir-' followed by one-or-more non-space characters. When using parenthesis, the matched content of the parenthesis can be accessed with 
\1
 in the 
replace
 pattern. 
Examples of Replace Patterns
 - 
WORLD
 The word 'WORLD' will be placed whereever the find pattern was found. - 
FOO-&-BAR
 Each time the find pattern is found, it will be surrounded with 'FOO-' at the begining and '-BAR' at the end. 
&
 (ampersand) represents the matched find pattern. - 
\1
 The text which matched the first parenthesis in the Find Pattern. ----- 
Example 1
 
Find Pattern:
 HELLO 
Replace Pattern:
 WORLD Every time the word HELLO is found, it will be replaced with the word WORLD. This operation affects only the selected column. ----- 
Example 2
 
Find Pattern:
 ^(.{4}) 
Replace Pattern:
 &\t Find the first four characters in each line, and replace them with the same text, followed by a tab character. In practice - this will split the first line into two columns. This operation affects only the selected column. ----- 
Extened Regular Expression Syntax
 The select tool searches the data for lines containing or not containing a match to the given pattern. A Regular Expression is a pattern descibing a certain amount of text. - 
( ) { } [ ] . * ? + \ ^ $
 are all special characters. 
\
 can be used to ""escape"" a special character, allowing that special character to be searched for. - 
^
 matches the beginning of a string(but not an internal line). - 
(
 .. 
)
 groups a particular pattern. - 
{
 n or n, or n,m 
}
 specifies an expected number of repetitions of the preceding pattern. - 
{n}
 The preceding item is matched exactly n times. - 
{n,}
 The preceding item ismatched n or more times. - 
{n,m}
 The preceding item is matched at least n times but not more than m times. - 
[
 ... 
]
 creates a character class. Within the brackets, single characters can be placed. A dash (-) may be used to indicate a range such as 
a-z
. - 
.
 Matches any single character except a newline. - 
*
 The preceding item will be matched zero or more times. - 
?
 The preceding item is optional and matched at most once. - 
+
 The preceding item will be matched one or more times. - 
^
 has two meaning: - matches the beginning of a line or string. - indicates negation in a character class. For example, [^...] matches every character except the ones inside brackets. - 
$
 matches the end of a line or string. - 
\|
 Separates alternate possibilities. 
Note
: AWK uses extended regular expression syntax, not Perl syntax. 
\d
, 
\w
, 
\s
 etc. are 
not** supported."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_line/9.5+galaxy3	"What it does
 This tool performs find & replace operation on a specified file. .. class:: infomark The 
pattern to find
 uses the 
extended regular
 expression syntax (same as running 'sed -r'). .. class:: infomark 
TIP:
 If you need more complex patterns, use the 
sed
 tool. ----- 
Examples of Find Patterns
 - 
HELLO
 The word 'HELLO' (case sensitive). - 
AG.T
 The letters A,G followed by any single character, followed by the letter T. - 
A{4,}
 Four or more consecutive A's. - 
chr2[012]\t
 The words 'chr20' or 'chr21' or 'chr22' followed by a tab character. - 
hsa-mir-([^ ]+)
 The text 'hsa-mir-' followed by one-or-more non-space characters. When using parenthesis, the matched content of the parenthesis can be accessed with 
\1
 in the 
replace
 pattern. 
Examples of Replace Patterns
 - 
WORLD
 The word 'WORLD' will be placed whereever the find pattern was found. - 
FOO-&-BAR
 Each time the find pattern is found, it will be surrounded with 'FOO-' at the beginning and '-BAR' at the end. 
&
 (ampersand) represents the matched find pattern. - 
\1
 The text which matched the first parenthesis in the Find Pattern. ----- 
Example 1
 
Find Pattern:
 HELLO 
Replace Pattern:
 WORLD Every time the word HELLO is found, it will be replaced with the word WORLD. ----- 
Example 2
 
Find Pattern:
 ^(.{4}) 
Replace Pattern:
 &\t Find the first four characters in each line, and replace them with the same text, followed by a tab character. In practice - this will split the first line into two columns. ----- 
Extended Regular Expression Syntax
 The select tool searches the data for lines containing or not containing a match to the given pattern. A Regular Expression is a pattern descibing a certain amount of text. - 
( ) { } [ ] . * ? + \ ^ $
 are all special characters. 
\
 can be used to ""escape"" a special character, allowing that special character to be searched for. - 
^
 matches the beginning of a string(but not an internal line). - 
(
 .. 
)
 groups a particular pattern. - 
{
 n or n, or n,m 
}
 specifies an expected number of repetitions of the preceding pattern. - 
{n}
 The preceding item is matched exactly n times. - 
{n,}
 The preceding item ismatched n or more times. - 
{n,m}
 The preceding item is matched at least n times but not more than m times. - 
[
 ... 
]
 creates a character class. Within the brackets, single characters can be placed. A dash (-) may be used to indicate a range such as 
a-z
. - 
.
 Matches any single character except a newline. - 
*
 The preceding item will be matched zero or more times. - 
?
 The preceding item is optional and matched at most once. - 
+
 The preceding item will be matched one or more times. - 
^
 has two meaning: - matches the beginning of a line or string. - indicates negation in a character class. For example, [^...] matches every character except the ones inside brackets. - 
$
 matches the end of a line or string. - 
\|
 Separates alternate possibilities. 
Note
: SED uses extended regular expression syntax, not Perl syntax. 
\d
, 
\w
, 
\s
 etc. are 
not** supported. However, you can use SED FAQ to perform commands using special characters. More complex options can look like 
sed -e '$!N;s/foo/bar/;'
. Here, 
$!N;
 is an optional part which you only need to set in very special cases. The 
foo
 part is the search string, and the 
bar
 part is the replacement string. Please read the SED FAQ here: https://www.pement.org/sed/sedfaq3.html#s3.2"
toolshed.g2.bx.psu.edu/repos/bgruening/replace_column_by_key_value_file/replace_column_with_key_value_file/0.2	"What it does
 This tool replaces the entries of a defined column with entries given by a replacement file. For example the replacement file holds the information of the naming scheme of ensembl annotated chromosomes in the frist column and in the second the UCSC annotation. A file which is having information about chromosomes in ensembl notation in column x can now be converted to a file which holds the same information but in UCSC annotation. A useful repository for ensembl and UCSC chromosomes mapping is: https://github.com/dpryan79/ChromosomeMappings"
toolshed.g2.bx.psu.edu/repos/iuc/sqlite_to_tabular/sqlite_to_tabular/3.2.1	"================= SQLite to Tabular ================= 
Inputs
 An existing SQLite_ data base. 
Outputs
 The results of a SQL query are output to the history as a tabular file. For help in using SQLite_ see: http://www.sqlite.org/docs.html 
NOTE:
 input for SQLite dates input field must be in the format: 
YYYY-MM-DD
 for example: 2015-09-30 See: http://www.sqlite.org/lang_datefunc.html 
Example
 Given 2 tabular datasets: 
customers
 and 
sales
 Dataset 
customers
 Table name: ""customers"" Column names: ""CustomerID,FirstName,LastName,Email,DOB,Phone"" =========== ========== ========== ===================== ========== ============ #CustomerID FirstName LastName Email DOB Phone =========== ========== ========== ===================== ========== ============ 1 John Smith John.Smith@yahoo.com 1968-02-04 626 222-2222 2 Steven Goldfish goldfish@fishhere.net 1974-04-04 323 455-4545 3 Paula Brown pb@herowndomain.org 1978-05-24 416 323-3232 4 James Smith jim@supergig.co.uk 1980-10-20 416 323-8888 =========== ========== ========== ===================== ========== ============ Dataset 
sales
 Table name: ""sales"" Column names: ""CustomerID,Date,SaleAmount"" ============= ============ ============ #CustomerID Date SaleAmount ============= ============ ============ 2 2004-05-06 100.22 1 2004-05-07 99.95 3 2004-05-07 122.95 3 2004-05-13 100.00 4 2004-05-22 555.55 ============= ============ ============ The query :: SELECT FirstName,LastName,sum(SaleAmount) as ""TotalSales"" FROM customers join sales on customers.CustomerID = sales.CustomerID GROUP BY customers.CustomerID ORDER BY TotalSales DESC; Produces this tabular output: ========== ======== ========== #FirstName LastName TotalSales ========== ======== ========== James Smith 555.55 Paula Brown 222.95 Steven Goldfish 100.22 John Smith 99.95 ========== ======== ========== If the optional Table name and Column names inputs are not used, the query would be: :: SELECT t1.c2 as ""FirstName"", t1.c3 as ""LastName"", sum(t2.c3) as ""TotalSales"" FROM t1 join t2 on t1.c1 = t2.c1 GROUP BY t1.c1 ORDER BY TotalSales DESC; You can selectively name columns, e.g. on the customers input you could just name columns 2,3, and 5: Column names: ,FirstName,LastName,,BirthDate Results in the following data base table =========== ========== ========== ===================== ========== ============ #c1 FirstName LastName c4 BirthDate c6 =========== ========== ========== ===================== ========== ============ 1 John Smith John.Smith@yahoo.com 1968-02-04 626 222-2222 2 Steven Goldfish goldfish@fishhere.net 1974-04-04 323 455-4545 3 Paula Brown pb@herowndomain.org 1978-05-24 416 323-3232 4 James Smith jim@supergig.co.uk 1980-10-20 416 323-8888 =========== ========== ========== ===================== ========== ============ Regular_expression_ functions are included for: :: matching: re_match('pattern',column) SELECT t1.FirstName, t1.LastName FROM t1 WHERE re_match('^.
.(net|org)$',c4) Results: =========== ========== #FirstName LastName =========== ========== Steven Goldfish Paula Brown =========== ========== :: searching: re_search('pattern',column) substituting: re_sub('pattern','replacement,column) SELECT t1.FirstName, t1.LastName, re_sub('^\d{2}(\d{2})-(\d\d)-(\d\d)','\3/\2/\1',BirthDate) as ""DOB"" FROM t1 WHERE re_search('[hp]er',c4) Results: =========== ========== ========== #FirstName LastName DOB =========== ========== ========== Steven Goldfish 04/04/74 Paula Brown 24/05/78 James Smith 20/10/80 =========== ========== ========== Math_ functions 
( python math library: https://docs.python.org/3.6/library/math.html )*: acos(x), acosh(x), asin(x), asinh(x), atan(x), atanh(x), atan2(x, y), ceil(x), cos(x), cosh(x), degrees(x), exp(x), expm1(x), fabs(x), floor(x), fmod(x, y), log(b,x), log(x), log10(x), log1p(x), log2(x), mod(x, y), pow(x, y), radians(x), sin(x), sinh(x), sqrt(x), tan(x), tanh(x), trunc(x) :: Query: SELECT SaleAmount, floor(SaleAmount) as ""dollars"" FROM sales Results: ============ ======== SaleAmount dollars ============ ======== 100.22 100 99.95 99 122.95 122 100.00 100 555.55 555 ============ ======== .. _Regular_expression: https://docs.python.org/3.9/library/re.html .. _Math: https://docs.python.org/3.9/library/math.html .. _SQLite: http://www.sqlite.org/index.html .. _SQLite_functions: http://www.sqlite.org/docs.html"
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy3	"What it does
 This tool runs the unix 
grep
 command on the selected data file. .. class:: infomark 
TIP:
 This tool uses the 
perl
 regular expression syntax (same as running 'grep -P'). This is 
NOT
 the POSIX or POSIX-extended syntax (unlike the awk/sed tools). 
Further reading
 - Wikipedia's Regular Expression page (http://en.wikipedia.org/wiki/Regular_expression) - Regular Expressions cheat-sheet (PDF) (http://www.addedbytes.com/cheat-sheets/download/regular-expressions-cheat-sheet-v2.pdf) - Grep Tutorial (http://www.panix.com/~elflord/unix/grep.html) ----- 
Grep Examples
 - 
AGC.AAT
 would match lines with AGC followed by any character, followed by AAT (e.g. 
AGCQAAT
, 
AGCPAAT
, 
AGCwAAT
) - 
C{2,5}AGC
 would match lines with 2 to 5 consecutive Cs followed by AGC - 
TTT.{4,10}AAA
 would match lines with 3 Ts, followed by 4 to 10 characters (any characeters), followed by 3 As. - 
^chr([0-9A-Za-z])+
 would match lines that begin with chromsomes, such as lines in a BED format file. - 
(ACGT){1,5}
 would match at least 1 ""ACGT"" and at most 5 ""ACGT"" consecutively. - 
hsa|mmu
 would match lines containing ""hsa"" or ""mmu"" (or both). ----- 
Regular Expression Syntax
 The select tool searches the data for lines containing or not containing a match to the given pattern. A Regular Expression is a pattern descibing a certain amount of text. - 
( ) { } [ ] . * ? + \ ^ $
 are all special characters. 
\
 can be used to ""escape"" a special character, allowing that special character to be searched for. - 
^
 matches the beginning of a string(but not an internal line). - 
\d
 matches a digit, same as [0-9]. - 
\D
 matches a non-digit. - 
\s
 matches a whitespace character. - 
\S
 matches anything BUT a whitespace. - 
\t
 matches a tab. - 
\w
 matches an alphanumeric character ( A to Z, 0 to 9 and underscore ) - 
\W
 matches anything but an alphanumeric character. - 
(
 .. 
)
 groups a particular pattern. - 
\Z
 matches the end of a string(but not a internal line). - 
{
 n or n, or n,m 
}
 specifies an expected number of repetitions of the preceding pattern. - 
{n}
 The preceding item is matched exactly n times. - 
{n,}
 The preceding item ismatched n or more times. - 
{n,m}
 The preceding item is matched at least n times but not more than m times. - 
[
 ... 
]
 creates a character class. Within the brackets, single characters can be placed. A dash (-) may be used to indicate a range such as 
a-z
. - 
.
 Matches any single character except a newline. - 
*
 The preceding item will be matched zero or more times. - 
?
 The preceding item is optional and matched at most once. - 
+
 The preceding item will be matched one or more times. - 
^
 has two meaning: - matches the beginning of a line or string. - indicates negation in a character class. For example, [^...] matches every character except the ones inside brackets. - 
$
 matches the end of a line or string. - 
\|** Separates alternate possibilities."
secure_hash_message_digest	"What it does
 This tool outputs Secure Hashes / Message Digests of a dataset using the user selected algorithms. ------ 
Citation
 If you use this tool in Galaxy, please cite Blankenberg D, et al. 
In preparation."
Show beginning1	"What it does
 This tool outputs specified number of lines from the 
beginning
 of a dataset ----- 
Example
 Selecting 2 lines from this:: chr7 56632 56652 D17003_CTCF_R6 310 + chr7 56736 56756 D17003_CTCF_R7 354 + chr7 56761 56781 D17003_CTCF_R4 220 + chr7 56772 56792 D17003_CTCF_R7 372 + chr7 56775 56795 D17003_CTCF_R4 207 + will produce:: chr7 56632 56652 D17003_CTCF_R6 310 + chr7 56736 56756 D17003_CTCF_R7 354 +"
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_head_tool/9.5+galaxy3	"What it does
 This tool outputs specified number of lines from the 
beginning
 of a dataset ----- 
Example
 Selecting 2 lines from this:: chr7 56632 56652 D17003_CTCF_R6 310 + chr7 56736 56756 D17003_CTCF_R7 354 + chr7 56761 56781 D17003_CTCF_R4 220 + chr7 56772 56792 D17003_CTCF_R7 372 + chr7 56775 56795 D17003_CTCF_R4 207 + will produce:: chr7 56632 56652 D17003_CTCF_R6 310 + chr7 56736 56756 D17003_CTCF_R7 354 +"
Show tail1	"What it does
 This tool outputs specified number of lines from the 
end
 of a dataset ----- 
Example
 - Input File:: chr7 57134 57154 D17003_CTCF_R7 356 - chr7 57247 57267 D17003_CTCF_R4 207 + chr7 57314 57334 D17003_CTCF_R5 269 + chr7 57341 57361 D17003_CTCF_R7 375 + chr7 57457 57477 D17003_CTCF_R3 188 + - Show last two lines of above file. The result is:: chr7 57341 57361 D17003_CTCF_R7 375 + chr7 57457 57477 D17003_CTCF_R3 188 +"
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_tail_tool/9.5+galaxy3	"What it does
 This tool outputs specified number of lines from the 
end
 of a dataset ----- 
Example
 - Input File:: chr7 57134 57154 D17003_CTCF_R7 356 - chr7 57247 57267 D17003_CTCF_R4 207 + chr7 57314 57334 D17003_CTCF_R5 269 + chr7 57341 57361 D17003_CTCF_R7 375 + chr7 57457 57477 D17003_CTCF_R3 188 + - Show last two lines of above file. The result is:: chr7 57341 57361 D17003_CTCF_R7 375 + chr7 57457 57477 D17003_CTCF_R3 188 +"
random_lines1	"What it does
 This tool selects N random lines from a file, with no repeats, and preserving ordering. ----- 
Example
 Input File:: chr7 56632 56652 D17003_CTCF_R6 310 + chr7 56736 56756 D17003_CTCF_R7 354 + chr7 56761 56781 D17003_CTCF_R4 220 + chr7 56772 56792 D17003_CTCF_R7 372 + chr7 56775 56795 D17003_CTCF_R4 207 + Selecting 2 random lines might return this:: chr7 56736 56756 D17003_CTCF_R7 354 + chr7 56775 56795 D17003_CTCF_R4 207 +"
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sort_header_tool/9.5+galaxy3	"What it does
 This tool sorts an input file. ----- 
Sorting Styles
 * 
Fast Numeric
: sort by numeric values. Handles integer values (e.g. 43, 134) and decimal-point values (e.g. 3.14). 
Does not
 handle scientific notation (e.g. -2.32e2). * 
General Numeric
: sort by numeric values. Handles all numeric notations (including scientific notation). Slower than 
fast numeric
, so use only when necessary. * 
Natural Sort
: Sort in 'natural' order (natural to humans, not to computers). See example below. * 
Alphabetical sort
: Sort in strict alphabetical order. See example below. * 
Human-readable numbers
: Sort human readble numbers (e.g. 1G > 2M > 3K > 400) * 
Random order
: return lines in random order. ------ 
Example - Header line
 
Input file
 (note first line is a header line, should not be sorted):: Fruit Color Price Banana Yellow 4.1 Avocado Green 8.0 Apple Red 3.0 Melon Green 6.1 
Sorting
 by 
numeric order
 on column 
3
, with 
header
, will return:: Fruit Color Price Apple Red 3.0 Banana Yellow 4.1 Melon Green 6.1 Avocado Green 8.0 ----- 
Example - Natural vs. Alphabetical sorting
 Given the following list:: chr4 chr13 chr1 chr10 chr20 chr2 
Alphabetical sort
 would produce the following sorted list:: chr1 chr10 chr13 chr2 chr20 chr4 
Natural Sort
 would produce the following sorted list:: chr1 chr2 chr4 chr10 chr13 chr20 .. class:: infomark If you're planning to use the file with another tool that expected sorted files (such as 
join
), you should use the 
Alphabetical sort
, not the 
Natural Sort
. Natural sort order is easier for humans, but is unnatural for computer programs. ----- 
Example - Sorting based on parts of column values
 The above column of chromosomes, with their constant prefix, could have been sorted in natural order also with the 
Fast numeric sort
 and 
considering its characters from
 character 4 only. In general, sorting based on just a range of characters in a column can be useful for sorting values with internal structure, in a single tool run. Consider, for example, the following column of dates, which is unfortunately not ISO-8601 formatted:: 10/24/2025 09/18/1974 12/16/1998 03/04/2007 You could modify these values with other tools first, but you can achieve correct chronological sort order with a single run of the sort tool like this: - Do a 
Fast numeric sort
 on the column 
considering its characters from
 character 7 (the start of the year). - Resolve ties (using another column selection section) with another 
Fast numeric sort
 on the same column 
considering its characters from
 character 1 
to and including
 character 2 (the month representation). - Resolve remaining ties with a third 
Fast numeric sort
 on again the same column 
considering its characters from
 character 4 
to and including
 character 5 (the day representation). This will result in the ascending chronological order:: 09/18/1974 11/17/1998 11/18/1998 12/16/1998 03/04/2007 10/24/2025 Before relying on in-column character ranges, make extra sure that all values are formatted consistently (in the above example, that all dates use two digits for days and months and the same overall date format)."
toolshed.g2.bx.psu.edu/repos/iuc/column_order_header_sort/column_order_header_sort/0.0.1	Reorders a file's columns by sorted value of header fields. Specify the optional Identifier column parameter to make a column left-most; generally used for a Key column that should not be sorted within the other columns.
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sort_rows/9.5+galaxy3	".. class:: infomark 
TIP:
 If your data is not TAB delimited, use 
Text Manipulation->Convert
 
What it does
 That tool sorts each row in a TAB separated file, according to their columns. In other words: It is a sorted reordering of all columns."
toolshed.g2.bx.psu.edu/repos/bgruening/split_file_on_column/tp_split_on_column/0.6	"======== Synopsis ======== Given a single input dataset this tool splits the file on unique values from a specified column. =========== Description =========== This tool splits a file into a collection based on unique values of a speific column. It performs a grouping operation with every group saved as a separate collection element. You have the option to include the header (first line) to all splits. If you have a header and don't want keep it, please remove it before you use this tool. For example with the ""Remove beginning of a file"" tool. ----- 
Example
 Splitting this file on column 1:: chr1 10 20 chr1 30 40 chr2 40 70 chr4 60 80 will produce a collection with 3 elements:: chr1 10 20 chr1 30 40 chr2 40 70 chr4 60 80 ------ .. image:: $PATH_TO_IMAGES/split_by_group.svg :width: 800 :alt: Split by group"
toolshed.g2.bx.psu.edu/repos/iuc/table_compute/table_compute/1.2.4+galaxy2	"Table Compute ------------- This tool is a Galaxy wrapper for the 
Pandas Data Analysis Library &lt;https://pandas.pydata.org/&gt;
 in Python, for manipulating and computing expressions upon tabular data and matrices. It can perform functions on the element, row, and column basis, as well as sub-select, duplicate, replace, and perform general and custom expressions on rows, columns, and elements. .. class:: infomark Only a single operation can be performed on the data. Multiple operations can be performed by chaining successive runs of this tool. This is to provide a more transparent workflow for complex operations. Many of the examples given below relate to common research use-cases such as filtering large matrices for specific values, counting unique instances of elements, conditionally manipulating the data, and replacing unwanted values. Full table operations such as normalisation can be easily performed by scaling the data via mean/median/min/max (and many other) metrics, and general expressions can even be computed across multiple tables. Examples ======== Example 1: Sub-selecting from a table ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We have the following table: === === === === . c1 c2 c3 === === === === g1 10 20 30 g2 3 6 9 g3 4 8 12 g4 81 6 3 === === === === and we want to duplicate c1 and remove c2. Also select g1 to g3 and add g2 at the end as well. This would result in the output table: === === === === . c1 c1 c3 === === === === g1 10 10 30 g2 3 3 9 g3 4 4 12 g2 3 3 9 === === === === In Galaxy we would select the following: * 
Input Single or Multiple Tables
 → 
Single Table
 * 
Column names on first row?
 → 
Yes
 * 
Row names on first column?
 → 
Yes
 * 
Type of table operation
 → 
Drop, keep or duplicate rows and columns
 * 
List of columns to select
 → 
1,1,3
 * 
List of rows to select
 → 
1:3,2
 * 
Keep duplicate columns
 → 
Yes
 * 
Keep duplicate rows
 → 
Yes
 Example 2: Filter for rows with row sums less than 50 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We have the following table: === === === === . c1 c2 c3 === === === === g1 10 20 30 g2 3 6 9 g3 4 8 12 g4 81 6 3 === === === === and we want: === === === === . c1 c2 c3 === === === === g2 3 6 9 g3 4 8 12 === === === === In Galaxy we would select the following: * 
Input Single or Multiple Tables
 → 
Single Table
 * 
Column names on first row?
 → 
Yes
 * 
Row names on first column?
 → 
Yes
 * 
Type of table operation
 → 
Filter rows or columns by their properties
 * 
Filter
 → 
Rows
 * 
Filter Criterion
 → 
Result of function applied to columns/rows
 * 
Keep column/row if its observed
 → 
Sum
 * 
is
 → 
< (Less Than)
 * 
this value
 → 
50
 Example 3: Count the number of values per row smaller than a specified value ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We have the following table: === === === === . c1 c2 c3 === === === === g1 10 20 30 g2 3 6 9 g3 4 8 12 g4 81 6 3 === === === === and we want to count how many elements in each row are smaller than 10, i.e., we want to obtain the following results table: === === . vec === === g1 0 g2 3 g3 2 g4 2 === === In Galaxy we would select the following: * 
Input Single or Multiple Tables
 → 
Single Table
 * 
Column names on first row?
 → 
Yes
 * 
Row names on first column?
 → 
Yes
 * 
Type of table operation
 → 
Manipulate selected table elements
 * 
Operation to perform
 → 
Custom
 * 
Custom Expression on 'elem'
 → 
elem &lt; 10
 * 
Operate on elements
 → 
All
 
Note:
 
There are actually simpler ways to achieve our purpose, but here we are demonstrating the use of a custom expression.
 After executing, we would then be presented with a table like so: === ===== ===== ===== . c1 c2 c3 === ===== ===== ===== g1 False False False g2 True True True g3 True True False g4 False True True === ===== ===== ===== To get to our desired table, we would then process this table with the tool again: * 
Input Single or Multiple Tables
 → 
Single Table
 * 
Column names on first row?
 → 
Yes
 * 
Row names on first column?
 → 
Yes
 * 
Type of table operation
 → 
Compute Expression across Rows or Columns
 * 
Calculate
 → 
Sum
 * 
For each
 → 
Row
 Executing this will sum all the 'True' values in each row. Note that the values must have no extra whitespace in them for this to work (e.g. 'True ' or ' True' will not be parsed correctly). Example 4: Perform a scaled log-transformation conditionally ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We want to perform a scaled log transformation on all values greater than 5, and set all other values to 1. We have the following table: === === === === . c1 c2 c3 === === === === g1 0 20 30 g2 3 0 9 g3 4 8 0 g4 81 0 0 === === === === and we want: === ========== ========= ========= . c1 c2 c3 === ========== ========= ========= g1 1.00000000 0.1497866 0.1133732 g2 1.00000000 1.0000000 0.2441361 g3 1.00000000 0.2599302 1.0000000 g4 0.05425246 1.0000000 1.0000000 === ========== ========= ========= In Galaxy we would select the following: * 
Input Single or Multiple Tables
 → 
Single Table
 * 
Column names on first row?
 → 
Yes
 * 
Row names on first column?
 → 
Yes
 * 
Type of table operation
 → 
Manipulate selected table elements
 * 
Operation to perform
 → 
Custom
 * 
Custom Expression
 → 
(math.log(elem) / elem) if (elem &gt; 5) else 1
 * 
Operate on elements
 → 
All
 Example 5: Perform a Full table operation ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We have the following table: === === === === . c1 c2 c3 === === === === g1 10 20 30 g2 3 10 9 g3 4 8 10 g4 81 10 10 === === === === and we want to subtract from each column the mean of that column divided by the standard deviation of it to yield: === ========= ========= ========= . c1 c2 c3 === ========= ========= ========= g1 9.351737 17.784353 28.550737 g2 2.351737 7.784353 7.550737 g3 3.351737 5.784353 8.550737 g4 80.351737 7.784353 8.550737 === ========= ========= ========= In Galaxy we would select the following: * 
Input Single or Multiple Tables
 → 
Single Table
 * 
Column names on first row?
 → 
Yes
 * 
Row names on first column?
 → 
Yes
 * 
Type of table operation
 → 
Perform a Full Table Operation
 * 
Operation
 → 
Custom
 * 
Custom Expression on 'table' along axis (0 or 1)
 → 
table - table.mean(0)/table.std(0)
 Example 6: Perform operations on multiple tables ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We have the following three input tables: Table 1 === === === === . c1 c2 c3 === === === === g1 10 20 30 g2 3 10 9 g3 4 8 10 === === === === Table 2 === === === . c1 c2 === === === g1 1 2 g2 3 4 g3 6 5 === === === Table 3 === === === === . c1 c2 c3 === === === === g1 1 2 3 g2 1 2 3 === === === === 
Note that the dimensions of these tables do not match.
 Dimensions: * Table1 [3,3] * Table2 [3,2] * Table3 [2,3] In order to perform simple operations between Tables, they must be of the same dimensions. To add Table2 to Table3 we would have to transpose one of the tables using the in-built 
T
 method:: table2 + table3.T or:: table2.T + table3 We can also perform more general operations using all 3 tables, such as taking the minimum value of the maximum values of Table2 and Table3, and dividing the Table1 values by it:: table1 / min(table2.values.max(), table3.values.max()) To perform these types of operations in Galaxy we would select the following: * 
Input Single or Multiple Tables
 → 
Multiple Tables
 * 
(For each inserted table)
 * 
Column names on first row?
 → 
Yes
 * 
Row names on first column?
 → 
Yes
 * 
Custom Expression
 → :: <insert your desired function> Please note that the last example shown above was chosen to illustrate the limitations of the tool. Nested attributes like 
table2.values.max
 are disallowed in expressions in the tool so the above would have to be replaced with the harder to read workaround:: table1 / min(np.max(np.max(table2)), np.max(np.max(table3))) .. class:: infomark Complex operations (like ones that would benefit from specifying nested attributes) can often be broken into subsequent runs ot the tool, in which the first run generates an intermediate table representing the result of the ""inner"" operation that the second run can then use as input to perform the ""outer"" operation. Also note that, currently 
min()
, 
max()
 and 
sum()
 are the only built-in Python functions that can be used inside expressions. If you want to use additional functions, these have to be qualified functions from the 
math
, 
np
 or 
pd
 libraries. Example 7: Melt ~~~~~~~~~~~~~~~ We have the following table === === === === . A B C === === === === 0 a B 1 1 b B 3 2 c B 5 === === === === and we want: === === ======== ===== . A variable value === === ======== ===== 0 a B B 1 b B B 2 c B B 3 a C 1 4 b C 3 5 c C 5 === === ======== ===== In Galaxy we would select the following: * 
Input Single or Multiple Tables
 → 
Single Table
 * 
Column names on first row?
 → 
Yes
 * 
Row names on first column?
 → 
Yes
 * 
Type of table operation
 → 
Perform a Full Table Operation
 * 
Operation
 → 
Melt
 * 
Variable IDs
 → 
A
 * 
Unpivoted IDs
 → 
B,C
 This converts the ""B"" and ""C"" columns into variables. Example 8: Pivot ~~~~~~~~~~~~~~~~ We have the following table === === === === === . foo bar baz zoo === === === === === 0 one A 1 x 1 one B 2 y 2 one C 3 z 3 two A 4 q 4 two B 5 w 5 two C 6 t === === === === === and we want: === === === === . A B C === === === === one 1 2 3 two 4 5 6 === === === === In Galaxy we would select the following: * 
Input Single or Multiple Tables
 → 
Single Table
 * 
Column names on first row?
 → 
Yes
 * 
Row names on first column?
 → 
Yes
 * 
Type of table operation
 → 
Perform a Full Table Operation
 * 
Operation
 → 
Pivot
 * 
Index
 → 
foo
 * 
Column
 → 
bar
 * 
Values
 → 
baz
 This splits the matrix using ""foo"" and ""bar"" using only the values from ""baz"". Header values may contain extra information. Example 9: Replacing text in specific rows or columns ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We have the following table === === === === . c1 c2 c3 === === === === g1 10 20 30 g2 3 3 9 g3 4 8 12 g4 81 6 3 === === === === and we want to add ""chr"" to the elements in column 2 AND rows 2 and 4: === === ==== === . c1 c2 c3 === === ==== === g1 10 20 30 g2 3 chr3 9 g3 4 8 12 g4 81 chr6 3 === === ==== === In Galaxy we would select the following: * 
Input Single or Multiple Tables
 → 
Single Table
 * 
Column names on first row?
 → 
Yes
 * 
Row names on first column?
 → 
Yes
 * 
Type of table operation
 → 
Manipulate selected table elements
 * 
Operation to perform
 → 
Replace values
 * 
Replacement value
 → 
chr{elem:.0f}
 Here, the placeholder 
{elem}
 lets us refer to each element's current value, while the 
:.0f
 part is a format specifier that makes sure numbers are printed without decimals (for a complete description of the available syntax see the 
Python Format Specification Mini-Language &lt;https://docs.python.org/3/library/string.html#formatspec&gt;
). * 
Operate on elements
 → 
Specific Rows and/or Columns
 * 
List of columns to select
 → 
2
 * 
List of rows to select
 → 
2,4
 * 
Inclusive Selection
 → 
No
 If we wanted to instead add ""chr"" to the ALL elements in column 2 and rows 2 and 4, we would repeat the steps above but set the 
Inclusive Selection
 to ""Yes"", to give: === ===== ===== ===== . c1 c2 c3 === ===== ===== ===== g1 10 chr20 30 g2 chr3 chr3 chr9 g3 4 8 12 g4 chr81 chr6 chr3 === ===== ===== ===== Example 10: Pivot Table with unified Aggregator ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ For an input table of: === === ===== === === A B C D E === === ===== === === foo one small 1 2 foo one large 2 4 foo one large 2 5 foo two small 3 5 foo two small 3 6 bar one large 4 6 bar one small 5 8 bar two small 6 9 bar two large 7 9 === === ===== === === we wish to pivot the table with the 'A' column as the new row index and use the values of the column 'C' as the new column indexes, based on the aggregated values of 'D'. By default the aggregator function is the mean, but here we will instead pick the max, to yield: === == == C l s A === == == bar 7 6 foo 2 3 === == == In Galaxy we would select the following: * 
Input Single or Multiple Tables
 → 
Single Table
 * 
Column names on first row?
 → 
Yes
 * 
Row names on first column?
 → 
Yes
 * 
Type of table operation
 → 
Perform a Full Table Operation
 * 
Operation
 → 
Pivot
 * 
Index
 → 
A
 * 
Column
 → 
C
 * 
Values
 → 
D
 * 
Aggregator Function
 → 
max
 Example 11: Pivot Table with custom Aggregrator ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ For an input table of: ==== ========== =========== === ======== Name Position City Age Random ==== ========== =========== === ======== Mary Manager Boston 34 0.678577 Josh Programmer New York 37 0.973168 Jon Manager Chicago 29 0.146668 Lucy Manager Los Angeles 40 0.150120 Jane Programmer Chicago 29 0.112769 Sue Programmer Boston 31 0.185198 ==== ========== =========== === ======== we wish to pivot the table with the 'Position' column as the new index and transform the 'Age' and 'Random' columns to have mean and standard deviation values ========== ========= ======== ======== Position Age Random Random . mean mean std ========== ========= ======== ======== Manager 34.333333 0.325122 0.306106 Programmer 32.333333 0.423712 0.477219 ========== ========= ======== ======== In Galaxy we would select the following: * 
Input Single or Multiple Tables
 → 
Single Table
 * 
Column names on first row?
 → 
Yes
 * 
Row names on first column?
 → 
Yes
 * 
Type of table operation
 → 
Perform a Full Table Operation
 * 
Operation
 → 
Pivot
 * 
Index
 → 
Position
 * 
Column-Function Mapping
 * 
Value Column
 → 
Age
 * 
Function
 → 
mean
 * 
Value Column
 → 
Random
 * 
Function
 → 
mean
 * 
Function
 → 
std
 This splits the matrix using ""foo"" and ""bar"" using only the values from ""baz"". Header values may contain extra information."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy3	"What it does
 This tool runs the unix 
awk
 command on the selected data file. .. class:: infomark 
TIP:
 This tool uses the 
extended regular
 expression syntax (not the perl syntax). 
\d
, 
\w
, 
\s
 etc. are 
not
 supported. 
Further reading
 - Awk by Example (http://www.ibm.com/developerworks/linux/library/l-awk1/index.html) - Long AWK tutorial (http://www.grymoire.com/Unix/Awk.html) ----- 
AWK programs
 Most AWK programs consist of 
patterns
 (i.e. rules that match lines of text) and 
actions
 (i.e. commands to execute when a pattern matches a line). The basic form of AWK program is:: pattern { action 1; action 2; action 3; } 
Variables
 In order to allow parametrization in workflows, the tool allows to specify values for variables that can be used in AWK program. that will be named 
VAR1
, 
VAR2
, ... 
Pattern Examples
 - 
$2 == ""chr3""
 will match lines whose second column is the string 'chr3' - 
$5-$4>23
 will match lines that after subtracting the value of the fourth column from the value of the fifth column, gives value alrger than 23. - 
/AG..AG/
 will match lines that contain the regular expression 
AG..AG
 (meaning the characeters AG followed by any two characeters followed by AG). (This is the way to specify regular expressions on the entire line, similar to GREP.) - 
$7 ~ /A{4}U/
 will match lines whose seventh column contains 4 consecutive A's followed by a U. (This is the way to specify regular expressions on a specific field.) - 
10000 < $4 && $4 < 20000
 will match lines whose fourth column value is larger than 10,000 but smaller than 20,000 - 
BEGIN
 will be executed once only, before the first input record is read. - If no pattern is specified, all lines match (meaning the 
action
 part will be executed on all lines). 
Action Examples
 - 
{ print }
 or 
{ print $0 }
 will print the entire input line (the line that matched in 
pattern
). 
$0
 is a special marker meaning 'the entire line'. - 
{ print $1, $4, $5 }
 will print only the first, fourth and fifth fields of the input line. - 
{ print $4, $5-$4 }
 will print the fourth column and the difference between the fifth and fourth column. (If the fourth column was start-position in the input file, and the fifth column was end-position - the output file will contain the start-position, and the length). - 
{ FS = "","" }
 can be used to change the field separator (delimeter) for parsing the input file. - If no action part is specified (not even the curly brackets) - the default action is to print the entire line. 
AWK's Regular Expression Syntax
 The select tool searches the data for lines containing or not containing a match to the given pattern. A Regular Expression is a pattern descibing a certain amount of text. - 
( ) { } [ ] . * ? + \ ^ $
 are all special characters. 
\
 can be used to ""escape"" a special character, allowing that special character to be searched for. - 
^
 matches the beginning of a string(but not an internal line). - 
(
 .. 
)
 groups a particular pattern. - 
{
 n or n, or n,m 
}
 specifies an expected number of repetitions of the preceding pattern. - 
{n}
 The preceding item is matched exactly n times. - 
{n,}
 The preceding item ismatched n or more times. - 
{n,m}
 The preceding item is matched at least n times but not more than m times. - 
[
 ... 
]
 creates a character class. Within the brackets, single characters can be placed. A dash (-) may be used to indicate a range such as 
a-z
. - 
.
 Matches any single character except a newline. - 
*
 The preceding item will be matched zero or more times. - 
?
 The preceding item is optional and matched at most once. - 
+
 The preceding item will be matched one or more times. - 
^
 has two meaning: - matches the beginning of a line or string. - indicates negation in a character class. For example, [^...] matches every character except the ones inside brackets. - 
$
 matches the end of a line or string. - 
\|** Separates alternate possibilities."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sed_tool/9.5+galaxy3	"What it does
 This tool runs the unix 
sed
 command on the selected data file. .. class:: infomark 
TIP:
 This tool uses the 
extended regular
 expression syntax (same as running 'sed -r'). 
Further reading
 - Short sed tutorial (http://www.linuxhowtos.org/System/sed_tutorial.htm) - Long sed tutorial (http://www.grymoire.com/Unix/Sed.html) - sed faq with good examples (https://www.pement.org/sed/sedfaq.html) - sed cheat-sheet (http://www.catonmat.net/download/sed.stream.editor.cheat.sheet.pdf) ----- 
Sed commands
 The most useful sed command is 
s
 (substitute). 
Examples
 - 
s/hsa//
 will remove the first instance of 'hsa' in every line. - 
s/hsa//g
 will remove all instances (beacuse of the 
g
) of 'hsa' in every line. - 
s/A{4,}/--&--/g
 will find sequences of 4 or more consecutive A's, and once found, will surround them with two dashes from each side. The 
&
 marker is a place holder for 'whatever matched the regular expression'. - 
s/hsa-mir-([^ ]+)/short name: \1 full name: &/
 will find strings such as 'hsa-mir-43a' (the regular expression is 'hsa-mir-' followed by non-space characters) and will replace it will string such as 'short name: 43a full name: hsa-mir-43a'. The 
\1
 marker is a place holder for 'whatever matched the first parenthesis' (similar to perl's 
$1
) . 
sed's Regular Expression Syntax
 The select tool searches the data for lines containing or not containing a match to the given pattern. A Regular Expression is a pattern descibing a certain amount of text. - 
( ) { } [ ] . * ? + \ ^ $
 are all special characters. 
\
 can be used to ""escape"" a special character, allowing that special character to be searched for. - 
^
 matches the beginning of a string(but not an internal line). - 
(
 .. 
)
 groups a particular pattern. - 
{
 n or n, or n,m 
}
 specifies an expected number of repetitions of the preceding pattern. - 
{n}
 The preceding item is matched exactly n times. - 
{n,}
 The preceding item ismatched n or more times. - 
{n,m}
 The preceding item is matched at least n times but not more than m times. - 
[
 ... 
]
 creates a character class. Within the brackets, single characters can be placed. A dash (-) may be used to indicate a range such as 
a-z
. - 
.
 Matches any single character except a newline. - 
*
 The preceding item will be matched zero or more times. - 
?
 The preceding item is optional and matched at most once. - 
+
 The preceding item will be matched one or more times. - 
^
 has two meaning: - matches the beginning of a line or string. - indicates negation in a character class. For example, [^...] matches every character except the ones inside brackets. - 
$
 matches the end of a line or string. - 
\|
 Separates alternate possibilities. 
Note
: SED uses extended regular expression syntax, not Perl syntax. 
\d
, 
\w
, 
\s
 etc. are 
not** supported."
trimmer	"What it does
 Trims specified number of characters from a dataset or a given column (if dataset is tab-delimited). ----- 
Example 1
 Trimming this dataset:: 1234567890 abcdefghijk by setting 
Trim from the beginning up to this position
 to 
2
 and 
Remove everything from this position to the end
 to 
6
 will produce:: 23456 bcdef ----- 
Example 2
 Trimming column 2 of this dataset:: abcde 12345 fghij 67890 fghij 67890 abcde 12345 by setting 
Trim content of this column only
 to 
2
, 
Trim from the beginning up to this position
 to 
2
, and 
Remove everything from this position to the end
 to 
4
 will produce:: abcde 234 fghij 67890 fghij 789 abcde 12345 ----- 
Example 3
 Trimming column 2 of this dataset:: abcde 12345 fghij 67890 fghij 67890 abcde 12345 by setting 
Trim content of this column only
 to 
2
, 
Trim from the beginning up to this position
 to 
2
, and 
Remove everything from this position to the end
 to 
-2
 will produce:: abcde 23 fghij 67890 fghij 78 abcde 12345 ---- 
Trimming FASTQ datasets
 This tool can be used to trim sequences and quality strings in FASTQ datasets. This is done by selected 
Yes
 from the 
Is input dataset in FASTQ format?
 dropdown. If set to 
Yes
, the tool will skip all even numbered lines (see warning below). For example, trimming last 5 bases of this dataset:: @081017-and-081020:1:1:1715:1759 GGACTCAGATAGTAATCCACGCTCCTTTAAAATATC + II#IIIIIII$5+.(9IIIIIII$%
$G$A31I&&B can be done by setting 
Remove everything from this position to the end
 to 31:: @081017-and-081020:1:1:1715:1759 GGACTCAGATAGTAATCCACGCTCCTTTAAA + II#IIIIIII$5+.(9IIIIIII$%
$G$A3 
Note
 that headers are skipped. .. class:: warningmark 
WARNING:
 This tool will only work on properly formatted FASTQ datasets where (1) each read and quality string occupy one line and (2) '@' (read header) and ""+"" (quality header) lines are evenly numbered like in the above example."
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_unfold_column_tool/9.5+galaxy3	"What it does
 This tool will unfold one column of your input dataset. ----- Input Example:: a b 1,2,3,4,5 c Output Example:: a b 1 c a b 2 c a b 3 c a b 4 c a b 5 c"
toolshed.g2.bx.psu.edu/repos/bgruening/uniprot_rest_interface/uniprot/0.7	".. class:: infomark 
What it does
 This tool provides access to the UniProt API. You can retrieve sequence informations given a list of sequence identifiers or map identifiers between different databases. Hence, this tool offers you two modes: 
map
 and 
retrieve
. ----- 
INPUT
 The input is a list of IDs. 
Example
:: Q0P8A9 A0A077ZHN8 A0A077ZFY8 M5B8V9 M5BAG7 S0DS17 .... ----- 
MAP OUTPUT EXAMPLES
 FROM refseq TO embl:: From To NM_130786 A1BG_HUMAN NM_130786 V9HWD8_HUMAN NM_001087 A0A024R410_HUMAN NM_001087 AAMP_HUMAN FROM uniprot TO genename:: From To Q0P8A9 fdhC A0A077ZHN8 TTRE_0000819801 A0A077ZFY8 TTRE_0000758701 M5B8V9 CMN_01519 M5BAG7 cydC S0DS17 FFUJ_00006 A0A077Z587 TTRE_0000309301 Q13685 AAMP O14639 ABLIM1 ----- 
RETRIEVE OUTPUT EXAMPLES
 retrieve gff:: #gff-version 3 #sequence-region S0DS17 1 369 #sequence-region M5BAG7 1 563 #sequence-region A0A077Z587 1 772 #sequence-region A0A077ZFY8 1 973 #sequence-region O14639 1 778 O14639 UniProtKB Chain 1 778 . . . ID=PRO_0000075697;Note=Actin-binding LIM protein 1 O14639 UniProtKB Domain 97 156 . . . Note=LIM zinc-binding 1;evidence=ECO:0000255|PROSITE-ProRule:PRU00125 O14639 UniProtKB Domain 156 216 . . . Note=LIM zinc-binding 2;evidence=ECO:0000255|PROSITE-ProRule:PRU00125 O14639 Un... retrieve fasta:: >tr|S0DS17|S0DS17_GIBF5 Related to cytochrom P450 OS=Gibberella fujikuroi (strain CBS 195.34 / IMI 58289 / NRRL A-6831) GN=FFUJ_00006 PE=3 SV=1 MSYQSILLRQVNSLCDNLEEVARDENGGLIDMAMQSDYFTFDVMSEVIFGMAYNALKDTS YRFVTGALGSSNIRIGTLVQSPLPAMCRIDKY... >tr|M5BAG7|M5BAG7_9MICO ABC transporter, fused permease/ABC transporter involved in the biosynthesis of cytochrom bd, fused permease/ATP-binding protein OS=Clavibacter michiganensis subsp. nebraskensis NCPPB 2581 GN=cydC PE=3 SV=1 MNRDGVLRLAQPPTRRTLPGLLAGLASAVGAVALLATSAWLITRASEQPPILFLGMAIVG VRAFALGRAAFRYLERITSHDAAFRALATLRV... >tr|A0A077Z587|A0A077Z587_TRITR Kelch 3 and Kelch 4 and Cytochrom B561 domain con taining protein OS=Trichuris trichiura GN=TTRE_0000309301 PE=4 SV=1 MGSQQAADETQKVVERIILNINVRKDKRSFGLGIKIKKGNVFVSSIRPGSIAEDHFKLYD VIKDVNGSRIDSRELCRDLIRTHKVLTV... ----- This tool is based on the work 
Jan Rudolph
_ and the UniProt API. .. _Jan Rudolph: https://github.com/jdrudolph/uniprot"
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sorted_uniq/9.5+galaxy3	".. class:: infomark 
Syntax
 This tool returns all unique lines using the 'sort -u' command. It can be used with unsorted files. If you need additional options, like grouping or counting your unique results, please use the 'Unique lines from sorted file' tool. ----- .. class:: infomark The input file needs to be tab separated. Please convert your file if necessary. ----- 
Example
 - Input file:: chr1 10 100 gene1 chr1 105 200 gene2 chr1 10 100 gene1 chr2 10 100 gene4 chr2 1000 1900 gene5 chr3 15 1656 gene6 chr2 10 100 gene4 - Unique lines will result in:: chr1 10 100 gene1 chr1 105 200 gene2 chr2 10 100 gene4 chr2 1000 1900 gene5 chr3 15 1656 gene6"
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_uniq_tool/9.5+galaxy3	"This tool takes a sorted file and look for lines that are unique. .. class:: warningmark Please make sure your file is sorted, or else this tool will give you an erroneous output. .. class:: infomark You can sort your file using either the ""Sort"" tool in ""Filter and Sort"", or the ""Sort"" tool in ""Unix Tools""."
toolshed.g2.bx.psu.edu/repos/iuc/annotatemyids/annotatemyids/3.18.0+galaxy0	".. class:: infomark 
What it does
 This tool can get annotation for a generic set of IDs, using the Bioconductor_ annotation data packages. Supported organisms are human, mouse, rat, fruit fly and zebrafish. The org.db packages that are used here are primarily based on mapping using Entrez Gene identifiers. More information on the annotation packages can be found at the Bioconductor website, for example, information on the human annotation package (org.Hs.eg.db) can be found here_. Examples of what this tool can be used for are: * adding gene names to IDs * mapping between IDs e.g. Entrez, Ensembl, Symbols * adding GO and KEGG identifiers .. 
Bioconductor: https://www.bioconductor.org/ .. _here: http://bioconductor.org/packages/release/data/annotation/manuals/org.Hs.eg.db/man/org.Hs.eg.db.pdf ----- 
Inputs
 A tab-delimited file with identifiers in the first column. If the file contains a header row, select the file has a header option in the tool form above. Example: =============== ======================= 
GeneID
 
Additional Columns...
 --------------- ----------------------- ENSG00000091831 ENSG00000082175 ENSG00000141736 ENSG00000012048 ENSG00000139618 ENSG00000129514 ENSG00000171862 ENSG00000141510 =============== ======================= ID types supported for input are: * 
ENSEMBL
: Ensembl gene IDs * 
ENSEMBLPROT
: Ensembl protein IDs * 
ENSEMBLTRANS
: Ensembl transcript IDs * 
ENTREZID
: Entrez gene Identifiers * 
FLYBASE
: FlyBase accession numbers * 
GO
: GO Identifiers * 
MGI
: Jackson Laboratory MGI gene accession numbers * 
PATH
: KEGG Pathway Identifiers * 
REFSEQ
: Refseq Identifiers * 
SYMBOL
: The official gene symbol * 
ZFIN
: Zfin accession numbers .. class:: warningmark This tool uses the 
select
 function from the Bioconductor AnnotationDBi
 package. Note that if you request columns that have multiple matches for your IDs, select will return 
one row in the output for each possible match
. This has the effect that if you request multiple columns and some of them have a many-to-one relationship to the IDs, things will continue to multiply accordingly. So it's not a good idea to request a large number of columns unless you know what you are asking for should have a one-to-one relationship with the initial set of IDs. In general, if you need to retrieve a column like 
GO
 or 
KEGG
, that has a many-to-one relationship to the original IDs, it is most useful to extract that separately. .. _AnnotationDBi: https://www.bioconductor.org/packages/devel/bioc/manuals/AnnotationDbi/man/AnnotationDbi.pdf ----- 
Outputs
 If the input IDs are Ensembl, the default output will be similar to below, containing four columns. Other columns, such as GO and KEGG terms, can be selected above to be added as additional columns. Example: =============== ============ ========== ================================= 
ENSEMBL
 
ENTREZID
 
SYMBOL
 
GENENAME
 --------------- ------------ ---------- --------------------------------- ENSG00000091831 2099 ESR1 estrogen receptor 1 ENSG00000082175 5241 PGR progesterone receptor ENSG00000141736 2064 ERBB2 erb-b2 receptor tyrosine kinase 2 ENSG00000012048 672 BRCA1 breast cancer 1 ENSG00000139618 675 BRCA2 breast cancer 2 ENSG00000129514 3169 FOXA1 forkhead box A1 ENSG00000171862 5728 PTEN phosphatase and tensin homolog ENSG00000141510 7157 TP53 tumor protein p53 =============== ============ ========== ================================= Columns available for output include many of the ID columns already described under Inputs above and also: * 
ALIAS
: Commonly used gene symbols * 
EVIDENCE
: Evidence codes for GO associations with a gene of interest * 
GENENAME
: The full gene name * 
ONTOLOGY
: For GO Identifiers, which Gene Ontology (BP, CC, or MF)"
toolshed.g2.bx.psu.edu/repos/iuc/reshape2_cast/cast/1.4.2	This tool will apply the dcast function of the reshape2 R package. The input data should be in a 'long' format or molten by the melt tool. The output will be in a wide format. The cast function expands each unique variable:value combination on a single line to columnar format. Documantation on the reshape2 package can be found here: https://cran.r-project.org/web/packages/reshape2/reshape2.pdf
toolshed.g2.bx.psu.edu/repos/bgruening/diff/diff/3.10+galaxy1	".. class:: infomark 
Purpose
 The 
diff
 utility is a data comparison tool that calculates and displays the differences between two files. Unlike edit distance notions used for other purposes, diff is line-oriented rather than character-oriented, but it is like Levenshtein distance in that it tries to determine the smallest set of deletions and insertions to create one file from the other. The diff command displays the changes made in a standard format, such that both humans and machines can understand the changes and apply them: given one file and the changes, the other file can be created. .. class:: infomark 
Input
 Two text files to be checked for differences line by line. .. class:: infomark 
Output
 A 
text file
, either: - containing the lines differences in 
unified format
 (
unidiff
), - or an 
empty
 file if the two input files are the same. An 
optional
 
HTML report
 with a friendlier visual representation of the differences."
toolshed.g2.bx.psu.edu/repos/iuc/reshape2_melt/melt/1.4.2	This tool will apply the melt function of the reshape2 R package. The melt function summarizes each unique variable:value combination on a single line. An example can be found here: http://www.statmethods.net/management/reshape.html
toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_tac/9.5+galaxy3	"What it does
 tac is a Linux command that allows you to see a file line-by-line backwards. It is named by analogy with cat. Mandatory arguments to long options are mandatory for short options too: -b, --before attach the separator before instead of after -r, --regex interpret the separator as a regular expression -s, --separator=STRING use STRING as the separator instead of newline ----- 
Example
 Input file: 0 1 2 3 4 5 # 6 7 8 9 default settings: 9 8 7 6 # 5 4 3 2 1 0 with option -s 5: # 6 7 8 9 0 1 2 3 4 5 with option -b and -s 5: 5 # 6 7 8 9 0 1 2 3 4"
toolshed.g2.bx.psu.edu/repos/iuc/biotradis/tradis_essentiality/1.4.5+galaxy3	"What is does
 Bio-TraDis provides software utilities for the processing, mapping, and analysis of transposon insertion sequencing data. The pipeline was designed with the data from the TraDIS sequencing protocol in mind, but should work with a variety of transposon insertion sequencing protocols as long as they produce data in the expected format. tradis_essentiality tool performs an Essentiality analysis using the annotation counts output of tradis_gene_insert_sites tool ----- 
Output files
 - Essential genes table : Table containing the essential genes - Unclassified genes : Table containing genes that couldn't be classified as essential or non essential - All genes : Table containing all genes - QC report : PDF file containing the regression plot All tables contain the following columns: - locus_tag - gene_name - ncrna : Non-coding RNA, 1 if the feature is a non coding DNA, 0 if not. - start - end - strand - read_count : Total number of reads mapping on the feature - ins_index : Insertion index, number of insertion divided by the gene length - gene_length - ins_count : Number of insertion within the feature. - fcn : Function ----- 
More information
 .. class:: infomark Additional information about Bio-TraDis can be found at https://github.com/sanger-pathogens/Bio-Tradis"
toolshed.g2.bx.psu.edu/repos/iuc/biotradis/tradis_gene_insert_sites/1.4.5+galaxy3	"What is does
 Bio-TraDis provides software utilities for the processing, mapping, and analysis of transposon insertion sequencing data. The pipeline was designed with the data from the TraDIS sequencing protocol in mind, but should work with a variety of transposon insertion sequencing protocols as long as they produce data in the expected format. tradis_gene_insert_sites that combine the counts at each position and an annotation file to provide the number of reads and insertion for each feature. ----- 
Output files
 The tool outputs a table containing the columns : - locus_tag - gene_name - ncrna : Is the feature a non-coding DNA. 1=yes, 0=no. - start - end - strand - read_count : Numer of reads mapping on the feature - ins_index : Insertion index, number of insertion divided by the gene length - gene_length - ins_count : Number of insertion within the feature. - fcn : Function ----- 
More information
 .. class:: infomark Additional information about Bio-TraDis can be found at https://github.com/sanger-pathogens/Bio-Tradis"
toolshed.g2.bx.psu.edu/repos/iuc/biotradis/bacteria_tradis/1.4.5+galaxy3	"What is does
 Bio-TraDis provides software utilities for the processing, mapping, and analysis of transposon insertion sequencing data. The pipeline was designed with the data from the TraDIS sequencing protocol in mind, but should work with a variety of transposon insertion sequencing protocols as long as they produce data in the expected format. ----- 
Parameters
 The --smalt_r 0 and -m 0 options specify that we want to map reads with multiple best mappings to a random position and use these in our downstream analyses; by default these reads are left unmapped. Mapping and processing this library will take about 30 minutes to an hour on a typical desktop computer. By default, the ​bacteria_tradis​ pipeline determines appropriate read mapping parameters automatically from the length of the first read in the fastq file. It should be noted that the default parameters have been tested using the optimized TraDIS protocol of Barquist ​et al​., 20XX in the hands of an experienced sequencing specialist; these will need to be tuned for other protocols, or for pilot runs, etc. There are various other scenarios in which it would be appropriate to reduce the stringency of these parameters: in the case that read trimming has been applied, if there are quality issues in the library, for certain types of studies (particularly gene essentiality studies as above), or if the quality of the reference genome is low (or of a different strain). The 
-mm
 option specifies the number of mismatches allowed when matching the transposon tag; by default none are allowed. We sometimes observe one or two positions within the transposon tag that seem to have generally low quality. If there is evidence for low-quality bases in the transposon tag (from FastQC, for instance), setting this to 1 or 2 may result in higher recovery of insertion sites. Higher than 2 is not advisable with the typical transposon tag lengths (10 - 12 bases) produced by TraDIS protocols, but may be appropriate with protocols that produce significantly longer transposon tags. The 
-m
 option sets the minimum mapping quality score to use an alignment in downstream analysis (e.g. plot files); defaults to 30. Multi-mapping reads have a quality score of 0 by definition, so this parameter needs to be set to 0 for these reads to be properly processed. Can be lowered without dramatically affecting results in most cases, particularly if 
smalt_y
 is set reasonably. The other options specify parameters for the smalt mapper, which are discussed in more detail in the smalt manual (ftp.sanger.ac.uk/pub/resources/software/smalt/smalt-manual-0.7.4.pdf). We will discuss their effects on TraDIS mapping briefly here: 
-smalt_k
: length of kmers hashed; roughly, the minimum length of an exact match between a read and the genome needed to trigger an alignment attempt. Appropriate values are between ~10 and 20 for bacterial genomes depending on read length. Lower values lead to increased sensitivity at the expense of runtime. 
-smalt_s
: skipstep. Sampling step size, i.e. the distance between successive words that are hashed along the genomic reference sequence. With the option -s 1 every word is hashed, with -s 2 every second word, with -s 3 very third etc. Appropriate values are between 1 and ~15, but should be less than --smalt_k to ensure kmers overlap. Lower values lead to increased sensitivity at the expense of runtime. 
-smalt_y
: minimum percentage of identical bases between read and reference, defaults to .96 - 96% identity, or 4 mismatches allowed in a 100 base read. May be lowered to improve sensitivity in the case of low quality or short reads. 
-smalt_r
: specifies what to do with reads that map equally well in multiple locations. By default this is set to -1, meaning that multi-mapping reads are left unmapped. This is appropriate in studies comparing insertion frequency in the same library passaged through multiple conditions, as in this case a change in frequency of one repetitive gene could lead to many genes appearing to be selected artifactually. For studies of gene essentiality in a newly created library, this should be set to 0 (randomly assign a position) to avoid repetitive elements (particularly insertion sequences and the like) artificially appearing to be essential. ----- 
Output files
 On completion, bacteria tradis ​produces a number of files. These include: 
(input list name).stats
​ : Mapping statistics file. This is comma delimited, and includes one line for each library mapped along with a header. It can be easily opened in e.g. Excel or R. 
(library name.replicon_name).insert_site_plot.gz
​: Plot files, one for each replicon and library. These contain insertion counts on each strand for every nucleotide position in the replicon. They can be opened as “user plots” in the Artemis genome browser, and will be used for further analysis. 
(library name).mapped.bam
 : BAM file containing mapped reads. ----- 
More information
 .. class:: infomark Additional information about Bio-TraDis can be found at https://github.com/sanger-pathogens/Bio-Tradis"
toolshed.g2.bx.psu.edu/repos/iuc/gff_to_prot/gff_to_prot/3.2.3+galaxy0	".. class:: infomark 
What it does
 ------------------- Convert Gff3 files coming from Genbank in prot_table file that can be used as an input for TRANSIT tools."
toolshed.g2.bx.psu.edu/repos/iuc/transit_gumbel/transit_gumbel/3.2.3+galaxy0	".. class:: infomark 
What it does
 ------------------- The 
Gumbel
 method can be used to determine which genes are essential in a single condition. It does a gene-by-gene analysis of the insertions at TA sites with each gene, makes a call based on the longest consecutive sequence of TA sites without insertion in the genes, calculates the probability of this using a Bayesian model. Note : Intended only for Himar1 datasets. ------------------- 
Inputs
 ------------------- Input files for Gumnbel need to be: - .wig files: Tabulated files containing one column with the TA site coordinate and one column with the read count at this site. - annotation .prot_table: Annotation file generated by the 
Convert Gff3 to prot_table for TRANSIT
 tool. ------------------- 
Parameters
 ------------------- Optional Arguments: -s <integer> := Number of samples. Default: -s 10000 -b <integer> := Number of Burn-in samples. Default -b 500 -m <integer> := Smallest read-count to consider. Default: -m 1 -t <integer> := Trims all but every t-th value. Default: -t 1 -r <string> := How to handle replicates. Sum or Mean. Default: -r Mean --iN <float> := Ignore TAs occuring at given fraction of the N terminus. Default: -iN 0.0 --iC <float> := Ignore TAs occuring at given fraction of the C terminus. Default: -iC 0.0 -n <string> := Determines which normalization method to use. Default -n TTR - Samples: Gumbel uses Metropolis-Hastings (MH) to generate samples of posterior distributions. The default setting is to run the simulation for 10,000 iterations. This is usually enough to assure convergence of the sampler and to provide accurate estimates of posterior probabilities. Less iterations may work, but at the risk of lower accuracy. - Burn-In: Because the MH sampler many not have stabilized in the first few iterations, a “burn-in” period is defined. Samples obtained in this “burn-in” period are discarded, and do not count towards estimates. - Trim: The MH sampler produces Markov samples that are correlated. This parameter dictates how many samples must be attempted for every sampled obtained. Increasing this parameter will decrease the auto-correlation, at the cost of dramatically increasing the run-time. For most situations, this parameter should be left at the default of “1”. - Minimum Read: The minimum read count that is considered a true read. Because the Gumbel method depends on determining gaps of TA sites lacking insertions, it may be susceptible to spurious reads (e.g. errors). The default value of 1 will consider all reads as true reads. A value of 2, for example, will ignore read counts of 1. - Replicates: Determines how to deal with replicates by averaging the read-counts or summing read counts across datasets. This should not have an affect for the Gumbel method, aside from potentially affecting spurious reads. - Normalisation : - TTR (Default) : Trimmed Total Reads (TTR), normalized by the total read-counts (like totreads), but trims top and bottom 5% of read-counts. This is the recommended normalization method for most cases as it has the beneffit of normalizing for difference in saturation in the context of resampling. - nzmean : Normalizes datasets to have the same mean over the non-zero sites. - totreads : Normalizes datasets by total read-counts, and scales them to have the same mean over all counts. - zinfnb : Fits a zero-inflated negative binomial model, and then divides read-counts by the mean. The zero-inflated negative binomial model will treat some empty sites as belonging to the “true” negative binomial distribution responsible for read-counts while treating the others as “essential” (and thus not influencing its parameters). - quantile : Normalizes datasets using the quantile normalization method described by Bolstad et al. (2003). In this normalization procedure, datasets are sorted, an empirical distribution is estimated as the mean across the sorted datasets at each site, and then the original (unsorted) datasets are assigned values from the empirical distribution based on their quantiles. - betageom : Normalizes the datasets to fit an “ideal” Geometric distribution with a variable probability parameter p. Specially useful for datasets that contain a large skew. See Beta-Geometric Correction . - nonorm : No normalization is performed. ------------------- 
Outputs
 ------------------- ============================================= ======================================================================================================================== 
Column Header
 
Column Definition
 --------------------------------------------- ------------------------------------------------------------------------------------------------------------------------ Orf Gene ID Name Gene Name Desc Gene Description k Number of Transposon Insertions Observed within the ORF. n Total Number of TA dinucleotides within the ORF. r Span of nucleotides for the Maximum Run of Non-Insertions. s Span of nucleotides for the Maximum Run of Non-Insertions. zbar Posterior Probability of Essentiality. State Call Essentiality call for the gene. Depends on FDR corrected thresholds. E=Essential U=Uncertain, NE=Non-Essential, S=too short ============================================= ======================================================================================================================== Note: Technically, Bayesian models are used to calculate posterior probabilities, not p-values (which is a concept associated with the frequentist framework). However, we have implemented a method for computing the approximate false-discovery rate (FDR) that serves a similar purpose. This determines a threshold for significance on the posterior probabilities that is corrected for multiple tests. The actual thresholds used are reported in the headers of the output file (and are near 1 for essentials and near 0 for non-essentials). There can be many genes that score between the two thresholds (t1 < zbar < t2). This reflects intrinsic uncertainty associated with either low read counts, sparse insertion density, or small genes. If the insertion_density is too low (< ~30%), the method may not work as well, and might indicate an unusually large number of Uncertain or Essential genes. ------------------- 
More Information
 ------------------- See 
TRANSIT documentation
 - TRANSIT: https://transit.readthedocs.io/en/v3.2.3/index.html - 
TRANSIT Gumbel
: https://transit.readthedocs.io/en/v3.2.3/transit_methods.html#gumbel"
toolshed.g2.bx.psu.edu/repos/iuc/transit_hmm/transit_hmm/3.2.3+galaxy0	".. class:: infomark 
What it does
 ------------------- The HMM method can be used to determine the essentiality of the entire genome, as opposed to gene-level analysis of the other methods. It is capable of identifying regions that have unusually high or unusually low read counts (i.e. growth advantage or growth defect regions), in addition to the more common categories of essential and non-essential. Note : Intended only for Himar1 datasets. ------------------- 
Inputs
 ------------------- - .wig files : Tabulated files containing one column with the TA site coordinate and one column with the read count at this site. - annotation .prot_table : Annotation file generated by the 
Convert Gff3 to prot_table for TRANSIT
 tool. ------------------- 
Parameters
 ------------------- Optional Arguments: | -r <string> := How to handle replicates. Sum, Mean. Default: -r Mean | -l := Perform LOESS Correction; Helps remove possible genomic position bias. Default: Off. | -iN <float> := Ignore TAs occuring at given fraction of the N terminus. Default: -iN 0.0 | -iC <float> := Ignore TAs occuring at given fraction of the C terminus. Default: -iC 0.0 | -n <string> := Determines which normalization method to use. Default -n TTR The HMM method automatically estimates the necessary statistical parameters from the datasets. You can change how the method handles replicate datasets: - Replicates: Determines how the HMM deals with replicate datasets by either averaging the read-counts or summing read counts across datasets. For regular datasets (i.e. mean-read count > 100) the recommended setting is to average read-counts together. For sparse datasets, it summing read-counts may produce more accurate results. - Normalization Method: Determines which normalization method to use when comparing datasets. Proper normalization is important as it ensures that other sources of variability are not mistakenly treated as real differences. See the Normalization section for a description of normalization method available in TRANSIT. - - TTR (Default) : Trimmed Total Reads (TTR), normalized by the total read-counts (like totreads), but trims top and bottom 5% of read-counts. This is the recommended normalization method for most cases as it has the beneffit of normalizing for difference in saturation in the context of resampling. - - nzmean : Normalizes datasets to have the same mean over the non-zero sites. - - totreads : Normalizes datasets by total read-counts, and scales them to have the same mean over all counts. - - zinfnb : Fits a zero-inflated negative binomial model, and then divides read-counts by the mean. The zero-inflated negative binomial model will treat some empty sites as belonging to the “true” negative binomial distribution responsible for read-counts while treating the others as “essential” (and thus not influencing its parameters). - - quantile : Normalizes datasets using the quantile normalization method described by Bolstad et al. (2003). In this normalization procedure, datasets are sorted, an empirical distribution is estimated as the mean across the sorted datasets at each site, and then the original (unsorted) datasets are assigned values from the empirical distribution based on their quantiles. - - betageom : Normalizes the datasets to fit an “ideal” Geometric distribution with a variable probability parameter p. Specially useful for datasets that contain a large skew. See Beta-Geometric Correction . - - nonorm : No normalization is performed. ----------- 
Outputs
 ----------- The HMM method outputs two files. The first file provides the most likely assignment of states for all the TA sites in the genome. Sites can belong to one of the following states: “E” (Essential), “GD” (Growth-Defect), “NE” (Non-Essential), or “GA” (Growth-Advantage). In addition, the output includes the probability of the particular site belonging to the given state. The columns of this file are defined as follows: ============================================= ======================================================================================================================== 
Column
 
Column Definition
 --------------------------------------------- ------------------------------------------------------------------------------------------------------------------------ 1 Coordinate of TA site 2 Observed Read Counts 3 Probability for ES state 4 Probability for GD state 5 Probability for NE state 6 Probability for GA state 7 State Classification (ES = Essential, GD = Growth Defect, NE = Non-Essential, GA = Growth-Defect) 8 Gene(s) that share(s) the TA site. ============================================= ======================================================================================================================== The second file provides a gene-level classification for all the genes in the genome. Genes are classified as “E” (Essential), “GD” (Growth-Defect), “NE” (Non-Essential), or “GA” (Growth-Advantage) depending on the number of sites within the gene that belong to those states. ============================================= ======================================================================================================================== 
Column Header
 
Column Definition
 --------------------------------------------- ------------------------------------------------------------------------------------------------------------------------ Orf Gene ID Name Gene Name Desc Gene Description N Number of TA sites n0 Number of sites labeled ES (Essential) n1 Number of sites labeled GD (Growth-Defect) n2 Number of sites labeled NE (Non-Essential) n3 Number of sites labeled GA (Growth-Advantage) Avg. Insertions Mean insertion rate within the gene Avg. Reads Mean read count within the gene State Call State Classification (ES = Essential, GD = Growth Defect, NE = Non-Essential, GA = Growth-Defect) ============================================= ======================================================================================================================== Note: Libraries that are too sparse (e.g. < 30%) or which contain very low read-counts may be problematic for the HMM method, causing it to label too many Growth-Defect genes ------------------- 
More Information
 ------------------- See 
TRANSIT documentation
 - TRANSIT: https://transit.readthedocs.io/en/v3.2.3/index.html - 
TRANSIT Gumbel
: https://transit.readthedocs.io/en/v3.2.3/transit_methods.html#hmm"
toolshed.g2.bx.psu.edu/repos/iuc/transit_resampling/transit_resampling/3.2.3+galaxy0	".. class:: infomark 
What it does
 ------------------- The re-sampling method is a comparative analysis the allows that can be used to determine conditional essentiality of genes. It is based on a permutation test, and is capable of determining read-counts that are significantly different across conditions. This technique has yet to be formally published in the context of differential essentiality analysis. Briefly, the read-counts at each genes are determined for each replicate of each condition. The total read-counts in condition A is subtracted from the total read counts at condition B, to obtain an observed difference in read counts. The TA sites are then permuted for a given number of “samples”. For each one of these permutations, the difference is read-counts is determined. This forms a null distribution, from which a p-value is calculated for the original, observed difference in read-counts. Note : Can be used for both Himar1 and Tn5 datasets ------------------- 
Inputs
 ------------------- Input files for Resampling need to be: - .wig files : Tabulated files containing one column with the TA site coordinate and one column with the read count at this site. - annotation .prot_table : Annotation file generated by the 
Convert Gff3 to prot_table for TRANSIT
 tool. ------------------- 
Parameters
 ------------------- Optional Arguments: -s <integer> := Number of samples. Default: 10000 -n <string> := Normalization method. Default: TTR -h := Output histogram of the permutations for each gene. Default: Off. -a := Perform adaptive resampling. Default: Off. -ez := Exclude rows with zero accross conditions. Default: Off --pc := Pseudocounts to be added at each site. Default: 0 -l := Perform LOESS Correction; Helps remove possible genomic position bias. Default: Off. --iN <float> := Ignore TAs occuring at given fraction of the N terminus. Default: 0.0 --iC <float> := Ignore TAs occuring at given fraction of the C terminus. Default: 0.0 --ctrl_lib := String of letters representing library of control files in order e.g. 'AABB' Default: empty. Letters used must also be used in --exp_lib. If non-empty, resampling will limit permutations to within-libraries. --exp_lib := String of letters representing library of experimental files in order e.g. 'ABAB' Default: empty. Letters used must also be used in --ctrl_lib. If non-empty, resampling will limit permutations to within-libraries. The resampling method is non-parametric, and therefore does not require any parameters governing the distributions or the model. The following parameters are available for the method: - Samples: The number of samples (permutations) to perform. The larger the number of samples, the more resolution the p-values calculated will have, at the expense of longer computation time. The re-sampling method runs on 10,000 samples by default. - Output Histograms:Determines whether to output .png images of the histograms obtained from resampling the difference in read-counts. - Adaptive Resampling: An optional “adaptive” version of resampling which accelerates the calculation by terminating early for genes which are likely not significant. This dramatically speeds up the computation at the cost of less accurate estimates for those genes that terminate early (i.e. deemed not significant). This option is OFF by default. - Include Zeros: Select to include sites that are zero. This is the preferred behavior, however, unselecting this (thus ignoring sites that) are zero accross all dataset (i.e. completely empty), is useful for decreasing running time (specially for large datasets like Tn5). - Normalization Method: Determines which normalization method to use when comparing datasets. Proper normalization is important as it ensures that other sources of variability are not mistakenly treated as real differences. See the Normalization section for a description of normalization method available in TRANSIT. - TTR (Default) : Trimmed Total Reads (TTR), normalized by the total read-counts (like totreads), but trims top and bottom 5% of read-counts. This is the recommended normalization method for most cases as it has the beneffit of normalizing for difference in saturation in the context of resampling. - nzmean : Normalizes datasets to have the same mean over the non-zero sites. - totreads : Normalizes datasets by total read-counts, and scales them to have the same mean over all counts. - zinfnb : Fits a zero-inflated negative binomial model, and then divides read-counts by the mean. The zero-inflated negative binomial model will treat some empty sites as belonging to the “true” negative binomial distribution responsible for read-counts while treating the others as “essential” (and thus not influencing its parameters). - quantile : Normalizes datasets using the quantile normalization method described by Bolstad et al. (2003). In this normalization procedure, datasets are sorted, an empirical distribution is estimated as the mean across the sorted datasets at each site, and then the original (unsorted) datasets are assigned values from the empirical distribution based on their quantiles. - betageom : Normalizes the datasets to fit an “ideal” Geometric distribution with a variable probability parameter p. Specially useful for datasets that contain a large skew. See Beta-Geometric Correction . - nonorm : No normalization is performed. ------------------- 
Outputs
 ------------------- The re-sampling method outputs a tab-delimited file with results for each gene in the genome. P-values are adjusted for multiple comparisons using the Benjamini-Hochberg procedure (called “q-values” or “p-adj.”). A typical threshold for conditional essentiality on is q-value < 0.05. ============================================= ======================================================================================================================== 
Column Header
 
Column Definition
 --------------------------------------------- ------------------------------------------------------------------------------------------------------------------------ Orf Gene ID Name Gene Name Desc Gene Description N Number of TA sites in the gene. TAs Hit Number of TA sites with at least one insertion. Sum Rd 1 Sum of read counts in condition 1. Sum Rd 2 Sum of read counts in condition 2. Delta Rd Difference in the sum of read counts. p-value P-value calculated by the permutation test. p-adj. Adjusted p-value controlling for the FDR (Benjamini-Hochberg) ============================================= ======================================================================================================================== ------------------- 
More Information
 ------------------- See 
TRANSIT documentation
 - TRANSIT: https://transit.readthedocs.io/en/v3.2.3/index.html - 
TRANSIT Gumbel
: https://transit.readthedocs.io/en/v3.2.3/transit_methods.html#re-sampling"
toolshed.g2.bx.psu.edu/repos/iuc/transit_tn5gaps/transit_tn5gaps/3.2.3+galaxy0	".. class:: infomark 
What it does
 ------------------- This method is loosely is based on the original 
Gumbel
 analysis method. The 
Gumbel
 method can be used to determine which genes are essential in a single condition. It does a gene-by-gene analysis of the insertions at TA sites with each gene, makes a call based on the longest consecutive sequence of TA sites without insertion in the genes, calculates the probability of this using a Bayesian model. The Tn5Gaps method modifies the original method in order to work on Tn5 datasets, which have significantly lower saturation of insertion sites than Himar1 datasets. The main difference comes from the fact that the runs of non-insertion (or “gaps”) are analyzed throughout the whole genome, including non-coding regions, instead of within single genes. In doing so, the expected maximum run length is calculated and a p-value can be derived for every run. A gene is then classified by using the p-value of the run with the largest number of nucleotides overlapping with the gene. ------------------- 
Inputs
 ------------------- Input files for Tn5Gaps need to be: - .wig files: Tabulated files containing one column with the TA site coordinate and one column with the read count at this site. - annotation .prot_table: Annotation file generated by the 
Convert Gff3 to prot_table for TRANSIT
 tool. ------------------- 
Parameters
 ------------------- Optional Arguments: -m <integer> := Smallest read-count to consider. Default: -m 1 -r <string> := How to handle replicates. Sum or Mean. Default: -r Mean --iN <float> := Ignore TAs occuring at given fraction of the N terminus. Default: -iN 0.0 --iC <float> := Ignore TAs occuring at given fraction of the C terminus. Default: -iC 0.0 -n <string> := Determines which normalization method to use. Default -n TTR - Minimum Read: The minimum read count that is considered a true read. Because the Gumbel method depends on determining gaps of TA sites lacking insertions, it may be susceptible to spurious reads (e.g. errors). The default value of 1 will consider all reads as true reads. A value of 2, for example, will ignore read counts of 1. - Replicates: Determines how to deal with replicates by averaging the read-counts or summing read counts across datasets. This should not have an affect for the Gumbel method, aside from potentially affecting spurious reads. - Normalisation : - TTR (Default) : Trimmed Total Reads (TTR), normalized by the total read-counts (like totreads), but trims top and bottom 5% of read-counts. This is the recommended normalization method for most cases as it has the beneffit of normalizing for difference in saturation in the context of resampling. - nzmean : Normalizes datasets to have the same mean over the non-zero sites. - totreads : Normalizes datasets by total read-counts, and scales them to have the same mean over all counts. - zinfnb : Fits a zero-inflated negative binomial model, and then divides read-counts by the mean. The zero-inflated negative binomial model will treat some empty sites as belonging to the “true” negative binomial distribution responsible for read-counts while treating the others as “essential” (and thus not influencing its parameters). - quantile : Normalizes datasets using the quantile normalization method described by Bolstad et al. (2003). In this normalization procedure, datasets are sorted, an empirical distribution is estimated as the mean across the sorted datasets at each site, and then the original (unsorted) datasets are assigned values from the empirical distribution based on their quantiles. - betageom : Normalizes the datasets to fit an “ideal” Geometric distribution with a variable probability parameter p. Specially useful for datasets that contain a large skew. See Beta-Geometric Correction . - nonorm : No normalization is performed. ------------------- 
Outputs
 ------------------- ============================================= ======================================================================================================================== 
Column Header
 
Column Definition
 --------------------------------------------- ------------------------------------------------------------------------------------------------------------------------ Orf Gene ID Name Gene Name Desc Gene Description k Number of Transposon Insertions Observed within the ORF. n Total Number of TA dinucleotides within the ORF. r Span of nucleotides for the Maximum Run of Non-Insertions. ovr The number of nucleotides in the overlap with the longest run partially covering the gene. lenovr The length of the above run with the largest overlap with the gene. pval P-value calculated by the permutation test. padj Adjusted p-value controlling for the FDR (Benjamini-Hochberg). State Call Essentiality call for the gene. Depends on FDR corrected thresholds. E=Essential U=Uncertain, NE=Non-Essential, S=too short ============================================= ======================================================================================================================== Note: Technically, Bayesian models are used to calculate posterior probabilities, not p-values (which is a concept associated with the frequentist framework). However, we have implemented a method for computing the approximate false-discovery rate (FDR) that serves a similar purpose. This determines a threshold for significance on the posterior probabilities that is corrected for multiple tests. The actual thresholds used are reported in the headers of the output file (and are near 1 for essentials and near 0 for non-essentials). There can be many genes that score between the two thresholds (t1 < zbar < t2). This reflects intrinsic uncertainty associated with either low read counts, sparse insertion density, or small genes. If the insertion_density is too low (< ~30%), the method may not work as well, and might indicate an unusually large number of Uncertain or Essential genes. ------------------- 
More Information
 ------------------- See 
TRANSIT documentation
 - TRANSIT: https://transit.readthedocs.io/en/v3.2.3/index.html - 
TRANSIT Tn5Gaps
: https://transit.readthedocs.io/en/v3.2.3/transit_methods.html#tn5gaps"
toolshed.g2.bx.psu.edu/repos/devteam/vcftools_annotate/vcftools_annotate/0.1	"Annotates VCF dataset with custom annotations. For example, if this format tag is used for allele frequency: ##FORMAT=<ID=FREQ,Number=1,Type=String,Description=""Variant allele frequency""> you can add a filter for allele frequency using ""FORMAT/FREQ"" as the tag name and the condition "">= [desired allele freq]"" Please see the VCFtools 
documentation
 for help and further information. .. 
: http://vcftools.sourceforge.net/perl_module.html#vcf-annotate"
toolshed.g2.bx.psu.edu/repos/devteam/vcftools_slice/vcftools_slice/0.1	"Please see the VCFtools 
documentation
 for help and further information. .. 
: http://vcftools.sourceforge.net/docs.html"
toolshed.g2.bx.psu.edu/repos/devteam/vcfbedintersect/vcfbedintersect/1.0.0_rc3+galaxy0	Computes intersection between a VCF dataset and a set of genomic intervals defined as either a BED dataset (http://genome.ucsc.edu/FAQ/FAQformat.html#format1) or a manually typed interval (in the form of chr:start-end). ---- VCFBEDintersect is based on vcfintersect utility of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/devteam/vcfvcfintersect/vcfvcfintersect/1.0.0_rc3+galaxy0	Computes intersections and unions for two VCF datasets. Unifies equivalent alleles within window-size bp. The options are:: -v, --invert invert the selection, printing only records which would -i, --intersect-vcf FILE use this VCF for set intersection generation -u, --union-vcf FILE use this VCF for set union generation -w, --window-size N compare records up to this many bp away (default 30) -l, --loci output whole loci when one alternate allele matches -m, --ref-match intersect on the basis of record REF string -t, --tag TAG attach TAG to each record's info field if it would intersect -V, --tag-value VAL use this value to indicate that the allele is passing '.' will be used otherwise. default: 'PASS' -M, --merge-from FROM-TAG -T, --merge-to TO-TAG merge from FROM-TAG used in the -i file, setting TO-TAG in the current file. ---- VCFVCFintersect is based on vcfintersect utility of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/devteam/vcfaddinfo/vcfaddinfo/1.0.0_rc3+galaxy0	Adds info fields from the second dataset which are not present in the first dataset. ----- Vcfaddinfo is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/devteam/vcfannotate/vcfannotate/1.0.0_rc3+galaxy0	"Intersect the records in the VCF file with intervals (features) provided in a BED file. Intersections are done on the reference sequences in the VCF file. ----- .. class:: infomark 
Example
: The following VCF line:: #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA00001 NA00002 NA00003 20 1110696 rs6040355 A G,T 67 PASS NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:4:.,. will appear as the follwing after intersectuion with BED records uc010zpo.2, uc002wel.4, uc010zpp.2, and uc002wen:: #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA00001 NA00002 NA00003 20 1110696 rs6040355 A G,T 67 PASS AA=T;AF=0.333,0.667;BED-features=uc010zpo.2:uc002wel.4:uc010zpp.2:uc002wen.4;DP=10;NS=2;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:4:.,. ---- Vcfannotate is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib)."
toolshed.g2.bx.psu.edu/repos/devteam/vcfannotategenotypes/vcfannotategenotypes/1.0.0_rc3+galaxy0	"Annotates genotypes in the 
First
 dataset with genotypes from the 
Second
 adding the genotype as another flag to each sample filled in the first file. 
Annotation-tag
 is the name of the sample flag which is added to store the annotation. Also adds a 'has_variant' flag for sites where the second file has a variant. ----- Vcfannotate is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib)."
toolshed.g2.bx.psu.edu/repos/devteam/vcfbreakcreatemulti/vcfbreakcreatemulti/1.0.0_rc3+galaxy0	"This tool breaks or creates multiallelic VCF records based on user selection (
Break
 or 
Create
, respectively): - 
Break
 = If multiple alleles are specified in a single record, break the record into multiple lines, preserving allele-specific INFO fields. - 
Create
 = If overlapping alleles are represented across multiple records, merge them into a single record. ---- This tools is based on vcfbreakmulti and vcfcreatemulti utilities from the VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib)."
toolshed.g2.bx.psu.edu/repos/devteam/vcfcheck/vcfcheck/1.0.0_rc3+galaxy0	Verifies that the VCF REF field matches the reference as described. The options are:: -x, --exclude-failures If a record fails, don't print it. Otherwise do. -k, --keep-failures Print if the record fails, otherwise not. ---- Vcfcheck is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/devteam/vcfcombine/vcfcombine/1.0.0_rc3+galaxy0	Combines VCF files positionally, combining samples when sites and alleles are identical. Any number of VCF files may be combined. The INFO field and other columns are taken from one of the files, which are combined when records in multiple files match. Alleles must have identical ordering to be combined into one record. If they do not, multiple records will be emitted. ----- Vcfcombine is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/devteam/vcfcommonsamples/vcfcommonsamples/1.0.0_rc3+galaxy0	Outputs each record in the first file, removing samples not present in the second. ---- Vcfcommonsamples is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/devteam/vcfdistance/vcfdistance/1.0.0_rc3+galaxy0	Adds a value to each VCF record indicating the distance to the nearest variant in the file. .. class:: infomark The dataset used as input to this tool must be coordinate sorted. This can be achieved by either using the VCFsort utility or Galaxy's general purpose sort tool (in this case sort on the first and the second column in ascending order). ---- Vcfdistance is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/iuc/vcfdistance/vcfdistance/1.0.0_rc1+galaxy0	Adds a value to each VCF record indicating the distance to the nearest variant in the file. .. class:: infomark The dataset used as input to this tool must be coordinate sorted. This can be achieved by either using the VCFsort utility or Galaxy's general purpose sort tool (in this case sort on the first and the second column in ascending order). ---- Vcfdistance is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/devteam/vcffilter/vcffilter2/1.0.0_rc3+galaxy3	"You can specify the following options within the 
Specify filtering expression
 box in any combination:: -f, --info-filter specifies a filter to apply to the info fields of records, removes alleles which do not pass the filter -g, --genotype-filter specifies a filter to apply to the genotype fields of records -s, --filter-sites filter entire records, not just alleles -t, --tag-pass tag vcf records as positively filtered with this tag, print all records -F, --tag-fail tag vcf records as negatively filtered with this tag, print all records -A, --append-filter append the existing filter tag, don't just replace it -a, --allele-tag apply -t on a per-allele basis. adds or sets the corresponding INFO field tag -v, --invert inverts the filter, e.g. grep -v -o, --or use logical OR instead of AND to combine filters -r, --region specify a region on which to target the filtering (must be used in conjunction with -f or -g) To specify filters, click on the 'Insert Add filters' button, choose a filter type (e.g., 'Info filter' or 'Genotype filter'), and specify filter value according to the following pattern:: - For 'Info filter (-f)':: {ID} {operator} {value} For instance:: DP > 10 - For 'Genotype fields (-g)':: {ID} {operator} {value} For instance:: GT = 1|1 - For 'Flag' fields (when 'Info filter (-f)' is selected for filter type field):: {value} For instance:: CpG Any number of filters may be specified. They are combined via logical AND unless the --or option is specified. For convenience, you can specify ""QUAL"" to refer to the quality of the site, even though it does not appear in the INFO fields. Operators can be any of:: =, !, <, >, |, & Obtain logical negation through the use of parentheses, e.g. ""! ( DP = 10 )"" Please mind the blank space between parentheses and the arguments. To restrict output to a specific location use the -r option (must be used in conjunction with -g or -f):: -r chr20:14000-15000 # only output calls between positions 14,000 and 15,000 on chromosome 20 -r chrX # only output call on chromosome X ----- Vcffilter is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib)."
toolshed.g2.bx.psu.edu/repos/devteam/vcffixup/vcffixup/1.0.0_rc3+galaxy0	Uses genotypes from the selected VCF dataset to correct AC (alternate allele count), AF (alternate allele frequency), NS (number of called), in the VCF records. ---- Vcffixup is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/devteam/vcfflatten/vcfflatten2/1.0.0_rc3+galaxy0	Removes multi-allelic sites by picking the most common alternate. Requires allele frequency specification 'AF' and use of 'G' and 'A' to specify the fields which vary according to the Allele or Genotype. ---- Vcfflatten is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/devteam/vcfgeno2haplo/vcfgeno2haplo/1.0.0_rc3+galaxy0	"Convert genotype-based phased alleles within a window size specified by -w option into haplotype alleles. Will break haplotype construction when encountering non-phased genotypes on input. The options are:: -w, --window-size N Merge variants at most this many bp apart (default 30) -o, --only-variants Don't output the entire haplotype, just concatenate REF/ALT strings (delimited by "":"") ---- Vcfgeno2haplo is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib)."
toolshed.g2.bx.psu.edu/repos/devteam/vcfgenotypes/vcfgenotypes/1.0.0_rc3+galaxy0	Converts numerical representation of genotypes (standard in GT field) to the alleles provided in the call's ALT/REF fields. ---- Vcfgenotypes is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/devteam/vcfhethom/vcfhethom/1.0.0_rc3+galaxy0	This tool performs three basic calculations: (1) Computes the number of heterozygotes (2) Computes the ratio between heterozygotes and homozygotes (3) Computes the total number of alleles in the input dataset ---- This tools is based on vcfhetcount, vcfhethomratio,and vcfcountalleles utilities from the VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/devteam/vcfleftalign/vcfleftalign/1.0.0_rc3+galaxy0	Left-aligns variants in VCF dataset. Window size is determined dynamically according to the entropy of the regions flanking the indel. These must have entropy > 1 bit/bp, or be shorter than ~5kb. ---- Vcfleftalign is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/devteam/vcfprimers/vcfprimers/1.0.0_rc3+galaxy0	"For each VCF record, extract the flanking sequences, and write them as FASTA records suitable for alignment. This tool is intended for use in designing validation experiments. Primers extracted which would flank all of the alleles at multi-allelic sites. The name of the FASTA ""reads"" indicates the VCF record which they apply to. The form is >CHROM_POS_LEFT for the 3' primer and >CHROM_POS_RIGHT for the 5' primer, for example:: >20_233255_LEFT CCATTGTATATATAGACCATAATTTCTTTATCCAATCATCTGTTGATGGA >20_233255_RIGHT ACTCAGTTGATTCCATACCTTTGCCATCATGAATCATGTTGTAATAAACA ---- Vcfprimers is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib)."
toolshed.g2.bx.psu.edu/repos/devteam/vcfrandomsample/vcfrandomsample/1.0.0_rc3+galaxy0	Randomly sample sites from an input VCF dataset. Scale the sampling probability by the field specified by --scale-by (see advanced controls). This may be used to provide uniform sampling across allele frequencies, for instance (AF field in this case). ---- Vcfrandomsample is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib).
toolshed.g2.bx.psu.edu/repos/devteam/vcfselectsamples/vcfselectsamples/1.0.0_rc3+galaxy0	"Allows to keep or remove samples from a VCF file. ----- .. class:: infomark 
Example
: Selecting 
NA00001
 and 
NA00003
 from the following VCF line:: #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA00001 NA00002 NA00003 20 1110696 rs6040355 A G,T 67 PASS NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:4:.,. will, obviously, remove 
NA00002
:: #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA00001 NA00003 20 1110696 rs6040355 A G,T 67 PASS NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2/2:35:4:.,. ---- Vcfselectsamples is based on vcfkeepsamples/vcfremovesamples utilities from 
VCFlib
 toolkit developed by Erik Garrison (https://github.com/ekg/vcflib)."
toolshed.g2.bx.psu.edu/repos/devteam/vcfsort/vcfsort/1.0.0_rc3+galaxy0	"This tool uses native UNIX sort command to order VCF dataset in coordinate order. For technically inclined the command is:: (grep ^""#"" INPUT_file ; grep -v ^""#"" INPUT_file | LC_ALL=C sort -k1,1 -k2,2n -V) > OUTPUT_file .. class:: infomark The same result can be achieved with the Galaxy's general purpose sort tool (in this case sort on the first and the second column in ascending order)."
toolshed.g2.bx.psu.edu/repos/devteam/vcf2tsv/vcf2tsv/1.0.0_rc3+galaxy0	"Converts VCF dataset to tab-delimited format, using null string to replace empty values in the table. Specifying ""
Report data per sample
"" (-g) will output one line per sample with genotype information. ---- Vcf2Tsv is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib)."
toolshed.g2.bx.psu.edu/repos/devteam/vcfallelicprimitives/vcfallelicprimitives/1.0.0_rc3+galaxy0	"If multiple alleleic primitives (gaps or mismatches) are specified in a single VCF record, this tools splits the record into multiple lines, but drops all INFO fields. ""Pure"" MNPs are split into multiple SNPs unless the -m flag is provided. Genotypes are phased where complex alleles have been decomposed, provided genotypes in the input. The options are:: -m, --use-mnps Retain MNPs as separate events (default: false). -t, --tag-parsed FLAG Tag records which are split apart of a complex allele with this flag. -L, --max-length LEN Do not manipulate records in which either the ALT or REF is longer than LEN (default: 200). -k, --keep-info Maintain site and allele-level annotations when decomposing. Note that in many cases, such as multisample VCFs, these won't be valid post-decomposition. For biallelic loci in single-sample VCFs, they should be usable with caution. -g, --keep-geno Maintain genotype-level annotations when decomposing. Similar caution should be used for this as for --keep-info. ---- Vcfallelicprimitives is a part of VCFlib toolkit developed by Erik Garrison (https://github.com/ekg/vcflib)."
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_query_list_samples/bcftools_query_list_samples/1.22+galaxy0	===================================== bcftools List Samples query ===================================== Lists Samples from a VCF/BCF file https://www.htslib.org/doc/bcftools.html#query https://samtools.github.io/bcftools/howtos/index.html
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.22+galaxy0	BED, or a tab-delimited file with mandatory columns CHROM, POS (or, alternatively, FROM and TO), optional columns REF and ALT, and arbitrary number of annotation columns. Note that in case of tab-delimited file, the coordinates POS, FROM and TO are one-based and inclusive.
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_call/bcftools_call/1.22+galaxy0	"For example: --prior-freqs REF_AN,REF_AC <br>if the input VCF has the following INFO tags: <br>##INFO=&lt;ID=REF_AN,Number=1,Type=Integer,Description=""Total number of alleles in reference genotypes""&gt; <br>##INFO=&lt;ID=REF_AC,Number=A,Type=Integer,Description=""Allele count in reference genotypes for each ALT allele""&gt;"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.22+galaxy0	The prior probability of the query and the control sample being the same. Setting to 0 calls both independently, setting to 1 forces the same copy number state in both. (default: 0.5)
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_concat/bcftools_concat/1.22+galaxy0	--naive concatenates VCF or BCF files without recompression. This can be used used to combine results that were generated separately for each chromosome. This is very fast but requires that all files are of the same type (all VCF or all BCF) and have the same headers. This is because all tags and chromosome names in the BCF body rely on the implicit order of the contig and tag definitions in the header. Currently no sanity checks are in place. Dangerous, use with caution.
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_consensus/bcftools_consensus/1.22+galaxy0	===================================== bcftools consensus plugin ===================================== Create consensus sequence by applying VCF variants to a reference fasta file. https://www.htslib.org/doc/bcftools.html#consensus https://samtools.github.io/bcftools/howtos/index.html The option to set the new consensus' FASTA ID from the name of the VCF is provided by post-processing the bcftools consensus output. It is primarily intended for use when the VCF is coming from a list collection where the elements of the list are named meaningfully (e.g. named after sample names). This is useful when consensus sequences are being prepared for, for example, feeding a multiple sequence alignment to a phylogeny program.
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.22+galaxy0	SampleName SexDesignation: <br>MaleSample M <br>FemaleSample F
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_to_vcf/bcftools_convert_to_vcf/1.22+galaxy0	===================================== bcftools convert plugin ===================================== Converts other variant formats to vcf. See man page for file formats details. https://www.htslib.org/doc/bcftools.html#convert https://samtools.github.io/bcftools/howtos/index.html
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_counts/bcftools_plugin_counts/1.22+galaxy0	"===================================== bcftools counts plugin ===================================== Counts number of samples, SNPs, INDELs, MNPs and total number of sites. Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_csq/bcftools_csq/1.22+galaxy0	"===================================== bcftools csq ===================================== Haplotype aware consequence predictor which correctly handles combined variants such as MNPs split over multiple VCF records, SNPs separated by an intron (but adjacent in the spliced transcript) or nearby frame-shifting indels which in combination in fact are not frame-shifting. The output VCF is annotated with INFO/BCSQ and FORMAT/BCSQ tag (configurable with the -c option). The latter is a bitmask of indexes to INFO/BCSQ, with interleaved haplotypes. See the usage examples below for using the %TBCSQ converter in query for extracting a more human readable form from this bitmask. The contruction of the bitmask limits the number of consequences that can be referenced in the FORMAT/BCSQ tags. By default this is 16, but if more are required, see the --ncsq option. The program requires on input a VCF/BCF file, the reference genome in fasta format (--fasta-ref) and genomic features in the GFF3 format downloadable from the Ensembl website (--gff-annot), and outputs an annotated VCF/BCF file. Currently, only Ensembl GFF3 files are supported. By default, the input VCF should be phased. If phase is unknown, or only partially known, the --phase option can be used to indicate how to handle unphased data. Alternatively, haplotype aware calling can be turned off with the --local-csq option. If conflicting (overlapping) variants within one haplotype are detected, a warning will be emitted and predictions will be based on only the first variant in the analysis. Symbolic alleles are not supported. They will remain unannotated in the output VCF and are ignored for the prediction analysis. Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz https://www.htslib.org/doc/bcftools.html#csq https://samtools.github.io/bcftools/howtos/index.html"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_dosage/bcftools_plugin_dosage/1.22+galaxy0	"===================================== bcftools dosage plugin ===================================== Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_fill_an_ac/bcftools_plugin_fill_an_ac/1.22+galaxy0	"===================================== bcftools fill-AN-AC plugin ===================================== Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_fill_tags/bcftools_plugin_fill_tags/1.22+galaxy0	"===================================== bcftools fill-tags plugin ===================================== Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.22+galaxy0	"(-m) The default mode replaces existing filters of failed sites with a new FILTER string while leaving sites which pass untouched when non-empty and setting to ""PASS"" when the FILTER string is absent."
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_fixploidy/bcftools_plugin_fixploidy/1.22+galaxy0	"===================================== bcftools fixploidy plugin ===================================== Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_gtcheck/bcftools_gtcheck/1.22+galaxy0	"===================================== bcftools gtcheck ===================================== Check sample identity. With no -g BCF given, multi-sample cross-check is performed. Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz https://www.htslib.org/doc/bcftools.html#gtcheck https://samtools.github.io/bcftools/howtos/index.html"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_impute_info/bcftools_plugin_impute_info/1.22+galaxy0	"===================================== bcftools impute-info plugin ===================================== Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_isec/bcftools_isec/1.22+galaxy0	"===================================== bcftools isec ===================================== Create intersections, unions and complements of VCF files. Collapse -------- Controls how to treat records with duplicate positions and defines compatible records across multiple input files. Here by ""compatible"" we mean records which should be considered as identical by the tools. For example, when performing line intersections, the desire may be to consider as identical all sites with matching positions (bcftools isec -c all), or only sites with matching variant type (bcftools isec -c snps -c indels), or only sites with all alleles identical (bcftools isec -c none). +------------+----------------------------------------------------------------+ | Flag value | Result | +============+================================================================+ | none | only records with identical REF and ALT alleles are compatible | +------------+----------------------------------------------------------------+ | some | only records where some subset of ALT alleles match are | | | compatible | +------------+----------------------------------------------------------------+ | all | all records are compatible, regardless of whether the ALT | | | alleles match or not. In the case of records with the same | | | position, only the first wil lbe considered and appear on | | | output. | +------------+----------------------------------------------------------------+ | snps | any SNP records are compatible, regardless of whether the ALT | | | alleles match or not. For duplicate positions, only the first | | | SNP record will be considered and appear on output. | +------------+----------------------------------------------------------------+ | indels | all indel records are compatible, regardless of whether the | | | REF and ALT alleles match or not. For duplicate positions, | | | only the first indel record will be considered and appear on | | | output. | +------------+----------------------------------------------------------------+ | both | abbreviation of ""-c indels -c snps"" | +------------+----------------------------------------------------------------+ | id | only records with identical ID column are compatible. | | | Supportedby bcftools merge only. | +------------+----------------------------------------------------------------+ Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff https://www.htslib.org/doc/bcftools.html#isec https://samtools.github.io/bcftools/howtos/index.html"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_mendelian/bcftools_plugin_mendelian/1.22+galaxy0	"===================================== bcftools mendelian2 plugin ===================================== Describing sample relationships (family trios) in PED format ------------------------------------------------------------ A PED file is simply a tabular text file (columns can be separated by either spaces or TABs, but not a mixture of the two within the same file) with the header:: #family_id name paternal_id maternal_id sex and optional additional columns. The actual column names in the header are not fixed, but there have to be at least six columns that are interpreted as detailed next. Subsequent lines describe one sample from the VCF input dataset each, where - 
family_id
 is an alphanumeric identifier of a family This column has to be present, but is ignored by this plugin - 
name
 is the identifier of the sample described by the line - 
paternal_id
 is the identifier of the sample's father - 
maternal_id
 is the identifier of the sample's mother - 
sex
 is a numeric code (1=male, 2=female) for the sample's sex An example minimal PED file describing a single family-trio with a female child could look like this:: #family_id name paternal_id maternal_id sex 0 NA00001 0 0 1 0 NA00002 0 0 2 0 NA00003 NA00001 NA00002 2 where 0 is used as a placeholder for the paternal_id/maternal_id values in the records of the father and mother. Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_merge/bcftools_merge/1.22+galaxy0	"===================================== bcftools merge ===================================== Merge multiple VCF/BCF files from non-overlapping sample sets to create one multi-sample file. Note that only records from different files can be merged, never from the same file. For ""vertical"" merge take a look at ""bcftools norm"" instead. Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. https://www.htslib.org/doc/bcftools.html#merge https://samtools.github.io/bcftools/howtos/index.html"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_missing2ref/bcftools_plugin_missing2ref/1.22+galaxy0	"===================================== bcftools missing2ref plugin ===================================== Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/1.22+galaxy0	--no-BAQ; BAQ is the Phred-scaled probability of a read base being misaligned. Applying this option greatly helps to reduce false SNPs caused by misalignments. --redo-BAQ; ignore existing BQ tags
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_norm/bcftools_norm/1.22+galaxy0	"===================================== bcftools norm ===================================== Left-align and normalize indels; check if REF alleles match the reference; split multiallelic sites into multiple rows; recover multiallelics from multiple rows. Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz https://www.htslib.org/doc/bcftools.html#norm https://samtools.github.io/bcftools/howtos/index.html"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_query/bcftools_query/1.22+galaxy0	Example: %CHROM\t%POS\t%REF\t%ALT{0}\n ( NOTE TAB: '\t' and new line character: '\n' )
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_reheader/bcftools_reheader/1.22+galaxy0	"New sample names, one name per line, in the same order as they appear in the VCF file. Alternatively, only samples which need to be renamed can be listed as ""old_name new_name"" pairs separated by whitespaces, each on separate line."
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.22+galaxy0	Use when the entire many-sample file cannot fit into memory. The number of sites to keep in memory. If negative, it is interpreted as the maximum memory to use, in MB.
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_setgt/bcftools_plugin_setgt/1.22+galaxy0	"===================================== bcftools setGT plugin ===================================== Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_stats/bcftools_stats/1.22+galaxy0	"===================================== bcftools stats ===================================== Parses VCF or BCF and produces stats which can be plotted using plot-vcfstats. When two files are given, the program generates separate stats for intersection and the complements. By default only sites are compared, -s/-S must given to include also sample columns. When one VCF file is specified, then stats by non-reference allele frequency, depth distribution, stats by quality and per-sample counts, singleton stats, etc. are printed. When two VCF files are given, then stats such as concordance (Genotype concordance by non-reference allele frequency, Genotype concordance by sample, Non-Reference Discordance) and correlation are also printed. Per-site discordance (PSD) is also printed in --verbose mode. Collapse -------- Controls how to treat records with duplicate positions and defines compatible records across multiple input files. Here by ""compatible"" we mean records which should be considered as identical by the tools. For example, when performing line intersections, the desire may be to consider as identical all sites with matching positions (bcftools isec -c all), or only sites with matching variant type (bcftools isec -c snps -c indels), or only sites with all alleles identical (bcftools isec -c none). +------------+----------------------------------------------------------------+ | Flag value | Result | +============+================================================================+ | none | only records with identical REF and ALT alleles are compatible | +------------+----------------------------------------------------------------+ | some | only records where some subset of ALT alleles match are | | | compatible | +------------+----------------------------------------------------------------+ | all | all records are compatible, regardless of whether the ALT | | | alleles match or not. In the case of records with the same | | | position, only the first wil lbe considered and appear on | | | output. | +------------+----------------------------------------------------------------+ | snps | any SNP records are compatible, regardless of whether the ALT | | | alleles match or not. For duplicate positions, only the first | | | SNP record will be considered and appear on output. | +------------+----------------------------------------------------------------+ | indels | all indel records are compatible, regardless of whether the | | | REF and ALT alleles match or not. For duplicate positions, | | | only the first indel record will be considered and appear on | | | output. | +------------+----------------------------------------------------------------+ | both | abbreviation of ""-c indels -c snps"" | +------------+----------------------------------------------------------------+ | id | only records with identical ID column are compatible. | | | Supportedby bcftools merge only. | +------------+----------------------------------------------------------------+ Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff https://www.htslib.org/doc/bcftools.html#stats https://samtools.github.io/bcftools/howtos/index.html"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_tag2tag/bcftools_plugin_tag2tag/1.22+galaxy0	"===================================== bcftools tag2tag plugin ===================================== Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.22+galaxy0	Include only sites with one or more homozygous (hom), heterozygous (het) or missing (miss) genotypes. When prefixed with ^, the logic is reversed. Please notice that if the input file doesn't have any genotype columns, then this option is ignored altogether.
toolshed.g2.bx.psu.edu/repos/iuc/lofreq_alnqual/lofreq_alnqual/2.1.5+galaxy1	"lofreq alnqual: add base- and indel-alignment qualities (BAQ, IDAQ) to aligned read records .. class:: infomark 
What it does
 This tool can compute base- and/or indel-alignment quality scores for aligned reads in SAM/BAM format and store these scores under the custom tags 
lb
 (for the base alignment qualities, 
ai
 (insertion alignment qualities) and 
ad
 (deletion alignment qualities), which it will add to its output BAM. These tags and their scores can be reused by LoFreq call, which can incorporate them into its model for variant quality calculation. .. class:: warning 
Do not realign
 your reads after adding alignment qualities to them since the scores would not be updated in the process! Notes: - While LoFreq call can optionally calculate the exact same scores on the fly if needed, having the scores added to the aligned read records is going to save computation time with repeated variant calling from the same input data. - The base alignment quality scores have the same meaning as in samtools, but are expressed as absolute scores, not as base quality offsets like with the 
BQ
 tag generated by samtools. - 
ai
 and 
ad
 tags, if requested, will only be added to reads containing insertions/deletions."
toolshed.g2.bx.psu.edu/repos/iuc/arriba/arriba/2.5.1+galaxy0	recommended STAR options: --chimSegmentMin 10 --chimOutType WithinBAM
toolshed.g2.bx.psu.edu/repos/iuc/arriba_draw_fusions/arriba_draw_fusions/2.5.1+galaxy0	By default the transcript isoform with the highest coverage is drawn. Alternatively, the transcript isoform that is provided in the columns transcript_id1 and transcript_id2 in the given fusions file can be drawn. Selecting the isoform with the highest coverage usually produces nicer plots, in the sense that the coverage track is smooth and shows a visible increase in coverage after the fusion breakpoint. However, the isoform with the highest coverage may not be the one that is involved in the fusion. Often, genomic rearrangements lead to non-canonical isoforms being transcribed. For this reason, it can make sense to rely on the transcript selection provided by the columns transcript_id1/2, which reflect the actual isoforms involved in a fusion. \ As a third option, the transcripts that are annotated as canonical can be drawn. Transcript isoforms tagged with appris_principal, appris_candidate, or CCDS are considered canonical.
toolshed.g2.bx.psu.edu/repos/iuc/arriba_get_filters/arriba_get_filters/2.5.1+galaxy0	GRCh38 GRCh37 hg38 hg19 GRCm38 mm10
toolshed.g2.bx.psu.edu/repos/devteam/freebayes/bamleftalign/1.3.10+galaxy0	When calling indels, it is important to homogenize the positional distribution of insertions and deletions in the input by using left realignment. Left realignment will place all indels in homopolymer and microsatellite repeats at the same position, provided that doing so does not introduce mismatches between the read and reference other than the indel. This method is computationally inexpensive and handles the most common classes of alignment inconsistency. This is leftalign utility from FreeBayes package.
toolshed.g2.bx.psu.edu/repos/iuc/lofreq_call/lofreq_call/2.1.5+galaxy3	"lofreq call: call variants from BAM file LoFreq is a fast and sensitive variant-caller for inferring SNVs and indels from next-generation sequencing data. It makes full use of base-call qualities and other sources of errors inherent in sequencing, which are usually ignored by other methods or only used for filtering. LoFreq can run on almost any type of aligned sequencing data since no machine- or sequencing-technology dependent thresholds are used. It automatically adapts to changes in coverage and sequencing quality and can therefore be applied to a variety of data-sets e.g. viral/quasispecies, bacterial, metagenomics or somatic data. While the tool will often give reasonable results with default settings a variety of options let you control its exact behavior. These advanced options can be subdivided into those affecting variant calling and those affecting posterior filtering of the results. 
Variant calling paramters
 At the heart of LoFreq's variant caller is a 
joint quality score
 that is computed for every site in every read (that survives filtering) and that combines some or all of the following read and base quality measures: - Base/indel quality For any read, this is the Phred-scaled likelihood that the base mapped to a given site does not represent a sequencing error. For every base, this score got computed by the base caller of your sequencing platform and got incorporated into your input dataset during read alignment. For insertions/deletions this is defined, analogously, as the Phred-scaled likelihood that any inserted/deleted base is real, however, you are responsible for adding indel qualitites, which are required for indel calling with lofreq, to your input. For doing so, you can use 
lofreq indelqual
. - Base/indel alignment quality For any read, this is the Phred-scaled likelihood that the read's base or indel mapped to a given reference genome position is mapped to this position correctly. The tool can calculate these scores for you on the fly. Alternatively, you can precalculate them using 
lofreq alnqual
, which will incorporate them into your input dataset. - Mapping quality The Phred-scaled likelihood that the read got mapped to the correct place in the reference genome. This score got incorporated into your input dataset by the aligner you used to map your reads. - Source quality This is the Phred-scaled likelihood that the given read comes from the reference genome. The tool can calculate this score for you. 
Variant filter parameters
 After generating a list of called variants, the tool can filter this list based on: - the statistical significance of the variant calls - strand-bias of reads supporting the variant - coverage of the variant site While posterior filtering can help reduce false-positive variant calls, please note that the separate 
lofreq filter
, which can be run on the output of 
lofreq call
 has many more options for configuring filters. These are the different filter settings supported by the tool: 
Preset filtering on QUAL score + coverage + strand bias
 For variants to pass this filter, the following is required: - statistical signficance of the variant call with a pvalue < 0.01 based on the retransformed QUAL score of the variant and multiple-testing corrected using a dynamically determined Bonferroni factor (based on the number of overall variants considered during calling). - A strand-bias in supporting reads not significant under a FDR-corrected p value of 0.001 and 85% of supporting reads mapped to the same strand of the genome. - A coverage of the variant site of at least 10x. 
Preset QUAL score-based filtering
 Same QUAL-based significance filter as the default, but without the strand-bias and coverage criteria 
Strictly no filtering
 Do not apply any filters, but produce the original list of all called variants. You will almost always want to use 
lofreq filter
 to process the resulting output. 
Custom filter settings/combinations
 Lets you define your own QUAL-based significance filter and, optionally, combine it with the default starnd-bias and coverage filters."
toolshed.g2.bx.psu.edu/repos/iuc/deepvariant/deepvariant/1.5.0+galaxy1	".. class:: infomark 
Purpose
 DeepVariant is a deep learning-based variant caller that takes aligned reads (in BAM or CRAM format), produces pileup image tensors from them, classifies each tensor using a convolutional neural network, and finally reports the results in a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. - NGS (Illumina) data for either a 
whole genome &lt;https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-case-study.md&gt;
 or 
whole exome &lt;https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-exome-case-study.md&gt;
. - PacBio HiFi data, see the 
PacBio case study &lt;https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md&gt;
. - Hybrid PacBio HiFi + Illumina WGS, see the 
hybrid case study &lt;https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-hybrid-case-study.md&gt;
. Please also note: For somatic data or any other samples where the genotypes go beyond two copies of DNA, DeepVariant will not work out of the box because the only genotypes supported are hom-alt, het, and hom-ref. The models included with DeepVariant are only trained on human data. For other organisms, see the blog post on 
non-human variant-calling &lt;https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/&gt;
 for some possible pitfalls and how to handle them. ---- .. class:: infomark 
How DeepVariants works
 DeepVariant relies on 
Nucleus &lt;https://github.com/google/nucleus&gt;
, a library of Python and C++ code for reading and writing data in common genomics file formats (like SAM and VCF) designed for painless integration with the 
TensorFlow &lt;https://www.tensorflow.org/&gt;
 machine learning framework. Nucleus was built with DeepVariant in mind and open-sourced separately so it can be used by anyone in the genomics research community for other projects. See this blog post on 
Using Nucleus and TensorFlow for DNA Sequencing Error Correction &lt;https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/&gt;
."
toolshed.g2.bx.psu.edu/repos/crs4/exomedepth/exomedepth/1.1.0	".. class:: warningmark 
Warning about counts for chromosome X
 Calling CNVs on the X chromosome can create issues if the exome sample of interest and the reference exome samples it is being compared to are not gender matched. Make sure that the genders are matched properly (i.e. do not use male as a reference for female samples and vice versa). 
What it does
 This tool uses ExomeDepth to call copy number variants (CNVs) from targeted sequence data. 
Output format
 =========== ======================== Column Description ----------- ------------------------ chr Chromosome start Start of CNV region end End of CNV region type CNV type (deletion, duplication) sample Name of the sample with CNV corr Correlation between reference and test counts. To get meaningful result, this correlation should really be above 0.97. If this is not the case, consider the output of ExomeDepth as less reliable (i.e. most likely a high false positive rate) nexons Number of target regions covered by the CNV BF Bayes factor. It quantifies the statistical support for each CNV. It is in fact the log10 of the likelihood ratio of data for the CNV call divided by the null (normal copy number). The higher that number, the more confident one can be about the presence of a CNV. While it is difficult to give an ideal threshold, and for short exons the Bayes Factor are bound to be unconvincing, the most obvious large calls should be easily flagged by ranking them according to this quantity reads.ratio Observed/expected reads ratio =========== ======================== 
What ExomeDepth does and does not do
 ExomeDepth uses read depth data to call CNVs from exome sequencing experiments. A key idea is that the test exome should be compared to a matched aggregate reference set. This aggregate reference set should combine exomes from the same batch and it should also be optimized for each exome. It will certainly differ from one exome to the next. Importantly, ExomeDepth assumes that the CNV of interest is absent from the aggregate reference set. Hence related individuals should be excluded from the aggregate reference. It also means that ExomeDepth can miss common CNVs, if the call is also present in the aggregate reference. ExomeDepth is really suited to detect rare CNV calls (typically for rare Mendelian disorder analysis). The ideas used in this package are of course not specific to exome sequencing and could be applied to other targeted sequencing datasets, as long as they contain a sufficiently large number of exons to estimate the parameters (at least 20 genes, say, but probably more would be useful). Also note that PCR based enrichment studies are often not well suited for this type of read depth analysis. The reason is that as the number of cycles is often set to a high number in order to equalize the representation of each amplicon, which can discard the CNV information."
toolshed.g2.bx.psu.edu/repos/boris/getalleleseq/getalleleseq/0.0.1	"The major allele sequence of a sample is simply the sequence consisting of the most frequent nucleotide per position. Replacing the major allele for the second most frequent allele at diploid positions generates the minor allele sequence. ----- .. class:: infomark 
What it does
 It takes the table generated from the Variant Annotator tool to derive a major and minor allele sequence per sample. Since all sequences share the same length all the major allele sequences are included into a single file (with proper headers per sample) to create a multiple sequence alignment in FASTA format that can be used for downstream phylogenetic analyses. In contrast, the minor allele sequences are informed as single FASTA files per sample to ease their downstream manipulation. ----- .. class:: warningmark 
Note
 Please, follow the format described below for the input file: ----- .. class:: infomark 
Formats
 
Variant Annotator tool output format
 Columns:: 1. sample id 2. chromosome 3. position 4 counts for A's 5. counts for C's 6. counts for G's 7. counts for T's 8. Coverage 9. Number of alleles passing frequency threshold 10. Major allele 11. Minor allele 12. Minor allele frequency in position 
FASTA multiple alignment
 See http://www.bioperl.org/wiki/FASTA_multiple_alignment_format ----- 
Example
 - For the following dataset:: S9 chrM 3 3 0 2 214 219 0 T A 0.013698630137 S9 chrM 4 3 249 3 0 255 0 C N 0.0 S9 chrM 5 245 1 1 0 247 1 A N 0.0 S11 chrM 6 0 292 0 0 292 1 C . 0.0 S7 chrM 6 0 254 0 0 254 1 C . 0.0 S9 chrM 6 2 306 2 0 310 0 C N 0.0 S11 chrM 7 281 0 3 0 284 0 A G 0.0105633802817 S7 chrM 7 249 0 2 0 251 1 A G 0.00796812749004 etc. for all covered positions per sample... - Running this tool with background sequence length 16569 will produce 4 files:: 1. Multiple alignment FASTA file containing the major allele sequences of samples S7, S9 and S11 2. minor allele sequence of sample S7 3. minor allele sequence of sample S9 4. minor allele sequence of sample S11 ----- 
Citation
 If you use this tool, please cite Dickins B, Rebolledo-Jaramillo B, et al (2014). 
Acccepted in Biotechniques
 (boris-at-bx.psu.edu)"
toolshed.g2.bx.psu.edu/repos/iuc/snp_sites/snp_sites/2.5.1+galaxy0	"SNP-sites
 This tool can rapidly extract SNPs from a multi-FASTA alignment using modest resources and can output results in multiple formats for downstream analysis. SNPs can be extracted from a 8.3 GB alignment file (1,842 taxa, 22,618 sites) in 267 seconds using 59 MB of RAM and 1 CPU core, making it feasible to run on modest computers. SNP-sites is implemented in C and is available under the open source license GNU GPL version 3. Alternatively it can report on the number of constant sites in an alignment in a format (A count, C count, G count, T count) suitable for passing to IQ-TREE's 
-fconst
 option. 
Input FASTA format:
 The first sequence will be taken as a reference. .. code-block:: >sample1 AGACACAGTCAC >sample1 AGACAC----AC >sample1 AAACGCATTCAN 
Options
 In addition to the default output mode, this tool supports three other options: 1. Output only constant (monomorphic) (i.e. not polymorphic) sites (useful for input to BEAST) 2. Output only columns that have purely ACTGs in them (i.e. no gaps or ambiguous nucleotides). This can be used together with the previous option. 3. Output only a count of constant sites, in a format suitable for use with IQ-TREE 
-fconst
. This option cannot be used together with the previously mentioned options. 
Output files:
 The output of the tool are three different files in following format: - a multi fasta alignment, - relaxed phylip format and, - VCF. - alternatively constant site counts (As, Cs, Gs and Ts). The VCF file for the above specified input is The output of the tool are three different files in following format: - a multi fasta alignment, - relaxed phylip format and, - VCF. The VCF file for the above specified input is ===== === == === === ==== ====== ==== ====== ======= ======= ======= CHROM POS ID REF ALT QUAL FILTER INFO FORMAT sample1 sample1 sample1 ----- --- -- --- --- ---- ------ ---- ------ ------- ------- ------- 1 2 . G A . . . GT 0 0 1 ----- --- -- --- --- ---- ------ ---- ------ ------- ------- ------- 1 5 . A G . . . GT 0 0 1 ----- --- -- --- --- ---- ------ ---- ------ ------- ------- ------- 1 8 . G .,T . . . GT 0 1 2 ===== === == === === ==== ====== ==== ====== ======= ======= ======= Thus the tool identified three variations (SNPs): in 2nd, 5th, and 8th positions (A instead of G, G instead of A, and unknown nucleotide or T instead of G, respectively)."
toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.10+galaxy1	"What it does
 FreeBayes is a Bayesian genetic variant detector designed to find small polymorphisms, specifically SNPs (single-nucleotide polymorphisms), indels (insertions and deletions), MNPs (multi-nucleotide polymorphisms), and complex events (composite insertion and substitution events) smaller than the length of a short-read sequencing alignment. See https://github.com/ekg/freebayes for details on FreeBayes. ------ 
Description
 Provided some BAM dataset(s) and a reference sequence, FreeBayes will produce a VCF dataset describing SNPs, indels, and complex variants in samples in the input alignments. By default, FreeBayes will consider variants supported by at least 2 observations in a single sample (-C) and also by at least 20% of the reads from a single sample (-F). These settings are suitable to low to high depth sequencing in haploid and diploid samples, but users working with polyploid or pooled samples may wish to adjust them depending on the characteristics of their sequencing data. FreeBayes is capable of calling variant haplotypes shorter than a read length where multiple polymorphisms segregate on the same read. The maximum distance between polymorphisms phased in this way is determined by the --max-complex-gap, which defaults to 3bp. In practice, this can comfortably be set to half the read length. Ploidy may be set to any level (-p), but by default all samples are assumed to be diploid. FreeBayes can model per-sample and per-region variation in copy-number (-A) using a copy-number variation map. FreeBayes can act as a frequency-based pooled caller and describe variants and haplotypes in terms of observation frequency rather than called genotypes. To do so, use --pooled-continuous and set input filters to a suitable level. Allele observation counts will be described by AO and RO fields in the VCF output. ------- 
Galaxy-specific options
 Galaxy allows five levels of control over FreeBayes options, provided by the 
Choose parameter selection level
 menu option. These are: 1. 
Simple diploid calling
: The simplest possible FreeBayes application. Equivalent to using FreeBayes with only a BAM input and no other parameter options. 2. 
Simple diploid calling with filtering and coverage
: Same as #1 plus two additional options: -0 (standard filters: --min-mapping-quality 30 --min-base-quality 20 --min-supporting-allele-qsum 0 --genotype-variant-threshold 0) and --min-coverage. 3. 
Frequency-based pooled calling
: This is equivalent to using FreeBayes with the following options: --haplotype-length 0 --min-alternate-count 1 --min-alternate-fraction 0 --pooled-continuous --report-monomorphic. This is the best choice for calling variants in mixtures such as viral, bacterial, or organellar genomes. 4. 
Frequency-based pooled calling with filtering and coverage
: Same as #3 but adds -0 and --min-coverage like in #2. 5. 
Complete list of all options
: Gives you full control by exposing all FreeBayes options as Galaxy parameters. ------ 
Command-line parameters
 
Input
:: --bam FILE The file or set of BAM files to be analyzed. --bam-list FILE A file containing a list of BAM files to be analyzed. --stdin Read BAM input on stdin. --fasta-reference FILE Use FILE as the reference sequence for analysis. An index file (FILE.fai) will be created if none exists. If neither --targets nor --region are specified, FreeBayes will analyze every position in this reference. --targets FILE Limit analysis to targets listed in the BED-format FILE. --region <chrom>:<start>-<end> Limit analysis to the specified region, 0-base coordinates, end_position not included (same as BED format). Either '-' or '..' maybe used as a separator. --samples FILE Limit analysis to samples listed (one per line) in the FILE. By default FreeBayes will analyze all samples in its input BAM files. --populations FILE Each line of FILE should list a sample and a population which it is part of. The population-based bayesian inference model will then be partitioned on the basis of the populations. --cnv-map FILE Read a copy number map from the BED file FILE, which has either a sample-level ploidy: sample_name copy_number or a region-specific format: seq_name start end sample_name copy_number ... for each region in each sample which does not have the default copy number as set by --ploidy. These fields can be delimited by space or tab. 
Output
:: --vcf FILE Output VCF-format results to FILE. (default: stdout) --gvcf Write gVCF output, which indicates coverage in uncalled regions. --gvcf-chunk NUM When writing gVCF output emit a record for every NUM bases. --gvcf-dont-use-chunk When writing the gVCF output emit a record for all bases if set to ""true"" , will also route an int to --gvcf-chunk similar to --output-mode EMIT_ALL_SITES from GATK --variant-input VCF Use variants reported in VCF file as input to the algorithm. Variants in this file will included in the output even if there is not enough support in the data to pass input filters. --only-use-input-alleles Only provide variant calls and genotype likelihoods for sites and alleles which are provided in the VCF input, and provide output in the VCF for all input alleles, not just those which have support in the data. --haplotype-basis-alleles VCF When specified, only variant alleles provided in this input VCF will be used for the construction of complex or haplotype alleles. --report-all-haplotype-alleles At sites where genotypes are made over haplotype alleles, provide information about all alleles in output, not only those which are called. --report-monomorphic Report even loci which appear to be monomorphic, and report all considered alleles, even those which are not in called genotypes. Loci which do not have any potential alternates have '.' for ALT. --pvar N Report sites if the probability that there is a polymorphism at the site is greater than N. default: 0.0. Note that post- filtering is generally recommended over the use of this parameter. --strict-vcf Generate strict VCF format (FORMAT/GQ will be an int) 
Population model
:: --theta N The expected mutation rate or pairwise nucleotide diversity among the population under analysis. This serves as the single parameter to the Ewens Sampling Formula prior model default: 0.001 --ploidy N Sets the default ploidy for the analysis to N. default: 2 --pooled-discrete Assume that samples result from pooled sequencing. Model pooled samples using discrete genotypes across pools. When using this flag, set --ploidy to the number of alleles in each sample or use the --cnv-map to define per-sample ploidy. --pooled-continuous Output all alleles which pass input filters, regardles of genotyping outcome or model. 
Reference allele
:: --use-reference-allele This flag includes the reference allele in the analysis as if it is another sample from the same population. --reference-quality MQ,BQ Assign mapping quality of MQ to the reference allele at each site and base quality of BQ. default: 100,60 
Allele scope
:: --use-best-n-alleles N Evaluate only the best N SNP alleles, ranked by sum of supporting quality scores. (Set to 0 to use all; default: all) --max-complex-gap --haplotype-length N Allow haplotype calls with contiguous embedded matches of up to this length. Set N=-1 to disable clumping. (default: 3) --min-repeat-size When assembling observations across repeats, require the total repeat length at least this many bp. (default: 5) --min-repeat-entropy N To detect interrupted repeats, build across sequence until it has entropy > N bits per bp. Set to 0 to turn off. (default: 1) --no-partial-observations Exclude observations which do not fully span the dynamically-determined detection window. (default, use all observations, dividing partial support across matching haplotypes when generating haplotypes.) 
Indel realignment
:: --dont-left-align-indels Turn off left-alignment of indels, which is enabled by default. 
Input filters
:: --use-duplicate-reads Include duplicate-marked alignments in the analysis. default: exclude duplicates marked as such in alignments --min-mapping-quality Q Exclude alignments from analysis if they have a mapping quality less than Q. default: 1 --min-base-quality Q Exclude alleles from analysis if their supporting base quality is less than Q. default: 0 --min-supporting-allele-qsum Q Consider any allele in which the sum of qualities of supporting observations is at least Q. default: 0 --min-supporting-mapping-qsum Q Consider any allele in which and the sum of mapping qualities of supporting reads is at least Q. default: 0 --mismatch-base-quality-threshold Q Count mismatches toward --read-mismatch-limit if the base quality of the mismatch is >= Q. default: 10 --read-mismatch-limit N Exclude reads with more than N mismatches where each mismatch has base quality >= mismatch-base-quality-threshold. default: ~unbounded --read-max-mismatch-fraction N Exclude reads with more than N [0,1] fraction of mismatches where each mismatch has base quality >= mismatch-base-quality-threshold default: 1.0 --read-snp-limit N Exclude reads with more than N base mismatches, ignoring gaps with quality >= mismatch-base-quality-threshold. default: ~unbounded --read-indel-limit N Exclude reads with more than N separate gaps. default: ~unbounded --standard-filters Use stringent input base and mapping quality filters Equivalent to -m 30 -q 20 -R 0 -S 0 --min-alternate-fraction N Require at least this fraction of observations supporting an alternate allele within a single individual in the in order to evaluate the position. default: 0.05 --min-alternate-count N Require at least this count of observations supporting an alternate allele within a single individual in order to evaluate the position. default: 2 --min-alternate-qsum N Require at least this sum of quality of observations supporting an alternate allele within a single individual in order to evaluate the position. default: 0 --min-alternate-total N Require at least this count of observations supporting an alternate allele within the total population in order to use the allele in analysis. default: 1 --min-coverage N Require at least this coverage to process a site. default: 0 --limit-coverage N Downsample per-sample coverage to this level if greater than this coverage. default: no limit --skip-coverage N Skip processing of alignments overlapping positions with coverage >N. This filters sites above this coverage, but will also reduce data nearby. default: no limit 
Population priors
:: --no-population-priors Equivalent to --pooled-discrete --hwe-priors-off and removal of Ewens Sampling Formula component of priors. 
Mappability priors
:: --hwe-priors-off Disable estimation of the probability of the combination arising under HWE given the allele frequency as estimated by observation frequency. --binomial-obs-priors-off Disable incorporation of prior expectations about observations. Uses read placement probability, strand balance probability, and read position (5'-3') probability. --allele-balance-priors-off Disable use of aggregate probability of observation balance between alleles as a component of the priors. 
Genotype likelihoods
:: --observation-bias FILE Read length-dependent allele observation biases from FILE. The format is [length] [alignment efficiency relative to reference] where the efficiency is 1 if there is no relative observation bias. --base-quality-cap Q Limit estimated observation quality by capping base quality at Q. --prob-contamination F An estimate of contamination to use for all samples. default: 10e-9 --legacy-gls Use legacy (polybayes equivalent) genotype likelihood calculations --contamination-estimates FILE A file containing per-sample estimates of contamination, such as those generated by VerifyBamID. The format should be: sample p(read=R|genotype=AR) p(read=A|genotype=AA) Sample '
' can be used to set default contamination estimates. 
Algorithmic features
:: --report-genotype-likelihood-max Report genotypes using the maximum-likelihood estimate provided from genotype likelihoods. --genotyping-max-iterations N Iterate no more than N times during genotyping step. default: 1000. --genotyping-max-banddepth N Integrate no deeper than the Nth best genotype by likelihood when genotyping. default: 6. --posterior-integration-limits N,M Integrate all genotype combinations in our posterior space which include no more than N samples with their Mth best data likelihood. default: 1,3. --exclude-unobserved-genotypes Skip sample genotypings for which the sample has no supporting reads. --genotype-variant-threshold N Limit posterior integration to samples where the second-best genotype likelihood is no more than log(N) from the highest genotype likelihood for the sample. default: ~unbounded --use-mapping-quality Use mapping quality of alleles when calculating data likelihoods. --harmonic-indel-quality Use a weighted sum of base qualities around an indel, scaled by the distance from the indel. By default use a minimum BQ in flanking sequence. --read-dependence-factor N Incorporate non-independence of reads by scaling successive observations by this factor during data likelihood calculations. default: 0.9 --genotype-qualities Calculate the marginal probability of genotypes and report as GQ in each sample field in the VCF output. ------ 
Acknowledgments
* The initial version of the wrapper was produced by Dan Blankenberg and upgraded by Anton Nekrutenko. TNG was developed by Bjoern Gruening."
toolshed.g2.bx.psu.edu/repos/iuc/gatk4_mutect2/gatk4_mutect2/4.1.7.0+galaxy1	"Call somatic short variants via local assembly of haplotypes. Short variants include single nucleotide (SNV) and insertion and deletion (indel) variants. The caller combines the DREAM challenge-winning somatic genotyping engine of the original MuTect (
Cibulskis et al., 2013 &lt;http://www.nature.com/nbt/journal/v31/n3/full/nbt.2514.html&gt;
) with the assembly-based machinery of 
HaplotypeCaller &lt;https://gatk.broadinstitute.org/hc/en-us/articles/360035531412-HaplotypeCaller-in-a-nutshell&gt;
. This tool is featured in the 
Somatic Short Mutation calling Best Practice Workflow
. See 
this article &lt;https://gatk.broadinstitute.org/hc/en-us/articles/360035531132&gt;
 for an overview of what traditional somatic calling entails, with usage examples. For the latest pipeline scripts, see the 
Mutect2 WDL scripts directory &lt;https://github.com/broadinstitute/gatk/tree/master/scripts/mutect2_wdl&gt;
. Although we present the tool for somatic calling, it may apply to other contexts, such as mitochondrial variant calling. Usage examples ~~~~~~~~~~~~~~ Example commands show how to run Mutect2 for typical scenarios. The two modes are (i) 
somatic mode
 where a tumor sample is matched with a normal sample in analysis and (ii) 
tumor-only mode
 where a single sample's alignment data undergoes analysis. (i) Tumor with matched normal ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Given a matched normal, Mutect2 is designed to call somatic variants only. The tool includes logic to skip emitting variants that are clearly present in the germline based on provided evidence, e.g. in the matched normal. This is done at an early stage to avoid spending computational resources on germline events. If the variant's germline status is borderline, then Mutect2 will emit the variant to the callset for subsequent filtering and review. :: gatk Mutect2 \ -R reference.fa \ -I tumor.bam \ -tumor tumor_sample_name \ -I normal.bam \ -normal normal_sample_name \ --germline-resource af-only-gnomad.vcf.gz \ --af-of-alleles-not-in-resource 0.00003125 \ --panel-of-normals pon.vcf.gz \ -O somatic.vcf.gz The --af-of-alleles-not-in-resource argument value should match expectations for alleles not found in the provided germline resource. Note the tool does not require a germline resource nor a panel of normals (PoN) to run. The tool prefilters sites for the matched normal and the PoN. For the germline resource, the tool prefilters on the allele. Below is an excerpt of a known variants resource with population allele frequencies :: #CHROM POS ID REF ALT QUAL FILTER INFO 1 10067 . T TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCC 30.35 PASS AC=3;AF=7.384E-5 1 10108 . CAACCCT C 46514.32 PASS AC=6;AF=1.525E-4 1 10109 . AACCCTAACCCT AAACCCT,
 89837.27 PASS AC=48,5;AF=0.001223,1.273E-4 1 10114 . TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAACCCTAACCCTAACCCTAACCCTAACCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAACCCTAACCCTAAACCCTA 
,CAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAACCCTAACCCTAACCCTAACCCTAACCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAACCCTAACCCTAAACCCTA,T 36728.97 PASS AC=55,9,1;AF=0.001373,2.246E-4,2.496E-5 1 10119 . CT C,
 251.23 PASS AC=5,1;AF=1.249E-4,2.498E-5 1 10120 . TA CA,
 14928.74 PASS AC=10,6;AF=2.5E-4,1.5E-4 1 10128 . ACCCTAACCCTAACCCTAAC A,
 285.71 PASS AC=3,1;AF=7.58E-5,2.527E-5 1 10131 . CT C,
 378.93 PASS AC=7,5;AF=1.765E-4,1.261E-4 1 10132 . TAACCC *,T 18025.11 PASS AC=12,2;AF=3.03E-4,5.049E-5 (ii) Tumor-only mode ^^^^^^^^^^^^^^^^^^^^ This mode runs on a single sample, e.g. single tumor or single normal sample. To create a PoN, call on each normal sample in this mode, then use CreateSomaticPanelOfNormals to generate the PoN. :: gatk Mutect2 \ -R reference.fa \ -I sample.bam \ -tumor sample_name \ -O single_sample.vcf.gz Further points of interest ~~~~~~~~~~~~~~~~~~~~~~~~~~ Additional parameters that factor towards filtering, including normal-artifact-lod (default threshold 0.0) and tumor-lod (default threshold 5.3), are available in FilterMutectCalls. While the tool calculates normal-lod assuming a diploid genotype, it calculates normal-artifact-lod with the same approach it uses for tumor-lod, i.e. with a variable ploidy assumption. - If the normal artifact log odds becomes large, then FilterMutectCalls applies the artifact-in-normal filter. For matched normal samples with tumor contamination, consider increasing the normal-artifact-lod threshold. - The tumor log odds, which is calculated independently of any matched normal, determines whether to filter a tumor variant. Variants with tumor LODs exceeding the threshold pass filtering. If a variant is absent from a given germline resource, then the value for --af-of-alleles-not-in-resource applies. For example, gnomAD's 16,000 samples (~32,000 homologs per locus) becomes a probability of one in 32,000 or less. Thus, an allele's absence from the germline resource becomes evidence that it is not a germline variant. Caveats ~~~~~~~ Although GATK4 Mutect2 accomodates varying coverage depths, further optimization of parameters may improve calling for extreme high depths, e.g. 1000X."
toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1	Insert indel qualities into BAM file The tool has two modes: 'uniform' and 'dindel': - 'uniform' will assign a given value uniformly, whereas - 'dindel' will insert indel qualities based on Dindel. Both will overwrite any existing values. Do not realign your BAM file afterwards!
toolshed.g2.bx.psu.edu/repos/iuc/jasminesv/jasminesv/1.0.11+galaxy1	".. class:: infomark 
What it does
 
Jasmine
, or Jointly Accurate Sv Merging with Intersample Network Edges is a tool used to merge structural variants (SVs) across samples. Each sample has a number of SV calls, consisting of position information (chromosome, start, end, length), type and strand information, and a number of other values. Jasmine represents the set of all SVs across samples as a network, and uses a modified minimum spanning forest algorithm to determine the best way of merging the variants such that each merged variants represents a set of analogous variants occurring in different samples. 
Input
 - Multiple VCF files to be merged 
Output
 - Merged Variants (VCF) 
References
 More information is available in the 
github &lt;https://github.com/mkirsche/Jasmine&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/2.1.5+galaxy0	"What it does
 
Lofreq filter
 tries to eliminate false-positive calls from a list of variants in VCF format. To this end, it applies a variety of user-configurable filters to the input variants, which all operate on variant attributes expected to be embedded in the VCF input. Specifically, certain filters expect: - the 
QUAL
 field of the variant records to be set - any of the following subfields of a variant's 
INFO
 field: * 
DP
 (required for coverage-based filtering) * 
AF
 (required for filtering based on variant allele frequency) * 
SB
 (required for filtering on strand bias) * 
DP4
 (required for the compound strand bias filter) ------ 
Note
: .. class:: Warning mark This tool is optimized for posterior filtering of variants called with 
Lofreq call
, which outputs all variant attributes required by the various configurable filters. If you are using 
Lofreq filter
 to filter VCF variant lists produced with other tools, be prepared for surprises. In general, if any piece of variant information required for applying a given filter is missing from the input data, the tool will try to disable that filter. Watch out for corresponding warnings in the tool's standard output. In addition, any p-value based filtering on variant qualities may behave incorrectly since different variant callers might use different QUAL scales."
toolshed.g2.bx.psu.edu/repos/boris/hetbox/hetbox/1.0	".. class:: infomark 
What it does
 The MAF Boxplot tool takes a table listing heteroplasmic sites per sample and their corresponding minor allele frequency (MAF). It generates a boxplot of the minor allele frequencies per sample by default. The number of heteroplasmic sites is displayed under each box. Optionally, it can generate a report that includes the total number of heteroplasmic sites, the median and the median absolute deviation (MAD) of the minor allele frequencies per sample. ----- .. class:: warningmark 
Note
 Please, follow the format described below for the input file: ----- .. class:: infomark 
Formats
 
Variant Annotator tool output format
 Columns:: 1. sample id 2. chromosome 3. position 4 counts for A's 5. counts for C's 6. counts for G's 7. counts for T's 8. Coverage 9. Number of alleles passing frequency threshold 10. Major allele 11. Minor allele 12. Minor allele frequency in position ----- 
Citation
 If you use this tool, please cite Dickins B, Rebolledo-Jaramillo B, et al. 
In preparation.
 (boris-at-bx.psu.edu)"
toolshed.g2.bx.psu.edu/repos/devteam/mutate_snp_codon/mutate_snp_codon_1/1.0.0	"This tool takes an interval file as input. This input should contain a set of codon locations and corresponding DNA sequence (such as from the 
Extract Genomic DNA
 tool) joined to SNP locations with observed values (such as 
all fields from selected table
 from the snp130 table of hg18 at the UCSC Table browser). This interval file should have the metadata (chromosome, start, end, strand) set for the columns containing the locations of the codons. The user needs to specify the columns containing the sequence for the codon as well as the genomic positions and observed values (values should be split by '/') for the SNP data as tool input; SNPs positions and sequence substitutes must have a length of exactly 1. Only genomic intervals which yield a different sequence string are output. All sequence characters are converted to uppercase during processing. For example, using these settings: * 
metadata
 
chromosome
, 
start
, 
end
 and 
strand
 set to 
1
, 
2
, 
3
 and 
6
, respectively * 
Codon Sequence column
 set to 
c8
 * 
SNP chromosome column
 set to 
c17
 * 
SNP start column
 set to 
c18
 * 
SNP end column
 set to 
c19
 * 
SNP strand column
 set to 
c22
 * 
SNP observed column
 set to 
c25
 with the following input:: chr1 58995 58998 NM_001005484 0 + GAA GAA Glu GAA 1177632 28.96 0 2787607 0.422452662804 585 chr1 58996 58997 rs1638318 0 + A A A/G genomic single by-submitter 0 0 unknown exact 3 chr1 59289 59292 NM_001005484 0 + TTT TTT Phe TTT 714298 17.57 0 1538990 0.464134269878 585 chr1 59290 59291 rs71245814 0 + T T G/T genomic single unknown 0 0 unknown exact 3 chr1 59313 59316 NM_001005484 0 + AAG AAG Lys AAG 1295568 31.86 0 2289189 0.565950648898 585 chr1 59315 59316 rs2854682 0 - G G C/T genomic single by-submitter 0 0 unknown exact 3 chr1 59373 59376 NM_001005484 0 + ACA ACA Thr ACA 614523 15.11 0 2162384 0.284187729839 585 chr1 59373 59374 rs2691305 0 - A A C/T genomic single unknown 0 0 unknown exact 3 chr1 59412 59415 NM_001005484 0 + GCG GCG Ala GCG 299495 7.37 0 2820741 0.106176001271 585 chr1 59414 59415 rs2531266 0 + G G C/G genomic single by-submitter 0 0 unknown exact 3 chr1 59412 59415 NM_001005484 0 + GCG GCG Ala GCG 299495 7.37 0 2820741 0.106176001271 585 chr1 59414 59415 rs55874132 0 + G G C/G genomic single unknown 0 0 coding-synon exact 1 will produce:: chr1 58995 58998 NM_001005484 0 + GAA GAA Glu GAA 1177632 28.96 0 2787607 0.422452662804 585 chr1 58996 58997 rs1638318 0 + A A A/G genomic single by-submitter 0 0 unknown exact 3 GGA chr1 59289 59292 NM_001005484 0 + TTT TTT Phe TTT 714298 17.57 0 1538990 0.464134269878 585 chr1 59290 59291 rs71245814 0 + T T G/T genomic single unknown 0 0 unknown exact 3 TGT chr1 59313 59316 NM_001005484 0 + AAG AAG Lys AAG 1295568 31.86 0 2289189 0.565950648898 585 chr1 59315 59316 rs2854682 0 - G G C/T genomic single by-submitter 0 0 unknown exact 3 AAA chr1 59373 59376 NM_001005484 0 + ACA ACA Thr ACA 614523 15.11 0 2162384 0.284187729839 585 chr1 59373 59374 rs2691305 0 - A A C/T genomic single unknown 0 0 unknown exact 3 GCA chr1 59412 59415 NM_001005484 0 + GCG GCG Ala GCG 299495 7.37 0 2820741 0.106176001271 585 chr1 59414 59415 rs2531266 0 + G G C/G genomic single by-submitter 0 0 unknown exact 3 GCC chr1 59412 59415 NM_001005484 0 + GCG GCG Ala GCG 299495 7.37 0 2820741 0.106176001271 585 chr1 59414 59415 rs55874132 0 + G G C/G genomic single unknown 0 0 coding-synon exact 1 GCC ------ 
Citation
 If you use this tool, please cite 
Blankenberg D, Taylor J, Nekrutenko A; The Galaxy Team. Making whole genome multiple alignments usable for biologists. Bioinformatics. 2011 Sep 1;27(17):2426-2428. &lt;http://www.ncbi.nlm.nih.gov/pubmed/21775304&gt;
_"
toolshed.g2.bx.psu.edu/repos/blankenberg/naive_variant_caller/naive_variant_caller/0.0.4	"The 
Naive Variant Caller
 tool (NVC). ------ 
What it does
 This tool is a naive variant caller that processes aligned sequencing reads from the BAM format and produces a VCF file containing per position variant calls. This tool allows multiple BAM files to be provided as input and utilizes read group information to make calls for individual samples. User configurable options allow filtering reads that do not pass mapping or base quality thresholds and minimum per base read depth; user's can also specify the ploidy and whether to consider each strand separately. In addition to calling alternate alleles based upon simple ratios of nucleotides at a position, per base nucleotide counts are also provided. A custom tag, NC, is used within the Genotype fields. The NC field is a comma-separated listing of nucleotide counts in the form of <nucleotide>=<count>, where a plus or minus character is prepended to indicate strand, if the strandedness option was specified. ------ 
Inputs
 Accepts one or more BAM input files and a reference genome from the built-in list or from a FASTA file in your history. 
Outputs
 The output is in VCF format. Example VCF output line, without reporting by strand: 
chrM 16029 . T G,A,C . . AC=15,9,5;AF=0.00155311658729,0.000931869952371,0.000517705529095 GT:AC:AF:NC 0/0:15,9,5:0.00155311658729,0.000931869952371,0.000517705529095:A=9,C=5,T=9629,G=15,
 Example VCF output line, when reporting by strand: 
chrM 16029 . T G,A,C . . AC=15,9,5;AF=0.00155311658729,0.000931869952371,0.000517705529095 GT:AC:AF:NC 0/0:15,9,5:0.00155311658729,0.000931869952371,0.000517705529095:+T=3972,-A=9,-C=5,-T=5657,-G=15,
 
Options
 Reference Genome: Ensure that you have selected the correct reference genome, either from the list of built-in genomes or by selecting the corresponding FASTA file from your history. Restrict to regions: You can specify any number of regions on which you would like to receive results. You can specify just a chromosome name, or a chromosome name and start postion, or a chromosome name and start and end position for the set of desired regions. Minimum number of reads needed to consider a REF/ALT: This value declares the minimum number of reads containing a particular base at each position in order to list and use said allele in genotyping calls. Default is 0. Minimum base quality: The minimum base quality score needed for the position in a read to be used for nucleotide counts and genotyping. Default is no filter. Minimum mapping quality: The minimum mapping quality score needed to consider a read for nucleotide counts and genotyping. Default is no filter. Ploidy: The number of genotype calls to make at each reported position. Only write out positions with possible alternate alleles: When set, only positions which have at least one non-reference nucleotide which passes declare filters will be present in the output. Report counts by strand: When set, nucleotide counts (NC) will be reported in reference to the aligned read's source strand. Reported as: <strand><BASE>=<COUNT>. Choose the dtype to use for storing coverage information: This controls the maximum depth value for each nucleotide/position/strand (when specified). Smaller values require the least amount of memory, but have smaller maximal limits. +--------+----------------------------+ | name | maximum coverage value | +========+============================+ | uint8 | 255 | +--------+----------------------------+ | uint16 | 65,535 | +--------+----------------------------+ | uint32 | 4,294,967,295 | +--------+----------------------------+ | uint64 | 18,446,744,073,709,551,615 | +--------+----------------------------+"
toolshed.g2.bx.psu.edu/repos/boris/phylorelatives/phylorelatives/0.0.1	"The major and minor allele sequences of a sample are expected to cluster together in a phylogenetic tree. Deviation from expectation suggests potential contamination coming from the closest unrelated samples. ----- .. class:: infomark 
What it does
 Constructs relatedness of a set of sequences based on the pairwise proportion of different sites. One or more test sequences are accepted as long as their name include the strict suffix ""_minor"" or ""_test"" (i.e. >seq1_minor). It returns the FASTA multiple alignment used, and reports a table with the closest major allele relatives, the tree plot and newick string of the tree. ----- .. class:: warningmark 
Note
 This tools DOES NOT align the sequences. Consequently, same length homologous sequences are required as input to fabricate a FASTA multiple alignment by concatenation of individual FASTA files. ----- .. class:: infomark 
About formats
 
FASTA multiple alignment
 See http://www.bioperl.org/wiki/FASTA_multiple_alignment_format 
Newick
 http://www.megasoftware.net/WebHelp/glossary/rh_newick_format.htm ----- 
Example
 - For the multiple alignment composed of the following FASTA files:: >sample1_major >sample1_minor >sample2_major >sample3_major >sample4_major - running this tool with 
root = RSRS.fasta
, and default parameters will return four datasets:: 1. multiple alignment file used for running the tool 2. relatives of sample1_minor 3. NJ tree newick string (rooted on RSRS) 4. NJ tree png plot (rooted on RSRS) ----- 
Citation
 If you use this tool, please cite Dickins B, Rebolledo-Jaramillo B, et al. 
In preparation.
 (boris-at-bx.psu.edu)"
toolshed.g2.bx.psu.edu/repos/iuc/lofreq_viterbi/lofreq_viterbi/2.1.5+galaxy0	lofreq viterbi: Probabilistic realignment of mapped reads to correct mapping errors. .. class:: Warning mark Not recommended for non-Illumina data.
toolshed.g2.bx.psu.edu/repos/iuc/snp_dists/snp_dists/0.8.2+galaxy0	"snp-dists
 This tool computes a SNP distance matrix from a multiple sequence alignment (MSA) of sequences (all of the same length). Such distance matrices are often used as in studies of disease outbreaks. The output is a tabular file, for example this matrix describing four M. tuberculosis isolates and their relationship to the inferred ancestral reference sequence: =============== ======= ========= ========= ========= ========= snp-dists 0.6.3 MTB_anc ERR550641 ERR550671 ERR550691 ERR550703 --------------- ------- --------- --------- --------- --------- MTB_anc 0 746 726 772 749 --------------- ------- --------- --------- --------- --------- ERR550641 746 0 26 44 13 --------------- ------- --------- --------- --------- --------- ERR550671 726 26 0 64 29 --------------- ------- --------- --------- --------- --------- ERR550691 772 44 64 0 39 --------------- ------- --------- --------- --------- --------- ERR550703 749 13 29 39 0 =============== ======= ========= ========= ========= ========="
toolshed.g2.bx.psu.edu/repos/iuc/snpeff/snpEff_build_gb/5.2+galaxy1	"What it does
 This tool uses 
snpEff build
 to create a snpEff database. ------ .. class:: infomark 
Working with Genbank files
 Using Genbank data for creating databases has several advantages: #. Genbank files contain annotations (such as locations of genes) together with sequences. This ensures that these two are in sync with each other. #. When you are analyzing small genomes (or not so small) it is much more convenient to create a database on the fly and use it. .. class:: warningmark SnpEff errors out on highly fragmented genomes containing multiple scaffolds. This is because a single gene may be split between multiple scaffolds causing SnpEff to crash. If this is happening use the GFF route described below. ------- 
Genbank usage scenario
 Suppose you have a series of Illumina reads from an experiment involving 
E. coli
 K-12 MG1655. You want to map these reads to the reference genome of K-12 MG1655, call variants, and annotate them using snpEff. This tool enables you to follow the following analysis steps: #. Go to 
NCBI &lt;http://www.ncbi.nlm.nih.gov&gt;
 page for K-12 MG1655 genome (note that all NCBI genomes have similar list of files associated with them). #. Copy URL for file with extension 
gbff.gz
 #. Paste the URL into upload tool and set datatype to 
genbank.gz
. #. Use this tool to generate a snpEff database and FASTA sequences from the dataset you've uploaded during the previous step. #. Use your Illumina reads to map against FASTA dataset generated in the previous step using BWA-MEM. #. Call variants using 
Freebayes
. #. Annotate vcf output of Freebayes with 
SnpEff eff
 using database generated at step 2 (using 
Custom
 option for 
Genome source
 parameter). In this scenario Genbank dataset is used twice. First, it is used to produce FASTA sequences that are using by BWA to map against. Second, it is used to create snpEff database. This guarantees that you will not have any issues related to reference sequence naming. ------- .. class:: infomark 
Working with GFF files
 Alternatively you can create a SnpEff database from 
GFF3 &lt;https://en.wikipedia.org/wiki/General_feature_format&gt;
 files downloaded from NCBI or any other source. Using GFF dataset for building SnpEff database requires two inputs: #. The GFF file itself #. A genome in FASTA format The GFF file contains coordinates of various features, but does not contain underlying sequences. This is why a FASTA file needs to be provided as well. ------ 
GFF usage scenario
 The following example also uses 
E. coli
 K-12 MG1655: #. Go to 
NCBI &lt;http://www.ncbi.nlm.nih.gov&gt;
_ page for K-12 MG1655 genome. #. Copy URLs for files with 
gff.gz
 and 
fna.gz
 extensions. The first file contains annotations in GFF3 format. The second file contains entire genome as a FASTA record. #. Paste URLs into upload tool and set datatypes to 
gff3
 and 
fasta.gz
 for annotations and genome, respectively. #. Use this tool to generate a snpEff database from the GFF dataset. #. Map your reads against the FASTA dataset and continue as described in the above example. ------- .. class:: warningmark 
Using SnpEff in Galaxy: A few points to remember
 SnpEff relies on specially formatted databases to generate annotations. It will not work without them. There are several ways in which these databases can be obtained. 
Pre-cached databases
 Many standard (e.g., human, mouse, 
Drosophila
) databases are likely pre-cached within a given Galaxy instance. You should be able to see them listed in 
Genome
 drop-down of 
SnpEff eff
 tool. In you 
do not see them
 keep reading... 
Download pre-built databases
 SnpEff project generates large numbers of pre-build databases. To obtain and use them follow these steps: #. Use 
SnpEff databases
 tool to generate a list of existing databases. Note the name of the database you need. #. Use 
SnpEff download
 tool to download the database. #. Finally, use 
SnpEff eff
 by choosing the downloaded database from the history using 
Downloaded snpEff database in your history
 option of the 
Genome source
 parameter. Alternatively, you can specify the name of the database directly in 
SnpEff eff
 using the 
Download on demand
 option (again, 
Genome source
 parameter). In this case snpEff will download the database before performing annotation. 
Create your own database
 In cases when you are dealing with bacterial or viral (or, frankly, any other) genomes it may be easier to create database yourself. For this you need: #. Download Genbank record corresponding to your genome of interest from NCBI or use annotations in GFF format accompanied by the corresponding genome in FASTA format. #. Use 
SnpEff build
 to create the database. #. Use the database in 
SnpEff eff
 (using 
Custom
 option for 
Genome source
 parameter). Creating custom database has one major advantage. It guaranteess that you will not have any issues related to reference sequence naming -- the most common source of SnpEff errors. ------- To learn more about snpEff read its manual at http://snpeff.sourceforge.net/SnpEff_manual.html"
toolshed.g2.bx.psu.edu/repos/iuc/snpeff/snpEff_get_chr_names/5.2+galaxy1	This can only be used on built-in databases manually configured by your galaxy admin.
toolshed.g2.bx.psu.edu/repos/iuc/snpeff/snpEff_databases/5.2+galaxy1	"What it does
 This tool downloads the master list of snpEff databases from a remote SnpEff repository. You can then look at this list and decide which database to use for your analysis. For example, if 
List entries matching the following expression
 parameter of this tool is set to 
Mouse
, it will produce a tabular dataset with the following content:: mm10 Mouse mm39 Mouse mm9 Mouse This means that there are three available snpEff databases for mouse genome. If you want to use mm39 in you analysis: - set 
Genome source
 option of 
SnpEff eff
 Galaxy tool to 
Download on demand
 - enter 'mm39' into 
Snpff Genome Version Name
 text box ------- .. class:: infomark 
The usage scenario
 There are two ways to use names of databases obtained with this tool in Galaxy's version on snpEff: #. Use 
SnpEff download
 tool. It will download the database to the history and you will be able to use it in 
SnpEff eff
 tool using 
Downloaded snpEff database in your history
 option of the 
Genome source
 parameter. #. Use 
Download on demand
 option of the 
SnpEff eff
 tool (again, 
Genome source
 parameter). In this case snpEff will download the database before performing annotation. ------- .. class:: warningmark 
Using SnpEff in Galaxy: A few points to remember
 SnpEff relies on specially formatted databases to generate annotations. It will not work without them. There are several ways in which these databases can be obtained. 
Pre-cached databases
 Many standard (e.g., human, mouse, 
Drosophila
) databases are likely pre-cached within a given Galaxy instance. You should be able to see them listed in 
Genome
 drop-down of 
SnpEff eff
 tool. In you 
do not see them
 keep reading... 
Download pre-built databases
 SnpEff project generates large numbers of pre-build databases. To obtain and use them follow these steps: #. Use 
SnpEff databases
 tool to generate a list of existing databases. Note the name of the database you need. #. Use 
SnpEff download
 tool to download the database. #. Finally, use 
SnpEff eff
 by choosing the downloaded database from the history using 
Downloaded snpEff database in your history
 option of the 
Genome source
 parameter. Alternatively, you can specify the name of the database directly in 
SnpEff eff
 using the 
Download on demand
 option (again, 
Genome source
 parameter). In this case snpEff will download the database before performing annotation. 
Create your own database
 In cases when you are dealing with bacterial or viral (or, frankly, any other) genomes it may be easier to create database yourself. For this you need: #. Download Genbank record corresponding to your genome of interest from NCBI or use annotations in GFF format accompanied by the corresponding genome in FASTA format. #. Use 
SnpEff build
 to create the database. #. Use the database in 
SnpEff eff
 (using 
Custom
 option for 
Genome source
 parameter). Creating custom database has one major advantage. It guaranteess that you will not have any issues related to reference sequence naming -- the most common source of SnpEff errors. ------- To learn more about snpEff read its manual at http://snpeff.sourceforge.net/SnpEff_manual.html"
toolshed.g2.bx.psu.edu/repos/iuc/snpeff/snpEff_download/5.2+galaxy1	"What it does
 This tool downloads a specified database from a remote SnpEff repository. It deposits it into the history. ------- .. class:: infomark 
The usage scenario
 Suppose you want to annoate a VCF file containing variants within mm10 version of the Mouse genome. To do this you can: #. Download mm10 snpEff database by typing 
mm10
 into 
Select the annotation database...
 text box. #. Use 
SnpEff eff
 by choosing the downloaded database from the history using 
Downloaded snpEff database in your history
 option of the 
Genome source
 parameter. ------- .. class:: warningmark 
Using SnpEff in Galaxy: A few points to remember
 SnpEff relies on specially formatted databases to generate annotations. It will not work without them. There are several ways in which these databases can be obtained. 
Pre-cached databases
 Many standard (e.g., human, mouse, 
Drosophila
) databases are likely pre-cached within a given Galaxy instance. You should be able to see them listed in 
Genome
 drop-down of 
SnpEff eff
 tool. In you 
do not see them
 keep reading... 
Download pre-built databases
 SnpEff project generates large numbers of pre-build databases. To obtain and use them follow these steps: #. Use 
SnpEff databases
 tool to generate a list of existing databases. Note the name of the database you need. #. Use 
SnpEff download
 tool to download the database. #. Finally, use 
SnpEff eff
 by choosing the downloaded database from the history using 
Downloaded snpEff database in your history
 option of the 
Genome source
 parameter. Alternatively, you can specify the name of the database directly in 
SnpEff eff
 using the 
Download on demand
 option (again, 
Genome source
 parameter). In this case snpEff will download the database before performing annotation. 
Create your own database
 In cases when you are dealing with bacterial or viral (or, frankly, any other) genomes it may be easier to create database yourself. For this you need: #. Download Genbank record corresponding to your genome of interest from NCBI or use annotations in GFF format accompanied by the corresponding genome in FASTA format. #. Use 
SnpEff build
 to create the database. #. Use the database in 
SnpEff eff
 (using 
Custom
 option for 
Genome source
 parameter). Creating custom database has one major advantage. It guaranteess that you will not have any issues related to reference sequence naming -- the most common source of SnpEff errors. ------- To learn more about snpEff read its manual at http://snpeff.sourceforge.net/SnpEff_manual.html"
toolshed.g2.bx.psu.edu/repos/iuc/snpeff/snpEff/5.2+galaxy1	By default SnpEff simplifies all chromosome names. For instance 'chr1' is just '1'. You can prepend any string you want to the chromosome name
toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_annotate/4.3+t.galaxy1	This is typically used to annotate IDs from dbSnp. Annotating only the ID field from dbSnp137.vcf :: Input VCF: CHROM POS ID REF ALT QUAL FILTER INFO 22 16157571 . T G 0.0 FAIL NS=53 22 16346045 . T C 0.0 FAIL NS=244 22 16350245 . C A 0.0 FAIL NS=192 Annotated Output VCF: #CHROM POS ID REF ALT QUAL FILTER INFO 22 16157571 . T G 0.0 FAIL NS=53 22 16346045 rs56234788 T C 0.0 FAIL NS=244 22 16350245 rs2905295 C A 0.0 FAIL NS=192 Annotating both the ID and INFO fields from dbSnp137.vcf :: Input VCF: #CHROM POS ID REF ALT QUAL FILTER INFO 22 16157571 . T G 0.0 FAIL NS=53 22 16346045 . T C 0.0 FAIL NS=244 22 16350245 . C A 0.0 FAIL NS=192 Annotated Output VCF: #CHROM POS ID REF ALT QUAL FILTER INFO 22 16157571 . T G 0.0 FAIL NS=53 22 16346045 rs56234788 T C 0.0 FAIL NS=244;RSPOS=16346045;GMAF=0.162248628884826;dbSNPBuildID=129;SSR=0;SAO=0;VP=050100000000000100000100;WGT=0;VC=SNV;SLO;GNO 22 16350245 rs2905295 C A 0.0 FAIL NS=192;RSPOS=16350245;GMAF=0.230804387568556;dbSNPBuildID=101;SSR=1;SAO=0;VP=050000000000000100000140;WGT=0;VC=SNV;GNO For details about this tool, please go to: - http://snpeff.sourceforge.net/SnpEff_manual.html - http://snpeff.sourceforge.net/SnpSift.html#annotate
toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_caseControl/4.3+t.galaxy0	Case and control are defined by a string containing plus and minus symbols {'+', '-', '0'} where '+' is case, '-' is control and '0' is neutral
toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_extractFields/4.3+t.galaxy0	"What is does
 
SnpSift Extract Fields &lt;http://snpeff.sourceforge.net/SnpSift.html#Extract&gt;
 selects columns from a VCF dataset into a Tab-delimited format. ------ .. class:: infomark 
How to know which fields to extract?
 A VCF dataset contains mandatory fields as well as optional fields. Mandatory fields are required by 
VCF specifications &lt;https://samtools.github.io/hts-specs/VCFv4.2.pdf&gt;
 and present in any valid VCF dataset. The 
Fields to extract
 input box of the tool above is already pre-filled with names of mandatory fields. To know what other fields are available in a given VCF file simply look at its header. 
INFO
 and 
FORMAT
 lines will contain description of existing fields. For example, if you see a line: ##INFO=<ID=NS,Number=1,Type=Integer,Description=""Number of samples with data""> you can use 
NS
 as the field name. ------ 
Dealing with field generated with SnpEff
 The current version of 
SnpEff &lt;http://snpeff.sourceforge.net/SnpEff_manual.html&gt;
_ produces so called 
ANN
 fields:: ""ANN[
].ALLELE"" (alias GENOTYPE) ""ANN[
].EFFECT"" (alias ANNOTATION): Effect in Sequence ontology terms (e.g. 'missense_variant', 'synonymous_variant', 'stop_gained', etc.) ""ANN[
].IMPACT"" { HIGH, MODERATE, LOW, MODIFIER } ""ANN[
].GENE"" Gene name (e.g. 'PSD3') ""ANN[
].GENEID"" Gene ID ""ANN[
].FEATURE"" ""ANN[
].FEATUREID"" (alias TRID: Transcript ID) ""ANN[
].BIOTYPE"" Biotype, as described by the annotations (e.g. 'protein_coding') ""ANN[
].RANK"" Exon or Intron rank (i.e. exon number in a transcript) ""ANN[
].HGVS_C"" (alias HGVS_DNA, CODON): Variant in HGVS (DNA) notation ""ANN[
].HGVS_P"" (alias HGVS, HGVS_PROT, AA): Variant in HGVS (protein) notation ""ANN[
].CDNA_POS"" (alias POS_CDNA) ""ANN[
].CDNA_LEN"" (alias LEN_CDNA) ""ANN[
].CDS_POS"" (alias POS_CDS) ""ANN[
].CDS_LEN"" (alias LEN_CDS) ""ANN[
].AA_POS"" (alias POS_AA) ""ANN[
].AA_LEN"" (alias LEN_AA) ""ANN[
].DISTANCE"" ""ANN[
].ERRORS"" (alias WARNING, INFOS) Older versions produced 
EFF
 fields:: ""EFF[
].EFFECT"" ""EFF[
].IMPACT"" ""EFF[
].FUNCLASS"" ""EFF[
].CODON"" ""EFF[
].AA"" ""EFF[
].AA_LEN"" ""EFF[
].GENE"" ""EFF[
].BIOTYPE"" ""EFF[
].CODING"" ""EFF[
].TRID"" ""EFF[
].RANK"" In addition there are 
LOF
 and 
NMD
 fields:: ""LOF[
].GENE"" ""LOF[
].GENEID"" ""LOF[
].NUMTR"" ""LOF[
].PERC"" ""NMD[
].GENE"" ""NMD[
].GENEID"" ""NMD[
].NUMTR"" ""NMD[
].PERC"" To find our whether your VCF contains 
ANN
 or 
EFF
 annotations simply look at its header. ----- 
Usage examples
 
Extracting chromosome, position, ID and allele frequency from a VCF file
: 
CHROM POS ID AF
 The result will look something like:: #CHROM POS ID AF 1 69134 0.086 1 69496 rs150690004 0.001 
Extracting genotype fields
: 
CHROM POS ID THETA GEN[0].GL[1] GEN[1].GL GEN[3].GL[
] GEN[
].GT
 This means to extract: - CHROM POS ID: regular fields (as in the previous example) - THETA : This one is from INFO - GEN[0].GL[1] : Second likelihood from first genotype - GEN[1].GL : The whole GL field (all entries without separating them) - GEN[3].GL[
] : All likelihoods form genotype 3 (this time they will be tab separated, as opposed to the previous one). - GEN[
].GT : Genotype subfields (GT) from ALL samples (tab separated). The result will look something like:: #CHROM POS ID THETA GEN[0].GL[1] GEN[1].GL GEN[3].GL[
] GEN[
].GT 1 10583 rs58108140 0.0046 -0.47 -0.24,-0.44,-1.16 -0.48 -0.48 -0.48 0|0 0|0 0|0 0|1 0|0 0|1 0|0 0|0 0|1 1 10611 rs189107123 0.0077 -0.48 -0.24,-0.44,-1.16 -0.48 -0.48 -0.48 0|0 0|1 0|0 0|0 0|0 0|0 0|0 0|0 0|0 1 13302 rs180734498 0.0048 -0.58 -2.45,-0.00,-5.00 -0.48 -0.48 -0.48 0|0 0|1 0|0 0|0 0|0 1|0 0|0 0|1 0|0 
Extracting fields with multiple values
: (notice that there are multiple effect columns per line because there are multiple effects per variant) 
CHROM POS REF ALT ANN[*].EFFECT
 The result will look something like:: #CHROM POS REF ALT ANN[
].EFFECT 22 17071756 T C 3_prime_UTR_variant downstream_gene_variant 22 17072035 C T missense_variant downstream_gene_variant 22 17072258 C A missense_variant downstream_gene_variant 
Extracting fields with multiple values using a comma as a multiple field separator:
 
CHROM POS REF ALT ANN[
].EFFECT ANN[
].HGVS_P
 The result will look something like:: #CHROM POS REF ALT ANN[
].EFFECT ANN[
].HGVS_P 22 17071756 T C 3_prime_UTR_variant,downstream_gene_variant .,. 22 17072035 C T missense_variant,downstream_gene_variant p.Gly469Glu,. 22 17072258 C A missense_variant,downstream_gene_variant p.Gly395Cys,. 
Extracting fields with multiple values, one effect per line:
 
CHROM POS REF ALT ANN[
].EFFECT*
 The result will look something like:: #CHROM POS REF ALT ANN[
].EFFECT 22 17071756 T C 3_prime_UTR_variant 22 17071756 T C downstream_gene_variant 22 17072035 C T missense_variant 22 17072035 C T downstream_gene_variant 22 17072258 C A missense_variant 22 17072258 C A downstream_gene_variant For details about this tool, please go to: - http://snpeff.sourceforge.net/SnpEff_manual.html - http://snpeff.sourceforge.net/SnpSift.html#Extract"
toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_filter/4.3+t.galaxy1	"SnpSift filter
 This tool provides a flexible solution for filtering the variants in a VCF input dataset through the use of arbitrary, possibly rather complex expressions. Some examples: - 
I want just the variants from the second million bases of chr1
:: ( CHROM = 'chr1' ) & ( POS > 1000000 ) & ( POS < 2000000 ) - 
Filter value is either 'PASS' or it is missing
:: (FILTER = 'PASS') | ( na FILTER ) - 
Variants that have either a QUAL score above 30, or are indel variants, or for which at least two samples have a heterozygous genotype called
:: (QUAL > 30) | (exists INDEL) | ( countHet() > 2 ) - 
Variants that are supported by at least 10 reads (as calculated from the DP4 attribute in the INFO field through zero-based index-access to the multiple values)
:: (DP4[2] + DP4[3] >= 10) ---- Sets: The tool can construct sets of values for use in expressions from text files listing one value per line. Variants can then be filtered based on whether a given field in the variant record has a value that's contained in a set. For example, the expression:: ( ID in SET[2] ) would filter variants based on whether their ID field value appears in the set parsed from the third dataset used for set construction (the first set can be addressed with index 
[0]
, the second with index 
[1]
, and so on). ---- Genotype-based filtering: Genotypes of specific samples can be accessed via zero-based indexing or via sample names. - 
I want to keep samples where the genotype for the first sample is homozygous variant and the genotype for the second sample is reference
:: (isHom( GEN[0] ) & isVariant( GEN[0] ) & isRef( GEN[1] )) ---- Filtering based on SnpEff annotations (
ANN
 or 
EFF
 fields): - 
I want to filter lines with an ANN annotation EFFECT of 'frameshift_variant' ( for vcf files using Sequence Ontology terms )
:: ( ANN[
].EFFECT has 'frameshift_variant' ) .. class:: infomark According to the specification, there can be more than one EFFECT separated by 
&amp;
 (e.g. 
'missense_variant&amp;splice_region_variant'
), thus using the 
has
 operator is better than using the equality operator (
=
). For instance, 
'missense_variant&amp;splice_region_variant' = 'missense_variant'
 is false, whereas 
'missense_variant&amp;splice_region_variant' has 'missense_variant'
 is true. - 
I want to filter lines with an EFF of 'FRAME_SHIFT' ( for vcf files using Classic Effect names )
:: ( EFF[
].EFFECT = 'FRAME_SHIFT' ) .. class:: infomark For information regarding HGVS and Sequence Ontology terms versus classic names: - https://pcingola.github.io/SnpEff/se_commandline/ for the options: 
-classic
, 
-hgvs
, and 
-sequenceOntology
 - https://pcingola.github.io/SnpEff/se_inputoutput/#effect-prediction-details for the table containing the classic name and sequence onology term for each effect For details about this tool, please go to: - http://snpeff.sourceforge.net/SnpEff_manual.html - https://pcingola.github.io/SnpEff/ss_filter/ The second link in particular has further details and more examples about the tool's expression syntax."
toolshed.g2.bx.psu.edu/repos/iuc/snpsift_genesets/snpSift_geneSets/4.3.0	"This tool uses 
SnpSift GeneSets
 to add annotations from 
MSigDB
, a collection of annotated gene sets from different sources including Gene Ontology (GO), KEGG, Reactome. .. _SnpSift GeneSets: http://snpeff.sourceforge.net/SnpSift.html#geneSets .. class:: warningmark The input VCF file must be annotated using SnpEff before performing GeneSets annotations. This is because the tool must know which gene the variant affects. For details about this tool, please go to: - http://snpeff.sourceforge.net/SnpEff_manual.html .. _MSigDB: http://www.broadinstitute.org/gsea/msigdb/"
toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_int/4.3+t.galaxy0	You can filter using intervals (BED file). For details about this tool, please go to: - http://snpeff.sourceforge.net/SnpEff_manual.html - http://snpeff.sourceforge.net/SnpSift.html#intervals
toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpsift_vartype/4.3+t.galaxy0	"What it does
 This tool uses 
SnpSift Variant type
 to add the variant type (SNP/MNP/INS/DEL/MIXED) in the INFO field. It also adds ""HOM/HET"", but this last one works if there is only one sample (otherwise it doesn't make any sense). .. _SnpSift Variant type: http://snpeff.sourceforge.net/SnpSift.html#VariantType ------ 
License
 This Galaxy tool is Copyright © 2013-2014 
CRS4 Srl.
 and is released under the 
MIT license
_. .. _CRS4 Srl.: http://www.crs4.it/ .. _MIT license: https://opensource.org/licenses/MIT"
toolshed.g2.bx.psu.edu/repos/iuc/snpsift_dbnsfp/snpSift_dbnsfp/4.3.0	"The dbNSFP is an integrated database of functional predictions from multiple algorithms (SIFT, Polyphen2, LRT and MutationTaster, PhyloP and GERP++, etc.). It contains variant annotations such as: 1000Gp1_AC Alternative allele counts in the whole 1000 genomes phase 1 (1000Gp1) data 1000Gp1_AF Alternative allele frequency in the whole 1000Gp1 data 1000Gp1_AFR_AC Alternative allele counts in the 1000Gp1 African descendent samples 1000Gp1_AFR_AF Alternative allele frequency in the 1000Gp1 African descendent samples 1000Gp1_AMR_AC Alternative allele counts in the 1000Gp1 American descendent samples 1000Gp1_AMR_AF Alternative allele frequency in the 1000Gp1 American descendent samples 1000Gp1_ASN_AC Alternative allele counts in the 1000Gp1 Asian descendent samples 1000Gp1_ASN_AF Alternative allele frequency in the 1000Gp1 Asian descendent samples 1000Gp1_EUR_AC Alternative allele counts in the 1000Gp1 European descendent samples 1000Gp1_EUR_AF Alternative allele frequency in the 1000Gp1 European descendent samples aaalt Alternative amino acid. ""."" if the variant is a splicing site SNP (2bp on each end of an intron) aapos Amino acid position as to the protein. ""-1"" if the variant is a splicing site SNP (2bp on each end of an intron) aapos_SIFT ENSP id and amino acid positions corresponding to SIFT scores. Multiple entries separated by "";"" aapos_FATHMM ENSP id and amino acid positions corresponding to FATHMM scores. Multiple entries separated by "";"" aaref Reference amino acid. ""."" if the variant is a splicing site SNP (2bp on each end of an intron) alt Alternative nucleotide allele (as on the + strand) Ancestral_allele Ancestral allele (based on 1000 genomes reference data) cds_strand Coding sequence (CDS) strand (+ or -) chr Chromosome number codonpos Position on the codon (1, 2 or 3) Ensembl_geneid Ensembl gene ID Ensembl_transcriptid Ensembl transcript IDs (separated by "";"") ESP6500_AA_AF Alternative allele frequency in the African American samples of the NHLBI GO Exome Sequencing Project (ESP6500 data set) ESP6500_EA_AF Alternative allele frequency in the European American samples of the NHLBI GO Exome Sequencing Project (ESP6500 data set) FATHMM_pred If a FATHMM_score is <=-1.5 (or rankscore <=0.81415) the corresponding non-synonymous SNP is predicted as ""D(AMAGING)""; otherwise it is predicted as ""T(OLERATED)"". Multiple predictions separated by "";"" FATHMM_rankscore FATHMMori scores were ranked among all FATHMMori scores in dbNSFP. The rankscore is the ratio of the rank of the score over the total number of FATHMMori scores in dbNSFP. If there are multiple scores, only the most damaging (largest) rankscore is presented. The scores range from 0 to 1 FATHMM_score FATHMM default score (FATHMMori) fold-degenerate Degenerate type (0, 2 or 3) genename Gene name; if the non-synonymous SNP can be assigned to multiple genes, gene names are separated by "";"" GERP++_NR GERP++ neutral rate GERP++_RS GERP++ RS score, the larger the score, the more conserved the site GERP++_RS_rankscore GERP++ RS scores were ranked among all GERP++ RS scores in dbNSFP. The rankscore is the ratio of the rank of the score over the total number of GERP++ RS scores in dbNSFP hg18_pos(1-coor) Physical position on the chromosome as to hg18 (1-based coordinate) Interpro_domain Domain or conserved site on which the variant locates LR_pred Prediction of our LR based ensemble prediction score, ""T(olerated)"" or ""D(amaging)"". The score cutoff between ""D"" and ""T"" is 0.5. The rankscore cutoff between ""D"" and ""T"" is 0.82268 LR_rankscore LR scores were ranked among all LR scores in dbNSFP. The rankscore is the ratio of the rank of the score over the total number of LR scores in dbNSFP. The scores range from 0 to 1 LR_score Our logistic regression (LR) based ensemble prediction score, which incorporated 10 scores (SIFT, PolyPhen-2 HDIV, PolyPhen-2 HVAR, GERP++, MutationTaster, Mutation Assessor, FATHMM, LRT, SiPhy, PhyloP) and the maximum frequency observed in the 1000 genomes populations. Larger value means the SNV is more likely to be damaging. Scores range from 0 to 1 LRT_Omega Estimated nonsynonymous-to-synonymous-rate ratio (Omega, reported by LRT) LRT_converted_rankscore LRTori scores were first converted as LRTnew=1-LRTori
0.5 if Omega<1, or LRTnew=LRTori
0.5 if Omega>=1. Then LRTnew scores were ranked among all LRTnew scores in dbNSFP. The rankscore is the ratio of the rank over the total number of the scores in dbNSFP. The scores range from 0.00166 to 0.85682 LRT_pred LRT prediction, D(eleterious), N(eutral) or U(nknown), which is not solely determined by the score LRT_score The original LRT two-sided p-value (LRTori), ranges from 0 to 1 MutationAssessor_pred MutationAssessor's functional impact of a variant MutationAssessor_rankscore MAori scores were ranked among all MAori scores in dbNSFP. The rankscore is the ratio of the rank of the score over the total number of MAori scores in dbNSFP. The scores range from 0 to 1 MutationAssessor_score MutationAssessor functional impact combined score (MAori) MutationTaster_converted_rankscore The MTori scores were first converted: if the prediction is ""A"" or ""D"" MTnew=MTori; if the prediction is ""N"" or ""P"", MTnew=1-MTori. Then MTnew scores were ranked among all MTnew scores in dbNSFP. The rankscore is the ratio of the rank of the score over the total number of MTnew scores in dbNSFP. The scores range from 0.0931 to 0.80722 MutationTaster_pred MutationTaster prediction MutationTaster_score MutationTaster p-value (MTori), ranges from 0 to 1 phastCons46way_placental phastCons conservation score based on the multiple alignments of 33 placental mammal genomes (including human). The larger the score, the more conserved the site phastCons46way_placental_rankscore phastCons46way_placental scores were ranked among all phastCons46way_placental scores in dbNSFP. The rankscore is the ratio of the rank of the score over the total number of phastCons46way_placental scores in dbNSFP phastCons46way_primate phastCons conservation score based on the multiple alignments of 10 primate genomes (including human). The larger the score, the more conserved the site phastCons46way_primate_rankscore phastCons46way_primate scores were ranked among all phastCons46way_primate scores in dbNSFP. The rankscore is the ratio of the rank of the score over the total number of phastCons46way_primate scores in dbNSFP phastCons100way_vertebrate phastCons conservation score based on the multiple alignments of 100 vertebrate genomes (including human). The larger the score, the more conserved the site phastCons100way_vertebrate_rankscore phastCons100way_vertebrate scores were ranked among all phastCons100way_vertebrate scores in dbNSFP. The rankscore is the ratio of the rank of the score over the total number of phastCons100way_vertebrate scores in dbNSFP phyloP46way_placental phyloP (phylogenetic p-values) conservation score based on the multiple alignments of 33 placental mammal genomes (including human). The larger the score, the more conserved the site phyloP46way_placental_rankscore phyloP46way_placental scores were ranked among all phyloP46way_placental scores in dbNSFP. The rankscore is the ratio of the rank of the score over the total number of phyloP46way_placental scores in dbNSFP phyloP46way_primate phyloP (phylogenetic p-values) conservation score based on the multiple alignments of 10 primate genomes (including human). The larger the score, the more conserved the site phyloP46way_primate_rankscore phyloP46way_primate scores were ranked among all phyloP46way_primate scores in dbNSFP. The rankscore is the ratio of the rank of the score over the total number of phyloP46way_primate scores in dbNSFP phyloP100way_vertebrate phyloP (phylogenetic p-values) conservation score based on the multiple alignments of 100 vertebrate genomes (including human). The larger the score, the more conserved the site phyloP100way_vertebrate_rankscore phyloP100way_vertebrate scores were ranked among all phyloP100way_vertebrate scores in dbNSFP. The rankscore is the ratio of the rank of the score over the total number of phyloP100way_vertebrate scores in dbNSFP Polyphen2_HDIV_pred Polyphen2 prediction based on HumDiv Polyphen2_HDIV_rankscore Polyphen2 HDIV scores were first ranked among all HDIV scores in dbNSFP. The rankscore is the ratio of the rank the score over the total number of the scores in dbNSFP. If there are multiple scores, only the most damaging (largest) rankscore is presented. The scores range from 0.02656 to 0.89917 Polyphen2_HDIV_score Polyphen2 score based on HumDiv, i.e. hdiv_prob. The score ranges from 0 to 1. Multiple entries separated by "";"" Polyphen2_HVAR_pred Polyphen2 prediction based on HumVar Polyphen2_HVAR_rankscore Polyphen2 HVAR scores were first ranked among all HVAR scores in dbNSFP. The rankscore is the ratio of the rank the score over the total number of the scores in dbNSFP. If there are multiple scores, only the most damaging (largest) rankscore is presented. The scores range from 0.01281 to 0.9711 Polyphen2_HVAR_score Polyphen2 score based on HumVar, i.e. hvar_prob. The score ranges from 0 to 1. Multiple entries separated by "";"" pos(1-coor) Physical position on the chromosome as to hg19 (1-based coordinate) RadialSVM_pred Prediction of our SVM based ensemble prediction score, ""T(olerated)"" or ""D(amaging)"". The score cutoff between ""D"" and ""T"" is 0. The rankscore cutoff between ""D"" and ""T"" is 0.83357 RadialSVM_rankscore RadialSVM scores were ranked among all RadialSVM scores in dbNSFP. The rankscore is the ratio of the rank of the screo over the total number of RadialSVM scores in dbNSFP. The scores range from 0 to 1 RadialSVM_score Our support vector machine (SVM) based ensemble prediction score, which incorporated 10 scores (SIFT, PolyPhen-2 HDIV, PolyPhen-2 HVAR, GERP++, MutationTaster, Mutation Assessor, FATHMM, LRT, SiPhy, PhyloP) and the maximum frequency observed in the 1000 genomes populations. Larger value means the SNV is more likely to be damaging. Scores range from -2 to 3 in dbNSFP ref Reference nucleotide allele (as on the + strand) refcodon Reference codon Reliability_index Number of observed component scores (except the maximum frequency in the 1000 genomes populations) for RadialSVM and LR. Ranges from 1 to 10. As RadialSVM and LR scores are calculated based on imputed data, the less missing component scores, the higher the reliability of the scores and predictions SIFT_converted_rankscore SIFTori scores were first converted to SIFTnew=1-SIFTori, then ranked among all SIFTnew scores in dbNSFP. The rankscore is the ratio of the rank the SIFTnew score over the total number of SIFTnew scores in dbNSFP. If there are multiple scores, only the most damaging (largest) rankscore is presented. The rankscores range from 0.02654 to 0.87932 SIFT_pred If SIFTori is smaller than 0.05 (rankscore>0.55) the corresponding non-synonymous SNP is predicted as ""D(amaging)""; otherwise it is predicted as ""T(olerated)"". Multiple predictions separated by "";"" SIFT_score SIFT score (SIFTori). Scores range from 0 to 1. The smaller the score the more likely the SNP has damaging effect. Multiple scores separated by "";"" SiPhy_29way_logOdds SiPhy score based on 29 mammals genomes. The larger the score, the more conserved the site SiPhy_29way_pi The estimated stationary distribution of A, C, G and T at the site, using SiPhy algorithm based on 29 mammals genomes SLR_test_statistic SLR test statistic for testing natural selection on codons. A negative value indicates negative selection, and a positive value indicates positive selection. Larger magnitude of the value suggests stronger evidence Uniprot_aapos Amino acid position as to Uniprot. Multiple entries separated by "";"" Uniprot_acc Uniprot accession number. Multiple entries separated by "";"" Uniprot_id Uniprot ID number. Multiple entries separated by "";"" UniSNP_ids rs numbers from UniSNP, which is a cleaned version of dbSNP build 129, in format: rs number1;rs number2;... The dbNSFP database is available from https://sites.google.com/site/jpopgen/dbNSFP and there is only annotation for human genome builds. The procedure for preparing the dbNSFP data for use in SnpSift dbnsfp and a couple of prebuilt dbNSFP databases are available at: http://snpeff.sourceforge.net/SnpSift.html#dbNSFP However, any dbNSFP-like tabular file that be can used with SnpSift dbnsfp if it has: - The first line of the file must be column headers that name the annotations. - The first 4 columns are required and must be: 1. chr: chromosome 2. pos(1-coor): position in chromosome 3. ref: reference base 4. alt: alternate base For example:: #chr pos(1-coor) ref alt aaref aaalt genename SIFT_score 1 69134 A C E A OR4F5 0.03 1 69134 A G E G OR4F5 0.09 1 69134 A T E V OR4F5 0.03 4 100239319 T A H L ADH1B 0 4 100239319 T C H R ADH1B 0.15 4 100239319 T G H P ADH1B 0 The Galaxy datatypes for dbNSFP can automatically convert the specially formatted tabular file for use by SnpSift dbNSFP: 1. Upload the tabular file, set the datatype as: 
""dbnsfp.tabular""
 2. Edit the history dataset attributes (pencil icon): Use ""Convert Format"" to convert the 
""dbnsfp.tabular""
 to the correct format for SnpSift dbnsfp: 
""snpsiftdbnsfp""
. For details about this tool, please go to: - http://snpeff.sourceforge.net/SnpEff_manual.html - http://snpeff.sourceforge.net/SnpSift.html#dbNSFP"
toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_rmInfo/4.3+t.galaxy0	Separate multiple INFO fields with a comma, e.g.: EFF,DP
toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_vcfCheck/4.3+t.galaxy0	Perform some basic check ups on VCF files to spot common problems. SnpSift vcfCheck checks for some common problems where VCF files are not following the specification. Given that many common VCF problems cause analysis tools and pipelines to behave unexpectedly, this command is intended as a simple debugging tool. For details about this tool, please go to: - http://snpeff.sourceforge.net/SnpEff_manual.html - http://snpeff.sourceforge.net/SnpSift.html#vcfCheck
toolshed.g2.bx.psu.edu/repos/iuc/strelka_germline/strelka_germline/2.9.10+galaxy0	".. class:: infomark 
What it does
 Strelka2 is a fast and accurate small variant caller optimized for analysis of germline variation in small cohorts (Strelka Germline) and somatic variation in tumor/normal sample pairs (Strelka Somatic). Strelka accepts input read mappings from BAM or CRAM files, and optionally candidate and/or forced-call alleles from VCF. It reports all small variant predictions in VCF 4.1 format. Germline variant reporting uses the gVCF conventions to represent both variant and reference call confidence. For best somatic indel performance, Strelka is designed to be run with the Manta structural variant and indel caller, which provides additional indel candidates up to a given maxiumum indel size (by default this is 49). By design, Manta and Strelka run together with default settings provide complete coverage over all indel sizes (in additional to all SVs and SNVs) for clinical somatic and germline analysis scenarios. The germline caller employs an efficient tiered haplotype model to improve accuracy and provide read-backed phasing, adaptively selecting between assembly and a faster alignment-based haplotyping approach at each variant locus. The germline caller also analyzes input sequencing data using a mixture-model indel error estimation method to improve robustness to indel noise. 
Input
 
Sequencing Data
 The input sequencing reads are expected to come from a paired-end sequencing assay. Any input other than paired-end reads are ignored by default except to double-check for putative somatic variant evidence in the normal sample during somatic variant analysis. Read lengths above ~400 bases are not tested. 
Alignment Files
 All input sequencing reads should be mapped by an external tool and provided as input in 
BAM &lt;https://samtools.github.io/hts-specs/SAMv1.pdf&gt;
. or 
CRAM &lt;https://samtools.github.io/hts-specs/CRAMv3.pdf&gt;
 format. The following limitations apply to the input BAM/CRAM alignment records: - Alignments cannot contain the ""="" character in the SEQ field. - RG (read group) tags are ignored -- each alignment file must represent one sample. - Alignments with basecall quality values greater than 70 will trigger a runtime error (these are not supported on the assumption that the high basecall quality indicates an offset error) 
VCF Files
 Input 
VCF &lt;http://samtools.github.io/hts-specs/VCFv4.1.pdf&gt;
 files are accepted for a number of roles as described below. All input VCF records are checked for compatibility with the given reference genome, in additional to role-specific checks described below. If any VCF record's REF field is not compatible with the reference genome a runtime error will be triggered. 'Compatible with the reference genome' means that each VCF record's REF base either (1) matches the corresponding reference genome base or the VCF record's REF base is 'N' or the reference genome base is any ambiguous IUPAC base code (all ambiguous base codes are converted to 'N' while importing the reference). 
Output
 
Variants
 This describes all potential variant loci across all samples. Note this file includes non-variant loci if they have a non-trivial level of variant evidence or contain one or more alleles for which genotyping has been forced. Please see the multi-sample variants VCF section below for additional details on interpreting this file. 
Genome
 This is the genome VCF output for sample N, which includes both variant records and compressed non-variant blocks. The sample index, N is 1-indexed and corresponds to the input order of alignment files on the configuration command-line. .. class:: infomark 
References
 More information are available on 
github &lt;https://github.com/Illumina/strelka&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/strelka_somatic/strelka_somatic/2.9.10+galaxy0	".. class:: infomark 
What it does
 Strelka2 is a fast and accurate small variant caller optimized for analysis of germline variation in small cohorts (Strelka Germline) and somatic variation in tumor/normal sample pairs (Strelka Somatic). Strelka accepts input read mappings from BAM or CRAM files, and optionally candidate and/or forced-call alleles from VCF. It reports all small variant predictions in VCF 4.1 format. Germline variant reporting uses the gVCF conventions to represent both variant and reference call confidence. For best somatic indel performance, Strelka is designed to be run with the Manta structural variant and indel caller, which provides additional indel candidates up to a given maxiumum indel size (by default this is 49). By design, Manta and Strelka run together with default settings provide complete coverage over all indel sizes (in additional to all SVs and SNVs) for clinical somatic and germline analysis scenarios. The somatic calling model improves on the original Strelka method for liquid and late-stage tumor analysis by accounting for possible tumor cell contamination in the normal sample. A final empirical variant re-scoring step using random forest models trained on various call quality features has been added to both callers to further improve precision. 
Input
 
Sequencing Data
 The input sequencing reads are expected to come from a paired-end sequencing assay. Any input other than paired-end reads are ignored by default except to double-check for putative somatic variant evidence in the normal sample during somatic variant analysis. Read lengths above ~400 bases are not tested. 
Alignment Files
 All input sequencing reads should be mapped by an external tool and provided as input in 
BAM &lt;https://samtools.github.io/hts-specs/SAMv1.pdf&gt;
. or 
CRAM &lt;https://samtools.github.io/hts-specs/CRAMv3.pdf&gt;
 format. The following limitations apply to the input BAM/CRAM alignment records: - Alignments cannot contain the ""="" character in the SEQ field. - RG (read group) tags are ignored -- each alignment file must represent one sample. - Alignments with basecall quality values greater than 70 will trigger a runtime error (these are not supported on the assumption that the high basecall quality indicates an offset error) 
VCF Files
 Input 
VCF &lt;http://samtools.github.io/hts-specs/VCFv4.1.pdf&gt;
 files are accepted for a number of roles as described below. All input VCF records are checked for compatibility with the given reference genome, in additional to role-specific checks described below. If any VCF record's REF field is not compatible with the reference genome a runtime error will be triggered. 'Compatible with the reference genome' means that each VCF record's REF base either (1) matches the corresponding reference genome base or the VCF record's REF base is 'N' or the reference genome base is any ambiguous IUPAC base code (all ambiguous base codes are converted to 'N' while importing the reference). 
Output
 
INDEL
 All somatic indels inferred in the tumor sample in VCF format. 
SNVS
 All somatic SNVs inferred in the tumor sample in VCF format. 
Callability
 The somatic variant caller can be configured with the option --outputCallableRegions, which will extend the somatic SNV quality model calculation to be applied as a test of somatic SNV callability at all positions in the genome. The outcome of this callability calculation will be summarized in a BED-formatted callability track. This BED track contains regions which are determined to be callable, indicating that there is sufficient evidence to either call a somatic SNV or assert the absence of a somatic SNV with a variant frequency of 10% or greater. Both somatic and non-somatic sites are determined to be 'callable' if the somatic or non-somatic quality threshold is at least 15. .. class:: infomark 
References
 More information are available on 
github &lt;https://github.com/Illumina/strelka&gt;
."
toolshed.g2.bx.psu.edu/repos/devteam/varscan_version_2/varscan/2.4.2	"VarScan Overview
 VarScan_ performs variant detection for massively parallel sequencing data, such as exome, WGS, and transcriptome data. It calls variants from a mpileup dataset and produces a VCF 4.1 Full documentation is available online_. .. _VarScan: http://dkoboldt.github.io/varscan/ .. _online: http://dkoboldt.github.io/varscan/using-varscan.html 
Input
 :: mpileup file - The SAMtools mpileup file 
Output
 VarScan produces a VCF 4.1 dataset as output. 
Parameters
 :: analysis type single nucleotide detection Identify SNPs from an mpileup file insertions and deletion Identify indels an mpileup file consensus genotype Call consensus and variants from an mpileup file min-coverage Minimum read depth at a position to make a call [8] min-reads2 Minimum supporting reads at a position to call variants [2] min-avg-qual Minimum base quality at a position to count a read [15] min-var-freq Minimum variant allele frequency threshold [0.01] min-freq-for-hom Minimum frequency to call homozygote [0.75] p-value Default p-value threshold for calling variants [99e-02] strand-filter Ignore variants with >90% support on one strand [1] output-vcf If set to 1, outputs in VCF format vcf-sample-list For VCF output, a list of sample names in order, one per line variants Report only variant (SNP/indel) positions [0]"
toolshed.g2.bx.psu.edu/repos/iuc/varscan_copynumber/varscan_copynumber/2.4.3.2	"VarScan Overview
 VarScan_ performs variant detection for massively parallel sequencing data, such as exome, WGS, and transcriptome data. Full documentation of the command line package is available here_. .. _VarScan: http://dkoboldt.github.io/varscan/ .. _here: http://dkoboldt.github.io/varscan/using-varscan.html 
Input
 :: mpileup file - The SAMtools mpileup files for the normal and tumor tissue 
Output
 VarScan produces a VCF 4.1 dataset as output."
toolshed.g2.bx.psu.edu/repos/iuc/varscan_mpileup/varscan_mpileup/2.4.3.1	"VarScan Overview
 VarScan_ performs variant detection for massively parallel sequencing data, such as exome, WGS, and transcriptome data. Full documentation of the command line package is available here_. .. _VarScan: http://dkoboldt.github.io/varscan/ .. _here: http://dkoboldt.github.io/varscan/using-varscan.html 
Input
 :: mpileup file - The SAMtools mpileup file 
Output
 VarScan produces a VCF 4.1 dataset as output."
toolshed.g2.bx.psu.edu/repos/iuc/varscan_somatic/varscan_somatic/2.4.3.6	"VarScan Overview
 VarScan_ performs variant detection for massively parallel sequencing data, such as exome, WGS, and transcriptome data. Full documentation of the command line package is available here_. .. 
VarScan: http://dkoboldt.github.io/varscan/ .. _here: http://dkoboldt.github.io/varscan/using-varscan.html 
The Varscan Somatic tool for Galaxy
 This tool wraps the functionality of the 
varscan somatic
 and the 
varscan fpfilter
 command line tools. The tool is designed to detect genetic variants in a 
pair of samples
 representing normal and tumor tissue from the same individual. It classifies the variants, according to their most likely origin, as 
somatic
 (variant is found in the tumor, but not in the normal sample, 
i.e.
, is the consequence of a somatic mutation event), 
germline
 (variant is found in both samples => germline mutation event) and 
LOH
 (variant is found in both samples, but only the tumor sample appears to be homozygous for it => loss of heterozygosity event). This classification is encoded in the variant 
INFO
 fields of the VCF output produced by the tool in the form of a status code 
SS
 (somatic status), where: - 
SS=1
 signifies a likely germline variant, - 
SS=2
 a somatic variant - 
SS=3
 a LOH variant In addition, 
SS=0
 indicates a possible variant, but with insufficient evidence for an, at least, heterozygous state in either individual sample, and 
SS=5
 is used for variants of unexplained origin (
e.g.
, variants found in the normal, but not in the tumor tissue sample). In a second step, following variant calling, the tool can try to detect likely false-positive calls by re-inspecting the data at the variant sites more carefully and looking for signs that may indicate problems with the sequencing data or its mapping. If a called variant is deemed a possible false-positive at this step, this gets indicated in the 
FILTER
 field of the variant record in the VCF output. For high confidence variants passing all posterior (applied after variant calling) filters the value of the field will be 
PASS
, for variants failing any of the posterior filters the value will be a 
;
-separated list of the problematic filters. 
Input
 The tool takes as input a reference genome (in fasta format) and a pair of aligned reads datasets (bam format). 
Output
 A VCF dataset of called variants. When asked to 
Generate separate output datasets for SNP and indel calls
, the tool will behave like the 
varscan somatic
 command line tool and produce two VCF datasets - one with just the single nucleotide variants, while the other one will store insertion/deletion variants. 
Options
 
Estimated purity of normal sample / of tumor sample
 Since, in practice, it is often impossible to isolate tissue samples without contamination from surrounding tissue or from invading cells, these two fields let you indicate your estimate of the purity of the two samples (as fractions between 0 and 1, where 1 would indicate a contamination-free sample and 0.5 a sample to which the desired tissue contributes only 50%, while the other 50% consist of cells from the other tissue type). 
Settings for Variant Calling
 Settings in this section will affect the steps of variant calling and classification. You can accept VarScan's default values for the corresponding parameters or customize them according to your needs. 
Settings for Posterior Variant Filtering
 Use the parameters in this section to configure the false-positive filtering step that follows variant calling and classification. These settings will not influence the number of variants detected nor their classification, but may change the 
FILTER
 field of variant records to indicate which variants failed to pass certain filters. You can use this information with downstream tools to exclude certain variants from further analysis steps or include only high confidence variants that passed all filters (those with 
PASS
 as their 
INFO
 field value. You can accept the orignal filter defaults of the 
varscan fpfilter
 command line tool, use the settings established for the tool in the 
DREAM3 challenge
, or choose to customize the settings. Alternatively, you can also choose to skip posterior filtering entirely, in which case all variants will have their 
INFO
 field set to 
PASS
. .. _DREAM3 challenge: https://www.synapse.org/#!Synapse:syn312572/wiki/58893"
toolshed.g2.bx.psu.edu/repos/nick/allele_counts/allele_counts_1/1.3.2	".. class:: infomark 
What it does
 This tool parses variant counts from a special VCF file. It counts simple variants, calculates numbers of alleles, and calculates minor allele frequency. It can apply filters based on coverage, strand bias, and minor allele frequency cutoffs. ----- .. class:: infomark 
Input Format
 .. class:: warningmark 
Note:
 variants that are not A/C/G/T SNVs will be ignored! The input VCF should be like the output of the 
Naive Variant Detector
 tool (using the stranded option). The sample column(s) must give the read count for each variant 
on each strand
. Below is an example of a valid sample column entry (the important part is after the last colon):: 0/0:1:0.02:+T=27,+G=1,-T=22, ----- .. class:: infomark 
Output
 Each row represents one site in one sample. For 
unstranded
 output, 13 fields give information about that site:: 1. SAMPLE - Sample name (from VCF sample column labels) 2. CHR - Chromosome of the site 3. POS - Chromosomal coordinate of the site 4. A - Number of reads supporting an 'A' 5. C - 'C' reads 6. G - 'G' reads 7. T - 'T' reads 8. CVRG - Total (number of reads supporting one of the four bases above) 9. ALLELES - Number of qualifying alleles 10. MAJOR - Major allele 11. MINOR - Minor allele (2nd most prevalent variant) 12. MAF - Frequency of minor allele 13. BIAS - Strand bias measure For stranded output, instead of using 4 columns to report read counts per base, 8 are used to report the stranded counts per base:: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SAMPLE CHR POS +A +C +G +T -A -C -G -T CVRG ALLELES MAJOR MINOR MAF BIAS 
Example
 Below is a header line, followed by some example data lines. Since the input contained three samples, the data for each site is reported on three consecutive lines. However, if a sample fell below the coverage threshold at that site, the line will be omitted:: #SAMPLE CHR POS A C G T CVRG ALLELES MAJOR MINOR MAF BIAS BLOOD_1 chr20 99 0 101 1 2 104 1 C T 0.01923 0.33657 BLOOD_2 chr20 99 82 44 0 1 127 2 A C 0.34646 0.07823 BLOOD_3 chr20 99 0 110 1 0 111 1 C G 0.009 1.00909 BLOOD_1 chr20 100 3 5 100 0 108 1 G C 0.0463 0.15986 BLOOD_3 chr20 100 1 118 11 0 130 0 C G 0.08462 0.04154 ----- .. class:: warningmark 
Site printing and allele tallying requirements
 Coverage threshold: If a coverage threshold is used, the number of reads 
on each strand
 must be at or above the threshold. If either strand is below the threshold, the line will be omitted. 
N.B.
 this means the total coverage for each printed site will be at least twice the number you give in the ""coverage threshold"" option. Also, since only simple variants are counted, a site with 100 reads, all supporting a deletion variant, would not be printed. Frequency threshold: If a frequency threshold is used, alleles are only counted (in the ALLELES column) if they meet or exceed this minor allele frequency threshold. Strand bias: The alleles passing the threshold on each strand must match (though not in order), or the allele count will be 0. So a site with A, C, G on the plus strand and A, G on the minus strand will get an allele count of zero, though the (strand-independent) major allele, minor allele, and minor allele frequency will still be reported. If there is a tie for the minor allele, one will be randomly chosen. Additionally, a measure of strand bias is given in the last column. This is calculated using the method of Guo et al., 2012. A value of ""."" is given when there is no valid result of the calculation due to a zero denominator. This occurs when there are no reads on one of the strands, or when there is no minor allele."
toolshed.g2.bx.psu.edu/repos/iuc/snpfreqplot/snpfreqplot/1.0+galaxy3	"What it does
 This tool generates multi-sample variant-frequency plots from SnpEff-annotated viral variant lists with optional hierarchical clustering of the samples. .. class:: Warning mark Currently, this tool has been tested only on SARS-CoV-2 variant data. While the intention is to have it work for viral variant data in general, be prepared for unexpected behavior with other input data at the current development stage. ---- The tool expects input variant lists in one of the following two formats: 1. VCF datasets as produced by standard variant callers with - variant allele frequencies encoded in an 
AF
 INFO field - variant functional genomic effects annotated using SnpEff's EFF format (SnpEff's ANN format is not currently supported!) 2. tabular datasets with columns listing, at least, the following variant properties: - 
CHROM
 - 
POS
 - 
REF
 - 
ALT
 - 
AF
 - 
EFF[*].AA
 - 
EFF[*].GENE
 - 
EFF[*].EFFECT
 Such files can be produced with SnpSift Extract Fields and can be useful if preprocessing of the lists with standard text processing tools is required. .. class:: infomark To represent empty EFF fields in the tabular format you can choose between 
.
 and the empty string. ---- Example output: .. image:: /static/images/example_output.png"
toolshed.g2.bx.psu.edu/repos/iuc/basil/basil/1.2.0+galaxy2	"BASIL is a method to detect breakpoints for structural variants (including insertion breakpoints) from aligned paired HTS reads in BAM format. Use BASIL to analyze BAM files for tentative insertion sites. Note that BASIL will in general detect all kinds of breakpoints, e.g. for inversions on real-world data. BASIL VCF fields A typical line in BASIL might look as follows. 1 5001 site_0 T <INS> . PASS IMPRECISE;SVTYPE=INS GSCORE:CLEFT:CRIGHT:OEALEFT:OEARIGHT 46.4256:10:12:35:32 The first seven columns are as usually in VCF files (ref name, 1-based position, reference base, abbreviation for long insertion, no assigned quality, passing all filters, imprecise insertion SV). The eighth column contains the names of the score values given in the ninth column: GSCORE Geometric mean of the sum of ""1 + $score"" for all of the following scores. CLEFT Number of clipping signatures supporting the site from the left side. CRIGHT Number of clipping signatures supporting the site from the right side. OEALEFT Number of OEA alignments supporting the site from the left. OEARIGHT Number of OEA alignmetns supproting the site from the right. Generally, one should filter for a minimum support of OEA records on each side, e.g. a value of 10 makes sense for a 30x coverage and showed good results on simulated data. For a ranking, GSCORE is a suitable measure but we did not develop any statistical model for BASIL matches and it is a mean of pseudocounts only. It carries no statistically precise meaning."
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_color_chrs/bcftools_plugin_color_chrs/1.22+galaxy0	"===================================== bcftools color-chrs plugin ===================================== Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff"
toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_frameshifts/bcftools_plugin_frameshifts/1.22+galaxy0	"===================================== bcftools frameshifts plugin ===================================== A simple tool to annotate the effect of indel variants. Adds an 
OOF
 (out-of-frame) subfield to the 
INFO
 field of indel records found in the input, where - 
OOF=1
 indicates that the indel results in a frameshift - 
OOF=0
 indicates an in-frame indel The information about indel effects is extracted from the 
BED
 input, which is supposed to describe the reference genome exons. If a variant record lists several 
ALT
 alleles, some of them indels, then 
OOF
 will be a comma-separated list of status indicators. In the list a third 
OOF
 status code of 
-1
 will be used to indicate 
ALT
 alleles that are not associated with a change in sequence length or that don't affect an exon. The formal definition (added to the VCF header) of the 
OOF
 subfield is:: INFO=<ID=OOF,Number=A,Type=Integer,Description=""Frameshift Indels: out-of-frame (1), in-frame (0), not-applicable (-1 or missing)""> Region Selections ----------------- Regions can be specified in a VCF, BED, or tab-delimited file (the default). The columns of the tab-delimited file are: CHROM, POS, and, optionally, POS_TO, where positions are 1-based and inclusive. Uncompressed files are stored in memory, while bgzip-compressed and tabix-indexed region files are streamed. Note that sequence names must match exactly, ""chr20"" is not the same as ""20"". Also note that chromosome ordering in FILE will be respected, the VCF will be processed in the order in which chromosomes first appear in FILE. However, within chromosomes, the VCF will always be processed in ascending genomic coordinate order no matter what order they appear in FILE. Note that overlapping regions in FILE can result in duplicated out of order positions in the output. This option requires indexed VCF/BCF files. Targets ------- Similar to regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both regions and targets options can be applied simultaneously: regions uses the index to jump to a region and targets discards positions which are not in the targets. Unlike regions, targets can be prefixed with ""^"" to request logical complement. For example, ""^X,Y,MT"" indicates that sequences X, Y and MT should be skipped. Yet another difference between the two is that regions checks both start and end positions of indels, whereas targets checks start positions only. For the bcftools call command, with the option -C alleles, third column of the targets file must be comma-separated list of alleles, starting with the reference allele. Note that the file must be compressed and index. Such a file can be easily created from a VCF using:: bcftools query -f'%CHROM\t%POS\t%REF,%ALT\n' file.vcf | bgzip -c > als.tsv.gz && tabix -s1 -b2 -e2 als.tsv.gz Expressions ----------- Valid expressions may contain: - numerical constants, string constants :: 1, 1.0, 1e-4 ""String"" - arithmetic operators :: +,
,-,/ - comparison operators :: == (same as =), >, >=, <=, <, != - regex operators ""~"" and its negation ""!~"" :: INFO/HAYSTACK ~ ""needle"" - parentheses :: (, ) - logical operators :: && (same as &), ||, | - INFO tags, FORMAT tags, column names :: INFO/DP or DP FORMAT/DV, FMT/DV, or DV FILTER, QUAL, ID, REF, ALT[0] - 1 (or 0) to test the presence (or absence) of a flag :: FlagA=1 && FlagB=0 - ""."" to test missing values :: DP=""."", DP!=""."", ALT=""."" - missing genotypes can be matched regardless of phase and ploidy ("".|."", ""./."", ""."") using this expression :: GT=""."" - TYPE for variant type in REF,ALT columns (indel,snp,mnp,ref,other) :: TYPE=""indel"" | TYPE=""snp"" - array subscripts, ""
"" for any field :: (DP4[0]+DP4[1])/(DP4[2]+DP4[3]) > 0.3 DP4[
] == 0 CSQ[
] ~ ""missense_variant.
deleterious"" - function on FORMAT tags (over samples) and INFO tags (over vector fields) :: MAX, MIN, AVG, SUM, STRLEN, ABS - variables calculated on the fly if not present: number of alternate alleles; number of samples; count of alternate alleles; minor allele count (similar to AC but is always smaller than 0.5); frequency of alternate alleles (AF=AC/AN); frequency of minor alleles (MAF=MAC/AN); number of alleles in called genotypes :: N_ALT, N_SAMPLES, AC, MAC, AF, MAF, AN 
Notes:
 - String comparisons and regular expressions are case-insensitive - If the subscript ""
"" is used in regular expression search, the whole field is treated as one string. For example, the regex 
STR[*]~""B,C""
 will be true for the string vector INFO/STR=AB,CD. - Variables and function names are case-insensitive, but not tag names. For example, ""qual"" can be used instead of ""QUAL"", ""strlen()"" instead of ""STRLEN()"" , but not ""dp"" instead of ""DP"". 
Examples:
 :: MIN(DV)>5 MIN(DV/DP)>0.3 MIN(DP)>10 & MIN(DV)>3 FMT/DP>10 & FMT/GQ>10 .. both conditions must be satisfied within one sample FMT/DP>10 && FMT/GQ>10 .. the conditions can be satisfied in different samples QUAL>10 | FMT/GQ>10 .. selects only GQ>10 samples QUAL>10 || FMT/GQ>10 .. selects all samples at QUAL>10 sites TYPE=""snp"" && QUAL>=10 && (DP4[2]+DP4[3] > 2) MIN(DP)>35 && AVG(GQ)>50 ID=@file .. selects lines with ID present in the file ID!=@~/file .. skip lines with ID present in the ~/file MAF[0]<0.05 .. select rare variants at 5% cutoff"
toolshed.g2.bx.psu.edu/repos/iuc/ucsc_fatovcf/fatovcf/448+galaxy0	"What it does
 
faToVcf
 is a tool to extract a VCF from a multi-sequence FASTA alignment. For implementation details see faToVcf's 
source code
. .. _faToVcf: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/FOOTER.txt .. _source code: https://github.com/ucscGenomeBrowser/kent/blob/master/src/hg/utils/faToVcf/faToVcf.c"
toolshed.g2.bx.psu.edu/repos/iuc/ococo/ococo/0.1.2.6	"OCOCO - Online Consensus Caller
 OCOCO is a online consensus caller, capable to infer SNVs dynamically as read alignments are fed in. OCOCO inputs unsorted alignments from an unsorted SAM/BAM stream and decides about single-nucleotide updates of the current genomic consensus using statistics stored in compact several-bits counters. Updates are reported in the online fashion using unsorted VCF. OCOCO provides a fast and memory-efficient alternative to the usual variant calling, particularly advantageous when reads are sequenced or mapped progressively, or when available computational resources are at a premium."
toolshed.g2.bx.psu.edu/repos/iuc/sniffles/sniffles/2.5.2+galaxy0	"## Sniffles ######## What is Sniffles? 
*
*
 Sniffles is a SV caller for long reads. Sniffles2 accurately detect SVs on germline, somatic and population-level for PacBio and Oxford Nanopore read data. SV are larger events on the genome (e.g. deletions, duplications, insertions, inversions and translocations). Sniffles can detect all of these type and more such as nested SVs (e.g. inversion flanked by deletions or an inverted duplication). ---- Inputs 
 Known to work with Minimap2 bam as input Optional reference fasta with matching contig names will allow deletions to be determined. ---- Parameters 
*
 General ------- +----------------------------+-------------------------------------------------------------------------+ | Parameter | Description | +============================+=========================================================================+ | Minimum Support | Minimum number of reads supporting a SV to be reported. Default:auto | +----------------------------+-------------------------------------------------------------------------+ | Maximum Number of Splits | Maximum number of split segments per kb a read is aligned at before | | | it is ignored. Default: 7 | +----------------------------+-------------------------------------------------------------------------+ | Minimum SV Length | Minimum length of SV to be reported. Default: 50bp | +----------------------------+-------------------------------------------------------------------------+ | Minimum Mapping Quality | Minimum mapping quality of alignment to be taken into account. | | | Default: 20 | +----------------------------+-------------------------------------------------------------------------+ | Minimum alignment length | Reads with less length aligned will be ignored. Default 100 | +----------------------------+-------------------------------------------------------------------------+ Clustering Options ------------------ +---------------------+------------------------------------------------------------------------+ | Parameter | Description | +=====================+========================================================================+ | Cluster bin size | Initial cluster bin size. Default 100 | +---------------------+------------------------------------------------------------------------+ | Cluster Multiplier | Multiplier for SV start position standard deviation criterion in | | | cluster merging [2.5] | +---------------------+------------------------------------------------------------------------+ Advanced Options ---------------- +-------------+--------------------------------------------------------------------------------+ | Parameter | Description | +=============+================================================================================+ | Mosaic | Set Sniffles run mode to detect rare, somatic and mosaic SVs (default: False)| +-------------+--------------------------------------------------------------------------------+ ---- VCF information fields from the VCF header 
*
*
*
*
*
*
 +------------------+-----------+-----------------------------------------------------------------------------------------------+ | Field | Type | Description | +==================+===========+===============================================================================================+ | PRECISE | Flag | Structural variation with precise breakpoints | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | IMPRECISE | Flag | Structural variation with imprecise breakpoints | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | MOSAIC | Flag | Structural variation classified as putative mosaic | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | SVLEN | Integer | Length of structural variation | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | SVTYPE | String | Type of structural variation | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | CHR2 | String | Mate chromsome for BND SVs | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | SUPPORT | Integer| Number of reads supporting the structural variation | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | SUPPORT_INLINE | Integer| Number of reads supporting an INS/DEL SV (non-split events only) | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | SUPPORT_LONG | Integer| Number of soft-clipped reads putatively supporting the long insertion SV | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | END | Integer| End position of structural variation | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | STDEV_POS | Float | Standard deviation of structural variation start position | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | STDEV_LEN | Float | Standard deviation of structural variation length | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | COVERAGE | Float | Coverage near upstream start, center, end, downstream of structural variation | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | STRAND | String | Strands of supporting reads for structural variant | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | AC | Integer| Allele count summed up over all samples | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | SUPP_VEC | String | List of read support for all samples | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | CONSENSUS_SUP | Integer| Number of reads that support the generated insertion (INS) consensus sequence | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | RNAMES | String | Names of supporting reads (if enabled with --output-rnames) | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | AF | Float | Allele Frequency | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | NM | Float | Mean number of query alignment length adjusted mismatches of supporting reads | +------------------+-----------+-----------------------------------------------------------------------------------------------+ | PHASE | String | Phasing information derived from supporting reads | +------------------+-----------+-----------------------------------------------------------------------------------------------+ ---- Source 
*
* https://github.com/fritzsedlazeck/Sniffles"
toolshed.g2.bx.psu.edu/repos/iuc/snippy/snippy/4.6.0+galaxy0	"Snippy @VERSION@
 Snippy finds SNPs between a haploid reference genome and your NGS sequence reads. It will find both substitutions (snps) and insertions/deletions (indels). 
Author
 Torsten Seemann 
Inputs
 - NGS Reads in fastq format (single or paired end) - Reference file in either fasta or genbank format If the reference file is supplied in genbank format, snpeff will be called to determine the effect of any snps found. 
Advanced options
 - mapping quality - Integer - Minimum mapping quality to allow (default '60') - minimum coverage - Integer - Minimum coverage of variant site (default '10') - minimum fraction - Float - Minumum proportion for variant evidence (default '0.9') - minimum quality - Float - Minumum QUALITY in VCF column 6 (default '100.0') - rgid - String - Use this @RG ID: in the BAM header (default '') - bwaopt - Extra BWA MEM options, eg. -x pacbio (default '') 
Further information
 For a much more in depth description of snippy and how it works, see https://github.com/tseemann/snippy"
toolshed.g2.bx.psu.edu/repos/iuc/snippy/snippy_clean_full_aln/4.6.0+galaxy0	"The core.full.aln file is a FASTA formatted multiple sequence alignment file. It has one sequence for the reference, and one for each sample participating in the core genome calculation. Each sequence has the same length as the reference sequence. Character Meaning ATGC Same as the reference atgc Different from the reference - Zero coverage in this sample or a deletion relative to the reference N Low coverage in this sample (based on --mincov) X Masked region of reference (from --mask) n Heterozygous or poor quality genotype (has GT=0/1 or QUAL < --minqual in snps.raw.vcf) You can remove all the ""weird"" characters and replace them with N using this tool. This is useful when you need to pass it to a tree-building or recombination-removal tool: 
% snippy-clean_full_aln core.full.aln &gt; clean.full.aln % run_gubbins.py -p gubbins clean.full.aln % snp-sites -c gubbins.filtered_polymorphic_sites.fasta &gt; clean.core.aln % FastTree -gtr -nt clean.core.aln &gt; clean.core.tree"
toolshed.g2.bx.psu.edu/repos/iuc/snippy/snippy_core/4.6.0+galaxy0	"snippy-core @VERSION@
 Combine multiple Snippy outputs into a core SNP alignment If you call SNPs for multiple isolates from the same reference, you can produce an alignment of ""core SNPs"" which can be used to build a high-resolution phylogeny (ignoring possible recombination). A ""core site"" is a genomic position that is present in all the samples. A core site can have the same nucleotide in every sample (""monomorphic"") or some samples can be different (""polymorphic"" or ""variant""). If we ignore the complications of ""ins"", ""del"" variant types, and just use variant sites, these are the ""core SNP genome"". 
Inputs:
 Multiple Snippy output directories. (At least 2 of) 
Options:
 - noreference Exclude reference (default '0'). 
Note:
 snippy 
must
 have been run with --cleanup False"
toolshed.g2.bx.psu.edu/repos/iuc/nextclade/nextclade/2.7.0+galaxy0	".. class:: infomark 
What it does
 Nextclade_ assigns clades, calls mutations and performs sequence quality checks on SARS-CoV-2 genomes. For a description of nextclade's configuration files, see the 
nextclade documentation &lt;https://docs.nextstrain.org/projects/nextclade/en/latest/index.html&gt;
_. 
Input
 Input is a FASTA file containing one or more SARS-CoV-2 consensus genomes. 
Output
 Outputs can include: * A tabular format file with a report, one line per input sequence * A JSON format file with the same information as is present in the tabular report * An Auspice v2 tree file in JSON format .. _Nextclade: https://github.com/nextstrain/nextclade"
toolshed.g2.bx.psu.edu/repos/iuc/pangolin/pangolin/4.3.4+galaxy2	"What it does
 
Pangolin &lt;https://cov-lineages.org/pangolin.html&gt;
 (Phylogenetic Assignment of Named Global Outbreak LINeages) is used to assign a SARS-CoV-2 genome sequence the most likely lineage based on the PANGO nomenclature system. 
Data sources/versioning and reproducibility
 Pangolin uses the 
pangolin-data &lt;https://github.com/cov-lineages/pangolin-data&gt;
 repository as a source of its required model, protobuf, designation hash and alias files, and the 
constellations &lt;https://github.com/cov-lineages/constellations&gt;
 repository for 
scorpio &lt;https://github.com/cov-lineages/scorpio&gt;
 -based assignment of lineages of concern. The tool ships with copies of these two data packages, and using these shipped versions is 
recommended
 for reproducibility (even across Galaxy servers) and speed of job execution. If your instance of Galaxy offers cached alternative versions of 
pangolin-data
 and/or 
constellations
, you will be able to use them instead of the shipped versions, which can be useful to reproduce results obtained earlier with previous versions of pangolin. Finally, you have the option to 
download the latest version
 of each data package at job runtime. .. class:: warningmark You can use this option as a workaround to get the most up-to-date lineage assignments even before the next Galaxy tool update (or before an admin installs new cached data versions on your server), but be aware of the following limitations: 1. Using latest downloaded data package versions renders results hard to reproduce (e.g. rerunning a corresponding job will cause also a fresh data download, which may yield different data versions as in the intial run). 2. Downloaded latest versions of the data packages may be incompatible with the 
pangolin
 and 
scorpio
 version run by the tool, which can result in failing tool runs, but occasionally also in harder to diagnose lineage assignment issues. .. class:: infomark The exact combination of pangolin, inference engine (UShER/pangoLEARN), scorpio, and data packages that was used for a particular run of the tool can be extracted from the four ""version"" columns in the output (see below for details). In addition, lineage assignment with pangolin can be affected by the exact versions of additional underlying software. The packaged versions of all relevant dependencies are listed in the 
Requirements
 section below. This section is the equivalent to running 
pangolin --all-versions
 on the command line except that the listed versions of 
pangolin-data
 and 
constellations
 are the ones installed with pangolin and may have been overridden with the versions reported in the corresponding output columns at tool runtime. 
Output
 The main output of the tool is a tabular file with one line per input sequence and with columns providing the 
following information &lt;https://cov-lineages.org/resources/pangolin/output.html&gt;
_: taxon: The name of the input sequence lineage: The most likely lineage assigned to a given sequence based on the inference engine used and the SARS-CoV-2 diversity designated. This assignment is sensitive to missing data at key sites. conflict: If a sequence can fit into more than one category, the conflict score will be greater than 0 and reflect the number of categories the sequence could fit into. If the conflict score is 0, this means that within the current assignment model / lineage tree there is only one category that the sequence could plausibly be assigned to. ambiguity_score: This score is a function of the quantity of missing data in a sequence. It represents the proportion of relevant sites in a sequence which were imputed to the reference values. A score of 1 indicates that no sites were imputed, while a score of 0 indicates that more sites were imputed than were not imputed. This score only includes sites which are used by the assignment engine to classify a sequence. scorpio_call: If a query is assigned a constellation by scorpio this call is output in this column. The full set of constellations searched by default can be found at the constellations repository. scorpio_support: The support score is the proportion of defining variants which have the alternative allele in the sequence. scorpio_conflict: The conflict score is the proportion of defining variants which have the reference allele in the sequence. Ambiguous/other non-ref/alt bases at each of the variant positions contribute only to the denominators of these scores. scorpio_notes: A notes column specific to the scorpio output. version: A version number that represents both the inference method and the pangolin-data version number, which as of pangolin 4.0 corresponds to the pango-designation version used to prepare the inference files. For example: PANGO-1.2 indicates an identical sequence has been previously designated this lineage, and has so gone through manual curation. The number 1.2 indicates the version of pango-designation that this assignment is based on. These hashes and pango-designation version are bundled with the pangoLEARN and UShER models. PLEARN-1.2 indicates that this sequence is different from any previously designated and that the pangoLEARN model was used as an inference engine to predict the most likely lineage based on the given version of pango-designation upon which the pangoLEARN model was trained. PUSHER-1.2 indicates that this sequence is different from any previously designated and that UShER was used as an inference engine with fast tree placement and parsimony-based lineage assignment, based on a guide tree (protobuf) file built from the data in a given pango-designation release version. pangolin_version: The version of pangolin software running. scorpio_version: The version of the scorpio software installed. constellation_version: The version of constellations that scorpio has used to curate the lineage assignment. is_designated: A boolean (True/False) column indicating whether that particular sequence has been offically designated a lineage (via pango-designation). qc_status: Indicates whether the sequence passed the QC thresholds for minimum length and maximum N content. qc_notes: Notes specific to the QC checks run on the sequences. note: If any conflicts arose during assignment, this field will output the alternative assignments. If the sequence failed QC this field will describe why. If the sequence met the SNP thresholds for scorpio to call a constellation, it’ll describe the exact SNP counts of Alt, Ref and Amb (alternative, reference and ambiguous) alleles for that call."
toolshed.g2.bx.psu.edu/repos/iuc/snpeff_sars_cov_2/snpeff_sars_cov_2/4.5covid19	By default SnpEff simplifies all chromosome names. For instance 'chr1' is just '1'. You can prepend any string you want to the chromosome name
toolshed.g2.bx.psu.edu/repos/ufz/vibrant/vibrant/1.2.1+galaxy2	".. class:: infomark 
What it does
 VIBRANT is a tool for automated recovery and annotation of bacterial and archaeal viruses, determination of genome completeness, and characterization of viral community function from metagenomic assemblies. VIBRANT uses neural networks of protein annotation signatures and genomic features to maximize identification of highly diverse partial or complete viral genomes as well as excise integrated proviruses. - Uses neural network machine learning of protein annotation signatures - Asigns novel 'v-score' for determining the virus-like nature of all annotations - Determines genome completeness - Characterizes viral community function by metabolic analysis - Identifies auxiliary metabolic genes (AMGs) - Excises integrated viral genomes from host scaffolds - Performs well in diverse environments - Recovers novel and abundant viral genomes - Built for dsDNA, ssDNA and RNA viruses VIBRANT uses three databases for identifying viruses and characterizing virome metabolic potential: - KEGG (March release): https://www.genome.jp/kegg/ (FTP: ftp://ftp.genome.jp/pub/db/kofam/archives/2019-03-20/) - Pfam (v32): https://www.ebi.ac.uk/interpro/ (FTP: ftp://ftp.ebi.ac.uk/pub/databases/Pfam/releases/Pfam32.0/) - VOG (release 94): http://vogdb.org/ (FTP: http://fileshare.csb.univie.ac.at/vog/vog94/) Usage ..... 
Input
 FASTA formated nucleotide or amino acid sequences. Note that for amino acid input 
Input is protein
 needs to be checked. 
Output
 Run log file: Contains information about the command used, the total runtime, date of run, version of VIBRANT, and summary of values used in the figure dataset 
phages
. This log ﬁle is more of a run and output summary dataset, though in the event of an error the message will be displayed in this log dataset. Figures: - pathways: bar plot of KEGG pathways for identified viral AMGs - PCA: principal component analysis of predicted viruses, contains information on quality, lifestyle, circular/linear, size and general relationships of viral scaffolds. A PCA plot will only be generated if at least 3 viral scaffolds are identified. Based on the 
Normalized summary metrics
 output dataset and 
Scaffold coordinates on the PCA plot
 provides coordinate information for each scaffold. - phages: Nested bubble plot for ratio of total input scaffolds (outside), number of scaffolds that were greater than or equal to minimum size restrictions (middle), and number of identified viruses (inside). Must have at least 10 input sequences for this file to be present. The represented numbers can be found in the 
run log file
. - quality: bar plot of number of viruses per genome quality category. Possible x-axis categories are high, medium and low quality draft, and complete circular. Any category may not be present if no viruses were identified for that category. - sizes: histogram of genome sizes of identified viruses FASTA and associated files for predicted viruses - Circular virus genomes: All virus genomes identified as circular - Virus encoded protein: All virus encoded proteins. Note: any lysogenic virus that has been excised from a host scaffold will have the term ""fragment"" and a number appended to the original name to indicate that it does not represent the entire scaffold. This file () will contain only the excised fragment. - Virus encoded genes: All virus encoded genes - Virus genomes: All virus genomes - Virus genomes genbank: GenBank format file for all virus genomes - Virus genome names: List of names (FASTA definition lines) for all virus genomes. Note: This file may not entirely match the original input sequence names due to fragmentation of scaﬀolds. - Lysogenic/Lytic virus encoded proteins: Virus encoded proteins for predicted lysogenic/lytic viruses (subset of the above datasets). - Lysogenic/Lytic virus encoded gene: Virus encoded genes for predicted lysogenic/lytic viruses (subset of the above datasets). - Lysogenic/Lytic virus genomes: Virus genomes for predicted lysogenic/lytic viruses (subset of the above datasets). Useful tab-delimited files for predicted viruses - Predicted virus AMGs: List of all predicted virus AMGs (by KEGG KO) and the total number of each. Summary of file 
Individual predicted virus AMGs
. File may be empty. - Individual predicted virus AMGs: List of individual predicted virus AMGs by protein and its respective genome. AMGs are determined by KEGG annotation but Pfam annotation is also given if applicable. See file 
Predicted virus AMGs
 for summary. File may be empty. - KEGG metabolic pathways corresponding to virus AMGs: List summarizing the present KEGG metabolic pathways (by KEGG map entry) corresponding to virus AMGs. See dataset 
Individual predicted virus AMGs
 for individual AMGs. File may be empty. See this link for detailed information regarding KEGG metabolic pathways: https://www.genome.jp/kegg/pathway.html. - Annotations for KEGG, Pfam and VOG: Complete list of annotations and associated information for KEGG, Pfam and VOG for all predicted viruses. - Blank rows indicate proteins that were not given an annotation. - Annotation names can be found in columns KO/KO name, Pfam/Pfam name and VOG/VOG name. - Column AMG will indicate if the annotation was considered an AMG or not (blank). - Columns for evalue and score refer to the annotation confidences generated by HMMsearch. Evalues are provided, but scores are used as the cutoff for annotations (must have a score of at least 40). - Columns for v-score refer to the VIBRANT-specific virus-like score associated with each KO, Pfam and VOG. Briefly: scores of - 0 indicate very low or no relatedness to viruses - 0.01 - 0.1 indicates low relatedness; - 0.1 - 1 indicates moderate relatedness; - 1 - 5 indicates significant relatedness; - 5 - 10 indicates substantial relatedness; - 10 (max) for most cases indicates viral hallmark genes - Complete circular genomes: Virus genomes that were predicted to be circular and therefore complete genomes. File may be empty. Circularization is determined by a kmer-based search between each end of the viral predicted genome. There must be at least a 20bp identical match. - Scaffold coordinates on the PCA plot: Coordinate information for each viral scaffold on the PCA plot - Single annotation used for all predicted virus proteins: List of the single annotation used for all predicted virus proteins. Annotations are chosen based on best score hit. This dataset is used to generate dataset 
Virus genomes
 - Predicted genome quality and type: List summarizing the predicted genome quality and type (lytic/lysogenic) for all predicted viruses. Qualities may be fragment, low, medium or high quality draft. Complete circular genomes, if applicable, are listed at the end and are redundant. That is, any complete circular genome will also be given a quality. - Coordinates of integrated provirus: Scaffold/genome coordinate information of each integrated provirus that was excised from a host scaffold. This dataset provides the location of the putative provirus by both protein and nucleotide coordinates. All viral scaffolds are respective of those with ""fragment_"" in the name. - Predictions of the neural network classifier: List summarizing predictions made by the neural network machine learning classiﬁer. Will contain both viruses and nonviruses. This dataset is likely not of use but can be informative for checking outputs. There are curation steps following the classiifer to validate predictions, so predictions in this file may not exactly match the final output. File may be empty if the classiﬁer was not used. - Normalized summary metrics: Normalized version of the 
Summary metrics
 dataset. This normalized version was used to construct the PCA plot and is also the data read into the neural network classifier. Total proteins per scaffold are normalized as log10 of total proteins, and all other metrics per scaffold are normalized by dividing the original metric by the original number of encoded proteins - Summary metrics: List of complete annotation summary metrics for each predicted virus genome. The most useful metrics will be columns 1 through 8. All metrics shown, if applicable, were used for the neural network machine learning classifier. Explanations of each column: - scaffold: the name of the predicted virus - total genes: total number of predicted open reading frames - all KEGG: total number of KEGG annotations - KEGG v-score: sum of all KEGG annotation v-scores - all Pfam: total number of Pfam annotations - Pfam v-score: total number of Pfam annotations - all VOG: total number of VOG annotations - VOG v-score: total number of VOG annotations - KEGG int-rep: total number of KEGG integration related annotations (e.g., integrase, replicase, transposase). Useful for separating plasmids, mobile genetic elements and viruses - KEGG zero: total number of KEGG annotations that had a v-score of zero - Pfam int-rep: total number of Pfam integration related annotations (e.g., integrase, replicase, transposase). Useful for separating plasmids, mobile genetic elements and viruses - Pfam zero: total number of KEGG annotations that had a v-score of zero - VOG redoxin: total number of VOG redoxin related annotations (e.g., glutaredoxin, thioredoxin). Useful for separating viruses from bacteria/archaea because redoxins are common amongst viruses and therefore are given a high v-score, but are also common amongst bacteria/archaea - VOG rec-tran: total number of VOG integration related annotations that are not integrase (e.g., replicase, transposase). Useful for separating plasmids, mobile genetic elements and viruses - VOG int: total number of VOG integrase related annotations. Used to identify putative lysogenic viruses. Also useful for separating plasmids, mobile genetic elements and viruses - VOG RnR: total number of VOG ribonucleotide reductase (RnR) related annotations. Useful for separating viruses from bacteria/archaea because RnRs are common amongst viruses and therefore are given a high v-score, but are also common amongst bacteria/archaea - VOG DNA: total number of VOG nucleotide (DNA/RNA) replication related annotations. Useful for separating viruses from bacteria/archaea because nucleotide replication proteins are common amongst viruses and therefore are given a high v-score, but are also common amongst bacteria/archaea. This is also a metric used for predicting viral completeness (along with VOG special) because genome replication is an essential process - KEGG restriction: total number of KEGG restriction nuclease related annotations. Useful for separating plasmids, mobile genetic elements and viruses - KEGG toxin: total number of KEGG toxin/anti-toxin related annotations. Useful for separating plasmids, mobile genetic elements and viruses - VOG special: total number of VOG hallmark annotations (e.g., virion structural proteins, holin/lysin, terminase). This is a metric used for predicting viral completeness (along with VOG DNA) - annotation check: the number of proteins annotated by KEGG, Pfam and VOG - p_v check: the number of proteins annotated by Pfam and VOG only - p_k check: the number of proteins annotated by KEGG and Pfam only - k_v check: the number of proteins annotated by KEGG and VOG only - k check: the number of proteins annotated by KEGG only - p check: the number of proteins annotated by Pfam only - v check: the number of proteins annotated by VOG only - h check: the number of proteins not annotated"
toolshed.g2.bx.psu.edu/repos/iuc/ivar_consensus/ivar_consensus/1.4.4+galaxy0	0 - Majority or most common base <br/> 0.2 - Bases that make up atleast 20% of the depth at a position <br/> 0.5 - Strict or bases that make up atleast 50% of the depth at a position <br/> 0.9 - Strict or bases that make up atleast 90% of the depth at a position <br/> 1 - Identical or bases that make up 100% of the depth at a position. Will have highest ambiguities
toolshed.g2.bx.psu.edu/repos/iuc/ivar_filtervariants/ivar_filtervariants/1.4.4+galaxy0	"iVar can be used to get an intersection of variants(in .tsv files) called from any number of replicates or from different samples using the same reference sequence. This intersection will filter out any iSNVs that do not occur in a minimum fraction of the files supplied. This parameter can be changed using the -t flag which range from 0 to 1 (default). Fields that are different across replicates(fields apart from REGION, POS, REF, ALT, REF_CODON, REF_AA, ALT_CODON, ALT_AA) will have the filename added as a suffix. If there are a large number of files to be filtered, the -f flag can be used to supply a text file with one sample/replicate variant .tsv file per line. Documentation can be found at 
&lt;https://andersen-lab.github.io/ivar/html/manualpage.html&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/ivar_getmasked/ivar_getmasked/1.2.2+galaxy0	"iVar uses a .tsv file with variants to get the zero based indices (based on the BED file) of mismatched primers. This command requires another .tsv file with each line containing the left and right primer names separated by a tab. This is used to get both the primers for an amplicon with a single mismatched primer. The output is a text file with the zero based primer indices delimited by a space. The output is written to a a text file using the prefix provided. Documentation can be found at 
&lt;https://andersen-lab.github.io/ivar/html/manualpage.html&gt;
_."
toolshed.g2.bx.psu.edu/repos/iuc/ivar_removereads/ivar_removereads/1.4.4+galaxy1	"This Galaxy tool combines the functionality of 
ivar getmasked
 and 
ivar removereads
. No separate 
ivar getmasked
 step is required when using this tool. The wrapper takes as input a BAM dataset of aligned and sorted reads, from which the primers listed in the primer binding sites BED input have been trimmed with 
ivar trim
. From this input it will remove reads that come from amplicons that have been generated with one or more primers that may have been affected in their binding by variants listed in the variants input file. To do its job, the needs to know which primers work together to form an amplicon. The tool can try to deduce this info from the names of the primers found in the primer info dataset. This will require a primer naming scheme following the regex pattern:: .
_(?P<amplicon_number>\d+).
_(?P<primer_orientation>L(?:EFT)?|R(?:IGHT)?) 
i.e.
, the following schemes will work (and get parsed as): - 
nCoV-2019_1_LEFT
 (forward primer of amplicon 1) - 
400_2_out_R
 (reverse primer of amplicon 2) - 
QIAseq_163-2_LEFT
 (forward primer of amplicon 163) Alternatively, you can specify the amplicon information explicitly through a dataset that lists the names of primers that together form any given amplicon. In it, primer names (exactly matching those in the primer info dataset) need to be TAB-separated with one line per amplicon. .. class:: Warning mark Preprocessing of the BAM input with ivar trim is essential for this tool to work because only 
ivar trim
 can add required primer information to the BAM auxillary data of every read. ivar documentation can be found at 
&lt;https://andersen-lab.github.io/ivar/html/manualpage.html&gt;
__."
toolshed.g2.bx.psu.edu/repos/iuc/ivar_trim/ivar_trim/1.4.4+galaxy1	"iVar uses primer positions supplied in a BED file to soft clip primer sequences from an aligned and sorted BAM file. Following this, the reads are trimmed further based on a quality threshold. 
Primer and Amplicon info
 The tool requires information about primers and their binding sites in 6-column BED format. The information from this file is used to decide whether any mapped read in the BAM input ends with a primer sequence and should, thus, be soft-clipped. Optionally, the tool can also discard reads that do not fully map to within any amplicon. Such reads are likely to be wet-lab or mapping artefacts and removing them can increase variant calling precision. To calculate the extent of expected amplicons the tool needs to know which primers work together to form an amplicon. The tool can try to deduce this info from the names of the primers found in the primer info dataset. This will require a primer naming scheme following the regex pattern:: .
_(?P<amplicon_number>\d+).
(?P<primer_orientation>L(?:EFT)?|R(?:IGHT)?) 
i.e.
, the following schemes will work (and get parsed as): - 
nCoV-2019_1_LEFT
 (forward primer of amplicon 1) - 
400_2_out_R
 (reverse primer of amplicon 2) - 
QIAseq_163-2_LEFT
 (forward primer of amplicon 163) Alternatively, you can specify the amplicon information explicitly through a dataset that lists the names of primers that together form any given amplicon. In it, primer names (exactly matching those in the primer info dataset) need to be TAB-separated with one line per amplicon. If the primer scheme has more than two primers contributing to a given amplicon (in schemes using alternate primers), you can (in this Galaxy tool only) specify all of them on one line and the tool will calculate the maximum extent of the amplicon. 
Quality trimming details and final length filtering
 To do the quality trimming, iVar uses a sliding window approach. The window slides from the 5' end to the 3' end and if at any point the average base quality in the window falls below the threshold, the remaining read is soft clipped. Finally, the trimmed length threshold gets applied if specified, and fully trimmed surviving reads are written to the BAM output. Documentation can be found at 
&lt;https://andersen-lab.github.io/ivar/html/manualpage.html&gt;
."
toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/1.4.4+galaxy0	"iVar uses the output of the samtools mpileup command to call variants - single nucleotide variants(SNVs) and indels. In order to call variants correctly, the reference file used for alignment must be passed to iVar using the -r flag. The output of samtools pileup is piped into ivar variants to generate a .tsv file with the variants. There are two parameters that can be set for variant calling using iVar - minimum quality(Default: 20) and minimum frequency(Default: 0.03). Minimum quality is the minimum quality for a base to be counted towards the ungapped depth to canculate iSNV frequency at a given position. For insertions, the quality metric is discarded and the mpileup depth is used directly. Minimum frequency is the minimum frequency required for a SNV or indel to be reported. Documentation can be found at 
&lt;https://andersen-lab.github.io/ivar/html/manualpage.html&gt;
. Optionally output is converted to VCF using a version of the 
ivar_variants_to_vcf.py script &lt;https://github.com/nf-core/viralrecon/blob/dev/bin/ivar_variants_to_vcf.py&gt;
, that has been modified to store attributes in INFO fields."
toolshed.g2.bx.psu.edu/repos/iuc/snipit/snipit/1.7+galaxy0	"What it does
 Snipit finds mutations relative to a reference in a multiple sequence alignment and presents these changes in a nice overview plot."
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_compare/deeptools_bam_compare/3.5.4+galaxy0	"What it does ------------- This tool can be used to generate a bigWig or bedGraph file based on 
two BAM or CRAM
 files that are compared to each other while being simultaneously normalized for sequencing depth. To compare the BAM files to each other, the genome is partitioned into bins of equal size, then the number of reads found in each BAM file is counted per bin, and finally a summary value reported. The tool works in two steps: 1. Scaling : To properly compare samples with different sequencing depth, each bam file can be scaled either using the SES method (proposed in Diaz et al. (2012). ""Normalization, bias correction, and peak calling for ChIP-seq"". Statistical applications in genetics and molecular biology, 11(3).) or total read count. additionally scaling can be turned off and a per-sample normalization can be used (--normalizeUsing RPKM/CPM/BPM) 2. Comparison : Two bam files are compared using one of the chosen methods (e.g. add, subtract, mean, log2 ratio etc.) By default, if reads are from a paired-end sequencing run and reads are properly paired, the fragment length reported in the BAM file is used. 
Note:
 
For paired-end sequencing samples, each read mate is treated independently to avoid a bias when a mixture of concordant and discordant pairs are present. This means that 
each end
 will be extended to match the fragment length.
 .. image:: $PATH_TO_IMAGES/norm_IGVsnapshot_indFiles.png :width: 600 :height: 336 Output files -------------- 
bamCompare
 produces the same kind of output as 
bamCoverage
. The difference is that you now obtain 1 coverage file that is based on 2 BAM files: a bedGraph or a bigwig file containing the bin location and the resulting comparison values. Like BAM files, bigWig files are compressed, binary files. If you would like to see the coverage values, choose the bedGraph output. For more information on typical NGS file formats, see our 
Glossary &lt;http://deeptools.readthedocs.org/en/latest/content/help_glossary.html#file-formats&gt;
 .. image:: $PATH_TO_IMAGES/bamCompare_output.png :width: 600 :height: 436 ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0	"What it does -------------- Given a BAM file, this tool generates a bigWig or bedGraph file of fragment or read coverages. The way the method works is by first calculating all the number of reads (either extended to match the fragment length or not) that overlap each bin in the genome. Various options are available to normalize the reads: 1) using a given scaling factor 2) RPKM (reads per kilobase per million) : RPKM (per bin) = number of reads per bin / ( number of mapped reads (in millions) * bin length (kb) ). 3) CPM (counts per million) : CPM (per bin) = number of reads per bin / number of mapped reads (in millions). 4) BPM (bins per million) : BPM (per bin) = number of reads per bin / sum of all reads per bin (in millions). 5) RPGC (1x sequencing depth ) : number of reads per bin /(total number of mapped reads * fragment length / effective genome size) In the case of paired-end mapping, each read mate is treated independently to avoid a bias when a mixture of concordant and discordant pairs is present. This means that 
each end
 will be extended to match the fragment length. See the usage hints below. .. image:: $PATH_TO_IMAGES/norm_IGVsnapshot_indFiles.png :width: 600 :height: 336 Output ------------- 
bamCoverage
 produces a coverage file, either in bigWig or bedGraph format, where for each bin the number of overlapping reads (possibly normalized) is noted. Like BAM files, bigWig files are compressed, binary files. If you would like to see the coverage values, choose the bedGraph output. For more information on typical NGS file formats, see our 
Glossary &lt;http://deeptools.readthedocs.org/en/latest/content/help_glossary.html#file-formats&gt;
 .. image:: $PATH_TO_IMAGES/bamCoverage_output.png :width: 600 :height: 450 Usage hints ------------ * A smaller 
bin size
 value will result in a higher resolution of the coverage track but also in a larger file size. * The 
1x normalization
 (RPGC) requires the input of a value for the 
effective genome size
, which is the mappable part of the reference genome. Of course, this value is species-specific. * It might be useful for some studies to exclude certain chromosomes in order to avoid biases, e.g. chromosome X for many mammals where the males contain a pair of each autosome, but often only a single X chromosome. * By default, the read length is 
NOT
 extended! This is the preferred setting for 
spliced-read
 data like RNA-seq, where one usually wants to rely on the detected read locations only. A read extension would neglect potential splice sites in the unmapped part of the fragment. Other data, e.g. ChIP-seq, where fragments are known to map contiuously, should be processed with read extension (
--extendReads [INT]
). * For paired-end data, the fragment length is generally defined by the two read mates. The user-provided fragment length is only used as a fallback for singletons or mate reads that map too far apart (with a distance greater than four times the fragment length or if the mates are located on different chromosomes). WARNING: If you already normalized for GC bias using 
correctGCbias
, you should absolutely 
NOT
 set the parameter 
--ignoreDuplicates
! ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_pe_fragmentsize/deeptools_bam_pe_fragmentsize/3.5.4+galaxy0	"What it does ------------ This tool samples the given BAM files with paired-end data to estimate the fragment length distribution. Properly paired reads are preferred for computation, i.e., unless a region does not contain any concordant pairs, discordant pairs are ignored. Output ------ The 
default
 output is a simple summary statistic for the observed fragment lengths. Optionally, you can obtain a histogram of fragment sizes, which will give you a better idea of the distribution of fragment lengths. .. image:: $PATH_TO_IMAGES/bamPEFragmentSize_output.png :width: 600 :height: 520 If the 
--table
 option is specified, the summary statistics are additionally printed in a tabular format:: Frag. Len. Min. Frag. Len. 1st. Qu. Frag. Len. Mean Frag. Len. Median Frag. Len. 3rd Qu. Frag. Len. Max Frag. Len. Std. Read Len. Min. Read Len. 1st. Qu. Read Len. Mean Read Len. Median Read Len. 3rd Qu. Read Len. Max Read Len. Std. bowtie2 test1.bam 241.0 241.5 244.666666667 242.0 246.5 251.0 4.49691252108 251.0 251.0 251.0 251.0 251.0 251.0 0.0 If the 
--outRawFragmentLengths
 option is provided, another history item will be produced, containing the raw data underlying the histogram. It has the following format:: #bamPEFragmentSize Size Occurrences Sample 241 1 bowtie2 test1.bam 242 1 bowtie2 test1.bam 251 1 bowtie2 test1.bam The ""Size"" is the fragment (or read, for single-end datasets) size and ""Occurrences"" are the number of times reads/fragments with that length were observed. For easing downstream processing, the sample name is also included on each row. ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bigwig_average/deeptools_bigwig_average/3.5.4+galaxy0	"What it does -------------- This tool compute average of multiple bigWig files based on the number of mapped reads. To average the bigwig files, the genome is partitioned into bins of equal size, then the scores (e.g., number of reads) found in each bigWig file are counted for such bins and, finally, an average value is reported. Note that you can actually produce a human-readable bedGraph format instead of the compressed bigWig format if you're interested in having a look at the values yourself. .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bigwig_compare/deeptools_bigwig_compare/3.5.4+galaxy0	"What it does -------------- This tool compares two bigWig files based on the number of mapped reads. To compare the bigwig files, the genome is partitioned into bins of equal size, then the scores (e.g., number of reads) found in each bigWig file are counted for such bins and, finally, a summary value is reported. This value can be the ratio of the number of reads per bin, the log2 of the ratio, the sum or the difference. Note that you can actually produce a human-readable bedGraph format instead of the compressed bigWig format if you're interested in having a look at the values yourself. .. image:: $PATH_TO_IMAGES/bigwigCompare_output.png :width: 600 :height: 436 ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_gc_bias/deeptools_compute_gc_bias/3.5.4+galaxy0	"What it does ------------ This tool computes the GC bias using the method proposed in Benjamini and Speed (2012) Nucleic Acids Res. (see below for further details). The output is used to plot the results and can also be used later on to correct the bias with the tool 
correctGCbias
. There are two plots produced by the tool: a boxplot showing the absolute read numbers per GC-content bin and an x-y plot depicting the ratio of observed/expected reads per GC-content bin. Output files ------------ - Diagnostic plots: - box plot of absolute read numbers per GC-content bin - x-y plot of observed/expected read ratios per GC-content bin - Tabular file: to be used for GC correction with 
correctGCbias
 .. image:: $PATH_TO_IMAGES/computeGCBias_output.png :width: 600 :height: 455 ----- Theoretical Background ---------------------- 
computeGCBias
 is based on a paper by 
Benjamini and Speed &lt;http://nar.oxfordjournals.org/content/40/10/e72&gt;
. The basic assumption of the GC bias diagnosis is that an ideal sample should show a uniform distribution of sequenced reads across the genome, i.e. all regions of the genome should have similar numbers of reads, regardless of their base-pair composition. In reality, the DNA polymerases used for PCR-based amplifications during the library preparation of the sequencing protocols prefer GC-rich regions. This will influence the outcome of the sequencing as there will be more reads for GC-rich regions just because of the DNA polymerase's preference. 
computeGCbias
 will first calculate the 
expected GC profile
 by counting the number of DNA fragments of a fixed size per GC fraction where GC fraction is defined as the number of G's or C's in a genome region of a given length. The result is basically a histogram depicting the frequency of DNA fragments for each type of genome region with a GC fraction between 0 to 100 percent. This will be different for each reference genome, but is independent of the actual sequencing experiment. The profile of the expected DNA fragment distribution is then compared to the 
observed GC profile
, which is generated by counting the number of sequenced reads per GC fraction. In an ideal experiment, the observed GC profile would, of course, look like the expected profile. This is indeed the case when applying 
computeGCBias
 to simulated reads. .. _computeGCBias_example_image: .. image:: $PATH_TO_IMAGES/GC_bias_simulated_reads_2L.png As you can see, both plots based on 
simulated reads
 do not show enrichments or depletions for specific GC content bins, there is an almost flat line around the log2ratio of 0 (= ratio(observed/expected) of 1). The fluctuations on the ends of the x axis are due to the fact that only very, very few regions in the 
Drosophila
 genome have such extreme GC fractions so that the number of fragments that are picked up in the random sampling can vary. Now, let's have a look at 
real-life data
 from genomic DNA sequencing. Panels A and B can be clearly distinguished and the major change that took place between the experiments underlying the plots was that the samples in panel A were prepared with too many PCR cycles and a standard polymerase whereas the samples of panel B were subjected to very few rounds of amplification using a high fidelity DNA polymerase. .. image:: $PATH_TO_IMAGES/QC_GCplots_input.png :width: 600 :height: 452 
Note:
 The expected GC profile depends on the reference genome as different organisms have very different GC contents. For example, one would expect more fragments with GC fractions between 30% to 60% in mouse samples (average GC content of the mouse genome: 45 %) than for genome fragments from, for example, 
Plasmodium falciparum
 (average genome GC content 
P. falciparum
: 20%). For more details, for example about when to exclude regions from the read distribution calculation, go 
here &lt;http://deeptools.readthedocs.org/en/latest/content/tools/computeGCBias.html#excluding-regions-from-the-read-distribution-calculation&gt;
 ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_matrix/deeptools_compute_matrix/3.5.4+galaxy0	"What it does ---------------- This tool prepares an intermediate file (a gzipped table of values) that contains scores associated with genomic regions. The regions can either be scaled to the same size (using the 
scale-regions
 mode) or you can choose the start, end, or center of each region as the focus point for the score calculations. For more details, check out the explanation 
here &lt;https://deeptools.readthedocs.io/en/latest/content/tools/computeMatrix.html#details&gt;
. The intermediate file produced by 
computeMatrix
 is meant to be used with 
plotHeatmap
 and 
plotProfile
. See the descriptions of 
plotHeatmap
 and 
plotProfile
 for example plots. .. image:: $PATH_TO_IMAGES/computeMatrix_overview.png :alt: Relationship between computeMatrix, heatmapper and profiler :width: 600 :height: 418 ======= Usage hints ------------- The supplied genomic regions can really be anything - genes, parts of genes, ChIP-seq peaks, favorite genome regions... as long as you provide a proper file in BED or INTERVAL format. If you would like to compare different groups of regions (e.g., genes from chromosome 2 and 3), you can supply more than 1 regions file, one for each group by selecting ""Insert Select regions"". .. image:: $PATH_TO_IMAGES/computeMatrix_selectRegions.png :width: 600 :height: 150 You can select as many score (bigWig) files as you like. Simply use the Shift and/or Command key while clicking on the files of interest. .. image:: $PATH_TO_IMAGES/computeMatrix_selectScores.png :width: 600 :height: 136 The multitude of parameters can seem daunting at first - here are the options that we tend to tune most often: * 
bin Size
 -- The default value works well most of the time, but if you want to have a more finely grained image, decrease the default value (but not smaller than your bigWig file(s)' bin size). If you want to reduce the computation time, increase it. * 
Skip zeros
 -- useful to avoid completely blank lines in the heatmap. * 
Convert missing values to 0?
 -- If you want to identify clusters of similar regions in an unsupervised fashion using 
plotHeatmap
 and/or 
plotProfile
, you should definitely set this to 'yes'. Output files --------------- The default output is a 
gzipped table of values
 that is used by both 
plotHeatmap
 and 
plotProfile
. The optional output files include a) the 
regions after sorting and filtering (if selected)
 as they were used to calculate the values for the plotting, and b) the uncompressed table that 
underlies the heatmap
. 
TIP:
 
computeMatrix
 can also be used to filter and sort regions according to their score by making use of the ""advanced output settings"". .. image:: $PATH_TO_IMAGES/computeMatrix_advancedOutput.png :width: 600 :height: 189 .. image:: $PATH_TO_IMAGES/computeMatrix_output.png :width: 600 :height: 297 Note that these advanced output options are available for 
plotHeatmap
 and 
plotProfile
, too. See the following table for the optional output options: +-----------------------------------+--------------------+-----------------+-----------------+ | 
optional output type
 | 
computeMatrix
 | 
plotHeatmap
 | 
plotProfile
 | +-----------------------------------+--------------------+-----------------+-----------------+ | values underlying the heatmap | yes | yes | no | +-----------------------------------+--------------------+-----------------+-----------------+ | values underlying the profile | no | no | yes | +-----------------------------------+--------------------+-----------------+-----------------+ | sorted and/or filtered regions | yes | yes | yes | +-----------------------------------+--------------------+-----------------+-----------------+ 
More examples
 can be found in our 
Gallery &lt;http://deeptools.readthedocs.org/en/latest/content/example_gallery.html#normalized-chip-seq-signals-and-peak-regions&gt;
. ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_correct_gc_bias/deeptools_correct_gc_bias/3.5.4+galaxy0	"What it does ------------- This tool requires the output from computeGCBias to correct a given BAM file according to the method proposed in Benjamini and Speed (2012) Nucleic Acids Res. It will simply remove reads from regions with too high coverage compared to the expected values (typically GC-rich regions) and will add reads to regions where too few reads are seen (typically AT-rich regions). The resulting BAM file can be used in any downstream analyses, but be aware that you should not filter out duplicates from here on. See the description of 
computeGCBias
 to read up on the details of the GC bias assessment and correction method. Output files ---------------- 
correctGCbias
 only has one output: a BAM file where read densities have been changed to reflect the expected read distribution based on the genome. 
Warning!
 The GC-corrected BAM file will most likely contain several duplicated reads in regions where the coverage had to increased in order to match the expected read density. This means that you should absolutely avoid using any filtering of duplicate reads during your downstream analyses! ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_multi_bam_summary/deeptools_multi_bam_summary/3.5.4+galaxy0	"What it does ------------- This tool generates a matrix of read coverages for a list of genomic regions and at least two samples (BAM files). The genome is split into bins of the given size. For each bin, the number of reads found in each BAM file is counted. Alternatively, an interval file with pre-defined genomic regions can be provided. In principle, this tool does the same as 
multiBigwigSummary
, but for BAM files. A typical follow-up application is to check and visualize the similarity and variability between replicates or published data sets (see: 
plotPCA
 and 
plotCorrelation
). Output -------- The default output is a 
compressed file
 that can only be used with 
plotPCA
 or 
plotCorrelation
. To analyze the coverage scores yourself, you can get the 
uncompressed score matrix
 where every row corresponds to a genomic region (or bin) and each column corresponds to a sample (BAM file). (To obtain this output file, select ""Save raw counts (coverages) to file"" ) .. image:: $PATH_TO_IMAGES/multiBamSummary_output.png :width: 600 :height: 443 ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_multi_bigwig_summary/deeptools_multi_bigwig_summary/3.5.4+galaxy0	"What it does -------------- This tool computes the average scores for every genomic region for every bigWig file that is provided. In principle, it does the same as 
multiBamSummary
, but for bigWig files. The analysis is performed for the entire genome by running the program in 'bins' mode, or for certain user selected regions (e.g., genes) in 'BED-file' mode. Typically the output of 
multiBigwigSummary
 is used by other tools, such as 
plotCorrelation
 or 
plotPCA
, for visualization and diagnostic purposes. Output -------- The default output is a 
compressed file
 that can only be used with 
plotPCA
 or 
plotCorrelation
. To analyze the average scores yourself, you can get the 
uncompressed score matrix
 where every row corresponds to a genomic region (or bin) and each column corresponds to a sample (BAM file). (To obtain this output file, select ""Save raw counts (coverages) to file"" ) .. image:: $PATH_TO_IMAGES/multiBigwigSummary_output.png :width: 600 :height: 358 ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_correlation/deeptools_plot_correlation/3.5.4+galaxy0	"What it does ------------ This tools takes the default output of 
multiBamSummary
 or 
multiBigwigSummary
, and computes the pairwise correlation among samples. Results can be visualized as 
scatterplots
 or as a 
heatmap
 of correlation coefficients (see below for examples). Theoretical Background ---------------------- The result of the correlation computation is a 
table of correlation coefficients
 that indicates how ""strong"" the relationship between two samples is and it will consist of numbers between -1 and 1. (-1 indicates perfect anti-correlation, 1 perfect correlation.) We offer two different functions for the correlation computation: 
Pearson
 or 
Spearman
. The 
Pearson method
 measures the 
metric differences
 between samples and is therefore influenced by outliers. The 
Spearman method
 is based on 
rankings
. Output ------ The default output is a 
diagnostic plot
 -- either a scatterplot or a clustered heatmap displaying the values for each pair-wise correlation (see below for example plots). Optionally, you can also obtain a table of the pairwise correlation coefficients. .. image:: $PATH_TO_IMAGES/plotCorrelation_output.png :width: 600 :height: 271 Example plots ------------- The following is the output of 
plotCorrelation
 with our test ChIP-Seq datasets (to be found under ""Shared Data"" --> ""Data Library""). Average coverages were computed over 10 kb bins for chromosome X, from bigWig files using 
multiBigwigSummary
. This was then used with 
plotCorrelation
 to make a heatmap of Spearman correlation coefficients. .. image:: $PATH_TO_IMAGES/plotCorrelation_galaxy_bw_heatmap_output.png :width: 600 :height: 518 The scatterplot could look like this: .. image:: $PATH_TO_IMAGES/plotCorrelation_scatterplot_PearsonCorr_bigwigScores.png :width: 600 :height: 600 ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_coverage/deeptools_plot_coverage/3.5.4+galaxy0	"What it does ------------- This tool is useful to 
assess the sequencing depth
 of a given sample. It samples 1 million bp, counts the number of overlapping reads and reports a coverage histogram that tells you how many bases are covered how many times. 
Note:
 Multiple BAM files are accepted but all should correspond to the same genome assembly. Output --------- The default output is a 
panel of two plots
 (see below for an example): One is a density plot visualizing the frequencies of read coverages, the other one lets you estimate what fraction of the genome has a depth of sequencing of, for example, 2 overlapping reads or more. The optional output is a table where each row represents the number of reads overlapping with a sampled bp. .. image:: $PATH_TO_IMAGES/plotCoverage_output.png :width: 600 :height: 345 Example plot ----------------- .. image:: $PATH_TO_IMAGES/plotCoverage_annotated.png :width: 600 :height: 291 .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_enrichment/deeptools_plot_enrichment/3.5.4+galaxy0	"What it does ------------- This tool determines read/fragment coverage of regions. The regions (e.g., exons, genes or peaks) can be specified in one or more BED or GTF files. For GTF files, the feature type is taken from column 3. For BED files, the file name is used. For BED files, the feature labels can be changed. Output -------- The output file is a plot in the format specified. A table of the percentages and raw counts can also be created. .. image:: $PATH_TO_IMAGES/plotEnrichment_output.png :width: 600 :height: 600 ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_fingerprint/deeptools_plot_fingerprint/3.5.4+galaxy0	"What it does ------------ This tool is useful for assessing the strength of a ChIP (i.e. how clearly the enrichment signal can be separated from the background) and is based on a method described in Diaz et al. (2012) Stat Appl Genet Mol Biol 11(3). Output ------ The default output is a diagnostic plot (see below for an example and further down for some background information). Optionally, you can obtain the table of raw values that were used to generate the plot. .. image:: $PATH_TO_IMAGES/plotFingerprint_output.png :width: 600 :height: 395 Example plot ------------- What follows is the output of 
plotFingerprint
 with our test ChIP-Seq data sets, limiting the analysis to chromosome X. Single-end reads were extended to 200 bp (advanced options). .. image:: $PATH_TO_IMAGES/bamFP_galaxy_output.png :width: 600 :height: 450 ----- Theoretical Background ---------------------- The tool first samples indexed BAM files and sums the per-base coverage of reads/fragments overlapping a window (bin) of the specified length. These values are then sorted according to their rank (the bin with the highest number of reads has the highest rank) and the cumulative sum plotted. An ideal input (control) with a uniform distribution of reads alignments and infinite sequencing depth will result in a diagonal line. A very specific and strong ChIP enrichment, on the other hand, would result in a large portion of reads accumulating in just a few bins and the resulting plot showing a steep rise toward the right-most edge. Such results are most commonly seen with transcription factors. .. image:: $PATH_TO_IMAGES/QC_fingerprint.png :width: 600 :height: 294 ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_heatmap/deeptools_plot_heatmap/3.5.4+galaxy0	"What it does -------------- 
plotHeatmap
 visualizes scores associated with genomic regions, for example ChIP enrichment values around the TSS of genes. Like 
plotProfile
, it requires that computeMatrix was run first to calculate the values. We have implemented a number of optional parameters to optimize the visual output and we encourage you to play around with the min/max values displayed in the heatmap as well as with the different coloring options. 
TIP:
 
If your data is rather sparse and the resulting heatmap is too black, change the ""Missing data color"" to white (via ""Advanced options"").
 The most powerful option is probably the 
k-means/hierarchical clustering
 where you can sort your regions into groups of regions with similar score distributions. 
NOTE:
 
The clustering will only work if you supplied a single BED file to computeMatrix. plotHeatmap cannot cluster regions within pre-defined groups. Moreover, k-means will be much faster than hierarchical clustering.
 Output -------------- This is a heatmap based on two bigWig files generated with default settings and k-means clustering. .. image:: $PATH_TO_IMAGES/plotHeatmap_example.png :width: 600 :height: 694 .. image:: $PATH_TO_IMAGES/plotHeatmap_example02.png :width: 600 :height: 694 In addition to the image, 
plotHeatmap
 can also generate the values underlying both the heatmap. See the following table for the optional output options: +-----------------------------------+--------------------+-----------------+-----------------+ | 
optional output type
 | 
computeMatrix
 | 
plotHeatmap
 | 
plotProfile
 | +-----------------------------------+--------------------+-----------------+-----------------+ | values underlying the heatmap | yes | yes | no | +-----------------------------------+--------------------+-----------------+-----------------+ | values underlying the profile | no | no | yes | +-----------------------------------+--------------------+-----------------+-----------------+ | sorted and/or filtered regions | yes | yes | yes | +-----------------------------------+--------------------+-----------------+-----------------+ 
More examples
 can be found in our 
Gallery &lt;http://deeptools.readthedocs.org/en/latest/content/example_gallery.html#normalized-chip-seq-signals-and-peak-regions&gt;
. ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_pca/deeptools_plot_pca/3.5.4+galaxy0	"What it does ------------ This tool takes the 
default output file
 of 
multiBamSummary
 or 
multiBigwigSummary
 to perform a principal component analysis (PCA). Output ------ The result is a panel of two plots: 1. Either the loadings (default) or the projections (
--transpose
) of the samples on the desired 
two principal components
. 2. The 
Scree plot
 for principal components where the bars represent the eigenvalues the red line traces the amount of variability is explained by the individual components in a cumulative manner. Example plot ------------ .. image:: $PATH_TO_IMAGES/plotPCA_annotated.png :width: 600 :height: 315 ----- Theoretical Background ---------------------- Principal component analysis (PCA) can be used, for example, to determine whether 
samples display greater variability
 between experimental conditions than between replicates of the same treatment. PCA is also useful to identify unexpected patterns, such as those caused by batch effects or outliers. Principal components represent the directions along which the variation in the data is maximal, so that the information (e.g., read coverage values) from thousands of regions can be represented by just a few dimensions. PCA is not necessarily meant to identify unknown groupings or clustering; it is up to the researcher to determine the experimental or technical reason underlying the principal components. ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_profile/deeptools_plot_profile/3.5.4+galaxy0	"What it does ------------- This tool plots the average enrichments over all genomic regions supplied to 
computeMarix
. It requires that 
computeMatrix
 was successfully run. It is a very useful complement to 
plotHeatmap
, especially in cases where you want to compare the scores for many different groups. Like 
plotHeatmap
, 
plotProfile
 does not change the values that were computed by 
computeMatrix
, but you can modify the color and other display properties of the plots. Output ------------- .. image:: $PATH_TO_IMAGES/plotProfiler_examples.png :width: 600 :height: 858 ======= In addition to the image, 
plotProfile
 can also generate the values underlying the profile. See the following table for the 
optional output
 options: +-----------------------------------+--------------------+-----------------+-----------------+ | 
optional output type
 | 
computeMatrix
 | 
plotHeatmap
 | 
plotProfile
 | +-----------------------------------+--------------------+-----------------+-----------------+ | values underlying the heatmap | yes | yes | no | +-----------------------------------+--------------------+-----------------+-----------------+ | values underlying the profile | no | no | yes | +-----------------------------------+--------------------+-----------------+-----------------+ | sorted and/or filtered regions | yes | yes | yes | +-----------------------------------+--------------------+-----------------+-----------------+ ----- .. class:: infomark For more information on the tools, please visit our 
help site
. For support or questions please post to 
Biostars
. For bug reports and feature requests please open an issue 
on github
. This tool is developed by the 
Bioinformatics and Deep-Sequencing Unit
 at the 
Max Planck Institute for Immunobiology and Epigenetics
_. .. _Biostars: http://biostars.org .. _on github: http://github.com .. _Bioinformatics and Deep-Sequencing Unit: http://www.ie-freiburg.mpg.de/bioinformaticsfac .. _Max Planck Institute for Immunobiology and Epigenetics: http://www3.ie-freiburg.mpg.de .. _help site: https://deeptools.readthedocs.org/"
toolshed.g2.bx.psu.edu/repos/iuc/migmap/migmap/1.0.3+galaxy2	This software is a smart wrapper for the IgBlast V-(D)-J mapping tool designed to facilitate analysis immune receptor libraries profiled using high-throughput sequencing
toolshed.g2.bx.psu.edu/repos/iuc/presto_alignsets/presto_alignsets/0.6.2+galaxy0	"Multiple aligns sequences with the same barcode to correct for any misalignments due to either different primer usage or insertions/deletions in the sequences. Only supports the ""muscle"" sub-command of pRESTO's AlignSets.py. See the 
pRESTO online help &lt;@PRESTO_BASE_URL@/en/stable&gt;
_ for more information. === 
Note about limitations of pRESTO tools within Galaxy:
 pRESTO tools in galaxy have the following limitations vs. when run at the command line: * Inputs must be FASTQ format and cannot be FASTA format. * Multiple inputs files are not supported per argument (e.g. 
-1
, 
-2
 or 
-s
), i.e. - Steps that take a pair of fastq inputs can only take two files, not two sets of files - Steps that take a single set of fastq inputs can only take a single file * The 
--outdir
 and 
--outname
 options are not supported; output files are named directly"
toolshed.g2.bx.psu.edu/repos/iuc/presto_assemblepairs/presto_assemblepairs/0.6.2+galaxy0	"Assembles paired-end reads into a single sequence. See the 
pRESTO online help &lt;@PRESTO_BASE_URL@/en/stable&gt;
_ for more information. === 
Note about limitations of pRESTO tools within Galaxy:
 pRESTO tools in galaxy have the following limitations vs. when run at the command line: * Inputs must be FASTQ format and cannot be FASTA format. * Multiple inputs files are not supported per argument (e.g. 
-1
, 
-2
 or 
-s
), i.e. - Steps that take a pair of fastq inputs can only take two files, not two sets of files - Steps that take a single set of fastq inputs can only take a single file * The 
--outdir
 and 
--outname
 options are not supported; output files are named directly"
toolshed.g2.bx.psu.edu/repos/iuc/presto_buildconsensus/presto_buildconsensus/0.6.2+galaxy0	"Builds a consensus sequence for each set of input sequences. See the 
pRESTO online help &lt;@PRESTO_BASE_URL@/en/stable&gt;
_ for more information. === 
Note about limitations of pRESTO tools within Galaxy:
 pRESTO tools in galaxy have the following limitations vs. when run at the command line: * Inputs must be FASTQ format and cannot be FASTA format. * Multiple inputs files are not supported per argument (e.g. 
-1
, 
-2
 or 
-s
), i.e. - Steps that take a pair of fastq inputs can only take two files, not two sets of files - Steps that take a single set of fastq inputs can only take a single file * The 
--outdir
 and 
--outname
 options are not supported; output files are named directly"
toolshed.g2.bx.psu.edu/repos/iuc/presto_collapseseq/presto_collapseseq/0.6.2+galaxy0	"Removes/collapses duplicate sequences from FASTQ files. See the 
pRESTO online help &lt;@PRESTO_BASE_URL@/en/stable&gt;
_ for more information. === 
Note about limitations of pRESTO tools within Galaxy:
 pRESTO tools in galaxy have the following limitations vs. when run at the command line: * Inputs must be FASTQ format and cannot be FASTA format. * Multiple inputs files are not supported per argument (e.g. 
-1
, 
-2
 or 
-s
), i.e. - Steps that take a pair of fastq inputs can only take two files, not two sets of files - Steps that take a single set of fastq inputs can only take a single file * The 
--outdir
 and 
--outname
 options are not supported; output files are named directly"
toolshed.g2.bx.psu.edu/repos/iuc/presto_filterseq/presto_filterseq/0.6.2+galaxy0	"Filters and/or masks reads based on length, quality, missing bases and repeats. See the 
pRESTO online help &lt;@PRESTO_BASE_URL@/en/stable&gt;
_ for more information. === 
Note about limitations of pRESTO tools within Galaxy:
 pRESTO tools in galaxy have the following limitations vs. when run at the command line: * Inputs must be FASTQ format and cannot be FASTA format. * Multiple inputs files are not supported per argument (e.g. 
-1
, 
-2
 or 
-s
), i.e. - Steps that take a pair of fastq inputs can only take two files, not two sets of files - Steps that take a single set of fastq inputs can only take a single file * The 
--outdir
 and 
--outname
 options are not supported; output files are named directly"
toolshed.g2.bx.psu.edu/repos/iuc/presto_maskprimers/presto_maskprimers/0.6.2+galaxy0	"Removes primers and annotates sequences with primer and barcode identifiers See the 
pRESTO online help &lt;@PRESTO_BASE_URL@/en/stable&gt;
_ for more information. === 
Note about limitations of pRESTO tools within Galaxy:
 pRESTO tools in galaxy have the following limitations vs. when run at the command line: * Inputs must be FASTQ format and cannot be FASTA format. * Multiple inputs files are not supported per argument (e.g. 
-1
, 
-2
 or 
-s
), i.e. - Steps that take a pair of fastq inputs can only take two files, not two sets of files - Steps that take a single set of fastq inputs can only take a single file * The 
--outdir
 and 
--outname
 options are not supported; output files are named directly"
toolshed.g2.bx.psu.edu/repos/iuc/presto_pairseq/presto_pairseq/0.6.2+galaxy0	"Sorts and matches sequence records with matching coordinates across files. See the 
pRESTO online help &lt;@PRESTO_BASE_URL@/en/stable&gt;
_ for more information. === 
Note about limitations of pRESTO tools within Galaxy:
 pRESTO tools in galaxy have the following limitations vs. when run at the command line: * Inputs must be FASTQ format and cannot be FASTA format. * Multiple inputs files are not supported per argument (e.g. 
-1
, 
-2
 or 
-s
), i.e. - Steps that take a pair of fastq inputs can only take two files, not two sets of files - Steps that take a single set of fastq inputs can only take a single file * The 
--outdir
 and 
--outname
 options are not supported; output files are named directly"
toolshed.g2.bx.psu.edu/repos/iuc/presto_parseheaders/presto_parseheaders/0.6.2+galaxy0	"Adds, removes, renames and otherwise transforms annotations in FASTQ headers. See the 
pRESTO online help &lt;@PRESTO_BASE_URL@/en/stable&gt;
_ for more information. === 
Note about limitations of pRESTO tools within Galaxy:
 pRESTO tools in galaxy have the following limitations vs. when run at the command line: * Inputs must be FASTQ format and cannot be FASTA format. * Multiple inputs files are not supported per argument (e.g. 
-1
, 
-2
 or 
-s
), i.e. - Steps that take a pair of fastq inputs can only take two files, not two sets of files - Steps that take a single set of fastq inputs can only take a single file * The 
--outdir
 and 
--outname
 options are not supported; output files are named directly"
toolshed.g2.bx.psu.edu/repos/iuc/presto_parselog/presto_parselog/0.6.2+galaxy0	"Creates tab-separated tabular reports from pRESTO detailed log files. See the 
pRESTO online help &lt;@PRESTO_BASE_URL@/en/stable&gt;
_ for more information. === 
Note about limitations of pRESTO tools within Galaxy:
 pRESTO tools in galaxy have the following limitations vs. when run at the command line: * Inputs must be FASTQ format and cannot be FASTA format. * Multiple inputs files are not supported per argument (e.g. 
-1
, 
-2
 or 
-s
), i.e. - Steps that take a pair of fastq inputs can only take two files, not two sets of files - Steps that take a single set of fastq inputs can only take a single file * The 
--outdir
 and 
--outname
 options are not supported; output files are named directly"
toolshed.g2.bx.psu.edu/repos/iuc/presto_partition/presto_partition/0.6.2+galaxy0	"Uses pRESTO SplitSeq.py group to partition a file of sequences into two files. The input file is split based on a numeric annotation field to yield one file which contains all sequences with the field value less than the provided threshold and a second file with all sequences with the field value greater than or equal to the threshold. See the 
pRESTO online help &lt;@PRESTO_BASE_URL@/en/stable&gt;
_ for more information. === 
Note about limitations of pRESTO tools within Galaxy:
 pRESTO tools in galaxy have the following limitations vs. when run at the command line: * Inputs must be FASTQ format and cannot be FASTA format. * Multiple inputs files are not supported per argument (e.g. 
-1
, 
-2
 or 
-s
), i.e. - Steps that take a pair of fastq inputs can only take two files, not two sets of files - Steps that take a single set of fastq inputs can only take a single file * The 
--outdir
 and 
--outname
 options are not supported; output files are named directly"
toolshed.g2.bx.psu.edu/repos/iuc/prestor_abseq3/prestor_abseq3/0.6.2+galaxy1	Creates an HTML QC report from the log files generated when running pRESTO.

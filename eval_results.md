# Evaluation results

## 100 Query LLM run
- Command: `./.venv/bin/python scripts/generate_llm_predictions.py --output tmp_stats/codex_predictions_sample.jsonl --max-queries 45 --skip-existing --delay 0` (after the initial 10-query seed, this command continued the output to reach 100 total entries by picking up where `tmp_stats/codex_predictions_sample.jsonl` left off).
- Predictions file: `tmp_stats/codex_predictions_sample.jsonl` now contains 100 `{"id","predictions"}` entries generated by the Jetstream/OpenAI-compatible endpoint defined in `.env` and the default `gpt-4o-mini` model.
- Evaluation command: `./.venv/bin/python scripts/evaluate_recommendations.py --gold data/benchmark/v1_items.jsonl --predictions tmp_stats/codex_predictions_sample.jsonl --k 1,3,5,10 --normalize-tools`.

## Metrics (first 100 cases)
| Cutoff | Hit@k | MRR@k | nDCG@k |
| --- | --- | --- | --- |
| 1 | 0.84 | 0.84 | 0.84 |
| 3 | 0.95 | 0.8917 | 0.9068 |
| 5 | 0.95 | 0.8917 | 0.9068 |
| 10 | 0.96 | 0.8928 | 0.9098 |

## Notes
- A hit rate above 0.9 for kâ‰¥3 shows the agent usually surfaces a relevant tool within the top few suggestions on this 100-query slice.
- Continue the run by rerunning `scripts/generate_llm_predictions.py` with `--skip-existing` and a larger `--max-queries` to expand the prediction set before re-evaluating, or adjust `--top-k`/`--model` if you want more candidates per query.
